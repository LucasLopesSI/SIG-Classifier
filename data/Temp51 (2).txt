Geoinformatica (2010) 14:135–162
DOI 10.1007/s10707-009-0079-2

Evaluating the beneﬁts of multimodal interface
design for CoMPASS—a mobile GIS

J. Doyle · M. Bertolotto · D. Wilson

Received: 3 April 2008 / Revised: 25 November 2008 /
Accepted: 19 February 2009 / Published online: 10 March 2009
© Springer Science + Business Media, LLC 2009

Abstract The context of mobility raises many issues for geospatial applications
providing location-based services. Mobile device limitations, such as small user
interface footprint and pen input whilst in motion, result in information overload
on such devices and interfaces which are difﬁcult to navigate and interact with.
This has become a major issue as mobile GIS applications are now being used by
a wide group of users, including novice users such as tourists, for whom it is essential
to provide easy-to-use applications. Despite this, comparatively little research has
been conducted to address the mobility problem. We are particularly concerned
with the limited interaction techniques available to users of mobile GIS which play
a primary role in contributing to the complexity of using such an application whilst
mobile. As such, our research focuses on multimodal interfaces as a means to present
users with a wider choice of modalities for interacting with mobile GIS applications.
Multimodal interaction is particularly advantageous in a mobile context, enabling
users of location-based applications to choose the mode of input that best suits their
current task and location. The focus of this article concerns a comprehensive user
study which demonstrates the beneﬁts of multimodal interfaces for mobile geospatial
applications.

Keywords GIS · Multimodal · HCI · Mobile · User evaluation

J. Doyle
University of Ottawa, Ottawa, Canada
e-mail: julie.doyle@ucd.ie

J. Doyle · M. Bertolotto (B)

University College Dublin, Dublin, Ireland
e-mail: michela.bertolotto@ucd.ie

D. Wilson
University of North Carolina at Charlotte, Charlotte, NC, USA
e-mail: davils@uncc.edu

136

1 Introduction

Geoinformatica (2010) 14:135–162

Many geospatial applications are inherently dependent on mobility. Users need
access to information and services provided by Geographic Information Systems
and Location Based Systems while on the move. It is while ‘in the ﬁeld’ that users
of such applications reap the most beneﬁts as geospatial applications allow them
to navigate and query their current surroundings. However, there are a number of
factors associated with mobility which contribute to complexity in using a geospatial
application. Mobility relates to both mobile devices and the problem of interacting
whilst moving. While the availability and usage of mobile devices has increased
signiﬁcantly in recent years, alongside advances in device technology, there are
still many limitations associated with such devices which can have negative effects
on the usability of mobile GIS. A signiﬁcant problem with mobile devices is that
they attempt to give people access to powerful computing services through small
interfaces, which typically have small visual displays and poor input techniques.
Furthermore, use in motion affects the usability of mobile GIS as it may be difﬁcult
to interact with such an application using a pen and virtual keyboard. As such,
a need arises to address human computer interaction challenges associated with
mobile device technology and mobile contexts when designing mobile geospatial
applications.

Our research is particularly concerned with the restricted methods of interaction
available to mobile GIS users, which have effects on how such users can interact
with applications dependent on location. Addressing this issue is critical in solving
the problem of interaction complexity associated with such applications, which
stems from the context of mobility. Solutions to such challenges include introducing
improved modes of interaction for mobile environments. The pen or stylus of a PDA
or Tablet PC alone may not be an adequate input mode to convey the intentions
of mobile GIS users. While in motion, it can be difﬁcult to interact with a GIS
application using a pen/stylus as users may be required to point precisely to small,
or relatively small, interface components in addition to inputting text via a virtual
keyboard. Such interaction can become cumbersome for users. Moreover, safety
concerns are raised for GIS users who are driving. Hence novel input techniques,
such as voice commands, are required.

Research has shown that multimodal interfaces can aid in considerably reducing
the complexity of GIS interfaces [1, 4, 9]. Multimodal interaction allows users to
interact in a manner similar to what they are used to when interacting with humans.
Using speech, gesture, touch and head and eye tracking, for example, allows for more
natural interaction.The beneﬁts of multimodal interfaces within mobile geospatial
environments are numerous. These include

–

Flexible interaction in mobile contexts. Multimodal interfaces allow users to
choose the most appropriate modality for carrying out varied spatial tasks in
contrasting environments, which provides users with greater freedom to exercise
control over how they interact. Therefore, they can choose to use the modality
that not only is more suited to their current task, but also is most intuitive to
them for this task.

Geoinformatica (2010) 14:135–162

137

–

Increased access to spatial information for non-professional user groups. Multi-
modal interfaces can help to ensure mobile GIS are accessible to people irre-
spective of skill level, age or sensory impairment. Voice commands and pointing
gestures may provide a more natural and intuitive means of communication
with a GIS, than stylus navigation through menus, for those users with limited
computing experience. In addition, information presented through different
modalities allows a user to adapt to the format of information display that suits
their own cognitive learning style or that they have sensory access to [18].

– Novel visualisation and interaction and increased efﬁciency for professional user
groups. Multimodal GIS may afford GIS professionals such as geographers,
surveyors and cartographers enhanced editing capabilities in the ﬁeld. For
example, a system that supports dictation can allow a user to efﬁciently enter
an annotation. Moreover, the increased efﬁciency multimodal applications hold
over their unimodal counterparts, is of great importance to GIS professionals
who must work under time constraints.
Increased robustness of geospatial applications. For example, mutual disambigua-
tion of input modalities enables the rectiﬁcation of errors made by individual
modalities [11]. Furthermore, multimodal applications support multimodal error
correction, which has been shown to be superior to unimodal error correction
both in terms of avoidance of and recovery from errors [15].

–

– User preference for multimodal interaction (over unimodal interaction). Our past
research has shown strong preferences by users to interact multimodally with
mobile GIS [1]. This was attributed to feelings of increased control over how
one can interact, increased ﬂexibility, increased efﬁciency and more intuitive
interaction.

The main contribution of this research is a comprehensive evaluation of how
multimodal input techniques can signiﬁcantly decrease the complexity of interacting
with a multifunctional GIS in mobile contexts for a wide range of users including
experts and non-experts. We do this through a case study which is provided by
the client component which we developed for the CoMPASS (Combining Mobile
Personalised Applications with Spatial Services) project.

CoMPASS is a complex GIS, providing full geospatial functionality through its
client interface. Not only does CoMPASS support traditional GIS functionality
such as navigation and querying but it also provides personalised map detail to
users in addition to supporting annotation functionality and Progressive Vector
Transmission. As such, it was fundamental that our interface provided intuitive
methods of interaction to carry out this functionality in mobile environments. These
are some of the challenges we addressed in the design and development of the
CoMPASS client multimodal interface.

The remainder of this article is organised as follows. Section 2 provides an
overview of some related research in the area of mobile GIS and multimodal
applications. The structure of the CoMPASS client and the functionality it supports
is outlined in Section 3. Sections 4, 5 and 6 provide details of evaluations of our
mobile client interface and highlight our results which demonstrate the beneﬁts
of multimodal interfaces for mobile GIS applications. Finally Section 7 presents
conclusions and future developments.

138

2 Related work

Geoinformatica (2010) 14:135–162

The array of available multimodal applications providing geospatial services has
broadened widely, and ranges from city navigation and way-ﬁnding for tourists, to
emergency planning and military simulation. In this section, we provide an overview
of current state of the art within the research realm of multimodal mobile GIS and
provide a detailed list of how CoMPASS compares with such applications.

DeepMap [8] is a prototype tourist guide system that has two facets. Firstly,
users can plan their upcoming trip through DeepMap’s web-based interface, from
their home. DeepMap consequently provides such users with a mobile-based version
of their prototype which can be used as a tourist guide for navigating the city of
Heidelberg. One of the primary goals of the DeepMap mobile tourist guide is to
provide users with a user interface that is both intuitive and allows easy access to
spatial information. The authors strive to achieve this goal through the use of both a
3D visualisation (in addition to 2D visualisations) and by providing multiple modes
of input and output for interaction. However, they found the 3D visualisation was
too complex to achieve on a mobile device such as a PDA due to computational per-
formance restrictions. With regard to the multimodal interaction, a natural language
interface is provided that allows free speech and real dialogs. However, no command
grammars are used. Grammars are important in any application supporting speech
recognition as they support more robust and accurate recognition. As such, speech
grammars are incorporated within CoMPASS.

A characteristic of mobile applications is the continually changing environmental
context, and hence changing noise levels, which can have serious impacts on appli-
cations incorporating speech. The aim of the research in [16] is to increase speech
recognition rates for users interacting with landmark objects in their vicinity and
the authors found that combining multiple input modalities produced consistent,
robust recognition rates. However, the main problem with such an application is that
it is simply a tour guide system providing navigation support and information on
landmark objects for tourists. Full geospatial functionality is not exploited. Similar
limitations apply to the BMW Personal Navigator (BPN) [7]. The BPN allows
users to plan an upcoming trip from their home computer, transfer the downloaded
information to a PDA device and to then use this information, both in-car and while
on foot, to navigate to an area of interest. While the PDA-based application allows
users to interact with their surroundings using both speech and gesture input, again
it is only used as a navigational aid, signiﬁcantly limiting the possibilities of such a
location-based application.

The issue of the complexity of GIS interfaces is addressed in [13], which describes
DAVE_G (Dialogue-Assisted Visual Environment for Geoinformation), a system
which uses a combination of speech and gesture to aid users in collaborative group
work when responding to a crisis situation in the context of emergency management.
As such, DAVE_G is a professional GIS application. However, its target users are
those without special GIS training, therefore addressing complexity is vital to the
usability of the system. The authors found that for a task such as querying, using
speech and gestures is more efﬁcient, and natural, than traditional querying methods
and hence more user-friendly. Results for such queries are instant and such efﬁciency

Geoinformatica (2010) 14:135–162

139

is a necessity for applications such as emergency management. Moreover, the advan-
tage of using gestures in addition to speech is that speech may not be completely
accurate when spatial information such as location needs to be speciﬁed. However,
such research is carried out on a large screen display, rather than on a mobile device.
Despite this, the DAVE_G system demonstrates that multimodal interaction can
reduce the complexity of a user’s task during an emergency management situation.

Multimodal interaction research tends to focus on how best to enhance an appli-
cation by using different input modalities. Very little research has been conducted
into multimodal feedback, however. The use of combining oral messages with visual
modalities for aiding target location tasks is described in [6]. The authors present
an experimental evaluation the aim of which is to assess the efﬁciency, accuracy and
usability of oral system messages, including brief spatial information, to help users
locate objects on crowded displays with minimal effort. The experiment consisted of
locating a pre-viewed photograph in a scene and selecting it as quickly as possible
using the mouse. Oral messages to help users were composed of short phrases such
as “On the left (of the screen)” or “At the bottom”. Providing oral feedback as a
supplement to visual stimuli proved to be both more efﬁcient and accurate than no
oral output. Moreover, evaluation subjects expressed preference for the inclusion
of oral messages for such a task. While this experiment was not spatially-based, it
demonstrates the utility of speech output for locating objects in a scene, and hence
could be applied to GIS.

Multimodal feedback/output within CoMPASS takes the form of conﬁrmation
messages (for example when a voice command has been interpreted correctly)
printed to an unobtrusive information bar at the bottom of the screen. In addition,
audio prompts are output to aid the user in completing complex tasks. Such audio
prompts consume less cognitive resources than pop-up windows displaying informa-
tion messages, as the user must pause their task to read the message. Pop-up windows
also take up a lot of screen space on small devices and so temporarily obstruct the
user’s view of their map. Speech prompts on the other hand do not distract from the
user’s main task, allowing them to continue to carry out their task while they receive
instructions to assist them further.

While each of the systems presented in this Section provide relevant research
into the area of multimodal GIS, they also suffer from certain limitations which we
attempt to address within CoMPASS. These limitations are summarised below:

– The vast majority of current mobile GIS applications are targeted at tourists
and as such support only limited geospatial functionality such as way-ﬁnding and
accessing semantic information regarding Points of Interest.
Such systems are typically implemented on PDA devices. While these devices
have the advantage of being small and lightweight, the available screen space for
displaying maps is extremely limited which can have effects on complexity and
the cognitive resources required to read mobile maps.

–

– There is a distinct lack of novel interaction techniques to support and enhance in-
teraction with such systems in a mobile context. Supporting multiple input modes
increases the ﬂexibility and choice users have in interacting and consequently can
enhance usability and efﬁciency.

140

Geoinformatica (2010) 14:135–162

– Many of those systems that do support multimodal interaction tend to lack
other features which CoMPASS provides. For example, the multimodal tour
guide applications presented lack full geospatial functionality providing only
navigation and information viewing, but not spatial annotation functionality, for
example. These systems target primarily tourist or similar non-expert users, while
other multimodal systems have been developed solely for use in a professional
context (e.g. DAVE_G). Moreover, whether developed for professional or
non-professional use, many of the multimodal systems presented are based
on handheld devices such as PDAs. Apart from the cognitive issues of map
reading associated with such hardware devices, they lack the sophistication to
allow professional users to carry out their work efﬁciently and effectively in
mobile contexts. Finally, the majority of systems presented do not support either
feedback or multimodal error correction, which are essential to ensure a robust
multimodal application.

We address the above limitations through the CoMPASS client interface. Pri-
marily, CoMPASS is a multi-functional GIS supporting not only navigation and
information retrieval but also manipulation, customisation, querying and annotating
of maps and map features. More detail on CoMPASS is provided in the following
section.

3 The CoMPASS system client

CoMPASS (Combining Mobile Personalised Applications with Spatial Services) is
a large scale GIS application incorporating standard GIS functionality such as navi-
gation, spatial querying and manipulation of vector-based spatial data. Unlike many
existing GIS applications however, CoMPASS further supports innovative function-
ality, including personalisation of spatial data, progressive transmission across wire-
less networks and the possibility to annotate map features. Moreover, CoMPASS
is designed as a mobile GIS, allowing users to beneﬁt from its functionality in a
mobile context. That is, users can download and interact with location-sensitive
spatial information on demand. Our main objective for the mobile client was to
develop a User Interface to integrate all CoMPASS functionality. As CoMPASS is
an inherently complex GIS it was essential that the client interface be easy-to-use,
intuitive and ﬂexible enough to allow both expert and non-expert users to visualise
and interact efﬁciently and effectively with geospatial data. This becomes even more
critical for applications dependent on location, as mobile interfaces have particular
needs due in part to the limitations of mobile devices discussed in Section 1, but also
to the continually changing context of location-based applications.

CoMPASS allows mobile users to connect to a remote server and download and
interact multimodally with personalised vector maps in GML (Geography Markup
Language) ﬁle format over wireless connections. GML is used within CoMPASS as
the data storage and exchange format. The primary reasons for this are that GML
is an open source, non-proprietary OGC standard and as such, using GML within
our system ensures that users of other GIS applications can access and use our
spatial data. We have successfully integrated improved human computer interaction

Geoinformatica (2010) 14:135–162

141

techniques to allow ﬂexible and efﬁcient interaction with spatial data in mobile
contexts. Supporting more than one input mode at our client interface provides our
users with the ﬂexibility to switch between interaction modes based on that which is
most intuitive to them, best suited to carrying out their current task, or best suited
to their immediate geophysical location. Speciﬁcally, in addition to traditional input
techniques (such as a pen/stylus) we support interaction through speech (commands
and dictation), pen gestures, combinations of speech and gesture and handwriting for
tasks requiring the user to enter textual information.

The adoption of speech interaction was possible through the integration of off-
the-shelf speech recognition software packages. Although such packages can be
easily plugged into existing applications, a number of challenges arise to provide
effective integration into a GIS, the most important being the intuitiveness of using
such functionality. This intuitiveness should be established at a number of different
levels. Firstly, it is critical to ensure that the availability of speech functionality at
the interface is evident to users. Moreover, it should be obvious to the user how to
turn such functionality on. The functionality should also be just as easily switched
off, if the user’s current context requires it (e.g. for a tourist user in a church).
An appropriate list of voice commands for interacting with the application must
be drawn up and integrated into the existing code. Such commands should be self-
explanatory to ensure intuitive user interaction. Moreover, it is vital that methods
are provided to support easy recovery from errors relating to speech tasks.

The structure of the CoMPASS Client Module is depicted in Fig. 1. The client
interface is the primary component, displaying geospatial data received from the
CoMPASS server and capturing user interactions. Processing of these interactions,
including navigation, manipulation, querying and annotation interactions, occurs at
the client. Results of such processing are subsequently visualised at the interface. The
various structural components are described in the following subsections.

3.1 CoMPASS server layer

One of the main purposes of the CoMPASS server layer is the transfer of geospatial
data to the client. As such, its relation to the client module is discussed further here.
Vector data from Tiger/Line 2000 ﬁles are stored in a remote database and used to
render maps. Much of the current work in geospatial systems tends to focus on the
display of raster maps within a relatively limited context, such as raster city maps with
touristic information and a user’s position therein. CoMPASS, however, exploits the
full potential of geospatial data by displaying vector maps hence providing a much
wider range of functionality. Raster images are usually preferable if the purpose
is simply visualisation. For actual object manipulation vector datasets are required.
Using vector data allows for map content to be divided into distinct map layers where
each layer corresponds to a speciﬁc map feature. This enables the capture of detailed
information each time a user interacts with the map, as each action can be associated
with a particular feature or set of features.

Within the CoMPASS server, the Tiger/Line 2000 ﬁles are converted into GML
ﬁle format. When a map is requested at the client interface the required information,
based on the user’s current location and their personal interests inferred from

142

Geoinformatica (2010) 14:135–162

Fig. 1 Structure of the client module

previous sessions, is extracted to create a personalised GML dataset. This dataset
is then transferred across the wireless network to the client device. Here, the GML
data is transformed into a suitable graphical format for presentation as a map.

Geoinformatica (2010) 14:135–162

143

3.2 The client interface

The functionality of the interface layer is threefold:

– Allow users to visualise map content;
– Allow users to interact with map features and
– Capture all of the user’s explicit and implicit interactions so as to create/update

their user proﬁle.

In this article we focus on the way users interact with map features (more details
on the overall functionality can be found in [17]). Once a user has been presented
with a map they can then begin to interact with individual, or groups of, map
features. As CoMPASS caters for multimodal interaction, users can interact with
maps using either pen-based input (gestures and handwriting) or voice-based input
(commands and dictation). We decided to integrate only active forms of interaction
(speech and pen) as opposed to passive (vision-based) as past research has shown
that active forms more reliably convey user intention than passive, which require
no input from the user but instead implicitly monitor their movements to infer
recognition [12]. Processing and interpretation of the various modes of input occurs
at the client. In addition, the client interface provides users with the visual results
of their queries/interactions as well as with feedback as a result of input processing.
Such feedback can take the form of audio or visual output and both prompts the
user for further input and informs them when their actions have been interpreted
correctly.

A typical scenario of user interaction might be as follows. A CoMPASS user
logs onto the system via their username. Their current location is obtained via
GPS and an interactive, personalised vector map is returned to them based on this
geographical position. It is then possible for users to dynamically interact with their
map. Available interactions include navigation through zooming and panning. This
is possible through buttons on the interface, by issuing a voice command to zoom
or pan and by drawing an area on the map where the user would like to focus. It
is also possible to re-centre the map at a particular location by simply clicking on
that map location. Manipulation of map features is possible through turning on/off
map features (such as highways, schools etc.) and changing the colour and size of map
features for individual map feature content personalisation. CoMPASS also supports
feature querying including highlighting speciﬁc features in an area deﬁned by the
user, highlighting a speciﬁc number of features closest to a given point, ﬁnding the
distance between two points and multimedia annotation creation and retrieval.

Other aspects of system functionality include an unobtrusive information bar at
the bottom of the interface displaying information on the user’s current position
(latitude and longitude) and the name of the feature the user is currently at (i.e.
what feature the pen is over). This prevents text cluttering the interface which
is of particular importance for mobile devices. Our user evaluation, described in
Section 4 demonstrates that this method of display is adequate even in a mobile
context, as almost all users, though mobile during the evaluations, stopped walking
while they were carrying out a speciﬁc task, allowing them to view the screen easily.
CoMPASS provides a help menu to aid users in interacting with the system. All of
the above-described functionality can be carried out using pen input, speech input
or a combination of both. For example, when inputting an annotation, a user might

144

Geoinformatica (2010) 14:135–162

use dictation to enter the annotation and then choose the spatial location on the map
to assign this annotation to, by pointing to that location on the map with the stylus.
Another user might prefer to input an annotation using the handwriting function of
CoMPASS, while yet another might prefer pen and virtual keyboard. This ability to
choose what mode of input to use, based on personal preference or context, ensures
a ﬂexible, easy to use interface. Providing two or more modes of input with parallel
functionality is of particular signiﬁcance in mobile environments where a particular
mode might be unsuitable or unfeasible. For example, speech input may not be
reliable in very noisy environments, or appropriate for a tourist visiting a church.
On the other hand, pen input might be cumbersome for users with a disability, such
as those who are vision impaired and may have difﬁculty pointing to small interface
objects.

3.3 Client processing

The CoMPASS client component is capable of handling and processing interactions
in the form of gesture, speech and handwriting. Currently, separate recognisers are
responsible for recognition of each mode of input and recognition occurs in sequence
rather than in parallel. However, future work will examine synchronous fusion of
inputs.

The Gesture Recogniser can process ‘intra-gestures’ i.e. pointing or selecting with
the stylus to locations or real-world objects (within the vicinity of the user’s current
location) on the Tablet PC screen. ‘Extra-gestures’ that allow users to point to
surrounding objects in their current environment are not currently supported. Intra-
gestures can take two forms within CoMPASS: pointing and dragging. Users can
point at objects to re-centre the map at this point, to discover the name and type of
objects, to specify what feature they would like to query or what feature they would
like to annotate. Dragging gestures specify a ‘zoom in’ on the area over which the pen
is dragged or, when used in conjunction with a query, to specify the area of interest
for the query.

Handwriting can be used within CoMPASS as a method to input annotations or
to correct errors during dictation of annotations. The handwriting recogniser can
process both block and cursive handwriting. If a word is not recognised correctly,
the user can choose from a list of alternatives simply by clicking on the word.
Such an input mode can prove both more efﬁcient than inputting long textual
annotations using the pen and virtual keyboard and more accurate than dictation
for an annotation task. Recognition is provided by an in-built handwriting recogniser
in the Tablet PC, which we have integrated with our code.

We have also integrated a speaker-independent speech module into the CoM-
PASS system, which is capable of human-computer interaction handling in real
time. The speech recogniser is capable of recognising two forms of speech input:
voice commands and dictation. Whereas voice commands (in English only) are
matched against a grammar ﬁle which contains a list of all the possible commands
for interacting with CoMPASS, hence supporting accurate and robust recognition,
dictation input is free-form. In other words, dictation is not matched against a rule-
grammar and so ideally supports free speech. As such, dictation is used within
CoMPASS for inputting annotations (as comments entered by users will depend
on their personal experience). However, as dictation grammars contain no speciﬁc

Geoinformatica (2010) 14:135–162

145

rules pertaining to what the user can say, they tend to be more error prone. It
is likely, particularly in outdoor mobile environments where a user’s locational
context is continually changing (and hence varying levels of noise are experienced),
that a relatively high percentage of the words spoken during dictation will not be
recognised correctly. Hence, it becomes crucial to provide methods for the user
to correct their voice annotation if necessary. It has been recognised that allowing
the user to switch modality during continuous speech error correction can result in
increased correction accuracy [10, 15]. This process is referred to as “multimodal
error correction”. CoMPASS leverages this technique in its multimodal interface.
If a user’s dictation is erroneous they can choose to correct it using either pen and
keyboard input, recording a voice memo or by handwriting (Fig. 2). It is very easy for
the user to switch interaction modes, with only a brief voice command or the touch of
a button required to switch from one mode to another. Throughout our evaluations,
we have not perceived any overheads in switching between modes. It is also very
easy for the user to recognise which modes of interaction are currently supported.
For example, when speech recognition is turned on, a speciﬁc icon appears on
the interface. Furthermore, switching modalities is not required for any particular
interaction, but rather a choice of the user.

Once the individual recognisers have processed and recognised incoming input,
the input must be interpreted and appropriate feedback provided. The Interpretation
and Feedback modules are responsible for this. Based on recognition of individual
inputs, the Interpretation module decides on the action that should be carried out at

Fig. 2 Error correction interface

146

Geoinformatica (2010) 14:135–162

the interface in response to this input. For example, if the user drags the pen over an
area of the map, this gesture is interpreted as a ‘zoom in’ action on the drawn area
and this action is subsequently carried out at the interface. Similarly, when a user
issues the command ‘highlight highways’ this is interpreted as a ‘highlight’ query on
the feature ‘highways’. This information is passed onto the Feedback module which
prompts the user at the interface to select an area of the map in which they are
interested in highlighting all highways. In addition feedback is provided in the form
of conﬁrmation when a voice command has been recognised or an action carried out
on the map.

4 CoMPASS client evaluation

The complexity of interacting with current mobile geospatial applications poses a
critical problem as such applications are now being used by a wide and varied group
of users. In developing CoMPASS, we distinguished between professional and non-
expert users and analysed what type of functionality each type of user would require
and in what contexts they would be likely to use CoMPASS. Professional users
include, GIS experts such as surveyors, cartographers and geographers. Such users
are considered expert users as they regularly use GIS technology and data to carry
out their work. Another type of professional user, for example, is a domain expert
who uses GIS in their work. For example, an engineer working for the Electricity
Supply Board. While such a user might work with a mobile GIS on a daily basis
(to ﬁx a light pole for example), they may not be expert GIS users as they only use
GIS technology as an auxiliary tool to map their data onto the underlying geographic
area. They would not necessarily use much of the sophisticated functionality included
in the system. However, daily use of an application will ensure that they will quickly
become familiar with that subset of the application’s functionality they require for
their work.

Non-expert mobile GIS users include tourists who may want to locate all museums
in their surrounding location, outdoor enthusiasts such as a ﬁsherman who may want
to view maps and associated information such as annotations left by other ﬁshermen
in the area recently or a family who are moving to a new city and want to locate
the nearest schools to their apartment. Hence, new challenges arise in the design of
geospatial interfaces for location-aware applications, which must be user friendly and
intuitive to appeal to this diverse range of users.

Once a fully working prototype of the CoMPASS application had been developed,
we set about designing an extensive user evaluation, the aim of which was to
conduct a large-scale experiment, incorporating all CoMPASS client functionality.
With regard to the development of the GUI for this client, our primary objective
was to provide all of our target users, both professional and non-expert, with an
interface that enhanced, rather than impeded, our users’ experience when viewing
and interacting with geospatial data in a mobile context. Contributing factors to an
enhanced geospatial experience include, but are not limited to, ease and efﬁciency of
task completion as well as overall user satisfaction. This evaluation served to demon-
strate that we have achieved this objective for tasks involving all possible CoMPASS
functionality through the provision of a multimodal interface. We hypothesised
that our multimodal mobile interface would support more efﬁcient and effective

Geoinformatica (2010) 14:135–162

147

interaction than a unimodal interface and that the ability to switch between modes
of interaction for different types of tasks would increase ease of task completion and
would hence elicit strong positive feedback from our users.

4.1 Subjects

Eighteen subjects participated in our study. Our subjects ranged from students
to researchers and IT specialists. All subjects were mobile whilst interacting and
were instructed to ‘walk as much as possible’ when carrying out their tasks. The
reason we stressed walking was because we noted that the majority of users in a
previous study, even though mobile, tended to stand in one place while carrying
out subtasks, only walking in between. Hence, each user walked around, moving
between areas of varying noise levels. The setting for our evaluation was the area
around the Computer Science department of our University. Thus, a typical subject
was instructed to begin the evaluation by walking around inside the department;
as the subject interacted with the system they then moved outdoors, ﬁnally making
their way back indoors to a cafeteria. This type of setting ensured that the subjects
were continuously mobile and that they experienced different levels of noise such as
people talking and background machinery. One constraint of our evaluation was that
our setting was conﬁned to those areas of the University with wireless access.

4.2 Tasks

As CoMPASS can be targeted to both professional and non-professional users,
subjects were evenly split into one of these two groups and given tasks based on
their user group so as to evaluate the broad range of functionality that might be
used by these two diverse groups of users. Each task was designed to ensure all
aspects of CoMPASS functionality was evaluated by users. Those users simulating
professionals were given the same task which related to a professional activity. This
group was further split into expert and non-expert professionals. While both types
of professional users carried out the same task, we were interested in determining if
the experts completed their tasks more efﬁciently and with less errors than the non-
expert professionals. Our non-professional user group simulated city tourists and as
such carried out typical tourist activity tasks.

Prior to beginning the evaluation, each participant was given a brief demonstra-
tion of the system, highlighting its main functionality. Each participant completed
three tasks: two training tasks and a main task. The purpose of the training tasks
was to allow the user to ‘practice’ interacting with the system to carry out geospatial
tasks. As such, it was permitted for subjects to ask questions during the training tasks.
For example, they could ask how to query a feature or what voice commands were
required to carry out certain functions. The average time taken per evaluation was 45
minutes, including demonstration, completing tasks and ﬁlling out a questionnaire.

During the ﬁrst training task, participants were requested to use solely pen input
to carry out the required sub-tasks, while for the second they were requested to use
multimodal input to carry out the same task, i.e. voice commands and pen input
where it was necessary to gesture to/select objects on the map. While both training
tasks were the same, the user was given a different start location for each to ensure
the need to navigate to locate features of interest. We did not record efﬁciency or

148

Geoinformatica (2010) 14:135–162

accuracy for these training tasks however. Their sole purpose was to provide the user
with the opportunity to carry out all subtasks using both unimodal and multimodal
input and hence to help the user in choosing a modality for similar subtasks during
their main task. An example of a training task for a professional user, such as a
surveyor, is as follows:

You have an assignment to test water pollution levels at Barr Lake. Locate the
lake on the map. You may want to remove some non-relevant features from the
map to aid navigation. Once you have located the lake perform a spatial query
on it to highlight it. Finally create an annotation regarding your ﬁndings at the
lake and assign it to the lake.

Having practiced interacting with CoMPASS both unimodally and multimodally,
each subject was asked to complete their main task. The main task was slightly more
complex in that it involved more sub-tasks, some of which the user had not ‘practiced’
during their training tasks. The user was given the choice of what mode of input to
use when completing sub-tasks based on their preference for a particular mode or
which mode they felt was better suited to completing certain subtasks, during the
training sessions. An example of a main task for a subject simulating a surveyor is as
follows:

You are currently at your ofﬁce at Federal Heights City Hall and you need
to go to McKay Lake to test water pollution levels. Locate both features on
the map. You may want to remove some non-relevant features from the map
to aid navigation. Once you have located both features perform a query to
ﬁnd the distance between the two. If you must drive there, perform a query
to highlight any highways in the vicinity of both your ofﬁce and the lake. You
need to know if any rivers surround the lake. Perform a highlight query on
rivers in the vicinity of the lake. You want to personalise some map content to
suit your taste—change the colour of the rivers feature-set to your preferred
colour. Finally, create an annotation regarding your ﬁndings at the lake and
assign it to the lake.

While some might argue that it is difﬁcult to judge user satisfaction through
planned experiments, such as ours, given that user behaviour during an evaluation
may be different from general use of a product, we believe planning evaluations
and carefully monitoring user behaviour are very important in effectively testing
usability, efﬁciency, effectiveness and satisfaction. Without given concrete tasks,
users are unlikely to test all functionality. Without some form of monitoring, whether
data logging or observation, it can not be accurately determined whether users
complete tasks. Finally, without formal methods of feedback from users, such as
questionnaires or interviews, it is difﬁcult to collect user satisfaction feedback.

4.3 Recorded information

The main objective of this evaluation was to determine the overall usability and
efﬁciency of the CoMPASS client for users of all levels of expertise in mobile
contexts. As subjects completed their 3 tasks, we monitored them and recorded their
actions in log ﬁles, allowing us to measure the time taken to complete tasks, the
error content (both system and user errors) for each task and speech recognition

Geoinformatica (2010) 14:135–162

149

rates. We would have liked to video-record subject evaluations but it was not
possible for the evaluator to both record and monitor user interactions at the same
time. Moreover the mobility made it difﬁcult to record subjects whilst moving and
impossible to record any interactions with the Tablet PC interface in sunlight. Even
without the video recording, the log ﬁles and monitoring ensured we were able
to collect the information we required. In addition, each user completed a post-
evaluation questionnaire, providing us with qualitative feedback on various aspects
of the evaluation.

While the log ﬁles served as a means for the evaluators to measure efﬁciency
and error rates, the questionnaire provided us with users’ subjective opinions of our
client GUI, multimodal interaction and how we might improve the client overall. The
‘Interface’ section of our questionnaire asked each subject to evaluate the interface
with regard to overall ease of use, the intuitiveness of what real-world objects map
features represented and the ease/intuitiveness of completing various geospatial
tasks such as navigating, querying, and annotating. This enabled us to determine
whether our interface provides an intuitive, efﬁcient and user-friendly environment
within which users can complete their geospatial tasks efﬁciently and effectively.

In effect, we were interested in determining the following:

–

Is our interface intuitive and easy to use? What effect does multimodality play in
this usability?

– Does multimodal interaction have an advantageous effect on interaction efﬁ-
ciency in a mobile context? That is, is multimodal interaction more efﬁcient than
unimodal interaction for a number of geospatial tasks?

– What is the users’ personal preferred mode of interaction, multimodal or uni-

modal, and what is the reasoning behind this?

– What is the overhead (if any) of switching from one input to another during a

task?

– How do users deal with the unique challenges that a mobile context introduces?
For example, how do users handle problems related to pen use whilst mobile, or
errors related to speech input?

– How well does the CoMPASS system respond to dictation input?
– How do users react to dictation errors? Do users attempt to repeat their words;
do they switch to pen and keyboard input? Do such errors frustrate users?

The results we obtained from monitoring and recording user interactions in a mobile
context are provided in the following section.

5 Results

Our primary objective with regard to this user evaluation was to assess all aspects of
the CoMPASS client to ensure maximum usability and efﬁciency for all CoMPASS
users whilst mobile in addition to gaining an insight into what aspects of our system
subjects liked/disliked. We adopt the ISO standard deﬁnition of usability which states
that usability is

The extent to which a product can be used by speciﬁed users to achieve speciﬁed
goals with effectiveness, efﬁciency and satisfaction in a speciﬁed context of
use. [5].

150

Geoinformatica (2010) 14:135–162

In the context of the CoMPASS client evaluations, usability denotes the ease
of use with which our intended users, both expert and non-expert, can use all
aspects of our system functionality in mobile contexts. We used both quantitative
(recording and monitoring interaction times, errors and error correction methods)
and qualitative (soliciting user opinions and preferences through a questionnaire)
methods to measure these parameters. We collected information regarding ease of
use of the interface, modality preference, efﬁciency of interaction for different user
groups, recognition rates and error rates. We made the following hypotheses:

–

Interface Ease of Use—We expected that, in assessing ease and intuitiveness
of various interface functionality, querying and annotation would score low-
est for intuitiveness as both involve more steps than navigation and feature
manipulation.

– Modality Preference—We hypothesised that the majority of users would prefer
multimodal interaction over unimodal for all functionality, with the possible
exception of annotation functionality.
Interaction Efﬁciency—With regard to efﬁciency for completing tasks, we ex-
pected that our four expert users would out-perform all other users as these users
had more knowledge of and experience interacting with CoMPASS.

–

– Error Evaluation—This aspect of our evaluation focused on how users attempt
to correct errors (i.e. by re-using the same modality or switching to a different
modality). We hypothesised that the majority of users would switch to avoid
repeated errors.

Our results are presented in the following sub-sections.

5.1 The interface

The user interface of an application allows users to visualise information and to
interact with it to achieve their tasks and goals. It is therefore a critical component
of any application and consequently should be designed and developed to ensure
maximum ease of use and intuitiveness. This is particularly important for mobile GIS
and applications providing location-based services as many current interfaces are
complex in their design and require much user training. We aimed to provide a user
interface to CoMPASS which would require minimal training while being intuitive
and usable at all levels of functionality: intuitiveness of map features, navigation,
manipulation of features, querying and annotating.

A section of our post-evaluation questionnaire was therefore devoted to the
subject’s experience with, and opinions of, the CoMPASS interface, whereby each
user ranked each of the above tasks on a scale of 1 to 5. The ﬁrst question we asked
subjects was to rate the overall ease of use of the interface for completing their
tasks, where 1 on the scale indicated difﬁculty and 5 indicated ease. Seven of our
eighteen users, or 38.89%, ranked the interface at 5. Of these seven users, 2 had
no experience using CoMPASS while 5 had used the system no more than twice
(for previous evaluations). With regard to familiarity with GIS applications, 1 user
from the seven had no experience with GIS, 3 users stated they were quite familiar
with GIS while the remaining 3 had little experience using such applications. 55.56%
(10 users) ranked the ease of use of our interface at 4. We considered four users
from this group to be expert users i.e they each had experience developing different

Geoinformatica (2010) 14:135–162

151

components of the CoMPASS system (such as the personalistion and annotation
services). However, 3 of these users were not familiar with the voice recognition
component of the interface (what commands to issue to carry out certain tasks) and
as such their comments on improving the interface related to this component of our
system. Finally, one user ranked interface ease of use at 3. This user was very familiar
with other GIS applications (e.g. Google Earth) and as such found it difﬁcult to adjust
to our interface.

Subsequent questions served to evaluate individual aspects of the interface and
its functionality. Subjects were asked to assess the intuitiveness of feature represen-
tation (i.e. was it obvious what objects map features represented). 50% (9 users)
rated intuitiveness at 5 (i.e very intuitive), 38.89% (7 users) ranked it at 4 while
the remaining two users ranked intuitiveness of feature representation at 3 and
at 2 respectively. Both users who gave a 2 and a 3 stated that while the layers panel
acted sufﬁciently as a map legend they would rather see the legend displayed as part
of the mapping interface rather than in a menu. However, this design decision was
based on the fact that we wanted to give more screen space to the map, as mobile
device interfaces are smaller than their desktop counterparts.

Ease of navigation was a further aspect of our interface which required assessment
as navigation is a critical task in any GIS application, allowing the user to move
around their geographical space to locate features/areas of interest. Moreover, dur-
ing a previous user evaluation, a number of users had reported problems regarding
the size of navigation GUI components when using the Tablet PC pen, which led to
navigation problems. As a result, we re-designed the interface to include larger GUI
components for navigation and we wanted to test if this re-design aided in easier
navigation.

From our eighteen users, 50% (9 users) considered navigating their map to be a
very easy task (i.e. they ranked it at 5). 11.11% (2 users) rated ease of navigation
at 3 while 5.56% (1 user) rated it at 2. While we hypothesised that querying and an-
notating functionality would score lower than navigation with regard to intuitiveness,
this was not the case. We further expected that there would be a correlation between
ease of navigation and users’ familiarity with GIS applications. However, again, this
was not true as those subjects who rated ease of navigation at 2 and 3 were very
familiar with GIS applications. Possible reasons for these ratings may be that users
were simulating movements in a geographical space rather than moving around their
actual surroundings. Moreover, if these subjects are familiar with navigating different
GISs, they might ﬁnd it difﬁcult to adjust to ours. It must be noted, however, that
while a minority of users ranked ease of navigation at 2 or 3, not one user attributed
this to difﬁculty of interacting with the navigation GUI components and as such, we
expect that our interface re-design had an effect on this.

Three remaining aspects of interface functionality that we were interested in
evaluating were the intuitiveness of map manipulation, querying and annotating.
Manipulation of map features included toggling features on/off and changing the
appearance of map features. Results showed a high rating for map manipulation
with 88.89% of users (16 users) ranking it at 5 and the remaining 11.11% (2 users)
ranking it at 4. Similar results were found for querying and annotating whereby
77.78% of subjects (14) rated intuitiveness of each of these tasks at 5 and 22.22%
(4 users) rated intuitiveness at 4. Overall, the results from this part of our evaluation
demonstrate that our interface is very easy to use, and intuitive for our target user
groups.

152

Geoinformatica (2010) 14:135–162

5.2 Multimodal interaction—efﬁciency and preference

An important requirement of our interface is that the functionality it supports can
be carried out efﬁciently. For example, a system may not be considered usable or
satisfactory if the user must apply a number of mouse/pen clicks to achieve their
task. Much related research in the area of multimodal interfaces for GIS has shown
efﬁciency advantages for multimodal over unimodal interaction [3, 14]. Indeed, this
was veriﬁed by our own results from a previous evaluation. Results of our tests to
determine the efﬁciency of multimodal versus unimodal interaction illustrated that
multimodal interaction using speech and pen was signiﬁcantly faster ( p < 0.01), and
therefore more efﬁcient than using the pen as a single modality [1]. On average,
efﬁciency improved by 12.21% when subjects used a combination of speech and pen
input. This increase in efﬁciency whilst interacting multimodally is made even more
signiﬁcant as none of the subjects had previous experience using the CoMPASS
multimodal interface. Hence, they were not familiar with the voice commands
required to perform certain interface actions. We can consequently expect that as
users become more familiar with the system, such as in the case of professional
users adopting the system as their working technology, efﬁciency rates will further
increase.

For this particular evaluation we were concerned with efﬁciency rates for different
user groups, namely experts, non-expert non-accented users and non-expert accented
users (i.e. non-native English speakers). We expected expert users to complete their
task in less time than non-expert users. Graphs depicting our results are shown in
Figs. 3–6.

Evaluation subjects were considered ‘expert’ users if they had extensive knowl-
edge of the CoMPASS system in addition to experience interacting with the system
on more or less a daily basis. As such, our expert users are those who have worked
on implementing different CoMPASS components (e.g. personalisation, annotation
indexing and retrieval, server implementation etc.). However, they did not neces-
sarily have experience using all aspects of system functionality, or the Tablet PC
itself. Our expert group consisted of 4 users, each of whom was a native English
speaker. Interaction times for each of these 4 individual users during their main tasks
are shown in Fig. 3. Each user interacted multimodally during this task and as such,
modality choice is not a factor in time differences.

Fig. 3 Efﬁciencies of
interaction for expert
users during main task

)
s
c
e
s
(
 
e
m
T

i

600

500

400

300

200

100

0

1

2

3

4

Users

Geoinformatica (2010) 14:135–162

153

Fig. 4 Efﬁciencies of
interaction for non-expert
non-accented users

Our ‘non-expert’ subjects were those who had no, or very limited, experience
using the CoMPASS system. For example, some of this group may have assisted
in previous user evaluations but would not have enough knowledge of the system to
be considered experts. In addition, each member of this group was a native English
speaker. Figure 4 depicts efﬁciency results for this group. Apart from one user (user
3 in our graph) each subject interacted multimodally during their main task. Again,
there is a wide range of task completion speeds evident amongst this user group
which suggests different levels of ability for carrying out various geospatial tasks
within CoMPASS. Overall, however, task completion times are sufﬁciently quick and
deemed efﬁcient by our users.

A further user group of interest were those non-expert users with an accent.
As each of these users chose to interact multimodally during their main task we
were interested in determining if using voice commands would signiﬁcantly increase
their time for task completion. The results for our 3 non-expert accented users are
presented in Fig. 5.

To gain a better insight into the differences in interaction times between our
various groups of users, Fig. 6 shows a comparison of average times across our
groups. As we expected, our expert users on average required less time to complete
their tasks than their non-expert counterparts (18.96% faster than non-accented
subjects and 25.84% faster than accented subjects). Similarly, accented users spent
8.5% longer completing their tasks than non-accented users. However, results of a
two-tailed paired T-test on the efﬁciency of expert subjects versus all non-expert
subjects reveals that the difference in efﬁciencies of the two groups is not signiﬁcant

Fig. 5 Efﬁciencies of
interaction for non-expert
accented users

154

Geoinformatica (2010) 14:135–162

Fig. 6 Average efﬁciencies
of interaction for different
user groups

for p = .05 or .01 (df = 16). As such we can conclude that, given minimal training,
our interface is easy to learn, allowing all types of users to complete tasks efﬁciently.
Moreover, the effectiveness of the recognition software improves with voice training
so we can consequently expect that if a user trains their voice to use this software,
recognition will increase, effectively increasing the overall efﬁciency of completing
tasks.

With regard to modality preference for carrying out geospatial tasks, 88.89%
of our subjects preferred interacting multimodally overall [2]. This preference was
attributed to the ﬂexibility and efﬁciency advantages multimodal interaction affords
over unimodal interaction. This trend was displayed across all geospatial tasks, i.e.
the vast majority of our users preferred interacting multimodally for geospatial tasks
involving querying, feature manipulation and annotating. However, preference for
pen input was greatest for annotation tasks, with 38.89% of subjects preferring
pen and keyboard input over dictation. This was not surprising, as dictation is not
sufﬁciently accurate to be considered an effective annotation input method in our
system. The remaining 62.11% of subjects who stated a preference for multimodal
annotation did so as they rated the efﬁciency of dictation input above the accuracy
of pen input, which was perceived as being time consuming and cumbersome. Given
this user feedback, we decided it was necessary to improve the annotation mechanism
within our system. This was achieved through providing multimodal error correction
at our dictation interface. A user evaluation, focusing on annotation input and our
error correction interface is described in the following section.

5.3 Error evaluation

In evaluating errors within our system, we did not focus our attention on the actual
rate of errors, but rather on how users attempted to correct these errors. In other
words, whether users attempted to correct their errors using the same input modality
(that was used when the mistake was made), or by switching modalities. In general,
error rate was very low, with a total of 12 errors being recorded across 11 subjects
during their main task. Each of these errors was user-, rather than system-, related.
Typical pen-related errors included choosing incorrect features from menus when
querying or not correctly selecting a feature to query before selecting an area on the

Geoinformatica (2010) 14:135–162

155

map, causing a zoom in rather than highlight action to occur. Errors related to speech
mostly consisted of issuing the wrong voice command, an error we expect would be
eliminated as users become more familiar with the available voice commands.

As we observed subjects interacting, we noted that 7 of these 11 subjects (with
errors) chose to rectify their errors using the same modality they had been using
when the error occurred. For 6 of these subjects, that modality was speech. The 4
remaining subjects chose to switch modalities to correct their errors: on 3 occasions
the modality was changed from pen to speech while it was changed from speech to
pen once.

Our post-evaluation questionnaire asked users whether, if in this situation, they
would correct errors using the same modality or switch modalities. We observed a
correlation between what users said they would do and their actual actions during
their main task. From our 18 subjects, 11 stated they would repeat the action/task
using the same modality at least once, but not more than twice, before switching.
The remaining 7 subjects reported that they would switch modalities immediately.
While monitoring users correcting errors, we perceived no overhead in switching
from one mode to another for correction. These results demonstrate that the ability
to switch between different modalities to correct errors is an important aspect of a
mobile application where errors relating to all input modalities are common due to
(amongst other factors) a user’s mobility.

6 Annotation input and error correction evaluation

One of the primary concerns users had with regard to the functionality of the
CoMPASS GUI was the annotation input mechanism. Ideally, it would be best for
users to input annotations using a real keyboard; however this is impractical in a
mobile context. As such, annotation input was possible through using the Tablet PC
pen and virtual keyboard or through dictation of annotations. However, annotation
creation through pen and virtual keyboard was perceived as being inefﬁcient and
cumbersome in a mobile context, while dictation of annotations was not sufﬁciently
accurate. As a result, we proposed two solutions to address this problem. The ﬁrst
was to provide alternative input methods to correct errors resulting from dictation
and the second was to make these input methods available at the annotation interface
level, thus providing the user with a wider range of input options for creating
annotations. Speciﬁcally, these alternative annotation input methods are handwriting
and voice memos. We subsequently conducted a user evaluation to determine if such
input methods would prove both efﬁcient and accurate when inputting annotations
and to ascertain which mode proved most favourable with users. For this part of the
experiment, we hypothesised that recording annotations as voice memos would be
the most efﬁcient method of annotation input, followed by handwriting and pen and
keyboard input.

Further motivation for carrying out this evaluation was to establish the efﬁciency
of error correction for dictation tasks. Dictation is used within CoMPASS for anno-
tating map features with text. As dictation is not matched against a rule grammar
it is more error prone than using voice commands. Therefore, it is necessary that an
interface provide methods allowing the user to correct errors resulting from dictation,

156

Geoinformatica (2010) 14:135–162

so as to ensure the correct annotation is stored in the system. We were interested in
ascertaining three things:

1. What mode of error correction is most efﬁcient for such a task. Available modes
are handwriting, recording a voice memo and using the pen and virtual keyboard.
Again, we expected that recording a voice memo would be most efﬁcient.

2. Which mode of correction do users prefer and which do they feel is most efﬁcient
and easy to use. We hypothesised that most users would choose to correct any
errors to dictation input using a voice memo (as we expected that this would be
the most efﬁcient mode of annotation input), followed by handwriting.

3. Whether dictating an annotation, including time for error correction, is more
efﬁcient than inputting the original annotation through pen and virtual keyboard.
We expected that this would be the case when correcting dictation errors using
either voice recording or handwriting, but not when correcting using pen and
keyboard.

We also recorded the accuracy of the user’s original dictation before correction so as
to gain some insight into exactly how accurate/inaccurate dictation is for such a task.
This evaluation served as a follow-up to the previous user evaluation. Eight
subjects from the previous experiment participated, 4 males and 4 females. All
participants were native English speakers. Each was given the task sheet presented in
Table 1. The task consisted of inputting and saving the given annotation in the system

Table 1 Annotation tasks

“The art exhibition at Thornton City Hall has many paintings by great artists such as Van Gogh.”

TASK 1 - Input this annotation using the pen and virtual keyboard. To do this you must

1. Choose pen and keyboard from the ’Create Annotation’ menu.
2. Open the virtual keyboard and enter the annotation.
3. Press send to save the annotation.

TASK 2 - Input this annotation using voice. To do this you must

1. Choose ‘Voice Annotation’ from the ‘Create Annotation’ menu.
2. Record your annotation and save.

TASK 3 - Input this annotation using handwriting. To do this you must

1. Choose ‘Handwriting’ from the ‘Create Annotation’ menu.
2. Write your annotation in the handwriting window.
3. Ensure it is correct before saving it.

TASK 4 - Input the same annotation using dictation. You must also correct any errors that result
from dictation so that the annotation you save is correct. To do this you must

1. Turn speech on.
2. Issue the command ‘Annotate’.
3. Speak your annotation.
4. Check if it is correct; if so say ‘Correct’, if not say ‘Incorrect’.
5. If incorrect, choose your preferred method of correction.
6. When correct, save.

Geoinformatica (2010) 14:135–162

157

through a number of different input modes. The ﬁrst required the user to enter the
annotation using the pen and virtual keyboard; the second consisted of recording a
voice memo concerning the feature of interest; handwriting was to be used to input
the annotation in the third task while dictation was the ﬁnal method of input. As
dictation is a highly error-prone mode of input for annotations users were requested
to correct their dictation if it contained any errors. The user was free to choose their
preferred mode of input for error correction of their dictated annotation.

6.1 Results

Results of this user evaluation are illustrated in Figs. 7–10. Our ﬁrst graph depicts
efﬁciencies of interaction across all input modality types for each of our 8 subjects
(Fig. 7). As expected, recording annotations as voice memos was the most efﬁcient
input mode for all subjects. This was followed by handwriting, which proved not only
efﬁcient but also an accurate mode of annotation input. Figure 8 provides a clearer
perspective on the difference in efﬁciency between pen and keyboard input and
dictation. Dictation not only takes into account user vocalisation of the annotation,
but also error correction if the annotation is erroneous. We had hypothesised that,
overall, the total time for dictation input and error correction would be less than the
time taken to input the same annotation using pen and virtual keyboard. However,
this was only the case for 4 of our subjects. Of these, 3 had corrected their dictation
by recording a voice memo while 1 chose handwriting. All 4 subjects whose dictation
time was slower than their pen and keyboard input time, chose handwriting as their
mode of correction for dictation errors. However, differences in interaction efﬁciency
were relatively small across all subjects.

We were also interested in ascertaining user preferences for the various available
modes of annotation input. We expected this would have a strong correlation with
the efﬁciency and accuracy subjects experienced with different input modes during

Fig. 7 User efﬁciency for different modes of annotation input

158

Geoinformatica (2010) 14:135–162

Fig. 8 Comparison of total
time for dictation versus
pen input

their annotation tasks. Following the evaluation, we conducted a short interview,
whereby the participant was asked a number of questions relating to their experience
and perceptions during the evaluation. When asked about their preferred annotation
input method, 5 users (62.5%) stated a preference for the voice memo input (Fig. 9a).
As anticipated, the reasons provided for this included ease of use, naturalness and
efﬁciency. The remaining 3 users (37.5%) preferred handwriting as an input mode
for annotations. Reasons provided were similar to those stated for voice memo
preference: for these users handwriting is both quick and easy. In addition, subjects
enjoyed the novelty of this type of input and were surprised by its accuracy.

Each subject’s ﬁnal annotation input task required them to dictate their anno-
tation, conﬁrm whether it was interpreted correctly or incorrectly and if necessary
correct the dictation. We recorded the percentage of accuracy for each user’s initial
dictated annotation, the results of which are illustrated in Fig. 9b. This graph shows
that there was a vast difference in accuracies between our subjects. For one of our
subjects, user 3 in our graph, no words were recognised correctly. With regard to
error correction the graph in Fig. 10a shows which modes of input users chose to
correct errors in their dictation, while Fig. 10b depicts the most efﬁcient mode for
this task. 5 users (62.5%) chose to correct their annotation using handwriting, while

a

s
r
e
s
U

 
f
o
 
e
g
a
t
n
e
c
r
e
P

70

60

50

40

30

20

10

0

b

y
c
a
r
u
c
c
A

90
80
70
60
50
40
30
20
10
0

Pen and
Keyboard

Voice Memo Handwriting

Dictation

1

2

3

6

7

8

4
5
Users

Fig. 9 a User preference for different modes of annotation input; b percentage accuracy of user
dictation

Geoinformatica (2010) 14:135–162

159

a

s
r
e
s
U

 
f
o
 
e
g
a
t
n
e
c
r
e
P

70

60

50

40

30

20

10

0

b
160

140

120

100

80

60

40

20

0

Pen and Keyboard

Voice Memo

Handwriting

Voice Memo

Handwriting

Fig. 10 a Chosen modes for correction of dictation input; b comparison of efﬁciencies of different
modes of error correction

the remaining 3 users (37.5%) chose to record a voice memo. Recording a voice
memo proved the most efﬁcient mode of error correction for a dictation task and
was 50.18% faster than handwriting, the only other mode used to correct errors
(Fig. 10b).

These results demonstrate that we have provided an accurate and efﬁcient way of
entering spatial annotations at the CoMPASS client interface. Moreover, the wide
range of choice for annotation input provides users with increased ﬂexibility for
performing such a task, allowing individual users to choose the method of annotation
input that is most appropriate/efﬁcient for their current environment. Our users have
shown to make use of this ﬂexibility to increase their task performance.

7 Conclusion

In recent years mobile GIS has been afforded a signiﬁcant increase in popularity
and has expanded greatly from its traditional usage context i.e. professional GIS
users such as surveyors and cartographers. A much wider and diverse range of users
now depend on mobile geospatial applications which provide location-based services.
These include for example tourists, outdoor enthusiasts and professional non-expert
GIS users such as electricians. As many traditional mobile GIS applications are
inherently complex, due in large part to limited interaction techniques for performing
geospatial tasks in a mobile context, it is critical to address such complexity for this
new age of mobile GIS use.

As a solution to this problem, we have addressed human computer interaction
challenges associated with GIS usage in a mobile context, focusing on the limited
modes of interaction available to mobile GIS users which can increase interaction
complexity. As such our research has investigated the use of multimodal interfaces
for mobile GIS and the associated issues and challenges relating to the design
and implementation of such interfaces. The advantages of multimodal interaction
for mobile geospatial, location-based tasks are numerous. They can improve users’
productivity and enhance their geospatial experience by allowing users to choose the
mode of interaction that best suits their current environment and task, in addition to
that which is most intuitive to them.

160

Geoinformatica (2010) 14:135–162

The main contribution of this article is a comprehensive evaluation of how
multimodal interfaces can contribute to reducing the complexity of interacting
with mobile, multi-functional GIS applications. Our results demonstrate that the
multimodal interface we developed played a signiﬁcant role in contributing to the
overall intuitiveness, user-friendliness, efﬁciency and ﬂexibility of our mobile GIS
application for different user groups, both novice and expert, carrying out different
tasks. Moreover, multimodal (as opposed to unimodal) interaction was rated highly
by users as the preferred mode of interaction for mobile geospatial tasks. While we
have shown that the speech and pen multimodal combination effectively addresses
usability issues concerning mobile GIS, it remains to be tested whether there is a
limit, with regard to the number of modalities, to the value of multimodal interaction
to the user. For example, the introduction of additional modalities, such as touch or
gaze-tracking might overload the user in terms of introducing complexity regarding
what modalities could, or should, be used in certain situations. Despite this, we
can conclude that as mobile mapping applications providing location-based services
become more readily available in the future, multimodal interfaces supporting
speech and pen input, such as that provided by CoMPASS, will play a vital role
in improving the usability of such applications. We perceive this will be the case
even as mobile devices change and become more advanced. Furthermore, we expect
CoMPASS will grow with the technologies of its time.

With regard to future work, we will consider the use of synchronous fusion on
inputs. If a CoMPASS user currently attempts to gesture at the same time as they
issue a voice command, errors will occur. While no such errors were perceived during
our user evaluations, synchronous fusion of inputs could increase the efﬁciency of
interaction as the user does not have to wait until after they have spoken to gesture,
as is currently the case for querying within CoMPASS.

Acknowledgements Research presented in this paper was funded by a Strategic Research Cluster
grant (07/SRC/I1168) by Science Foundation Ireland under the National Development Plan. The
authors gratefully acknowledge this support.

References

1. Doyle J, Bertolotto M, Wilson D (2007) A survey of multimodal interfaces for mobile mapping
applications (book chapter). In: Meng L, Zipf A, Winter S (eds) Map-based mobile services—
interactivity and usability, chapter 8. Springer-Verlag book series in geo-information and cartog-
raphy. Springer, Berlin Heidelberg New York, pp 146–166

2. Doyle J, Bertolotto M, Wilson D (2007) Multimodal interaction—improving usability and efﬁ-
ciency in a mobile GIS context. In: International conference on advances in computer-human
interaction (ACHI 08). IEEE, New York, pp. 63–68

3. Egenhofer M (1997) Query processing in spatial-query-by-sketch. J Vis Lang Comput 8(4):

403–424

4. Fuhrmann S, MacEachren A, Dou J, Wang K, Cox A (2005) Gesture and speech-based maps to
support use of GIS for crisis management: a user study. In: AutoCarto 2005. Cartography and
Geographic Information Society, Gaithersburg

5. International Organisation for Standardization (2007) ISO 9241-11—Guidance on usability.

http://www.iso.org. Accessed 2007

6. Keiffer S, Carbonell N (2006) Oral messages improve visual search. In: AVI ’06, the working

conference on advanced visual interfaces. ACM, New York, pp 369–372

7. Kreuger A, Butz A, Muller C, Stahl C, Wasinger R, Steinberg KE, Dirschl A (2004) The
connected user interface: realizing a personal situated navigation service. In: 9th international
conference on intelligent user interfaces (IUI 04). ACM, New York, pp 161–168

Geoinformatica (2010) 14:135–162

161

8. Malaka M, Zipf A (2000) DEEP MAP—challenging IT research in the framework of a tourist
information system. In: 7th international congress on tourism and communication technologies
in tourism (ENTER 2000). Springer LNCS, Berlin Heidelberg New York, pp 15–27

9. Oviatt S (1996) Multimodal interfaces for dynamic interactive maps. In: SIGCHI conference on

human factors in computing systems, Vancouver, 13–18 April 1996, pp 95–102

10. Oviatt S (2000) Taming recognition errors with a multimodal interface. In: Communications of

the ACM, vol 43, no 9. ACM, New York, pp 45–51

11. Oviatt S (1999) Mutual disambiguation of recognition errors in a multimodal architecture.

In: CHI ’99. ACM, New York, pp 576–583

12. Oviatt S (2003) Multimodal interfaces. In: Jacko J, Sears A (eds) Handbook of human–computer

interaction. North Holland, Amsterdam, pp 286–304

13. Rauschert I, Sharma R, Fuhrmann S, Brewer I, MacEachren A (2002) Approaching a new multi-
modal GIS interface. In: 2nd international conference on GIS (GIScience), Boulder, September
2002

14. Sharma R, Pavlovic V, Huang TS (1998) Toward a multimodal human computer interface. Proc

IEEE 86(5):853–869

15. Suhm B, Myers B, Waibel A (2001) Multimodal error correction for speech user interfaces.
In: ACM transactions on computer-human interaction (TOCHI), vol 8, no 1. ACM, New York,
pp 60–98

16. Wasinger R, Stahl C, Kreuger A (2003) M3I in a pedestrian navigation and exploration system.
In: 5th international symposium on human computer interaction with mobile devices (Mobile
HCI 03). Springer LNCS, Berlin Heidelberg New York, pp 343–346

17. Wilson D, Doyle J, Weakliam J, Bertolotto M, Lynch D (2006) Personalised maps in multimodal
mobile GIS. International Journal of Web Engineering and Technology, sepcial issue on Web
and Wireless GIS. Inderscience Publishers 3(2):196–216

18. Wickens CD, Baker P (1995) Cognitive issues in virtual reality. In: Furness TA, Bakerﬁeld W
(eds) Virtual environments and advanced interface design. Oxford University Press, Oxford,
pp 514–541

Dr. Julie Doyle received a PhD in Computer Science from University College Dublin, Ireland.
Her thesis focused on improving the usability and interactivity of mobile geospatial applications.
Subsequently she worked as a postdoctoral fellow at the School of IT and Engineering, University
of Ottawa, Canada, where she developed a framework to ensure the long term preservation of 3D
digital data. Currently she is a postdoctoral fellow in HCI and Systems Engineering in the UCD
School of Computer Science and Informatics. Her main research interests include the design of
interfaces for mobile geospatial applications, multimodal HCI, HCI for healthcare technologies,
usability and long term data preservation.

162

Geoinformatica (2010) 14:135–162

Dr. Michela Bertolotto received a PhD in Computer Science from the University of Genoa (Italy)
in 1998. Subsequently she worked as a Postdoctoral Research Associate in the National Center for
Geographic Information and Analysis (NCGIA) at the University of Maine. Since 2000 she has been
a faculty member at the School of Computer Science and Informatics of University College Dublin.
Her research interests include web-based and wireless GIS, spatio-temporal data modelling and 3D
interfaces.

Dr. David C. Wilson is an Assistant Professor in the College of Computing and Informatics at the
University of North Carolina at Charlotte. Dr. Wilson is an expert in intelligent software systems
and a leading name in the ﬁeld of Case-Based Reasoning. Dr. Wilson’s research centers on the
development of intelligent software systems to bridge the gaps between human information needs
and the computational resources available to meet them. It involves the coordination of intelligent
systems techniques (artiﬁcial intelligence, machine learning, etc.) with geographic, multimedia,
database, internet, and communications systems in order to elicit, enhance, apply, and present
relevant task-based knowledge.

