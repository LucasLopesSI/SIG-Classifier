This article was downloaded by: [New York University]
On: 27 May 2015, At: 15:34
Publisher: Taylor & Francis
Informa Ltd Registered in England and Wales Registered Number: 1072954 Registered
office: Mortimer House, 37-41 Mortimer Street, London W1T 3JH, UK

International Journal of Geographical
Information Science
Publication details, including instructions for authors and
subscription information:
http://www.tandfonline.com/loi/tgis20

Optimization of mobile radioactivity
monitoring networks
G. B.M. Heuvelink a , Z. Jiang a , S. De Bruin a & C. J.W.
Twenhöfel b
a Environmental Sciences Group , Wageningen University and
Research Centre , Wageningen, the Netherlands
b National Institute for Public Health and the Environment ,
Bilthoven, the Netherlands
Published online: 10 Mar 2010.

To cite this article: G. B.M. Heuvelink , Z. Jiang , S. De Bruin & C. J.W. Twenhöfel (2010)
Optimization of mobile radioactivity monitoring networks, International Journal of Geographical
Information Science, 24:3, 365-382

To link to this article:  http://dx.doi.org/10.1080/13658810802646687

PLEASE SCROLL DOWN FOR ARTICLE

Taylor & Francis makes every effort to ensure the accuracy of all the information (the
“Content”) contained in the publications on our platform. However, Taylor & Francis,
our agents, and our licensors make no representations or warranties whatsoever as to
the accuracy, completeness, or suitability for any purpose of the Content. Any opinions
and views expressed in this publication are the opinions and views of the authors,
and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content
should not be relied upon and should be independently verified with primary sources
of information. Taylor and Francis shall not be liable for any losses, actions, claims,
proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or
howsoever caused arising directly or indirectly in connection with, in relation to or arising
out of the use of the Content.

This article may be used for research, teaching, and private study purposes. Any
substantial or systematic reproduction, redistribution, reselling, loan, sub-licensing,
systematic supply, or distribution in any form to anyone is expressly forbidden. Terms &

Conditions of access and use can be found at http://www.tandfonline.com/page/terms-
and-conditions

Downloaded by [New York University] at 15:34 27 May 2015 International Journal of Geographical Information Science
Vol. 24, No. 3, March 2010, 365–382

Optimization of mobile radioactivity monitoring networks

G.B.M. Heuvelinka*, Z. Jianga, S. De Bruina and C.J.W. Twenho¨felb

aEnvironmental Sciences Group, Wageningen University and Research Centre, Wageningen,
the Netherlands; bNational Institute for Public Health and the Environment, Bilthoven,
the Netherlands

(Received 5 November 2008; final version received 17 November 2008)

In case of a nuclear accident, decision makers rely on high-resolution and accurate
information about the spatial distribution of radioactive contamination surrounding the
accident site. However, the static nuclear monitoring networks of many European countries
are generally too coarse to provide the desired level of spatial accuracy. In the Netherlands,
authorities are considering a strategy in which measurement density is increased during an
emergency using complementary mobile measuring devices. This raises the question,
where should these mobile devices be placed? This article proposes a geostatistical
methodology to optimize the allocation of mobile measurement devices, such that the
expected weighted sum of false-positive and false-negative areas (i.e. false classification
into safe and unsafe zones) is minimized. Radioactivity concentration is modelled as the
sum of a deterministic trend and a zero-mean spatially correlated stochastic residual. The
trend is defined as the outcome of a physical atmospheric dispersion model, NPK-PUFF.
The residual is characterized by a semivariogram of differences between the outputs of
various NPK-PUFF model runs, designed to reflect the effect of uncertainty in NPK-PUFF
meteorological inputs (e.g. wind speed, wind direction). Spatial simulated annealing is used
to obtain the optimal monitoring design,
in which accessibility of sampling sites
(e.g. distance to roads) is also considered. Although the methodology is computationally
demanding, results are promising and the computational load may be considerably reduced
to compute optimal mobile monitoring designs in nearly real time.

Keywords: emergency; geostatistics; sampling design; spatial simulated annealing

1.

Introduction

The Chernobyl nuclear accident in 1986 and the more recent increased risk of terrorist attacks
with dirty bombs underline the need for radiological monitoring networks and mobile
measuring facilities, capable of measuring and mapping radioactivity for a region or country.
In the Netherlands, the National Institute for Public Health and the Environment (RIVM)
operates the Dutch National Radioactivity Monitoring network. The network consists of 153
ambient dose rate monitors and 14 (cid:2)/(cid:3) air-sampling monitors (Twenho¨fel et al. 2005). In a
routine monitoring situation, measurements are carried out every 10 minutes and are imme-
diately available to the radiological back-office stationed at RIVM. The 153 dose rate monitors
are more or less uniformly spread across the country, with an increased density near the
Borssele nuclear power plant in the southwest and along the borders with Germany and
Belgium. When radiation levels are above a critical threshold, the RIVM issues a warning to

*Corresponding author. Email: gerard.heuvelink@wur.nl

ISSN 1365-8816 print/ISSN 1362-3087 online
# 2010 Taylor & Francis
DOI: 10.1080/13658810802646687
http://www.informaworld.com

Downloaded by [New York University] at 15:34 27 May 2015 366

G.B.M. Heuvelink et al.

the national and local authorities. After confirmation and careful evaluation, it may then be
decided to activate the national emergency response plan for nuclear accidents (Van Sonderen
1997, Twenho¨fel et al. 2005). Effective counter-measure strategies are aimed at immediate
protection of the human population (e.g. by sheltering, evacuation, and iodine prophylaxis),
safeguarding food production chains (e.g. by preventing harvest and dairy products from
entering the food market, or by sheltering or relocating cattle), and other preventative measures
designed to minimize long-term environmental remediation efforts. Measurements of radio-
activity are used in conjunction with physical dispersion models to predict the spatial distribu-
tion of radioactive contamination and to make prognoses of the radiological situation in the
area surrounding an accidental release of radionuclides. However, because the static monitor-
ing network was designed to cover the whole country, it is too coarse to provide accurate local
information. Additional measurements from mobile measuring devices or sensors, placed in
the vicinity of the accident site and the forecasted contamination plume, are required to
improve prediction (i.e. reduce prediction error) and obtain an accurate delineation of the
spatial distribution of contamination. In order to benefit most from the extra mobile devices,
the devices must be optimally spatially allocated.

Optimization of spatial sampling designs begins with the definition of a criterion to
evaluate the suitability of a given design. Criteria that evaluate the estimation of a spatial
aggregate, such as the mean, median, or frequency distribution of the target variable over the
region of interest, can be optimized using design-based statistical approaches (Stevens and
Olsen 2004, Stehman et al. 2005, De Gruijter et al. 2006, Theobald et al. 2007). Model-based
approaches are generally preferred when the objective is to predict values at point locations or
when the objective is to create high-resolution maps of the target variable, as in the case of a
radioactive release. McBratney and Webster (1981) and McBratney et al. (1981) presented an
algorithm that optimizes the spacing of a regular grid and a triangular lattice design, given an a
priori semivariogram and a maximum tolerable kriging prediction error variance. Yfantis et al.
(1987) showed that a hexagonal grid is more efficient than a regular grid or triangular lattice in
cases where the sampling density is sparse and when there is little spatial correlation (i.e. a
semivariogram with a large nugget). However, the gain in efficiency using a hexagonal design
is marginal and seldomly outweighs the ease of grid sampling in practical applications. Spruill
and Candela (1990) estimated the prediction accuracy of chloride concentration in ground-
water by removing and adding locations to an existing sampling network. Similarly, ordinary
co-kriging between mercury sediment concentration and sediment grain size index was used
by Ben-Jemaa et al. (1995) to redesign an existing network by minimizing the average
prediction error variance. Van Groenigen et al. (1999) adopted the mean kriging variance as
an optimization criterion to sample the texture and phosphate content on a river terrace. Brus
and Heuvelink (2007) extended the framework to universal kriging, in which case trend
parameter estimation is optimized simultaneously. These studies all assumed that
the
covariance function or semivariogram is known and used the average (kriging) prediction
error variance as a criterion to obtain the optimal design. More recently sampling design
optimization was extended to situations where the semivariogram parameters must also be
estimated from the sample (Zhu and Stein 2006).

Minimization of the average prediction error variance may not be the most appropriate choice
for radioactivity monitoring and mapping because uncertainty about the predicted radiation level
only affects decision making when the probability distribution of the prediction straddles the
intervention level. Large uncertainties associated with predictions far below or much above the
intervention level have no negative consequences when it is practically certain that the radio-
activity is below or above the threshold (see also Rogerson et al. 2004). Therefore, a more sensible
criterion would be to minimize the costs of wrong decisions caused by false classifications

Downloaded by [New York University] at 15:34 27 May 2015 International Journal of Geographical Information Science

367

(Goovaerts et al. 1997, Van Meirvenne and Goovaerts 2001). The optimal sampling design
minimizes the costs associated with false-positive (i.e. when the predicted concentration is greater
than the intervention level while the real value is smaller) and false-negative decisions (i.e. when
the predicted concentration is smaller than the intervention level while the real value is greater). In
the case of radioactive contamination, the costs associated with false negatives are likely to be
greater than those of false positives. Moreover, sampling design optimization must also take
geographical constraints of sensor placement into account.

This article presents a sampling design optimization method that minimizes the weighted
sum of costs of wrong decisions under imposed geographical constraints. The methodology
is illustrated with a fictitious example of an accident at the Borssele nuclear facility in the
Netherlands. Although the focus is on optimizing a radioactivity monitoring network, the
methodologies presented apply more generally to spatial optimization problems encountered
in geography and environmental science (Wang et al. 2002, Rogerson et al. 2004, Kumar
2007, McCarthy 2008, Ter Braak et al. 2008).

2. Methodology
The methodological framework to compute optimal locations for mobile measuring devices
has four main components as explained in Sections 2.1–2.4. These four components are
modelling the spatial distribution of radioactive concentrations and the associated uncer-
tainties, use of mobile measurements to improve spatial prediction of the concentrations,
definition of a cost criterion for evaluation of a sampling design, and spatial simulated
annealing (SSA). The integration of these four components into an iterative sampling design
optimization algorithm is discussed in Section 2.5.

2.1. Modelling the post-accident spatial distribution of radionuclide concentration and
its uncertainty

The RIVM has been conducting research into the dispersion of radionuclides for decades
with the development of a radioactive nuclide distribution model, PUFF (Van Egmond and
Kesseboom 1983, Verver and De Leeuw 1992). In cooperation with the Royal Dutch
Meteorological Institute (KNMI), PUFF was extended with provisions to allow coupling
with a meteorological model, the High Resolution Limited Area Model (HIRLAM, http://
hirlam.org/). The resulting NPK-PUFF model describes the atmospheric transport of
gaseous- and aerosol-bound radioactive nuclides from one or multiple point release sources
and calculates the concentration and deposition of radioactive materials over the threatened
area. The model produces gridded output that can be used to estimate (time-integrated) air and
ground radioactivity concentrations, which can subsequently be used in a dose estimation
model to calculate human population exposure to radioactivity.

The dynamics of radioactive dispersion in space and time can be reasonably well
predicted with the NPK-PUFF model, but uncertainties in meteorological conditions and
other inputs will propagate to the output (Heuvelink 1998, Eleveld et al. 2007). Moreover,
errors in the model structure mean that even if the model input were known exactly, model
predictions would still differ from reality. To take these uncertainties into account, the true
concentration at the time of interest may be represented as the sum of a deterministic trend
(the NPK-PUFF prediction) and a stochastic residual:

ZðxÞ ¼ ZPUFFðxÞ þ "ðxÞ

(1)

Downloaded by [New York University] at 15:34 27 May 2015 368

G.B.M. Heuvelink et al.

where Z is the true concentration, ZPUFF is the NPK-PUFF output, " is the difference between
the true concentration and NPK-PUFF output, and x is a spatial coordinate. The true value of
the residual " is unknown and it is therefore represented by a random function, whose
probability distribution must be defined such that it reflects the uncertainty about the NPK-
PUFF output (Brown and Heuvelink 2007, Heuvelink et al. 2007). To facilitate identifica-
tion of the probability distribution of ", it is common practice in geostatistics to assume that
the residual is normally distributed, second-order stationary and isotropic. Although the
stationarity assumption is a crude approximation of the real error structure, it is used here to
simplify the estimation of the probability distribution of ". Stationarity and isotropy indicate
that the mean and variance of " are constant in space and that the correlation between "(x)
and "(x + h) only depends on the Euclidean distance |h| between x and x + h (Goovaerts
1997). Here, it is also reasonable to assume that " has a mean of zero, because the NPK-
PUFF model is calibrated such that it will not systematically over- or underestimate the true
concentration. However, " will be spatially correlated. For instance, errors in wind speed and
wind direction displace the contamination plume as a whole and thus create local anomalies
where the concentration is systematically over- or underestimated.

The spatial covariance structure of " can be characterized by a semivariogram.
Parameters of the semivariogram (i.e. the nugget, sill, range and shape) can be estimated
as follows. First, NPK-PUFF is run with the most likely meteorological parameter input
values for a given scenario to create a reference run. Next, the main inputs of NPK-PUFF
are varied such that their values reflect the uncertainty in these inputs. Multiple runs of
NPK-PUFF are then executed using this range of input values and each of the results is
subtracted from the output of the reference run. Semivariograms of the resulting residuals
are computed and plotted. Suitable values for the semivariogram parameters can then be
derived from these experimental semivariograms, either through eyeball fitting or using
statistical estimation techniques such as weighted least squares and maximum likelihood
(Chile`s and Delfiner 1999).

2.2. Using mobile measurements to improve the NPK-PUFF prediction
In the absence of measurements, a reference run of the NPK-PUFF model will be used to
predict the radionuclide concentration at any given location, simply because no other
information is available. However, in a real emergency situation, measurements will be
available too, and information contained in these measurements can be used to improve
predictions of radioactive contamination and population exposure. A simple data
assimilation approach, not nearly as advanced as dynamic data assimilation techniques
based on the Kalman filter (Huang and Cressie 1996, Bertino et al. 2003, Heuvelink et al.
2006), can be used to combine model predictions with measured data as follows. At each
measurement location, the difference between the measured concentration and the con-
centration predicted by NPK-PUFF is computed. Next, these differences are interpolated
using simple kriging, and the interpolated residual is added to the NPK-PUFF output. The
resulting map reproduces the observed values at the measurement locations because
kriging is an exact
interpolator. It also adjusts the NPK-PUFF model outputs at
non-measurement locations. The degree of adjustment is larger near measurement loca-
tions and when spatial correlation is large. The final map uses both information contained
in the NPK-PUFF output and information from the observed measurements. Since it uses
both sources of information, it will generally be closer to the true concentration than the
map produced by the reference run.

Downloaded by [New York University] at 15:34 27 May 2015 International Journal of Geographical Information Science

369

True negative

y

Predicted plume

True plume

False positive

True positive

e

a ti v

g

e

n

e

a l s

F

x

Figure 1. Graphical illustration of geographical areas of false-positive and false-negative decisions.
Solid line delineates the true plume, dashed line the predicted plume.

2.3. Optimization criterion

As stated in the introduction, the objective of this research is to select sampling locations that
minimize costs associated with false-positive and false-negative decisions. False-positive
decisions occur when a pre-defined threshold is exceeded due to model error or uncertainty,
resulting in unnecessary measures, such as the evacuation of people from an area that is in
reality safe. Conversely, costs associated with false-negative decisions are that measures are
not taken while in fact they were necessary. These costs are likely greater than false-positive
costs because they affect the health and lives of people.

Any location within the study area can fall into one of four categories: false positive,
false negative, true positive, and true negative (Figure 1). The objective here is to minimize a
weighted sum of the areas occupied by false positives and negatives:

C ¼ (cid:2) (cid:2) area ðfalse positiveÞ þ ð1 (cid:3) (cid:2)Þ (cid:2) area ðfalse negativeÞ

(2)

where C are the costs and the cost ratio (cid:2)/(1 – (cid:2)) is likely to be (much) smaller than 1.

2.4. Spatial simulated annealing
Obtaining the spatial sampling design that minimizes Equation (2) requires the use of a
numerical optimization algorithm. Many algorithms have been developed for solving non-
linear optimization problems. Examples are genetic algorithms, neural networks, tabu
search, particle swarm optimization, and simulated annealing (Glover 1989, Hornik et al.
1989, Lee and Ellis 1996, Schmitt 2001, Coello et al. 2004, Vrugt and Robinson 2007).
Simulated annealing (Kirkpatrick et al. 1983) is a random search technique akin to finding
the optimum by the physical process of annealing. The optimization problem is solved
iteratively by starting with an arbitrary design and repeatedly trying out new designs that are
slight modifications of the previous design. Candidate designs that reduce the cost criterion
are favourably accepted, which ensures that the iteration gradually progresses to designs
with low costs, ideally the one with the lowest cost.

Spatial Simulated Annealing (SSA) is the spatial extension of simulated annealing (Van

Groenigen and Stein 1998). It has five main steps:

Downloaded by [New York University] at 15:34 27 May 2015 370

G.B.M. Heuvelink et al.

(1) Start with a (random) initial sampling design S0 and compute the associated costs

C(S0).

(2) Given the design Sk, construct a candidate new sampling design Sk+1 by moving a
randomly chosen sampling location over a distance h. The direction of h is randomly
chosen and its length is a random number between zero and a maximum shift. The
maximum shift is gradually decreased while the SSA iteration progresses.

(3) Compute the objective function C(Sk+1) for the new design. If C(Sk+1) , C(Sk) then
accept the new design, else accept the new design with a certain probability (this is to
ensure that the algorithm can escape from local optima). If the new design is
accepted, then increase k by 1. The probability of accepting worsening designs is
gradually decreased as the SSA iteration progresses.

(4) Return to step (2), using the new design Sk+1 as starting point if it was accepted and

using the old design Sk otherwise.

(5) Stop after a fixed number of iterations or when some other stopping criterion is

satisfied. Store the design that had the smallest cost.

Candidate sampling locations must obviously lie within the study area. Moreover, candidate
locations may also be selected based on additional topographic conditions. For instance,
mobile radioactivity measurement devices must be located in open areas and within a
maximum distance from public roads.

2.5. Sampling design optimization algorithm

The main steps of the approach used to compute the optimal sampling design can now be
summarized as follows. First, an initial design is chosen. Next, the observations at the
sampling locations are used to adjust the NPK-PUFF output as described in Section 2.2.
Geographical areas of false-positive and false-negative decisions are computed and the cost
criterion Equation (2) is evaluated. SSA is used to iteratively search for the optimal design.
There is one problem, though. In order to adjust NPK-PUFF with the measurements and
compute the areas of false-positive and false-negative decisions, the true concentration must
be known throughout the study area. In reality, it will not be known. However, as indicated in
Section 2.1, the true concentration can be characterized by a probability distribution through
Equation (1). Therefore, a Monte Carlo simulation approach can be used: many possible
realities are simulated, the costs associated with the sampling design are evaluated for all
possible realities, and the average costs are calculated. If the number of simulated realities is
sufficiently large, then the average costs will approximate the expected costs associated with
the sampling design. It is sensible to define the optimal design as the one for which the
expected costs are minimal.

In summary, the following step-by-step approach describes how the optimal sampling

design is obtained:

(1) Compute the reference NPK-PUFF output for the research area of interest.
(2) Identify all candidate sampling locations by applying the topographic conditions to

all locations within the study area.

(3) Identify the semivariogram of the stochastic residual (as described in Section 2.1).
(4) Generate a large number (N (cid:4) 100) of realizations of the stochastic residual field
using unconditional sequential Gaussian simulation (Goovaerts 1997) and add these
to the NPK-PUFF reference output, thus creating a set of N simulated concentration
fields (‘possible realities’).

Downloaded by [New York University] at 15:34 27 May 2015 International Journal of Geographical Information Science

371

(5) Construct an initial sampling design by (randomly) selecting a given number of

locations (n) from the candidate sampling locations.

(6) For each of the N realizations, derive the residual at the n sampling locations by
subtracting the NPK-PUFF reference value from the values in the possible reality
and interpolate the residuals using simple kriging. Add the interpolated residual
field to the NPK-PUFF reference output to obtain a predicted concentration map.
(7) For each of the N realizations, compare the predicted map with the known
concentration map (i.e. the possible reality) and compute the associated area of
false-positive and false-negative decisions and the cost criterion Equation (2).
(8) Compute the expected costs of the sampling design by averaging the costs over all

N simulations.

(9) Generate a new candidate sampling design using spatial simulated annealing;
compute the associated costs by repeating steps (6) through (8) and compare the
costs with those of the previous design; decide whether to accept or reject the new
design as specified in the SSA scheme.

(10) Repeat step (9) for as many iterations as specified by the SSA algorithm and store

the design with the minimum expected costs.

3. Results

3.1.

Introduction to the case study

Consider the hypothetical example where at time t = 0 an accident occurs at the nuclear
power plant of Borssele in the Netherlands. A south-west wind with velocity 3.0 ms-1 moves
a plume of released Cs-137 in the north-east direction. Suppose that the RIVM needs to
predict the spatial distribution of the radioactive concentration at t = 5 hours after the
accident. The prediction for t = 5 made by the NPK-PUFF model is given in Figure 2. The
figure also shows the boundary of the plume where the radioactivity is above the (fictitious)
intervention level of 1.0 · 105 Bq/m3. However, the NPK-PUFF output is no more than a
prediction which is uncertain to some degree. Therefore, it is decided to place 25 mobile
measurement devices within the area to improve prediction. Vans carrying the mobile
devices are on standby to drive to the selected measurement locations and can place
the devices within a few hours. The measurement readings at t = 5 can then be used to
adjust the predicted plume and produce a more accurate map. The sampling design optimization
methodology described in Section 2 can now be applied to derive the optimal locations for
the 25 devices.

The candidate sampling locations are confined to a finite set of nodes on a 1200-m
resolution grid of the area (100 · 100 km2). In addition, nodes that are not in open terrain or
are not within 300 m of a road are excluded. This reduces the set of candidate sampling
locations to 3147 points (Figure 2).

3.2. Six scenarios
Thirty error fields were computed by subtracting the reference NPK-PUFF output from 30
alternative NPK-PUFF model outputs. The alternative runs were obtained by changing
meteorological parameters (e.g. wind speed, wind direction, boundary layer height) and
emission inputs (e.g. emission height). The resulting error fields were randomly sampled,
and experimental semivariograms were computed (Figure 3). Differences between the sills

Downloaded by [New York University] at 15:34 27 May 2015 372

G.B.M. Heuvelink et al.

N

0

10

20

30 km

Borssele nuclear power plant

Candidate sampling locations

Land

Water

Plume

1.0.105 Bq/m3 intervention level

Figure 2. The study area is taken as a 100 · 100-km2 area in the south-west of the Netherlands. The
Borssele nuclear power plant is located in the south-west corner. The grey tone cloud overlaid on the
study area is the NPK-PUFF reference plume of radionuclide concentration, five hours after the
fictitious accident took place. Dots represent candidate locations where mobile measurement devices
can be placed.

of the semivariograms are substantial, indicating that NPK-PUFF output is more sensitive to
changes in some parameters than in others (wind speed in particular turns out to be a critical
parameter). Therefore, three different values for the semivariogram sill were taken, reflecting
the low, medium, and high uncertainty case. The semivariogram shape was in all cases a
spherical semivariogram with zero nugget variance and correlation length (range) of 30 km.
The choice of a spherical semivariogram may seem inappropriate because the NPK-PUFF
model is an advection–diffusion model which has a smooth behaviour at short lags and so
will differences between model runs with different parameter settings. However, a non-
differentiable variogram was used to also partially capture model error, which may show
substantial erratic behaviour. Two values for the cost ratio (cid:2)/(1 – (cid:2)) were used, one where the
costs of false-positive and false-negative decisions are equal and one where false-negative
decisions are five times more costly than false-positive decisions. The resulting six scenarios
are summarized in Table 1.

Downloaded by [New York University] at 15:34 27 May 2015 International Journal of Geographical Information Science

373

]
2
)
3
m
/
q
B
(
7
0
1
×
[
 
e
c
n
a
i
r
a
v
m
e
S

i

40

35

30

25

20

15

10

5

0

0

5

10

30

35

40

15

20
Distance (km)

25

Figure 3. Experimental semivariograms computed from the differences between the reference and
the perturbed NPK-PUFF outputs.

Table 1. Six scenarios with different settings for the degree of uncertainty and cost ratio.

Low uncertainty
sill = 1 · 107 (Bq/m3)2

Medium uncertainty
sill = 5 · 107 (Bq/m3)2

High uncertainty
sill = 1 · 108 (Bq/m3)2

Cost ratio (cid:2)/(1–(cid:2)) = 1/1
Vost ratio (cid:2)/(1–(cid:2)) = 1/5

Scenario 1a
Scenario 1b

Scenario 2a
Scenario 2b

Scenario 3a
Scenario 3b

3.3. Optimization with spatial simulated annealing

The SSA algorithm starts with an arbitrary initial sampling design for which the costs are
calculated. The algorithm then randomly perturbs the design by moving a randomly chosen
measurement device in a random direction, computes the cost of the new design, and
compares it with the cost of the previous design. Here, the distance over which the device
is displaced was drawn from a uniform distribution with a minimum distance of zero and an
initial maximum distance of 3 km. The maximum distance decreases exponentially as the
iteration progresses, where the exponential decay parameter was chosen such that the
maximum displacement distance halves every 400 iterations. Designs that reduce the costs
are always accepted, designs that increase the cost are accepted with a probability that
initially is equal to 0.2 but that decreases exponentially with a decay parameter that halves
the acceptance probability every 140 iterations. The SSA settings used were partly based on
the literature (Kirkpatrick 1984) and partly on critical examination of results from a testing
phase. The number of simulated realities used in the Monte Carlo analysis was taken as
N = 100. Figure 4 shows the development of the costs as the iterations progressed for
scenario 1a. In total, 2000 annealing iterations were done. The graph shows that the cost
function gradually decreased, but note also that several worsening designs were accepted.
Apparently, in this particular case near iteration step 400, the algorithm accepted a new
design that had a much larger cost than the previous design, which undid almost all of the
gain achieved in the first 400 runs. Nonetheless, towards the end of the iteration, the cost was
substantially reduced from an initial cost of 910 to a final cost of 498. Note also that the final

Downloaded by [New York University] at 15:34 27 May 2015 374

G.B.M. Heuvelink et al.

)

m
k
 
4
–
0
1
(
 
t
s
o
c
 
l
a
t
o
t
 
d
e
t
c
e
p
x
E

1000

900

800

700

600

500

0

500

1000

1500

2000

Simulated annealing iteration

Figure 4. Expected cost as a function of spatial simulated annealing iteration steps. Graph corre-
sponds to scenario 1a (low uncertainty and cost ratio 1/1).

600 iterations did not diminish the cost substantially. This suggests that the algorithm
converged to the optimal final design although there is no guarantee that the global optimum
has been reached.

Figure 5 shows the initial and final sampling designs for scenario 1a (low uncertainty and
cost ratio = 1/1). The final design concentrates observation locations along the boundary of
the reference contamination plume. This is because these are the areas where the probability
of making a false-positive or false-negative decision is the biggest. The greatest decrease of
these probabilities is achieved by placing mobile measurement devices in the boundary area,
so that locally a more accurate prediction of the radionuclide concentration can be made.
Taking measurements far away from the plume or in the centre of the plume has no added
value because at these locations one is already practically certain that the concentration is
below or above the intervention value. Note also that most observations are placed along
those parts of the plume where its boundary is wide and the uncertainty about the exact
location of the plume boundary is the greatest. This is where collecting additional observa-
tions pays off most. The final configuration of sampling points is not symmetric. This may
be caused by the fact that only N = 100 simulated realities were used or because the simulated
annealing had not converged to a global optimum, but the most likely cause is that sampling
is confined to locations that satisfy the topographic criteria (e.g. notice from Figure 2 that
north of the reference plume lies an area that cannot be sampled).

The probabilities of false-positive and false-negative decisions are also indicated in grey
tones in Figure 5. These probabilities are smaller than 50% in all cases and much smaller
than 50% in the optimized case. The smaller probabilities for the optimized sampling design
confirm the result of Figure 4, because the cost is the spatial average of the probabilities of
false decisions. Figure 5 also shows that the probability of false-negative decisions is large
just outside the reference plume, whereas false-positive decisions are large just inside the
plume. This is as expected, because a false-negative decision means that it is decided that the
concentration is below the intervention level whereas in reality it is above. This mainly
occurs just outside the reference plume. However, note that points within the reference

Downloaded by [New York University] at 15:34 27 May 2015 International Journal of Geographical Information Science

375

False-negative

False-positive

l

a
i
t
i
n
I

s
n
o
i
t
a
r
e
t
i
 

A
S
 
0
0
0
2
 
r
e
t
f

A

Probability of false classification

(cid:129)

Sample location

0

0.25

0.5

Threshold NPK-PUFF plume

Figure 5.
Initial (top) and final (bottom) sampling designs for scenario 1a. Boundary of the reference
plume and maps of probabilities of false-negative decision (left) and false-positive decision (right) are
also presented.

plume can have a non-zero probability of a false-negative decision, because the decision is
based on the adjusted plume and not on the reference plume (Section 2.2). Points outside the
plume can have a non-zero probability of a false-positive decision for the same reason.

3.4. Comparison of scenarios
Figure 6 shows the optimized sampling designs for the low, medium, and high uncertainty
scenarios, where the cost ratio equals 1/1 as before. Not surprisingly, the figure shows that
the optimal design spreads out when uncertainty increases. This is due to the increasing
occurrence of radioactive contamination above the intervention level at large distances from
the reference plume. In the high uncertainty case, the optimal design resembles a uniform
distribution of points over the study area, excluding the North Sea and the centre of the
plume. Note that the probability of making false decisions is smaller near the observation
points.

Downloaded by [New York University] at 15:34 27 May 2015 376

G.B.M. Heuvelink et al.

False-negative

False-positive

Cost ratio α/(1–α) = 1/1

l
l
i

s
w
o
L

l
l
i

s
m
u
d
e
M

i

l
l
i

s

h
g
H

i

Probability of false classification

Sample location

0

0.25

0.5

Threshold NPK-PUFF plume

Figure 6. Optimal sampling designs for the low (top), medium (middle), and high (bottom) uncer-
tainty case, case of cost ratio equal to 1/1.

Figure 7 presents the same results as Figure 6, but then for the scenarios where the cost
ratio equals 1/5. Recall that these are scenarios where the cost of a false-negative decision is
five times greater than that of a false-positive decision. At first impression, the optimal

Downloaded by [New York University] at 15:34 27 May 2015 International Journal of Geographical Information Science

377

False-negative

False-positive

Cost ratio α/(1–α) = 1/5

l
l
i

s
 
w
o
L

l
l
i

s
m
u
d
e
M

i

l
l
i

s

h
g
H

i

Probability of false classification

Sample location

0

0.25

0.5

Threshold NPK-PUFF plume

Figure 7. Optimal sampling designs for the low (top), medium (middle), and high (bottom) uncer-
tainty case, case of cost ratio equal to 1/5.

sampling designs are not very different from those presented in Figure 6. However, closer
inspection shows that there is an increased tendency to locate observations outside the
reference plume when a false-negative decision is more costly than a false-positive decision.
For instance, for scenario 1a (low uncertainty case and cost ratio 1/1), 10 points out of 25 are

Downloaded by [New York University] at 15:34 27 May 2015 378

G.B.M. Heuvelink et al.

Table 2. Summary of expected cost for initial and final sampling designs for all scenarios.

Initial design

Final design

False-
negative
costs

False-
positive
costs

Total
expected
costs

False-
negative
costs

False-
positive
costs

Total
expected
costs

Cost
improvement
(%)

Scenario

1a
1b
2a
2b
3a
3b

251
418
2311
3852
5251
8751

205
68
518
173
1451
484

455
486
2829
4024
6702
9235

143
223
2023
3148
4172
6448

106
42
443
216
1619
702

249
265
2466
3365
5791
7150

45
45
13
16
14
23

located within the plume, whereas only five points are inside the plume for scenario 1b
(when the cost ratio is 1/5). When false-negative decisions are more costly than false-
positive decisions, it makes sense to focus on reduction of the probability of false-negative
decisions and hence place more observations outside the reference plume.

Table 2 presents the expected costs for all scenarios, both for the initial and the final
sampling design. Clearly, the expected cost increases when uncertainty increases, because
the probability of making a false decision is greater when the uncertainty is large. The cost
improvement is large for the low uncertainty scenarios, whereas it is only modest for the
medium and high uncertainty scenarios. Indeed for the low uncertainty scenario, the final
sampling design is quite different from the initial design, whereas for the other two
cases the difference is smaller (the initial random design already has a fair spatial
coverage of the study area). Note also that the expected cost for scenarios where the
cost ratio equals 1/5 is greater than when the ratio equals 1/1. This occurs because the
probability of making a false-negative decision is generally greater than that of making
a false-positive decision (Figures 6 and 7).

4. Discussion and conclusions

This study presented a sampling design optimization methodology that aims to minimize the
expected area of false predictions. The procedure employs a stochastic simulation approach
to simulate possible realities using a geostatistical model and uses SSA for optimization of
the sampling design. The computational effort is large because each simulated annealing
step requires that a kriging interpolation is carried out N times, where N equals the number of
Monte Carlo simulation steps. In this study, N was equal to 100, whereas the number of
simulated annealing steps was 2000. Both numbers need to be increased to reach more
precise results although the results obtained were plausible and suggest that these are not that
far off from the exact solution. Additional runs are needed to confirm this. In fact, this was
already partially done because we executed the algorithm many times in the testing phase,
each time with a different initial design, and obtained very similar final designs.

In the current setting, the algorithm required about 20 hours computing time on a
standard personal computer. Clearly, for real-world emergency applications, solutions are
needed much faster than that. For instance, in an operational mode, the RIVM must decide in
less than half an hour where mobile devices must be placed. However, the implementation of
the computer code can be made much more efficient by clever programming, and a more
powerful computer can be used. For instance, the initial sampling design can be taken from a
previous analysis or guessed by an expert such that it is likely to be closer to the optimum

Downloaded by [New York University] at 15:34 27 May 2015 International Journal of Geographical Information Science

379

than a random design. Also, the kriging step can be speeded up by taking into account that
for each iteration step only one sampling location is replaced, meaning that only a single row
and column of the kriging matrix change. This can be exploited when solving the kriging
system with a block Gaussian elimination algorithm (Golub and Van Loan 1996). The
algorithm is also suitable for parallel computing approaches, meaning that the computation
time can potentially be reduced by several orders of magnitude.

The spatial distribution of the optimal sampling design varied greatly between the
uncertainty scenarios. In the low uncertainty case, all observations were placed in the
immediate vicinity of the plume boundary, because this was the only area where it was not
known whether the contamination was above or below the intervention level. The observa-
tions were mainly placed along that part of the plume boundary where it is the widest,
because this is where most gain can be achieved. An interesting result was also that when
false-negative decisions are more costly than false-positive decisions, observations tend to
move outside the reference plume. The movement leads to a further reduction of the
probability of false-negative decisions, which are predominantly made just outside the
plume.

Results were less appealing for the medium and high uncertainty case. Here, the
optimal designs were more or less uniformly distributed across the study area, excluding
only the inside of the reference plume and parts of the study area where no observations
could be placed (e.g. the North Sea). However, it is not sensible to place observations at
locations that are in the opposite wind direction from the release site. Apparently, for these
scenarios the variance of the residual " in Equation (1) was so large that concentrations
above the intervention level were simulated at locations where NPK-PUFF predicts a zero
concentration. This is indeed the case, because for the medium uncertainty case the
standard deviation of " is half the intervention level, whereas for the high uncertainty
case it is equal to the intervention level. Since " was assumed normally distributed, the
simulated value of " will be greater than the intervention level in about 2 out of 100 cases
for the medium uncertainty case and for 16 out of 100 cases for the high uncertainty case.
It then becomes rewarding to place mobile devices far away from the plume, in all
directions. Thus, the optimized designs for the medium and high uncertainty case are
not wrong but a logical consequence from the assumptions made. In fact, they pinpoint to
weaknesses in the error model used.

The variance of " was estimated by visual inspection of multiple semivariograms
computed from differences between the reference NPK-PUFF run and alternative runs
where some of the major NPK-PUFF inputs had been modified. This is a sensible approach,
but the assumption of a stationary error field " that was also made is not realistic. Clearly,
differences between NPK-PUFF runs will be larger near the plume, and it is therefore much
more realistic to also let the variance of the residual " be larger near the reference plume. This
is important because the sensitivity of the optimized sampling design to the spatial distribu-
tion of the magnitude of the NPK-PUFF error is large. In future work, more elaborate
representations of the error associated with the model output should therefore be used. For
instance, one possibility is to define realistic probability distributions characterizing the
uncertainty about the inputs to the NPK-PUFF model (e.g. wind speed and wind direction),
propagate these through the NPK-PUFF model (Heuvelink 1998), and use the resulting
outputs as possible realities, possibly augmented with a moderate model error represented by
an additive residual random function. This approach will likely yield more realistic results,
particularly for the medium and high uncertainty cases, but it will increase the computational
complexity dramatically because it requires that NPK-PUFF is run many times (several
hundreds at least).

Downloaded by [New York University] at 15:34 27 May 2015 380

G.B.M. Heuvelink et al.

Other improvements to this work can also be identified. First, in this work a single time
instance was chosen (i.e. five hours after the accident took place) for which the weighted
expected area of false-positive and false-negative decisions was minimized. In practice, the
intervention level is based on time-integrated values of radionuclide concentration, because
this reflects the radioactive dose received by the population. A logical extension would
therefore be to integrate the cost criterion over time so that a design is obtained, which
minimizes the expected areas of false decisions for dose values instead of concentrations.
Integrating the criterion over time is not fundamentally more difficult but it will increase
computing time and requires that the temporal correlation of the stochastic residual is
modelled as well. It will also not be difficult to further improve the criterion by introducing
weights proportional to the population density and/or agricultural use of land.

Second, in this work it was assumed that only mobile measurements are available to
adjust the reference NPK-PUFF plume. In reality, measurements of the static network (i.e.
the 153 devices that are more or less uniformly distributed over the Netherlands) are also
available and should be used. The optimization algorithm can easily cope with this and will
likely avoid placing mobile devices near static devices, to avoid redundant information. In
addition, the number of mobile devices required can also be optimized. In this study, a fixed
number of 25 devices was used, but the optimization algorithm can be adapted such that it
yields the minimum number of mobile devices required to reach a specified level of
maximum acceptable costs.

Third, in the case where false-negative decisions are more costly than false-positive
decisions, it may be sensible to take a risk-aversive approach and classify a location as
positive even when the probability of the concentration being above the intervention level is
smaller than 50%. This will decrease the expected area of false-negative decisions at the
expense of an increased expected area of false-positive decisions. Overall, this may pay off
when the cost of a false-negative decision is greater than that of a false-positive decision. In
fact, it is not difficult to deduce from Equation (2) that the smallest expected cost is obtained
when the threshold probability is set equal to (cid:2).

These extensions to the basic algorithm are all feasible and will bring the methodology
closer to an operational tool to be used by the RIVM. However, in spite of the simplifications
made, the methodology and results presented in this article are valuable in themselves and
have provided much insight to the RIVM. Moreover, the generic approach that was taken
means that the sampling design optimization methodology is not restricted to radioactivity
monitoring studies but can be applied more widely in the GI sciences.

Acknowledgements
This research was partially funded by the Dutch Research Programme ‘Space for Geo-Information’
(RGI), projects RGI-182 and RGI-302, and by the European Commission, under the Sixth Framework
Programme, by the contract no. 033811 with the DG INFSO, action Line IST-2005-2.5.12 ICT for
Environmental Risk Management. We thank Stephanie Melles and anonymous referees for valuable
comments.

References
Ben-Jemaa, F., Marino, M., and Loaiciga, H., 1995. Sampling design for contaminant distribution in

lake sediments. Journal of Water Resource Planning Management, 121, 71–79.

Bertino, L., Evensen, G., and Wackernagel, H., 2003. Sequential data assimilation techniques in

oceanography. International Statistical Review, 71, 223–241.

Downloaded by [New York University] at 15:34 27 May 2015 International Journal of Geographical Information Science

381

Brown, J.D. and Heuvelink, G.B.M., 2007. The Data Uncertainty Engine (DUE): a software tool for
assessing and simulating uncertain environmental variables. Computers & Geosciences, 33,
172–190.

Brus, D.J. and Heuvelink, G.B.M., 2007. Optimization of sample patterns for universal kriging of

environmental variables. Geoderma, 138, 86–95.

Chile`s, J.-P. and Delfiner, P., 1999. Geostatistics: modeling spatial uncertainty. New York: Wiley.
Coello, C.A.C., Pulido, G.T., and Lechuga, M.S., 2004. Handling multiple objectives with particle

swarm optimization. IEEE Transactions on Evolutionary Computation, 8, 256–279.

De Gruijter, J.J., et al., 2006. Sampling for natural resource monitoring. Berlin: Springer.
Eleveld, H., Kok, Y.S., and Twenho¨fel, C.J.W., 2007. Data assimilation, sensitivity and uncertainty
analyses in the Dutch nuclear emergency management system: a pilot study. International Journal
of Emergency Management, 4, 551–563.

Glover, F., 1989. Tabu search – part I. ORSA Journal of Computing, 1, 190–206.
Golub, G.H. and Van Loan, C.F., 1996. Matrix computations. Baltimore: John Hopkins.
Goovaerts, P., 1997. Geostatistics for natural resources evaluation. New York: Oxford University Press.
Goovaerts, P., Webster, R., and Dubois, J.P., 1997. Assessing the risk of soil contamination in the Swiss

Jura using indicator geostatistics. Environmental and Ecological Statistics, 4, 31–48.

Heuvelink, G.B.M., 1998. Error propagation in environmental modelling with GIS. London: Taylor &

Francis.

124–137.

Heuvelink, G.B.M., Brown, J.D., and Van Loon, E.E., 2007. A probabilistic framework for represent-
ing and simulating uncertain environmental variables. International Journal of GIS, 21, 497–513.
Heuvelink, G.B.M., et al., 2006. Space-time Kalman filtering of soil redistribution. Geoderma, 133,

Hornik, K., Stinchcombe, M., and White, H., 1989. Multilayer feedforward networks are universal

approximators. Neural Networks, 2, 359–366.

Huang, H.C. and Cressie, N., 1996. Spatio-temporal prediction of snow water equivalent using the

Kalman filter. Computational Statistics & Data Analysis, 22, 159–175.

Kirkpatrick, S., 1984. Optimization by simulated annealing: quantitative studies. Journal of Statistical

Kirkpatrick, S., Gerlatt, C.D., and Vecchi, M.P., 1983. Optimization by simulated annealing. Science,

Kumar, N., 2007. Spatial sampling design for a demographic health survey. Population Research and

Lee, Y.M. and Ellis, J.H., 1996. Comparison of algorithms for nonlinear integer optimization.
Application to monitoring network design. Journal of Environmental Engineering–ASCE, 122,
524–531.

McBratney, A.B. and Webster, R., 1981. The design of optimal sampling schemes for local estimation
and mapping of regionalized variables. II. Program and examples. Computers & Geosciences, 7,
335–365.

McBratney, A.B., Webster, R., and Burgess, T.M., 1981. The design of optimal sampling schemes for
local estimation and mapping of regionalized variables.I. Theory and method. Computers &
Geosciences, 7, 331–334.

McCarthy, M.P., 2008. Spatial sampling requirements for monitoring upper-air climate change with

radiosondes. International Journal of Climatology, 28, 985–993.

Rogerson, P.A., et al., 2004. Optimal sampling design for variables with varying spatial importance.

Geographical Analysis, 36, 177–194.

Schmitt, L.M., 2001. Theory of genetic algorithms. Theoretical Computer Science, 259, 1–61.
Spruill, T.B. and Candela, L., 1990. Two approaches to design of monitoring networks. Ground Water,

28, 430–442.

Stehman, S.V., Sohl, T.L., and Loveland, T.R., 2005. An evaluation of sampling strategies to improve
precision of estimates of gross change in land use and land cover. International Journal of Remote
Sensing, 26, 4941–4957.

Stevens, D.L. and Olsen, A.R., 2004. Spatially balanced sampling of natural resources. Journal of the

American Statistical Association, 99, 262–278.

Ter Braak, C.J., Brus, D.J., and Pebesma, E.J., 2008. Comparing sampling patterns for kriging
the spatial mean temporal trend. Journal of Agricultural and Environmental Statistics, 13,
159–176.

Physics, 34, 975–986.

220, 671–680.

Policy Review, 26, 581–599.

Downloaded by [New York University] at 15:34 27 May 2015 382

G.B.M. Heuvelink et al.

Theobald, D.M., et al., 2007. Using GIS to generate spatially balanced random survey designs for

natural resource applications. Environmental Management, 40, 134–146.

Twenho¨fel, C.J.W., et al., 2005. Operation of the Dutch 3rd generation national radioactivity monitor-

ing network. Spatial interpolation comparison 2004 report, 19–34 (EUR 21595 EN).

Van Egmond, N.D. and Kesseboom, H., 1983. Mesoscale air pollution dispersion models.
II. Lagrangian PUFF-model and comparison with Eulerian GRID model. Atmospheric
Environment, 17, 265–274.

Van Groenigen, J.W., Siderius, W., and Stein, A., 1999. Constrained optimization of soil sampling for

minimization of the kriging variance. Geoderma, 87, 239–259.

Van Groenigen, J.W. and Stein, A., 1998. Constrained optimization of spatial sampling using

continuous simulated annealing. Journal of Environmental Quality, 27, 1078–1086.

Van Meirvenne, M. and Goovaerts, P., 2001. Evaluating the probability of exceeding a site-specific soil

cadmium contamination threshold. Geoderma, 102, 75–100.

Van Sonderen, J.F., 1997. Monitoring strategy in support of radiological emergency management.

Radiation Protection Dosimetry, 73, 115–118.

Verver, G.H.L. and De Leeuw, F.A.A.M., 1992. An operational puff dispersion model. Atmospheric

Environment, 26, 3179–3193.

Vrugt, J.A. and Robinson, B.A., 2007. Improved evolutionary optimization from genetically adaptive
multimethod search. Proceedings of the National Academy of Sciences of the United States of
America, 104, 708–711.

Wang, J., et al., 2002. Spatial sampling design for monitoring the area of cultivated land. International

Journal of Remote Sensing, 23, 263–284.

Yfantis, E.A., Flatman, G.T., and Behar, J.V., 1987. Efficiency of kriging estimation for square,

triangular, and hexagonal grids. Mathematical Geology, 19, 183–205.

Zhu, Z. and Stein, M.L., 2006. Spatial sampling design for prediction with estimated parameters.

Journal of Agricultural Biological and Environmental Statistics, 11, 24–44.

Downloaded by [New York University] at 15:34 27 May 2015 