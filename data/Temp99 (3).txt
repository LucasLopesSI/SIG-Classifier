DOI: 10.1111/tgis.12557  

R E S E A R C H   A R T I C L E

Spatially constrained regionalization with 
multilayer perceptron

Michael Govorov1 ‚ÄÉ|   Giedrƒó Beconytƒó2 ‚ÄÉ|   Gennady Gienko3‚ÄÉ|   
Viktor Putrenko4

1Department of Geography, Vancouver 
Island University, Nanaimo, BC, Canada

2 Institute of Geosciences, Vilnius 
University, Vilnius, Lithuania

3Department of Geomatics, University of 
Alaska Anchorage, Anchorage, Alaska

4World Data Center for Geoinformatics 
and Sustainable Development, National 
Technical University of Ukraine, Kiev, 
Ukraine

Correspondence
Michael Govorov, Vancouver Island 
University, 900 Fifth Street, Nanaimo V9R 
5S5, BC, Canada.
Email: govorovm@viu.ca

Abstract

In this article, multilayer perceptron (MLP) network models 

with spatial constraints are proposed for regionalization of 

geostatistical point data based on multivariate homogeneity 

measures. The study focuses on non‚Äêstationarity and auto‚Äê

correlation in spatial data. Supervised MLP machine learning 

algorithms with spatial constraints have been implemented 

and tested on a point dataset. MLP spatially weighted classi‚Äê

fication models and an MLP contiguity‚Äêconstrained classifi‚Äê

cation model are developed to conduct spatially constrained 

regionalization.  The  proposed  methods  have  been  tested 

with an attribute‚Äêrich point dataset of geological surveys in 

Ukraine.  The  experiments  show  that  consideration  of  the 

spatial effects, such as the use of spatial attributes and their 

respective whitening, improve the output of regionalization. 

It is also shown that spatial sorting used to preserve spatial 

contiguity leads to improved regionalization performance.

1‚ÄÉ| ‚ÄÉI NTRO D U C TI O N

Regionalization  is  a  spatially  constrained  adjacency  classification  problem  (Johnston,  1970).  Regionalization  is 

applied in different fields. In this study, regionalization is the classification of spatial objects (points, lines, poly‚Äê

gons, voxels, or pixels) into a smaller number of geographic regions based on their characteristics and the set of 

contiguity  or  adjacency  constraints.  Different  spatial  characteristics  such  as  location,  dimension,  distribution, 

topological dependences (including connectivity), and relative density can be used as the criteria for regionaliza‚Äê

tion.  Unsupervised  clustering  and  supervised  classification  are  the  most  common  strategies  of  regionalization. 

1048 ‚ÄÉ|‚ÄÉ  

wileyonlinelibrary.com/journal/tgis 

Transactions in GIS. 2019;23:1048‚Äì1077.

¬© 2019 John Wiley & Sons Ltd‚ÄÉ‚ÄÇ ‚ÄÉ| ‚ÄÉ1049

Interest in the methods that employ machine learning remains strong, and new methods of regionalization have 

been  proposed  in  recent  years  (Chavent,  Kuentz‚ÄêSimonet,  Labenne,  &  Saracco,  2017;  Guo  &  Wang,  2011;  He, 

Ling, Zhang, & Gong, 2018; Miele, Picard, & Dray, 2014). Most machine learning methods belong to the group of 

unsupervised clustering methods that use unlabeled data with spatial constraints (Guo, 2008; Oliver & Webster, 

1989; Openshaw, 1977).

The  multilayer  perceptron  (MLP)  is  a  supervised  feedforward  artificial  neural  network  (ANN)  method.  The 

ANN‚Äîand particularly MLP‚Äîregained the attention of researchers due to the successes of deep learning. During 

the last decades, ANNs have been used extensively without modification in a wide range of environmental and so‚Äê

cial applications that use regionalized variables. MLP networks use nonlinear universal approximation algorithms 

that  capture  the  functional  relationships  using  observations  as  input  and  labeled  variables  as  output.  As  most 

regionalization methods seek approximated near‚Äêoptimal solutions (Anselin & Rey, 2014; Guo & Wang, 2011), the 

MLP can be a suitable approach.

Multiple applications of MLP for classification in the geoscience domain (Almhdi, Valigi, Gulbinas, Westphal, & 

Reuter, 2007; Lee, An, Yu, & Oh, 2014; Maier, Jain, Dandy, & Sudheer, 2010; Silva, Horta, Leal, & Oliveira, 2017) 

use the conventional MLP algorithms without taking into account the spatial effects (autocorrelation or heteroge‚Äê

neity) and spatial contiguity. Concerning the spatial aspect, there are some studies of the MLP models regarding 

its resistance to spatial autocorrelation (Santibanez, Kloft, & Lakes, 2015) and heterogeneity (de Bollivier, Dubois, 

Maignanb,  &  Kanevski,  1997;  Gilardi  &  Bengio,  2000)  for  regression  tasks.  Some  models  use  MLP  and  kriging 

methods in combination to improve the interpolation of the numeric variables (Jia, Zhou, Su, Yi, & Wang, 2018; 

Kanevski, 2008). Our study contributes to the wider use of spatially enhanced MLP methods for classification.

In  this  article,  we  introduce  the  enhanced  models  of  multivariate  regionalization  using  MLP  networks.  We 

implemented the supervised MLP algorithms to define homogeneous regions using multiple environmental vari‚Äê

ables and a labeled training dataset. The improved regionalization models were tested on the regionalization of 

point observation data (geostatistical data). Regionalization has been validated using a multivariate homogeneity 

measure. Our approach increases the efficiency of spatially constrained regionalization with MLP and improves 

the regionalization outcome due to consideration of spatial effects inherent in the data by:

1.  adding  spatial  weights  to  the  MLP  input  neurons;

2.  whitening input data while taking into consideration spatial weights;

3.  applying contiguity constraints via ordering the observation points by local neighborhoods and using them for 

MLP mini‚Äêbatch modeling; and

4.  adding spatial weights to the MLP output neurons by means of post‚Äêclassification smoothing.

The proposed methods have been tested on a point dataset derived from the results of the geological surveys in 

Ukraine carried out by the State Enterprise ‚ÄúKirovgeologija‚Äù (2004). The survey studied the concentration of natural 

uranium (a mixture of three isotopes, uranium‚Äê234, uranium‚Äê235, and uranium‚Äê238) in the groundwater in Ukraine. 

Some  areas  in  Ukraine  have  high  concentrations  of  natural  uranium,  therefore  the  groundwater  in  these  areas  is 

potentially unsafe for drinking. Concentrations of 0.08 mg/L and higher are potentially dangerous to human health. 

Therefore, it is important to understand the distribution of uranium and its possible impact on the quality of drinking 

water. The survey database consists of 24 variables (environmental indicators such as mineralization, precipitation, 

relief, and others) collected at 6,546 sampling points throughout Ukraine. The regionalization of Ukraine by the con‚Äê

centration of uranium in the groundwater enables us to determine which regions do not meet quality standards, and 

thus better control the problem. Regionalization of uranium in groundwater can also be applicable to searching for 

mineral uranium deposits, as it identifies areas with similar geological and environmental conditions.

The  remainder  of  this  article  is  organized  as  follows.  In  Section  2,  a  standard  MLP  neural  network  model 

is described in detail. Then, background and related works in constrained spatially based clustering and use of 

GOVOROV et al.1050‚ÄÉ |‚ÄÉ ‚ÄÉ‚ÄÇ

ANN (including MLP) for spatial regression tasks are reviewed. In Section 3, the methods of spatially constrained 

regionalization with MLP networks are proposed and discussed, followed by a description of the case study in 

Section 4. The final discussion and conclusions are provided in Section 5.

2‚ÄÉ| ‚ÄÉBAC KG RO U N D A N D R E L ATE D  WO R K S

2.1‚ÄÉ|‚ÄÉMLP neural network model

The single layer perceptron (SLP) learning model, invented in 1957, was developed out of a simpler binary linear 

classifier, which consisted of two inputs and one output layer of neurons (Rosenblatt, 1958). Later, in the 1960s, 

the SLP model was extended to the more successful MLP model by the addition of at least one hidden layer be‚Äê

tween the input and the output layers. MLP‚Äêrelated research experienced a resurgence in the 1980s. MLPs try 

to simulate the nonlinear signal propagation process in human neurons. An MLP consists of generic units called 

neurons form layers. The layers are capable of propagating data and modifying themselves. An MLP is a type of 

feedforward ANN where the outputs are deterministic functions of the inputs. It contains one or more intermedi‚Äê

ate hidden layers of generic elements, or perceptrons, that separate the input and output layers (Bishop, 2006; 

Goodfellow, Bengio, & Courville, 2016; Haykin, 2008; Ripley, 2012).

MLP can be used for regression and classification tasks. An MLP for classification tasks can be viewed as a 
logistic regression classifier where the input vector of variables X(m) is first transformed into one or more hidden 
layers  i  using one or more learnt nonlinear transformationsùõæii. Each transformation with an activation function ùõæi 
projects the input data into a space where it becomes linearly separable. Intermediate data from the last hidden 
layer is then transformed in the output layer of the binary vector of variables Y(m) using a learnt nonlinear transfor‚Äê
mation ŒìI, which can be different from the s activation function used for transformations of intermediate layers. 
Sometimes  a  single  hidden  layer  is  sufficient  to  make  an  MLP  an  efficient  approximator.  However,  using  many 

hidden layers may bring benefits, as is the case in deep learning (Goodfellow et al., 2016; Hinton, Osindero, & Teh, 

2006).

The MLP network model consists of the following layers.
The input layer is a set of J0 = P units, a0,1, ..., a0,J0, with a0,1 = xj, where:

‚Ä¢  Ji is the number of perceptrons/neurons in layer i , for example for the input layer J0 = P and for the output layer 

JI = R without counting the bias terms;

‚Ä¢  I is the number of layers without counting the input layer;
‚Ä¢  a(m)

i,j  is neuron  j of layer  i , with observations m;  j = 0, ..., Ji, i = 0, ..., I, and m = 1,... M, where M is the number of ob‚Äê
servations for each neuron;
, ..., x(m)
P
, ..., y(m)
R

 is a set of P input vector xj neurons with set of observations m for the input layer i = 0; and
)
 is a set of R output vector yj neurons with set of observations m for the output layer i = I.
)

x(m)
1
(
y(m)
1

‚Ä¢  X(m)
i=0
‚Ä¢  Y(m)
i=I

=

=

(

An ith hidden layer is a set of Ji neurons of ai,1, ..., ai:, with a(m)

i,k = ùõæi

s(m)
i,k

 and s(m)
)

i,k =

(

‚àë

Ji‚àí1
j=0

wi,j,ka(m)

i‚àí1,j, where ai‚àí1,0 = 1 and:

si,k

‚Ä¢  ùõæi
‚Ä¢  wi,j,k is the propagated synaptic weight from layer i ‚àí 1 to layer i  of neuron k. No weights connect a(m)

 is an activation or transformation function for layer i ;
)

i‚àí1,j to the bias 

(
a(m)
i,0 , so there is no wi,j,0 for any  j; and

‚Ä¢  the synaptic weight vector W contains all weights 
(

w1,0,1,w1,0,2, ‚Ä¶ ,wI,JI‚àí1,JI

.

)

The output layer is a set of JI = R neurons aI,1, ..., aI:JI, with a(m)

= ùõæI

I,k

s(m)
I:k

 and s(m)
I,k
)

(

=

‚àë

Ji‚àí1
j=0

wI,j,ka(m)

i‚àí1,j, where ai‚àí1,0 = 1.

GOVOROV et al.‚ÄÉ‚ÄÇ ‚ÄÉ| ‚ÄÉ1051

An MLP performs a series of functional transformations Œì that transform the input vector X(m)

i=0 of neurons with 
i=I  of neurons with size R. This feedforward propagation of information through the 

size P into the output vector Y(m)
network is shown in Figure 1.

An  MLP  can  employ  arbitrary  activation  or  transformation  functions.  The  choice  of  activation  function  is 
determined by the nature of the data and the assumed distribution of target variables Y(m)
i=I  (Haykin, 2008). Most 
commonly, feedforward MLPs are trained using an error backpropagation algorithm that changes the propagation 

weights  W.  Most  backpropagation  training  algorithms  comprise  an  iterative  procedure  for  minimization  of  an 

error loss function, with adjustments to the weights being made in two sequences (Ripley, 2012). The error flow 
is propagated backwards from output vector Y(m)
i=0 modifying the MLP weights according to 
the captured dependencies from data. In supervised learning, the output aI:1, ..., aI,R can be compared with target 
labeled values y1, ..., yR through a training error or loss function L (w,y,a). There are different options to match the 
output layer activation function and the corresponding error function. For multi‚Äêclass classification, the softmax 

i=I  to input vector  X(m)

activation function for the output layer with the corresponding multiclass cross‚Äêentropy error loss function should 

be used (Bishop, 2006; LISA Lab, 2015). Many algorithms have been proposed to train MLPs through error back‚Äê

propagation optimization adjustments, but the most commonly used algorithms are gradient‚Äêbased (Bishop, 2006; 

Haykin, 2008; Ripley, 2012).

Backpropagation training proceeds through at least one complete pass of the data record, and then the search 

can be stopped according to stopping criteria. Several early stopping criteria can be set and checked in the par‚Äê

ticular predefined order (Bengio, 2012; LeCun, Bottou, Orr, & M√ºller, 2012). The three main types of training are 

batch, online (or stochastic), and mini‚Äêbatch training. Different early stopping criteria can be applied in different 

orders respectively for batch, online, or mini‚Äêbatch training (Alpaydin, 2010; Haykin, 2008).

The batch training algorithm updates the synaptic weights W only after passing all training data records M. 

After each such update, the gradient is re‚Äêevaluated for the new weight vector and the process repeats until one 

of the stopping criteria is met. The error loss function is defined with respect to the training set. Each step requires 

that the entire training set is processed in order to evaluate the derivatives in the error function.

The stochastic (‚Äúonline‚Äù) training algorithm updates the synaptic weights one at a time after every single train‚Äê

ing data record. It continuously obtains a record and updates the weights until one of the stopping criteria is met. 

If all the records are used once and none of the stopping criteria are met, then the process continues by recycling 

the data records. Stochastic training is the preferred method for basic gradient descent backpropagation for sev‚Äê

eral  reasons.  However,  this  method  has  some  disadvantages,  for  example  some  acceleration  backpropagation 

techniques like conjugate gradient can operate only in batch learning (LeCun et al., 2012).

F I G U R E   1 ‚ÄÉFeedforward propagation perceptron network with two hidden layers

GOVOROV et al.1052‚ÄÉ |‚ÄÉ ‚ÄÉ‚ÄÇ

Mini‚Äêbatch training is a trade‚Äêoff between stochastic and batch gradient descent training. For highly redundant 

training sets, mini‚Äêbatch training is nearly always the best choice (LeCun et al., 2012). The mini‚Äêbatch training algo‚Äê

rithm divides the training data records into groups of approximately equal size, then updates the synaptic weights 

after processing each group. Mini‚Äêbatch training starts with a small batch that is used to calculate the model error 

and update the model coefficients. Then the batches increase in size as the training proceeds. Depending on the 

implementation, the gradient may be summed up over the mini‚Äêbatches or the average gradient calculated, thus 

the variance of the gradient is reduced. Mini‚Äêbatch learning smooths out some of the noise compared to stochastic 

gradient descent, but not all of it. This allows mini‚Äêbatch training to better discover the local minima of the cost 

function, which can be beneficial to prevent overtraining (LeCun et al., 2012).

The disadvantage of mini‚Äêbatch training compared to stochastic and batch training is that the mini‚Äêbatch size 
B is added as a network hyperparameter and it needs to be estimated. There are different recommendations on 
choosing a mini‚Äêbatch size. B may interact slightly with other hyperparameters. Masters and Luschi (2018) rec‚Äê

ommend a batch size of 32 or smaller. They state that ‚Äúthe presented results confirm that using small batch sizes 

achieves the best training stability and generalization performance, for a given computational cost, across a wide 

range of experiments. In all cases, the best results have been obtained with batch sizes of 32 or smaller, often as 
small as equals of 2 or 4.‚Äù Once B is selected, it can be fixed while the other hyperparameters can be optimized 

further (Bengio, 2012).

Many network hyperparameters are involved in the design of MLP networks (number of layers, number of 

hidden units for each layer, type of activation functions, etc.) and are used for the tuning of the chosen backprop‚Äê

agation algorithm (loss error function, learning rate, size of the mini‚Äêbatch, etc.). Some of the hyperparameters 

(e.g., the ‚Äúbest‚Äù number of hidden units) can be optimized, but some are not feasible for optimization and should 

be explored as both optimization and generalization performance problems. Over the last 40 years, various rules 

of  thumb  have  been  proposed  for  choosing  hyperparameters  in  a  neural  network  (Bengio,  2012;  LeCun  et  al., 

2012; Montavon, Orr, & M√ºller, 2012). There are also several search methods for expert or automatic optimiza‚Äê

tion of the hyperparameter space (Bergstra, Yamins, & Cox, 2013). We applied them to work with environmental 

autocorrelated variables.

In our research we paid specific attention to the validation of the regions in the supervised classification. There 

are at least three groups of criteria to judge various aspects of cluster validity: external, internal, and relative mea‚Äê

sures (Alpaydin, 2010; Halkidi, Batistakis, & Vazirgiannis, 2001):

1.  based  on  external  indices  that  measure  the  extent  to  which  the  cluster  labels  match  the  externally 

supplied  class  labels  (e.g.,  overall  accuracy  and  Kappa  coefficients  calculated  from  the  confusion  matrix 

between  the  cluster  labels  and  a  test  or  expert  set);

2.  based on internal indices that measure the goodness of a clustering structure without respect to external infor‚Äê

mation [e.g., sum of squared error (SSE), F measure, cross‚Äêentropy error, cluster cohesion, cluster separation, 

3.  based on relative indices that compare two different clustering results. External and internal indices can be used 

Silhouette coefficient]; and

for comparison.

The Silhouette coefficient is an internal index that reflects both cohesion and separation of clusters. It can be calcu‚Äê

lated for the individual points, individual clusters, and the entire set of clusters (Rousseeuw, 1987). For an individual 
point m, the Silhouette coefficient is calculated as:

Sil (m) = 1‚àía‚àïb, if a < b, or Sil (m) = b‚àïa ‚àí 1, if a ‚â• b, where

‚Ä¢  a is the average distance of m to the points in its cluster, and
‚Ä¢  b is the smallest average distance of m to all points in any other cluster, of which m is not a member.

GOVOROV et al.‚ÄÉ‚ÄÇ ‚ÄÉ| ‚ÄÉ1053

We used average Silhouette width AvgSil = 1
 as a measure of overall quality of the regionalization. The 
R
ÔøΩ
value of AvgSil ranges from ‚àí1 to +1, where a high value indicates that a strong clustering structure has been found 

mj
ÔøΩ

R
j=1 Sil

‚àë

and negative and low positive values specify that no substantial structure has been found (Kaufman and Rousseeuw, 

1990). Section 4 includes more information on the internal and relative indices used for this study.

The above discussed MLP models have been used in the geospatial sciences since the 1990s (Gopal, 2017). 

However, for the classification tasks, known MLP algorithms do not take into account the spatial effects, thus 

ignoring the fact that nearby units may be more strongly correlated than distant units. Spatial properties are con‚Äê

sidered in the other two classes of methods:

1.  Unsupervised  clustering,  where  the  spatial  effect  has  also  been  widely  studied  (discussed  in  Section  2.2); 

2.  MLP and radial‚Äêbasis function (RBF) (Haykin, 2008) networks for regression tasks. Studies on how MLP numeric 

results can be improved using kriging interpolation methods are reviewed in Section 2.3.

Experiences from these domains were used to develop the spatially constrained MLP classification models proposed 

and

in Section 3.

2.2‚ÄÉ|‚ÄÉConstrained spatial clustering

Constrained unsupervised clustering is a method for grouping similar units on a univariate or multivariate surface 

into clusters that satisfy additional requirements (e.g., delimiting homogeneous regions by grouping observation 

units into clusters that are adjacent in space or time) (Legendre, 1987). Many constrained unsupervised clustering 

solutions were developed as modifications of conventional clustering techniques, such as K‚Äêmeans or hierarchical 

agglomerating clustering.

There are four main spatially constrained clustering approaches: spatially weighted classification, contiguity‚Äê

constrained classification, post‚Äêclassification smoothing, and model‚Äêbased spatial clustering (Hu & Sung, 2005; 

Simbahan & Dobermann, 2006). There are several comprehensive reviews of regionalization methods that use 

constrained clustering (Basu, Davidson, & Wagstaff, 2008; Davidson & Basu, 2007; Duque, Ramos, & Surinach, 

2007).

Spatially  weighted  classification  is  the  addition  of  spatial  information  into  input  datasets,  followed  by  the 

derivation of clusters based on similarity of the attributes. This approach uses conventional clustering methods, 

with spatial information as the primary measure of similarity, so that the clusters tend to be spatially compact but 

not necessarily contiguous.

There are at least three ways to achieve contiguity in a spatially weighted classification.

1.  Using  geographic  coordinates  of  sample  units  as  an  additional  pair  of  classification  variables  and  assign‚Äê

ing  appropriate  weights  to  the  geographic  coordinates  (Berry,  1966;  Govorov  &  Malikov,  1986;  Webster 

&  Burrough,  1972).  This  allows  separating  the  samples  that  have  similar  attributive  information  but  are 

geographically  distant  from  one  another.

2.  Creating spatially explicit lagged variables from the original variables by using spatial weights (Anselin & Rey 

2014).  Spatial  weights  can  be  computed  based  on  the  distance  between  samples  to  create  distance  band 

weights and inverse distance function weights (e.g., inverse or negative exponential distance functions). Then 

spatial weights, which take into account the values observed at neighboring sample locations, can be applied 

to the original variables within distance bands or on k‚Äênearest neighbors. Spatially lagged variables can then be 

used to perform clustering.

3.  Applying some form of spatial weighting function to modify the separation distances between the sampling 

sites. This option of spatially weighted classification is performed in four stages. First, a similarity/dissimilarity 

GOVOROV et al.1054‚ÄÉ |‚ÄÉ ‚ÄÉ‚ÄÇ

matrix  is  computed  between  all  pairs  of  spatial  samples  from  the  original  data.  Then,  similarities/dissimilari‚Äê

ties  are  modified  spatially  by  a  function  of  geographical  separation  to  form  a  new  matrix.  This  measure  can 

be  weighted  as  a  function  of  the  geographic  separation  between  individual  samples  to  ensure  spatial  conti‚Äê

nuity  of  the  formed  clusters  (e.g.,  by  an  inverse  square  weighting  or  exponential  function),  or  by  incorpora‚Äê

tion of known autocorrelation among data from uni‚Äê or multivariate variograms into their spatial classification 

(Bourgault, Marcotte, & Legendre, 1992; Caeiro, Goovaerts, Painho, & Costa Caeiro, 2003; Oliver & Webster, 

1989;).  Principal  component  semivariograms  can  be  used  instead  of  variable‚Äêspecific  semivariograms.  In  the 

third stage, the principal coordinate analysis is used on the modified matrix to calculate principal coordinates. 

These  coordinates  are  then  used  on  the  fourth  stage  within  a  clustering  algorithm  to  perform  classification 

(Oliver & Webster, 1989).

The  second  approach  is  contiguity‚Äêconstrained  clustering.  This  approach  may  use  conventional  clustering  meth‚Äê

ods (e.g., hierarchical or partitioning) that are modified to explicitly enforce contiguity during the clustering process. 

Spatial contiguity can be achieved by creating cluster sets from neighbors (Ferligoj & Batagelj, 1982; Openshaw, 1977; 

Openshaw & Rao, 1995). The most recent contiguity‚Äêconstrained clustering algorithms perform bottom‚Äêup hierarchi‚Äê

cal clustering of neighbors not only in order to construct a spatially contiguous tree, but also to optimize the objective 

function to find the best cut of the contiguous tree. These algorithms include spatial kluster analysis by tree edge re‚Äê

moval (SKATER) (Assuncao, Neves, Camara, & Freitas, 2006) and dynamically constrained agglomerative clustering and 

partitioning (REDCAP) (Guo, 2008; Guo & Wang, 2011). Thus, unconstrained hierarchical agglomerative clustering 

algorithms have been adapted to take several contiguity constraints into account. Sample units are represented by 

a graph, and spatial contiguity algorithms prune the graph to get contiguous clusters. The advantage of graph‚Äêbased 

techniques is that they make spatial adjacency an essential part of the clustering procedure.

Post‚Äêclassification smoothing (or filtering) is the third constrained clustering approach, but it only considers 

spatial  relationships  among  objects  (Legendre,  Ellingsen,  Bjornbom,  &  Casgrain,  2002).  The  approach  is  widely 

used in the processing of remote sensing data (Atkinson & Naser, 2010; Li, Zang, Zhang, Li, & Wu, 2014). In the first 

stage, the dataset is classified into different groups using conventional clustering techniques (e.g., the k‚Äênearest 

neighbor, k‚ÄêNN method). In the second stage, adjacent members in the same group are combined into patches 

or filtered into homogenous regions. Basically, in the first stage spatial constraints are ‚Äúrelaxed,‚Äù and then in the 

second stage the optimized first‚Äêstage outcomes are modified to satisfy the spatial continuity requirements while 

maintaining the original partition as much as possible (He et al., 2018).

The fourth approach is to build a model that encompasses spatial information with constraints. Model‚Äêbased 

approaches include methods from machine learning, such as conceptual clustering (COBWEB) (Fisher, 1987), Cohen 

self‚Äêorganizing maps (SOM) (Kohonen, 2001), and the max‚ÄêP‚Äêregions model (Duque, Anselin, & Ray, 2012).

We applied the described approaches to incorporate the spatial information in the MLP network and tested 

them for regionalization of distribution of uranium on the surface and in the groundwater in Ukraine.

2.3‚ÄÉ|‚ÄÉUse of spatial constraints with MLP for regression tasks

The use of MLPs with local constraints in spatial interpolation analysis has been proposed by de Bollivier et al. 
(1997). One method was to incorporate spatial knowledge in an MLP by training a network based on local k‚Äênear‚Äê

est points found within the distance band. Further, to improve the accuracy of interpolation, it was proposed to 

train MLPs based on the local geostatistical results obtained from the inverse distance method (IDW) or kriging 

interpolation.

Another  interpolation  approach  with  MLPs  is  the  neural  network  residual  kriging  (NNRK)  model  (Kanevski, 

2008; Kanevski, Pozdnoukhov, & Timonin, 2008). A hybrid NNRK model uses both MLP and geostatistical meth‚Äê

ods in order to improve the interpolation of the numeric variables. In the first stage of NNRK, an MLP is used to 

model nonlinear global trends of input observations; as results, residuals are obtained at the MLP training points, 

GOVOROV et al.‚ÄÉ‚ÄÇ ‚ÄÉ| ‚ÄÉ1055

which are used further in the geostatistical analysis. Next, a kriging method is applied to interpolate the MLP's 

residuals. In the last step, predicted residuals from kriging are added to the MLP prediction global outputs. Thus, 

the MLP is used to model global trends, and kriging takes into account the spatial structures that have not been 

modeled by the MLP. NNRK models have been described in detail by Kanevski, Arutyunyan, Bolshov, Demyanov, 

and Maignan (1996).

interpolation.

Another  approach  was  used  to  improve  interpolation  accuracy  of  the  spatial  distribution  characteristics  of 

metal(loid)s in the regional soil. The authors combined two methods: kriging interpolation and a backpropagation 

neural  network  (Jia  et  al.,  2018).  They  used  ANN  first  to  densify  training  data  points  and  then  applied  kriging 

interpolation on training and densification data points. This method succeeded in increasing the accuracy of the 

Hu and Sung (2005) modified the layer's weight of conventional RBF networks to improve the accuracy of spa‚Äê

tial prediction (regression) tasks. They suggested four different models to fuse spatial information at three differ‚Äê

ent layers of RBF. The input layer's fusion model replaces each RBF network input with the weighted average of its 

neighbors. The input layer's fusion model was tried by Leung, Hennessey, and Drosopoul (2000) for regular lattice 

(polygons) data and Hu and Sung (2005) adapted it for irregular lattice data. The two models for a hidden layer's 

fusions were implemented by modifying weights from the hidden RBF layer into the output layer based on local 

autocorrelation values of outputs. The output fusion model replaced each RBF network output with the weighted 

output's average of its neighbors, similar to the post‚Äêprocessing smoothing. Experiments show that fusion at the 

hidden layer gives the best result (Hu and Sung, 2005).

3‚ÄÉ| ‚ÄÉM E TH O D O LO G Y: S PATI A L E N H A N C E M E NT O F M LP N E T WO R K  FO R 
R EG I O N A LIZ ATI O N

3.1‚ÄÉ|‚ÄÉSpatially weighted input layer MLP

In a feedforward backpropagation MLP, weights of hidden layers are learned from labels and cannot be changed 

based on spatial constraints like they were changed in an RBF network by Hu and Sung (2005). Activation func‚Äê

tions, the structure of the network, or both should be modified to be sensitive to spatial constraints, as is done 

for example in convolution neural networks (CNNs) (Goodfellow et al., 2016). In order to avoid modifications within 

hidden layers:

1.  spatial  information  can  be  fused  into  neurons  of  the  MLP's  input  layer,  or

2.  spatial filtering can be applied to categorical variables of the output layer.

We offer three different approaches to fusing spatial information into an input vector of neurons  X(m)
foresee data whitening.

i=0. All of them 

1.  Including  spatial  coordinates  B(m)
i=0

i=0  of  the  observation  samples  as  two  additional  attributes  of  the  input 
vector.  However,  it  is  difficult  to  determine  the  weights  of  the  input  variables  a  priori,  including  locational 

, L(m)

variables.  We  follow  the  general  recommendations  on  transformations  of  input  values  in  order  to  improve 

the  performance  of  the  backpropagation  MLP.

‚Ä¢  The input variables are centered by subtracting the mean so that the averages over the training set are close 
i=0;J is the mean 
i=0:j is the standard deviation of neuron j, is applied in order to have zero mean and a standard deviation 

to zero. To rescale dependent variables, the standardization 
and ùúé(m)
of 1.

i=0:j, where  ÃÑX(m)

X(m)
i=0:j
(

‚àí ÃÑX(m)
i=0:j

‚àïùúé(m)

)

GOVOROV et al.1056‚ÄÉ |‚ÄÉ ‚ÄÉ‚ÄÇ

‚Ä¢  The input variables are normalized to a standard deviation of 1 so that their covariances are about the same. 
)) proce‚Äê

X(m)
Prior to the standardization, the normalization 
i=0:j
(

‚àí mini=0:j X(m)
i=0:j
)
dures are applied to have the input values scaled between 0 and 1.

‚àí mini=0:j X(m)

maxi=0:j X(m)

i=0:j

i=0:j

)

(

‚àï

‚Ä¢  If possible, input variables are decorrelated (Bengio, 2012; LeCun et al., 2012).

 We proposed to include not only geographic or Cartesian coordinates B(m)
i=0
depict measures of spatial autocorrelation (e.g. local Moran I coefficients).

, L(m)

i=0, but also the spatial attributes that 

2. Applying principal components analysis (PCA) or factor analysis (FA) in order to remove the linear correlations 

between the input variables. This approach allows for simplifying the MLP model. The number of input neurons 

can be decreased by replacing input variables with factors whose eigenvalues exceed a specified value, or by 

using other extraction criteria (Bro & Smilde, 2014; Jolliffe, 2002). Numerous studies show that the accuracy 

rate of the classification can be improved by the use of principal component or factor scores as the input neuron 

values (or without any deterioration), even if a much simpler structure of MLP is used (Dureja, Singh, & Bhatti, 

2014; Oja, 1982).

The classical PCA of a covariance matrix assigns more weight to variables with larger variances. The variables with 

large weights tend to have larger loadings on the first component and smaller residual correlations than the variables 

with small weights. To avoid giving greater weight than appropriate to the variables with the largest variances, we 
standardized each variable by dividing it by its standard deviation (i.e. using z scores) before running the PCA as sug‚Äê

gested by Jolliffe and Cadima (2016).

3.   Treating non‚Äêspatial and spatial input variables differently. Non‚Äêspatial and spatial input variables describing 

the same observations are divided into two groups of variables. The weights of variables can be different in 

values for each group. We propose four options for dealing with non‚Äêspatial and spatial input variables in order 

to get spatially enabled variables for an MLP input layer.

‚Ä¢  Weighting variables or observations by means of weighted PCA (WPCA) (Bailey, 2012; Delchambre, 2015). The 

weights describing the known uncertainty of variables, observations, or both can be incorporated into the 
PCA. Different weights w(m)
i=0:j can be applied to each element/cell of the input data matrix, or a weight can 
be assigned to each variable to reflect its importance (Bailey, 2012; Delchambre, 2015). The set of weights 
can be stored in a diagonal matrix W(m)

i=0,j that can be symmetrical or asymmetrical.

For our purpose, non‚Äêspatial and spatial variables were assigned different weights. The weights were ad‚Äê

justed between the two groups iteratively in order to satisfy the contiguity constraints. There is no analyti‚Äê

cal solution for calculating the WPCA matrix if it is asymmetrical. We used an iterative method proposed by 
Gabriel and Zamir (1979). If the weights of both variables and observations vary, but are symmetric (wj:m = wm:j),  
then the analytic solution is still possible according to Koren and Carmel (2004).

‚Ä¢  Balancing the influence of each group of variables by means of multiple factor analysis (MFA) (Pag√®s, 2014). MFA 

allows for a different number of variables in different groups. It balances the influence of each group of vari‚Äê

ables, taking into consideration the relationships between the groups of variables. Integrated PCA scores 

are derived from the observations. Pag√®s (2014) proposed that a classical PCA is first performed separately 

on each group of variables. Each original spatial and non‚Äêspatial group is then standardized by dividing all 

their elements by the square root of the first eigenvalue obtained from the respective group's PCA:

, where ùúÜ(m)

1,j,t is the first eigenvalue of the tth group of variables.

X(m)
i=0,(t)
ùúÜ(m)
1,j,t

‚àö

GOVOROV et al.‚ÄÉ‚ÄÇ ‚ÄÉ| ‚ÄÉ1057

In our case of T = 2, the first group includes non‚Äêspatial attributes and the second group includes spatial 

attributes.  Standardized  matrices  have  a  first  singular  value  equal  to  1.  Then  the  standardized  datasets  are 

merged to form a unique matrix, and a global classical PCA is performed on this matrix. Finally, estimated global 

PCA scores are used as inputs for the MLP.

‚Ä¢  Calculating  spatially  lagged  variables  as  a  weighted  sum  of  the  values  observed  at  neighboring  locations. 

Spatially  lagged  variables  are  weighted  sums  of  the  values  observed  at  neighboring  locations.  The 

weights are based on distance measures (Anselin & Rey, 2014). In fact, the spatially lagged variables are 

M

i=0

‚àë

as 

m=1 w(m:k)

i=0 , where w(m:k)
x(m:k)

the outcomes of the weighted k‚ÄêNN algorithm for regression. Spatially lagged variables are calculated 
i=0  is the spatial weight calculated between the mth and kth observations by 
using a function of geographical separation. The separation function is derived from inverse distance 
weighting  functions  [e.g. w(m:k)
],  where dm:k  is  a  separation  distance  between  the mth  and kth 
i=0
observations, and p is a power constant; or from some kernel distance‚Äìdecay functions [e.g. Gaussian 
w(m:k)
], where hm is the bandwidth around the mth observation. Then the spa‚Äê
i=0
)

(dm:k‚àïhm)2
2

= 1
/

dp
m:k

tially lagged variables are normalized by rows and used as MLP input.

2ùúã
(

exp

1‚àï2

=

‚àí

(

)

‚Ä¢  Using  local  geographically  weighted  principal  components  (GWPC)  (Harris,  Brunsdon,  &  Charlton,  2011). 

A  localized  version  of  weighted  PCA,  geographically  weighted  PCA  (GWPCA),  is  performed  on  a  local 

neighborhood of each observation. The observation weights are generated using a kernel function (e.g., 

a bi‚Äêsquare function) as proposed by Harris et al. (2011). It is the simplest version of the WPCA, because 
the  weights  of  the  variables  are  equal.  The  weights w(m)
i=0  are  associated  with  non‚Äêspatial  attributes  of 
w(m)
each observation, and the weighted mean ùúá = 1
x(m) is subtracted from the data to center it. 
i=0
XTWX, where 
Then classical standard PCA is applied for analysis of the weighted covariance matrix 

w(m)
i=0 ‚àë

‚àë

X is the data matrix with variables in columns and M observations x(m) in rows, and W = diag

diagonal matrix of the weights. According to Harris et al. (2011), GWPCA takes into account a certain 

spatial heterogeneity and spatial autocorrelation.

1
w(m)
i=0

‚àë

w(m)
i=0

(

 is the 
)

A more complex process called spatially weighted principal component analysis (SWPCA) provides options to use 

not only the local spatial weights as proposed by Harris et al. (2011), but also the global spatial weights, which are the 

weights of individual features (Guo, Ahn, & Zhu, 2014). SWPCA is the implementation of WPCA with elements of 

supervised PCA (SPCA). SPCA is a classical PCA applied on trained selected features (Bair, Hastie, Paul, & Tibshirani, 
2006). The weights are either normalized by rows to have values scaled between 0 and 1 as w(m)
i=0:j, or 
i=0:j
normalized for all elements of the matrix as w(m)
‚àï
i=0:j

m=1 w(m)
i=0:j.
In the last two options that use kernel weight functions, bandwidths are estimated using distances hm or km 
nearest neighbors. For each observation, these controlling parameters are fixed (bandwidth distance or number 

m=1 w(m)

P
j=1

‚àë

‚àë

‚àë

‚àï

M

M

of neighbors) or adaptive (varying distance or number of neighbors). There are several methods of how bandwidth 

can be selected adaptively with hyperparameter optimization algorithms (Davies, Marshall, & Hazelton, 2017; Shi, 

2010). Fixed or adaptive parameters are estimated from the data through an optimization of loss/gain criteria (e.g. 

minimization of the size of the scores of those components corresponding to lower eigenvalues for local GWPCA), 

or through incremental search (e.g. finding a distance where the intensity of spatial clustering is maximum for the 
parameter's estimations of spatially lagged variables or finding a heuristically optimal number k of nearest neigh‚Äê

bors, based on minimization of RMSE using cross validation).

For embedding the spatial information into neurons of the MLP input layer, we have tested all these approaches.

3.2‚ÄÉ|‚ÄÉContiguity‚Äêconstrained MLP classification

In this study we used mini‚Äêbatches to train an MLP network in stochastic gradient descent propagations through 

the  networks'  mini‚Äêbatches. The issue was how to form these mini‚Äêbatches. We proposed using a space‚Äêfilling 

GOVOROV et al.1058‚ÄÉ |‚ÄÉ ‚ÄÉ‚ÄÇ

curve (SFC) for ordering the observation points by local neighborhoods, then splitting the ordered sequence into 

mini‚Äêbatches used for training an MLP network. Thus, the partition size p was used as the mini‚Äêbatch size B.

Mini‚Äêbatch training methods are explicitly dependent on the order of observations. Normally, the order should 

be randomized to minimize the order effects (LeCun et al., 2012). However, in the case of spatial autocorrela‚Äê

tion, the order of the points should be preserved in order to keep the nearest neighbors within the same mini‚Äê

batch. Thus, the two‚Äêdimensional dataset is ordered employing a space‚Äêfilling curve to account for spatial locality. 

Observation points that are close together in space are stored close to each other. The same observations will 

always appear in a mini‚Äêbatch together.

We propose the following methodological steps to execute the contiguity‚Äêconstrained MLP classification.

1.  Apply  an  SFC  to  map  the  multi‚Äêdimensional  space  of  randomly  distributed  point  observations  with  whitening  at‚Äê

tributes  into  one‚Äêdimensional  space.  An  SFC  starts  with  a  basic  curve  (a  set  of  segments)  on  an  n‚Äêdimensional 

square  grid  of  side  2  (can  be  a  non‚Äêsquare  grid).  Normally,  the  curve  visits  every  point  in  the  grid  exactly 

once  without  crossing  itself.  It  has  two  free  ends  that  may  be  joined  with  other  paths.  The  basic  curve  is 
said  to  be  of  level/order  1.  To  derive  a  curve  of  level  i,  each  vertex  of  the  basic  curve  is  replaced  by  a 
curve  of  level  i ‚àí 1,  which  may  be  appropriately  rotated  and  reflected  to  fit  the  higher‚Äêlevel  curve.  Formally, 

an  SFC  is  a  continuous  function  with  endpoints  whose  domain  is  the  unit  interval  (0,  1)  (Sagan,  1994).  An 

SFC  completely  fills  the  area  of  interest.  Space‚Äêfilling  curves  are  used  for  numerous  IT  and  GIS  applications, 

including  nearest‚Äêneighbor  spatial  queries  to  implement  efficient  search  indexing  (Chen  &  Chang,  2011).

Examples of SFCs are the Peano curve, Hilbert curve, and reflected binary gray‚Äêcode (RBG) curve. These curves 

have different capabilities to support data clustering. However, there is no total ordering that preserves spatial locality 

(Moon, Jagadish, Faloutsos, & Saltz, 2001; Xu & Tirthapura, 2014).

2.  Split  an  SFC  linear  array  of  ordered  point  observations  into  p  similarly  sized  partitions,  in  which  a  high  

percentage  of  pairs  are  the  geometrical  neighbors.  A  partition  corresponds  to  the  level  of  sub‚Äêsquares  of 

the  curve.

3.   Perform batch normalization of the input variables. Normalization significantly improves the training performance. 

Batch normalization (batchnorm) has become a standard procedure for training deep networks. The batchnorm 

uses the mean and variance of the mini‚Äêbatch to normalize the activations and takes the form 
‚éõ
‚éú
‚éú
‚éù

in a given mini‚Äêbatch of b observations, where ùõæ and ùõΩ are trainable parameters learned using the other model 
weights, and epsilon is a small constant (Ioffe & Szegedy, 2015).

‚àíùúñ ‚éû
‚éü
‚éü
‚é†

ùúé(b)
i=0:j
ÔøΩ

ÔøΩ

ÔøΩ

2

‚àóùõæ + ùõΩ 

X(b)
i=0:j

‚àíX

(b)

i‚àí0:j

The normalized activations corresponding to an input observation will depend on the other observations in the 

mini‚Äêbatch. The stochastic uncertainty of the batch statistics also acts as a regularizer that can improve the general‚Äê
ization. However, standard batchnorm is applicable to train a sufficiently large mini‚Äêbatch size (e.g. B = 32) (Wu & He, 
2018). For small mini‚Äêbatch sizes (e.g. B = 2), the specific normalization techniques can be used (e.g. moving averages, 

group normalization, or renormalization) (Ioffe, 2017; Wu & He, 2018).

4.    Train  the  MLP  network.  Spatially  autocorrelated  or  heterogeneous  data  are  not  independent  and  not  always 

similarly distributed. This means that each spatial variable can have a different probability distribution and all 

the variables can be mutually dependent. In the presence of spatial effects in a dataset, mini‚Äêbatch training with 

batchnorm on near‚Äêneighbor observations can be helpful in several ways: it adapts the input activations of a 

layer to the change in distributions or it shifts internal covariates of layer inputs; it regularizes the network model 

and reduces the dependence of gradients on the scale of initial values. The network training converges faster if 

its inputs are whitened (i.e., linearly transformed to have zero means and unit variances) (Ioffe & Szegedy, 2015). 

GOVOROV et al.By mini‚Äêbatch whitening on the local neighborhood, the inputs to each layer remove the spatial effects of the 

internal covariate shift to achieve the fixed distributions of the inputs. Some experimental results using SFC for 

mini‚Äêbatch training of an MLP network are demonstrated in the next section.

‚ÄÉ‚ÄÇ ‚ÄÉ| ‚ÄÉ1059

3.3‚ÄÉ|‚ÄÉSpatially weighted output's layer MLP

We suggested applying a spatial filter that is similar to post‚Äêclassification smoothing. The spatial filter is applied to the 

predicted values from the MLP output layer in order to smooth the results and further improve contiguity and compact‚Äê

ness of regions. There are many filter‚Äêbased post‚Äêprocessing techniques to fuse the outputs with the labels of the near‚Äê

est neighbors. The most commonly used post‚Äêclassification techniques include majority filter and other types of spatial 

filters (e.g., likelihood class filter), probability label relaxation, and cellular automata (Lu & Weng, 2007; Salah, 2017; Su, 

2016). We chose to test the majority filter with different neighborhood parameters on classification of the MLP outputs.

A more robust approach is based on the cluster k‚ÄêNN consistency measures: for any data object in a cluster, its 

k‚Äênearest neighbors should also be in the same cluster (Ding & He, 2004). Applied to the MLP output using a k‚ÄêNN 

consistency measure, this method can improve the accuracy of clustering.

In the following section, the proposed methodology is demonstrated on real‚Äêworld geostatistical point data. 

Figure 2 depicts the chain of methods applied for the case study presented next.

4‚ÄÉ| ‚ÄÉC A S E S T U DY:  D I S TR I B U TI O N  O F  N AT U R A L U R A N I U M I N U K R A I N E

4.1‚ÄÉ|‚ÄÉThe dataset

The geostatistical point dataset was derived from geological surveys in Ukraine carried out by State Enterprise 

‚ÄúKirovgeologija‚Äù (2004) (Makarenko, 2000). The data on concentration of uranium in groundwater in Ukraine have 

been collected because the radioactivity of natural waters associated with natural and anthropogenic factors is 

one of the main components of the ecological condition in Ukraine.

The database consists of 24 environmental variables collected at 6,546 sampling points in Ukraine. Figure 3 il‚Äê

lustrates the distribution of the sampling points and the relative amount of natural uranium represented using two 

different categorized visualization methods. The variables included in the dataset are the concentrations (mg/L) of 

U, SO4, Cl, NO3, Cu, Fe, Mg, Zn, NH4, NO2, PO4, Cr, HCO3, F, As, the mineralization of water (mg/L), the hardness 
of water (mg/L), precipitation (per year, in mm), elevation (m), slope (¬∞), average annual temperature (¬∞C), depth 

F I G U R E   2 ‚ÄÉApplication of spatially enhanced MLP for regionalization of distribution of uranium

GOVOROV et al.1060‚ÄÉ |‚ÄÉ ‚ÄÉ‚ÄÇ

 
a
t
a
D

 
.
)
t
h
g

i
r
(
 
l

a
v
r
e
t
n

i
 
l

 

a
c
i
r
t
e
m
o
e
g
d
n
a
 
)
r
e
t
n
e
c
(
 
e

l
i
t
n
u
q

i

 

 
:
a
t
a
d
d
e
z
i
r
o
g
e
t
a
c
 
d
n
a
 
)
t
f
e
l
(
 
s
t
n
o
p
y
e
v
r
u
s
 
l

 

i

a
c
i

l

g
o
o
e
G

 
.

i

e
n
a
r
k
U
n

 

i
 

i

m
u
n
a
r
u

 
l

 

a
r
u
t
a
n
 
f
o
n
o
i
t
u
b
i
r
t
s
i
D
‚ÄÉ
3

 

E
R
U
G

I
F

‚Äù
a

j
i

l

g
o
o
e
g
v
o
r
i
K
‚Äú
 
e
s
i
r
p
r
e
t
n
E
e
t
a
t
S
 
f
o
y
s
e
t
r
u
o
c

 

 

GOVOROV et al.TA B L E   1 ‚ÄÉThe factors and groups of the environmental indicators

Factor

Environmental variables

1

2

3

4

5

6

Mineralization and hardness of water

Metals dissolved in water

groundwater

Organic compounds in water

Geomorphological characteristics

Climatic conditions of territory and formation of surface and 

Climatic

Mineral compounds and satellite elements of uranium

Geochemical

‚ÄÉ‚ÄÇ ‚ÄÉ| ‚ÄÉ1061

Group

Mineralogical

Geochemical

Mineralogical

Geological/geomorphological

of  the  sampling  points  (m),  percentage  of  humus  in  the  soil,  and  the  volume  of  natural  groundwater  resources  
(l/s/km2). All these variables were measured in ratio or interval continuous scale. Some values of the environmen‚Äê

tal variables were missing: HCO3 (570 points), F (280 points), As (3 points). The points with missing values of these 
variables were excluded from MLP modeling.

This dataset has been explored in a previous study (Govorov, Putrenko, & Gienko, 2016), where several spatial 

statistical methods were employed to assess the impact of the environmental variables on the spatial distribution 

of uranium (U) in the groundwater in Ukraine. The methods were exploratory spatial data analysis (ESDA), global 

and local geographically weighted factor analysis based on PCA, global and local geographically weighted correla‚Äê

tion and regression (GWR) analysis based on ordinary least squares (OLS) regression, and autoregressive models 

of spatial econometrics (Govorov et al., 2016).

4.2‚ÄÉ|‚ÄÉExploratory spatial data analysis

ESDA resulted in several findings about the environmental variables. Some findings did not meet the criteria for 

normality,  some  were  significantly  correlated  (multi‚Äêcollinear),  and  some  showed  spatial  dependences.  Spatial 

autocorrelation methods were used to identify the patterns. Moran's I and Getis‚ÄìOrd analysis (Getis & Ord, 1992; 

Moran, 1950) revealed that the distribution of uranium can be described as highly clustered with statistical signifi‚Äê
cance. Moran's I index is 0.5475 (p value = 0.0) and observed general G = 0.00007 (p value = 0.0). This means that 

there is positive spatial autocorrelation. The fact that the data is spatially clustered had been taken into account 

by using spatial variables, spatial weighting, and mini‚Äêbatches in regression and classification models to eliminate 

the bias produced by spatial autocorrelation.

Global correlation analyses revealed significant associations between several of the environmental variables. 

The  highest  global  correlation  coefficients  of  uranium  concentration  were  obtained  with  the  following  param‚Äê

eters:  percentage  of  humus  (r  =  0.52),  temperature  (r  =  0.51),  precipitation  (r  =  ‚àí0.50),  and  volume  of  natural 

groundwater resources (r = ‚àí0.56).

Local  weighted  correlation  analysis  revealed  spatial  heterogeneity  of  the  data.  Local  weighted  correlations 

between  different  variables  form  complex  spatial  patterns  and  anomalies.  Thus,  the  values  of  the  correlation 

coefficients inherited high non‚Äêstationarity and had to be modeled using local methods. For example, the global 

coefficient of correlation shows a very low relationship between uranium and depth of the sampling points (only 

r  =  0.02),  but  the  local  coefficients  of  correlation  range  from  ‚àí0.71  to  0.42,  indicating  a  very  high  association 

between these two variables in some areas (Govorov et al., 2016). The presence of spatial effects in the measure‚Äê

ments required spatially receptive classification techniques.

Factor  analysis  was  applied  in  order  to  determine  the  impact  of  particular  variables  on  the  distribution  of 

uranium in Ukraine. Analysis of the original 24 environmental variables revealed six principal factors that shape 

the distribution of uranium. These six factors, whose eigenvalues exceed 1, explain nearly 74% of the variability 

GOVOROV et al.1062‚ÄÉ |‚ÄÉ ‚ÄÉ‚ÄÇ

 

i

e
n
a
r
k
U
n

 

i

i
 
s
e
c
n
v
o
r
p
 
c
i
n
e
g
o

l
l

 

a
t
e
m
e
h
t
 
d
n
a
 
)

4
0
0
2

 
,
a

j
i

l

g
o
o
e
g
v
o
r
i
K

(
 
)
t
f
e
l
(
 
e
n
a
r
k
U

i

 
r
e
t
a
w
d
n
u
o
r
g
n

 

i
 

y
t
i

v

i
t
c
a
o
d
a
r
 
l

i

a
r
u
t
a
n
 
f
o
 
s
l
e
v
e

l
 
t
n
e
r
e
f
f
i
d
h
t
i

 

i

w
 
s
n
o
g
e
r
 
x
S
‚ÄÉ
4

i

 

E
R
U
G

I
F

)

8
7
9
1

 
,

n
o
n
A

(
 
)
t
h
g

i
r
(

GOVOROV et al.‚ÄÉ‚ÄÇ ‚ÄÉ| ‚ÄÉ1063

in the original 24 variables. The Kaiser‚ÄìMeyer‚ÄìOlkin measure of sampling adequacy (= 0.803) and Bartlett's test 

(p values with a significance of 0.000) indicate that factor analysis has been useful. A description of the factors 

(Govorov et al., 2016) is presented in Table 1.

It was found that the concentration of uranium has a strong global and local correlation with eight environ‚Äê

mental variables: rain precipitation, percentage of humus, hardness of water, concentration of F, Fe, SO4, and As, 
and mineralization of water. These eight variables described nearly 42% of the overall variability of the dependent 

variable according to the linear OLS models. The use of local correlation improves the assessment of the relation‚Äê

ships between the environmental variables. A GWR model based on the main eight variables explained nearly 50% 

of the variability of the data. In further experiments a GWR model, which was built using six principal components, 

described nearly 56% of the variability. Autoregressive models yielded better results, with the spatial lag model 

(SLM) explaining nearly 61% of the variability of the dependent variable. Analysis of Moran's I indices on the resid‚Äê

uals of OLS, GWR, and SLM models showed their significant autocorrelation.

According  to  Anon  (1978),  Makarenko  (2000),  and  ‚ÄúKirovgeologija‚Äù  (2004),  allocation  of  natural  uranium  in 

Ukrainian groundwater can be divided into six regions (Figure 4, left). They were delineated based on the anal‚Äê

ysis of geological structures and metallogenic provinces (Figure 4, right). Ukrainian Carpathians, Volyn‚ÄêPodolsk, 

Eastern Ukrainian, and the Black Sea and Crimean provinces represent zones with low concentrations of uranium 

and natural radioactivity. High levels of uranium are observed in the Ukrainian Shield and Donetsk province. This 

partitioning is based entirely on the geology of each territory, and does not take into account the impact of many 

external factors, such as climatic and environmental components. When multiple variables are used for clustering, 

it is reasonable to evaluate the hypothesis of the natural number of groups (clusters) using quantitative measures. 

The effectiveness of grouping of the source data was evaluated using the Calinski‚ÄìHarabasz pseudo F statistics 

to measure within‚Äêgroup similarity and between‚Äêgroup differences. The analysis showed that the most effective 

grouping can be achieved with a minimum of four clusters. Five and six clusters also result in high effectiveness of 

grouping, especially with the ISODATA clustering algorithm.

MLP does not require linearity and normality. The clustered and multi‚Äêcollinear nature of the variables has 

been  taken  into  account  in  a  spatially  enabled  classification  process  with  MLP.  PCA  and  factor  analysis  have 

been applied for decorrelation/whitening. The spatial effect was taken into account using spatial variables, spatial 

weighting, and mini‚Äêbatches.

4.3‚ÄÉ|‚ÄÉOutcomes and validations

Different MLP models were tested in order to find the best MLP configuration for the regionalization using the 

Ukrainian dataset described above. Prior to regionalization, MLP training sites were identified within the six re‚Äê

gions described above. The sampling points within these training sites were labeled respectively. A feedforward 

MLP with backpropagation learning algorithm was conducted using 54.8% of the labeled dataset points for train‚Äê

ing, 28.4% for validation, and 16.8% for testing. The total number of labeled points provided by experts and used 

for training was 398. The validation sample is an independent set of data points used to track errors during the 

training to prevent overtraining. The test sample is a disjoint set of data points that were not used to build the 

model. This set was used for the assessment of the final MLP model.

In order to compare different MLP models used for regionalization, the experiments were carried out using the 

following MLP hyperparameters that are common for all the models used.

‚Ä¢  Softmax output‚Äôs activation function ùõæ

 for transformation to the output layer with correspond‚Äê

ing multiclass cross‚Äêentropy error function ET (w) =
a set of categorical outputs. Here Œîr is a set of sub‚Äêvectors of Y(m)
variable, and s is the number of categories of a variable.

‚àë

exp(sj)
M
m=1 Em (w), where Em (w) = ‚àí

r‚ààŒîc y(m)
i=I  containing one‚Äêof‚Äêc coded  rth categorical 

 and Œîc is 

log

‚àë

ÔøΩ

r

am
I:r
y(m)
r ÔøΩ

exp(sk)

=

sk
ÔøΩ

ÔøΩ

j‚ààŒîr

‚àë

GOVOROV et al.1064‚ÄÉ |‚ÄÉ ‚ÄÉ‚ÄÇ

‚Ä¢  Hyperbolic tangent activation function ùõæ (s) = tanh (s) = es‚àíe‚àís

es+e‚àís for transformation within hidden layers of neurons.

X(m)
‚Ä¢  The standardization method 
i=0:j
(

‚àí ÃÑX(m)
i=0:j

‚àïùúé(m)

)

i=0:j for rescaling continuous ordinal and interval variables.

‚Ä¢  Gradient descent optimization algorithm for estimation of the synaptic weights.

‚Ä¢  The estimation algorithm for automatic selection of the number of neurons in the hidden layer, and relaxed 

computational time.

Batch and mini‚Äêbatch training methods were used for training the MLP. We used internal indices of cluster validity as 

MLP minimization cost measures: cross‚Äêentropy error and percentage of incorrect predictions. The average Silhouette 
width coefficient AvgSil over all data was used as an internal measure for cluster validation.

A  series  of  experiments  was  conducted  in  order  to  find  the  optimal  regionalization.  The  experiments  with 

notable results were as follows.

1.  Batch  MLP  using  all  24  original  environmental  variables  and  MLP  using  the  most  significant  environmental 

variables  (precipitation,  humus,  hardness  of  water,  concentrations  of  F,  Fe,  SO4,  As,  and  mineralization  of 
water,  that  all  together  explained  42%  of  the  variability  in  the  linear  regression  model),  without  spatial 

2.  Batch MLP using all 24 original environmental variables and MLP using the most significant environmental vari‚Äê

enhancement  (Table  2).

ables with two spatial coordinates (Table 3).

3.  Batch MLP using all 24 original environmental variables and MLP using the most significant environmental vari‚Äê

ables with two spatial coordinates and local Moran's I index (Table 4).

4.  Batch MLP models based on the seven factors extracted from the preprocessed 24 original environmental vari‚Äê

ables and two spatial coordinates: ML and image factoring methods (Table 5).

5.  Mini‚Äêbatch MLP models based on the five factors extracted from the preprocessed 24 original environmental 

variables and two spatial coordinates: ML and image factoring methods (Table 6).

For better visual presentation of the six regions, point data were interpolated using the Euclidean allocation 

interpolation  technique.  Presented  raster  maps  represent  the  regionalization  results  in  a  more  comprehensive 

manner than the patterns of classified points. The points with missing values of some environmental variables 

have been excluded from MLP analysis on the original variables.

The average negative  AvgSil values indicate that those sample points might have been assigned to a wrong 

cluster. At the next step, two spatial variables (coordinates of points) were introduced as additional MLP inputs. 

The results are shown in Table 3.

These experiments yielded a low positive AvgSil value but, compared to the previous experiment, it has sig‚Äê

nificantly improved and changed from negative to positive. It is worth noting that the cross‚Äêentropy error and 

percentage of incorrect predictions are fairly constant across the training, validation, and testing samples. This 

gives us confidence that the model is not overtrained (Ripley, 2012).

In the next series, one more spatial variable, local Moran's I, was added to the input dataset. Moran's I is the 

measure of spatial autocorrelation based on both locations and attribute values of the features (Moran, 1950). 

Local Moran's I is calculated for each observation point on the respective neighbors. The optimal fixed bandwidth 

distance was computed using the average distance method for 30 neighbors for each feature. It equals approxi‚Äê

mately 29 km. The optimal fixed bandwidth distance could not be estimated using the incremental spatial auto‚Äê

correlation approach because the autocorrelations of the tested variables increased continuously as the distance 

increased, and did not produce peaks.

The MLP results are shown in Table 4. As seen, the introduction of the autocorrelation measure (Moran's I) as 

one of the MLP input variables did not improve the classification results.

GOVOROV et al.‚ÄÉ‚ÄÇ ‚ÄÉ| ‚ÄÉ1065

t
u
p
t
u
O

l
i

S
g
v
A

8
8
0
0
‚àí

.

y
r
a
m
m
u
s
 
l

e
d
o
M

6
3
0

 

.
 
r
o
r
r
E
y
p
o
r
t
n
E
 
s
s
o
r
C

 
:

i

g
n
n
a
r
T

i

 
l

a
t
n
e
m
n
o
r
i

v
n
e

 
l

a
n
g

i

 

i
r
o
4
2
h
t
i

 

 

w
P
L
M

l

s
e
b
a

i
r
a
v

l

e
d
o
M

l

s
e
b
a

i
r
a
v

 
l

a
t
n
e
m
n
o
r
i

 

 

v
n
e
e
h
t
 
n
o
d
e
s
a
b
 
s
l
e
d
o
m
P
L
M
h
c
t
a
B
‚ÄÉ
2

 

 

 

E
L
B
A
T

 
)

0
0
0

.
(
 

n
o
i
r
e
t
i
r
c
 
o
i
t
a
r
 
r
o
r
r
e
g
n
n
a
r
T

 

i

i

 
:

l

 

d
e
s
U
e
u
R
g
n
p
p
o
t
S

 

i

%
0
0

.

 
:
s
n
o
i
t
c
i
d
e
r
P
 
t
c
e
r
r
o
c
n
I
 
t
n
e
c
r
e
P

.

%
5
1
 
s
n
o
i
t
c
i
d
e
r
P
 
t
c
e
r
r
o
c
n
I
 
t
n
e
c
r
e
P

 
:

g
n
i
t
s
e
T

6

 

 
:
r
e
y
a
L
n
e
d
d
H
n

 

i

i
 
s
t
i
n
U

 
f
o
 
r
e
b
m
u
N

%
9
0

.

 
:
s
n
o
i
t
c
i
d
e
r
P
 
t
c
e
r
r
o
c
n
I
 
t
n
e
c
r
e
P

.

 

1
1
3
1
 
r
o
r
r
E
y
p
o
r
t
n
E
 
s
s
o
r
C

 
:

n
o
i
t
a
d

i
l

a
V

.

5
0
0
0
0
0

.

:

0

 
:

e
m

i

i
t
 
g
n
n
a
r
T

i

d
e
v
e
h
c
a

i

.

%
5
1
 
s
n
o
i
t
c
i
d
e
r
P
 
t
c
e
r
r
o
c
n
I
 
t
n
e
c
r
e
P

 
:

g
n
i
t
s
e
T

8

 

 
:
r
e
y
a
L
n
e
d
d
H
n

 

i

i
 
s
t
i
n
U

 
f
o
 
r
e
b
m
u
N

%
7
2

.

 
:
s
n
o
i
t
c
i
d
e
r
P
 
t
c
e
r
r
o
c
n
I
 
t
n
e
c
r
e
P

.

 

1
6
4
6
1
 
r
o
r
r
E
y
p
o
r
t
n
E
 
s
s
o
r
C

 
:

n
o
i
t
a
d

i
l

a
V

2
2

.

0
0
0
0

.

:

0

 
:

e
m

i

i
t
 
g
n
n
a
r
T

i

r
o
r
r
e
n

 

i
 

e
s
a
e
r
c
e
d

8
5
1
0
‚àí

.

.

 

4
2
8
0
1
 
r
o
r
r
E
y
p
o
r
t
n
E
 
s
s
o
r
C

 
:

i

g
n
n
a
r
T

i

 

 

o
n
h
t
i

w

 
)
s
(

p
e
t
s
 
e
v

i
t
u
c
e
s
n
o
c
 
0
0
5

 
:

l

 

d
e
s
U
e
u
R
g
n
p
p
o
t
S

 

i

%
5
0

.

 
:
s
n
o
i
t
c
i
d
e
r
P
 
t
c
e
r
r
o
c
n
I
 
t
n
e
c
r
e
P

 
 
t
n
a
c
i
f
i
n
g

i
s
 
t
s
o
m
8
h
t
i

 

 

 

w
P
L
M

l

s
e
b
a

i
r
a
v

 
l

a
t
n
e
m
n
o
r
i

v
n
e

GOVOROV et al.1066‚ÄÉ |‚ÄÉ ‚ÄÉ‚ÄÇ

t
u
p
t
u
O

l
i

S
g
v
A

6
7
0
0

.

7
3
0

 

.
 
r
o
r
r
E
y
p
o
r
t
n
E
 
s
s
o
r
C

 
:

i

g
n
n
a
r
T

i

 

d
n
a
 
s
e
b
a

l

i
r
a
v

 
l

a
t
n
e
m
n
o
r
i

v
n
e

 
l

a
n
g

i

 

i
r
o
4
2
h
t
i

 

 

w
P
L
M

i

s
e
t
a
n
d
r
o
o
c
 
l

a

i
t
a
p
s
 
2

y
r
a
m
m
u
s
 
l

e
d
o
M

l

e
d
o
M

i

s
e
t
a
n
d
r
o
o
c
 
l

a

i
t
a
p
s
 
d
n
a
 
s
e
b
a

l

i
r
a
v

 
l

a
t
n
e
m
n
o
r
i

 

 

v
n
e
e
h
t
 
n
o
d
e
s
a
b
 
s
l
e
d
o
m
P
L
M
h
c
t
a
B
‚ÄÉ
3

 

 

 

E
L
B
A
T

 

n
o
i
r
e
t
i
r
c
 
o
i
t
a
r
 
r
o
r
r
e
g
n
n
a
r
T

 

i

i

 
:

l

 

d
e
s
U
e
u
R
g
n
p
p
o
t
S

 

i

%
0
0

.

 
:
s
n
o
i
t
c
i
d
e
r
P
 
t
c
e
r
r
o
c
n
I
 
t
n
e
c
r
e
P

.

5
0
0
0
0
0

.

:

0

 
:

e
m

i

i
t
 
g
n
n
a
r
T

i

d
e
v
e
h
c
a
 
)

i

0
0
0

.
(

.

%
5
1
 
s
n
o
i
t
c
i
d
e
r
P
 
t
c
e
r
r
o
c
n
I
 
t
n
e
c
r
e
P

 
:

g
n
i
t
s
e
T

5

 

 
:
r
e
y
a
L
n
e
d
d
H
n

 

i

i
 
s
t
i
n
U

 
f
o
 
r
e
b
m
u
N

%
0
0

.

 
:
s
n
o
i
t
c
i
d
e
r
P
 
t
c
e
r
r
o
c
n
I
 
t
n
e
c
r
e
P

5
8
4

 

.
 
r
o
r
r
E
y
p
o
r
t
n
E
 
s
s
o
r
C

 
:

n
o
i
t
a
d

i
l

a
V

 

n
o
i
r
e
t
i
r
c
 
o
i
t
a
r
 
r
o
r
r
e
g
n
n
a
r
T

 

i

i

 
:

l

 

d
e
s
U
e
u
R
g
n
p
p
o
t
S

 

i

%
0
0

.

 
:
s
n
o
i
t
c
i
d
e
r
P
 
t
c
e
r
r
o
c
n
I
 
t
n
e
c
r
e
P

.

4
1
0
0
0
0

.

:

0

 
:

e
m

i

i
t
 
g
n
n
a
r
T

i

d
e
v
e
h
c
a
 
)

i

0
0
0

.
(

.

%
0
0
 
s
n
o
i
t
c
i
d
e
r
P
 
t
c
e
r
r
o
c
n
I
 
t
n
e
c
r
e
P

 
:

g
n
i
t
s
e
T

8

 

 
:
r
e
y
a
L
n
e
d
d
H
n

 

i

i
 
s
t
i
n
U

 
f
o
 
r
e
b
m
u
N

%
0
0

.

 
:
s
n
o
i
t
c
i
d
e
r
P
 
t
c
e
r
r
o
c
n
I
 
t
n
e
c
r
e
P

5
9
0

 

.
 
r
o
r
r
E
y
p
o
r
t
n
E
 
s
s
o
r
C

 
:

n
o
i
t
a
d

i
l

a
V

6
6
0
0

.

7
3
0

 

.
 
r
o
r
r
E
y
p
o
r
t
n
E
 
s
s
o
r
C

 
:

i

g
n
n
a
r
T

i

‚Äê
i
r
a
v

 
l

a
t
n
e
m
n
o
r
i

v
n
e
 
t
n
a
c
i
f
i
n
g

i
s
 
t
s
o
m
8
h
t
i

 

 

 

w
P
L
M

i

s
e
t
a
n
d
r
o
o
c
 
l

a

 

i
t
a
p
s
 
2
d
n
a
 
s
e
b
a

l

GOVOROV et al.‚ÄÉ‚ÄÇ ‚ÄÉ| ‚ÄÉ1067

%
0
0

.

 
:
s
n
o
i
t
c
i
d
e
r
P
 
t
c
e
r
r
o
c
n
I
 
t
n
e
c
r
e
P

 

 
r
o
r
r
e
g
n
n
a
r
T

i

i

 
:

l

 

d
e
s
U
e
u
R
g
n
p
p
o
t
S

 

i

d
e
v
e
h
c
a
 
)

i

0
0
0

.
(
 

n
o
i
r
e
t
i
r
c
 
o
i
t
a
r

.

0
1
0
0
0
0

.

:

0

 
:

e
m

i

i
t
 
g
n
n
a
r
T

i

%
0
0

.

 
:
s
n
o
i
t
c
i
d
e
r
P
 
t
c
e
r
r
o
c
n
I
 
t
n
e
c
r
e
P

 
s
n
o
i
t
c
i
d
e
r
P
 
t
c
e
r
r
o
c
n
I
 
t
n
e
c
r
e
P

 
:

g
n
i
t
s
e
T

6
2
0

 

.
 
r
o
r
r
E
y
p
o
r
t
n
E
 
s
s
o
r
C

 
:

n
o
i
t
a
d

i
l

a
V

%
0
0

.

5

 

 
:
r
e
y
a
L
n
e
d
d
H
n

 

i

i
 
s
t
i
n
U

 
f
o
 
r
e
b
m
u
N

%
0
0

.

 
:
s
n
o
i
t
c
i
d
e
r
P
 
t
c
e
r
r
o
c
n
I
 
t
n
e
c
r
e
P

 

 
r
o
r
r
e
g
n
n
a
r
T

i

i

 
:

l

 

d
e
s
U
e
u
R
g
n
p
p
o
t
S

 

i

d
e
v
e
h
c
a
 
)

i

0
0
0

.
(
 

n
o
i
r
e
t
i
r
c
 
o
i
t
a
r

.

3
1
0
0
0
0

.

:

0

 
:

e
m

i

i
t
 
g
n
n
a
r
T

i

%
0
0

.

 
:
s
n
o
i
t
c
i
d
e
r
P
 
t
c
e
r
r
o
c
n
I
 
t
n
e
c
r
e
P

 
s
n
o
i
t
c
i
d
e
r
P
 
t
c
e
r
r
o
c
n
I
 
t
n
e
c
r
e
P

 
:

g
n
i
t
s
e
T

3
0
2

 

.
 
r
o
r
r
E
y
p
o
r
t
n
E
 
s
s
o
r
C

 
:

n
o
i
t
a
d

i
l

a
V

%
0
0

.

7

 

 
:
r
e
y
a
L
n
e
d
d
H
n

 

i

i
 
s
t
i
n
U

 
f
o
 
r
e
b
m
u
N

l
i

S
g
v
A

9
6
0
0

.

t
u
p
t
u
O

y
r
a
m
m
u
s
 
l

e
d
o
M

l

e
d
o
M

7
3
0

 

.
 
r
o
r
r
E
y
p
o
r
t
n
E
 
s
s
o
r
C

 
:

i

g
n
n
a
r
T

i

 
l

a

i
t
a
p
s
 
2

 
,
s
e
b
a

l

i
r
a
v

 
l

a
t
n
e
m
n
o
r
i

v
n
e

 
l

a
n
g

i

 

i
r
o
4
2
h
t
i

 

 

w
P
L
M

)
l

a
t
o
t
 
n

i
 
s
e
b
a

l

i
r
a
v
7
2

 

‚Äô

(
 
I
 
s
n
a
r
o
M

 
l

a
c
o

l
 

d
n
a
 
,
s
e
t
a
n
d
r
o
o
 c

i

s
e
t
u
b
i
r
t
t
a
 
l

a

i
t
a
p
s
 
d
n
a
 
s
e
b
a

l

i
r
a
v

 
l

a
t
n
e
m
n
o
r
i

 

 

v
n
e
e
h
t
 
n
o
d
e
s
a
b
 
s
l
e
d
o
m
P
L
M
h
c
t
a
B
‚ÄÉ
4

 

 

 

E
L
B
A
T

3
7
0
0

.

3
3
1

 

.
 
r
o
r
r
E
y
p
o
r
t
n
E
 
s
s
o
r
C

 
:

i

g
n
n
a
r
T

i

 

2

 
,
s
e
b
a

l

i
r
a
v

 
l

a
t
n
e
m
n
o
r
i

v
n
e
 
t
n
a
c
i
f
i
n
g

i
s
 
t
s
o
m
8
h
t
i

 

 

 

w
P
L
M

)
l

a
t
o
t
 
n

i
 
s
e
b
a

l

i
r
a
v
1
1

 

‚Äô

(
 
I
 
s
n
a
r
o
M

 
l

a
c
o

l
 

d
n
a
 
,
s
e
t
a
n
d
r
o
o
c
 
l

i

a

i
t
a
p
 s

GOVOROV et al.1068‚ÄÉ |‚ÄÉ ‚ÄÉ‚ÄÇ

l
i

S
g
v
A

1
7
1
0

.

t
u
p
t
u
O

y
r
a
m
m
u
s
 
l

e
d
o
M

l

e
d
o
M

l

s
e
b
a

 

 

i
r
a
v
6
2
d
e
s
s
e
c
o
r
p
e
r
p
 
f
o
 
s
r
o
t
c
a
f
 
e
h
t
 
n
o
d
e
s
a
b
 
s
l
e
d
o
m
P
L
M
h
c
t
a
B
‚ÄÉ
5

 

 

 

 

E
L
B
A
T

7
3
0

 

.
 
r
o
r
r
E
y
p
o
r
t
n
E
 
s
s
o
r
C

 
:

i

g
n
n
a
r
T

i

 
s
e
b
a

l

i
r
a
v

 
l

a
t
n
e
m
n
o
r
i

v
n
e

 
l

a
n
g

i

 

 

i
r
o
4
2
m
o
r
f
 
s
r
o
t
c
a
f
 
7
h
t
i

 

 

w
P
L
M

 

 

 

L
M
y
b
d
e
t
c
a
r
t
x
e
e
r
a
 
s
r
o
t
c
a
F

 

i

 
.
s
e
t
a
n
d
r
o
o
c
 
l

a

i
t
a
p
s
 
2
d
n
a

 

 

d
o
h
t
e
m
g
n
i
r
o
t
c
a
f

 

o
i
t
a
r
 
r
o
r
r
e
g
n
n
a
r
T

 

i

i

 
:

l

 

d
e
s
U
e
u
R
g
n
p
p
o
t
S

 

i

%
0
0

.

 
:
s
n
o
i
t
c
i
d
e
r
P
 
t
c
e
r
r
o
c
n
I
 
t
n
e
c
r
e
P

d
e
v
e
h
c
a
 
)

i

0
0
0

.
(
 

n
o
i
r
e
t
i
r
c

.

4
0
0
0
0
0

.

:

0

 
:

e
m

i

i
t
 
g
n
n
a
r
T

i

.

%
0
0
 
s
n
o
i
t
c
i
d
e
r
P
 
t
c
e
r
r
o
c
n
I
 
t
n
e
c
r
e
P

 
:

g
n
i
t
s
e
T

7

 

 
:
r
e
y
a
L
n
e
d
d
H
n

 

i

i
 
s
t
i
n
U

 
f
o
 
r
e
b
m
u
N

%
0
0

.

 
:
s
n
o
i
t
c
i
d
e
r
P
 
t
c
e
r
r
o
c
n
I
 
t
n
e
c
r
e
P

2
2
0

 

.
 
r
o
r
r
E
y
p
o
r
t
n
E
 
s
s
o
r
C

 
:

n
o
i
t
a
d

i
l

a
V

 

o
i
t
a
r
 
r
o
r
r
e
g
n
n
a
r
T

 

i

i

 
:

l

 

d
e
s
U
e
u
R
g
n
p
p
o
t
S

 

i

%
0
0

.

 
:
s
n
o
i
t
c
i
d
e
r
P
 
t
c
e
r
r
o
c
n
I
 
t
n
e
c
r
e
P

d
e
v
e
h
c
a
 
)

i

0
0
0

.
(
 

n
o
i
r
e
t
i
r
c

.

4
0
0
0
0
0

.

:

0

 
:

e
m

i

i
t
 
g
n
n
a
r
T

i

.

%
0
0
 
s
n
o
i
t
c
i
d
e
r
P
 
t
c
e
r
r
o
c
n
I
 
t
n
e
c
r
e
P

 
:

g
n
i
t
s
e
T

7

 

 
:
r
e
y
a
L
n
e
d
d
H
n

 

i

i
 
s
t
i
n
U

 
f
o
 
r
e
b
m
u
N

%
0
0

.

 
:
s
n
o
i
t
c
i
d
e
r
P
 
t
c
e
r
r
o
c
n
I
 
t
n
e
c
r
e
P

2
2
0

 

.
 
r
o
r
r
E
y
p
o
r
t
n
E
 
s
s
o
r
C

 
:

n
o
i
t
a
d

i
l

a
V

9
7
1
0

.

7
3
0

 

.
 
r
o
r
r
E
y
p
o
r
t
n
E
 
s
s
o
r
C

 
:

i

g
n
n
a
r
T

i

 
s
e
b
a

l

i
r
a
v

 
l

a
t
n
e
m
n
o
r
i

v
n
e

 
l

a
n
g

i

 

 

i
r
o
4
2
m
o
r
f
 
s
r
o
t
c
a
f
 
7
h
t
i

 

 

w
P
L
M

F

 

I
 
y
b
d
e
t
c
a
r
t
x
e
e
r
a
 
s
r
o
t
c
a
F

 

i

 
.
s
e
t
a
n
d
r
o
o
c
 
l

a

i
t
a
p
s
 
2
d
n
a

 

GOVOROV et al.‚ÄÉ‚ÄÇ ‚ÄÉ| ‚ÄÉ1069

l
i

S
g
v
A

1
9
1
0

.

t
u
p
t
u
O

y
r
a
m
m
u
s
 
l

e
d
o
M

l

e
d
o
M

l

s
e
b
a

 

 

i
r
a
v
6
2
d
e
s
s
e
c
o
r
p
e
r
p
 
f
o
 
s
r
o
t
c
a
f
 
e
h
t
 
n
o
d
e
s
a
b
 
s
l
e
d
o
m
P
L
M
h
c
t
a
b
‚Äê
i
n
M
‚ÄÉ
6

 

 

 

i

 

E
L
B
A
T

6
0
2

 

.
 
r
o
r
r
E
y
p
o
r
t
n
E
 
s
s
o
r
C

 
:

i

g
n
n
a
r
T

i

‚Äê
n
e
m
n
o
r
i

v
n
e

 
l

a
n
g

i

 

 

i
r
o
4
2
m
o
r
f
 
s
r
o
t
c
a
f
 
5
h
t
i

 

 

 

w
P
L
M
h
c
t
a
b
‚Äê
i
n
M

i

 

 

d
e
t
c
a
r
t
x
e
e
r
a
 
s
r
o
t
c
a
F

i

 
.
s
e
t
a
n
d
r
o
o
c
 
l

a

 

i
t
a
p
s
 
2
d
n
a
 
s
e
b
a

l

i
r
a
v

 
l

a
t

g
n
i
r
o
t
c
a
f
 
L
M
y
b

 

 
f
o
 
r
e
b
m
u
n
m
u
m
x
a
M

 

i

 
:

l

 

d
e
s
U
e
u
R
g
n
p
p
o
t
S

 

i

%
0
0

.

 
:
s
n
o
i
t
c
i
d
e
r
P
 
t
c
e
r
r
o
c
n
I
 
t
n
e
c
r
e
P

d
e
d
e
e
c
x
e
 
)

0
0
0
1

,

(
 
s
h
c
o
p
e

.

4
4
0
0
0
0

.

:

0

 
:

e
m

i

i
t
 
g
n
n
a
r
T

i

.

%
0
0
 
s
n
o
i
t
c
i
d
e
r
P
 
t
c
e
r
r
o
c
n
I
 
t
n
e
c
r
e
P

 
:

g
n
i
t
s
e
T

5

 

 
:
r
e
y
a
L
n
e
d
d
H
n

 

i

i
 
s
t
i
n
U

 
f
o
 
r
e
b
m
u
N

%
0
0

.

 
:
s
n
o
i
t
c
i
d
e
r
P
 
t
c
e
r
r
o
c
n
I
 
t
n
e
c
r
e
P

3
4
1

 

.
 
r
o
r
r
E
y
p
o
r
t
n
E
 
s
s
o
r
C

 
:

n
o
i
t
a
d

i
l

a
V

 

e
v

i
t
u
c
e
s
n
o
c
 
0
0
5

 
:

l

 

d
e
s
U
e
u
R
g
n
p
p
o
t
S

 

i

r
o
r
r
e
n

 

i
 

e
s
a
e
r
c
e
d
o
n
h
t
i

 

 

w

 
)
s
(

p
e
t
s

%
0
0

.

 
:
s
n
o
i
t
c
i
d
e
r
P
 
t
c
e
r
r
o
c
n
I
 
t
n
e
c
r
e
P

%
0
0

.

 
:
s
n
o
i
t
c
i
d
e
r
P
 
t
c
e
r
r
o
c
n
I
 
t
n
e
c
r
e
P

5
6
1

 

.
 
r
o
r
r
E
y
p
o
r
t
n
E
 
s
s
o
r
C

 
:

n
o
i
t
a
d

i
l

a
V

6
2

.

0
0
0
0

.

:

0

 
:

e
m

i

i
t
 
g
n
n
a
r
T

i

.

%
0
3
 
s
n
o
i
t
c
i
d
e
r
P
 
t
c
e
r
r
o
c
n
I
 
t
n
e
c
r
e
P

 
:

g
n
i
t
s
e
T

6

 

 
:
r
e
y
a
L
n
e
d
d
H
n

 

i

i
 
s
t
i
n
U

 
f
o
 
r
e
b
m
u
N

1
8
1
0

.

0
6
3

 

.
 
r
o
r
r
E
y
p
o
r
t
n
E
 
s
s
o
r
C

 
:

i

g
n
n
a
r
T

i

‚Äê
n
e
m
n
o
r
i

v
n
e

 
l

a
n
g

i

 

 

i
r
o
4
2
m
o
r
f
 
s
r
o
t
c
a
f
 
5
h
t
i

 

 

 

w
P
L
M
h
c
t
a
b
‚Äê
i
n
M

i

 

 

d
e
t
c
a
r
t
x
e
e
r
a
 
s
r
o
t
c
a
F

i

 
.
s
e
t
a
n
d
r
o
o
c
 
l

a

 

i
t
a
p
s
 
2
d
n
a
 
s
e
b
a

l

i
r
a
v

 
l

a
t

F

I
 
y
b

GOVOROV et al.1070‚ÄÉ |‚ÄÉ ‚ÄÉ‚ÄÇ

Next,  an  experiment  was  conducted  using  the  factors  extracted  from  24  original  environmental  variables 

plus spatial coordinates of each point. A common problem in the use of factor and PCA analyses is to define the 

number of factors or components to retain. Determining the number of factors or components to retain was ac‚Äê

complished based on the following five criteria proposed by Jolliffe (2002) and Bro and Smilde (2014):

1.  Kaiser's  eigenvalue  greater  than  unity  (>1.0);

2.  components within the sharp descent from Cattell's scree plot test;

3.  at least 70% of total variability explained;

4.  conformity with Bartlett's test of sphericity test; and

5.  meaningfulness of each dimension.

The experiments with different criteria and factor extraction methods show that four to seven factors or components 

sufficiently represent the same information contained in the 24 original variables and two coordinates. The results 

yielded by different PCA and factor analysis algorithms were in some disagreement, therefore several factoring algo‚Äê

rithms were applied. Then the external criterion representing the measures of the MLP minimization cost and average 
AvgSil was applied for the selection of the algorithm with best performance.

The best exploratory factoring was found by employing image factoring (IF) and maximum‚Äêlikelihood (ML) fac‚Äê

tor extraction methods. The IF method is an empirical factor analysis in which each variable is predicted from the 

others using multiple regression based on the correlation matrix of predicted variables as communalities, rather 

than a function of hypothetical factors (Guttman, 1953). In ML factor extraction, population estimates for fac‚Äê

tor loadings that have the greatest probability of yielding a sample with the observed correlation matrix‚Äîwhere 

the correlations are weighted by the inverse of the uniqueness of the variables‚Äîare calculated (Harman, 1976; 

Joreskog, 1977). The use of ML methods is strongly recommended when nonlinear dependences exist between 

several variables. When factor analysis is used in an exploratory way to summarize relationships, assumptions on 

distributions are not enforced, thus the impact of a prior assumption of a multivariate normal distribution dimin‚Äê

ishes as the number of data records increases (Stroyny & Rowe, 2002). It has also been found that the ML estima‚Äê

tion procedure in factor analysis is not sensitive to changes in the distributions considered (Fuller & Hemmerle, 

1966).

For both factor extraction methods, the Varimax rotation was performed in order to get factors linearly more 

independent, and the Anderson‚ÄìRubin method was used to estimate factor scores to ensure that they are uncor‚Äê

related and standardized. In factor analysis, missing values of three specified variables were replaced by the mean 

values. The total variance explained by the seven factors from ML and IF factoring is 75%. The network training 

shows much better results when the inputs are decorrelated and whitened (i.e., linearly transformed to have zero 

means and unit variances). The best MLP results on factor inputs are shown in Table 5.

Whitening the variables in the input layer improves the classification results substantially. However, the aver‚Äê

age Silhouette AvgSil < 0.25 over points of all clusters indicates that no substantial structure was found.

In the next set of experiments, all 6,546 point records were sorted using the space‚Äêfilling curve schema shown 

in Figure 5. This sorting option is implemented in ArcGIS software. In this schema, an algorithm visits 24 locations 

of a smaller neighborhood first before moving on to the next neighborhood (Esri, 2018). Then, factor scores of 

sorted records were used to run mini‚Äêbatch MLP models with 24 records per mini‚Äêbatch. Experiments were also 

conducted using a mini‚Äêbatch size of 96 records, but validation indices were lower than with 24 records per mini‚Äê

batch. The best results, achieved using 24 records per mini‚Äêbatch, are shown in Table 6.

In these experiments, the values of AvgSil measures were improved compared to the classification results pre‚Äê

sented in Table 5. It must be noted that mini‚Äêbatch training takes longer than batch training. This can be an issue 

for processing large datasets. The mini‚Äêbatch MLP with five factors extracted from ML factoring on 24 original 

environmental variables and two spatial coordinates produced the best regionalization result.

GOVOROV et al.‚ÄÉ‚ÄÇ ‚ÄÉ| ‚ÄÉ1071

F I G U R E   5 ‚ÄÉSpace‚Äêfilling curve schema used to sort environmental observation points (Esri, 2018)

Experiments with spatially lagged variables derived as a weighted sum of the values observed at neighbor‚Äê

ing locations based on distance measures (Anselin & Rey, 2014) did not improve the MLP model based on the 

validation criterion used. The best results were achieved using the spatial lagged variables with row‚Äêstandard‚Äê

ized weights computed with a fixed distance‚Äêband neighborhood. The MLP results with spatially lagged variables 

smooth the original input variables within neighboring locations.

For the final set of experiments, the majority spatial filters were applied on the best achieved MLP output 

layer, which was the mini‚Äêbatch MLP with five factors from 24 original environmental variables plus two spatial 

coordinates that were extracted by the ML factoring method (Table 5). To smooth the outcome of spatially con‚Äê

strained MLP classification, several bandwidth distances were tried. The best result was achieved with a distance 

band of 29 km, which was derived from an optimized outlier analysis (incremental Anselin local Moran's I). The 

value of the distance band was based on a number of points that had less than eight neighbors (Anselin, 1995; Esri, 
2018). However, the smoothing of the output classification reduces the average Silhouette coefficient AvgSil from 

0.191 to 0.102. The best filtered map is shown in Figure 6.

The  experiments  were  conducted  using  SPSS  with  R  and  Python  extensions,  ArcGIS  Desktop,  GeoDa,  and 

MatLab software packages. MLP models were implemented with the SPSS Neural Networks tools (IBM, 2018).

5‚ÄÉ| ‚ÄÉD I S CU S S I O N  A N D CO N C LU S I O N S

Different MLP models were explored to find the best clustering results in the sequential model‚Äêbased optimiza‚Äê

tion framework. Applying ANN supervised learning techniques resulted in substantially enhanced regionalization 

based on internal and external indices. Validation and comparison of classification results were performed with 

two  approaches.  First,  the  external  indices,  such  as  cross‚Äêentropy  error  and  percentage  incorrect  predictions, 

GOVOROV et al.1072‚ÄÉ |‚ÄÉ ‚ÄÉ‚ÄÇ

F I G U R E   6 ‚ÄÉPost‚Äêprocessed classification map derived from majority focal statistics of the best MLP outcomes 
with bandwidth radius of 29 km and AvgSil = 0.102

were  used  to  measure  the  extent  to  which  cluster  labels  match  externally  supplied  class  labels.  Second,  the  

internal Silhouette index was used to measure the goodness of clustering structure without reference to exter‚Äê

nal information. The study produced a series of classification maps that show homogeneous regions of primary 

environmental variables (or their factors) based on their relationships with the concentration of natural uranium 

in groundwater.

The main insights and assumptions related to regionalization are as follows:

1.  Predictive  models  embrace  three  equally  important  factors:  the  dataset  used  to  train  the  model,  the 

parameters  that  best  describe  the  target,  and  the  learning  algorithm.

2.  Models should be driven by concepts like natural similarity based on the first law of geography that its k‚Äênear‚Äê

est neighbors should also be in the same cluster. However, assumptions about the model cannot be treated as 

referring to ‚Äúunderlying truth,‚Äù but rather serve to characterize the methods based on them. We support the 

statement by Hennig and Liao (2013) that an expectation that the application of automatic classification meth‚Äê

ods will enforce true spatial and multivariable structures can be deceptive.

3.  MLP  supervised  classification  very  much  depends  on  the  expert  labeled  training  and  the  datasets  used  for 

validation. The cluster labels are assigned by the procedures that do not fully take into account the underlying 
relationship of phenomena under study. Because of this, the internal index AvgSil used for unsupervised clas‚Äê

sification may not show that a reasonable clustering structure has been found. However, the index can be used 

for comparisons of the supervised clustering.

GOVOROV et al.‚ÄÉ‚ÄÇ ‚ÄÉ| ‚ÄÉ1073

4.  Too many application‚Äêbased decisions have to be made for an MLP classification, particularly regarding stand‚Äê

ardization,  transformation,  weighting  of  variables,  and  algorithmic  parameters.  Some  of  these  decisions,  but 

not all of them, depend on the nature of the dataset. Furthermore, not all the structural information about the 

dataset is known or explicitly discoverable. Even when classification analysis is applied as an exploratory tool, 

the interpretation of the findings can be problematic and inconclusive.

5.  The characteristics of the input data can have a strong effect on the MLP performance. Whitening the source 

data (standardization, normalization, removing nonlinearities, uniformization, decorrelation, spatial declutter‚Äê

ing, etc.) produces a strong positive effect. In our experiments, the shallow MLP network architecture with one 

hidden layer showed the best results provided that whitening of the input layer was performed. Adding two or 

more layers to the MLP network architecture did not improve the classification results.

6.  The number of factors in the decorrelation process was chosen based not only on the five quantitative crite‚Äê

ria described in Section 4.3, but also taking into account the meaningfulness of each dimension. Thus, the 24 

original  environmental  variables  can  be  assembled  in  four  factor  groups  such  as  mineralogical,  geochemical, 

climatic, geological (structural), and geomorphological. Spatial attributes make a separate group. Experimental 

incremental factoring in our case showed that the best MLP models are derived with five and seven factors.

7.  Shuffling the data removes possible drifts in stochastic and mini‚Äêbatch learnings. However, spatial dependences 

or autocorrelation within datasets of regionalized variables should be taken into account. Moreover, it is not 

always clear if drift should be removed or if a natural order should be preserved. It can also be unclear how 

to account for the spatial effects. The comparison of mini‚Äêbatch and batch training methods based on internal 

indices revealed that ordering points by spatial neighborhood leads to improved regionalization result.

The experiments conducted in this study show that removal of attributive dependences significantly improves the 

regionalization outputs. The use of spatial attributes and their respective whitening allows for improving the region‚Äê

alization even further. Spatial sorting that is used in MLP guided training to preserve spatial locality also leads to an 

improved regionalization performance.

Shallow learning, or MLP with only one hidden layer, works better to regionalize data using the investigated 

MLP models. Attempts to increase the number of hidden layers did not improve validation indices. Another ap‚Äê

proach, that is now under our investigation, is to learn to encode the components of a random point's regionalized 

variables: first to encode global feature trends and then to encode fewer local features, and so on, and at the same 

time take into consideration the key property of spatial data, which is that nearby local features are more strongly 

correlated than distant features. Greedy layer‚Äêwise training and CNN types of deep learning ANN could be applied 

to learn a hierarchy in spatial patterns for classification purposes.

Michael Govorov 

 https://orcid.org/0000‚Äê0001‚Äê6602‚Äê6869 

Giedrƒó Beconytƒó 

 https://orcid.org/0000‚Äê0003‚Äê3036‚Äê0544 

O R C I D

R E F E R E N C E S

Almhdi, K., Valigi, P., Gulbinas, V., Westphal, R., & Reuter, R. (2007). Classification with artificial neural networks and sup‚Äê
port vector machines: Application to oil fluorescence spectra. In Z. Bochenek (Ed.), New developments and challenges 
in remote sensing (pp. 413‚Äì431). Rotterdam, the Netherlands: Millpress.

Alpaydin, E. (2010). Introduction to machine learning (2nd ed.). Cambridge, MA: MIT Press.
Anon.  (1978).  Atlas  of  the  natural  environment  and  natural  resources  of  Ukraine.  Moscow,  Russia:  State  Department  of 

Geodesy and Cartography, USSR.

Anselin, L. (1995). Local Indicators of Spatial Association ‚Äì LISA. Geographical Analysis, 27(2), 93‚Äì115.

GOVOROV et al.1074‚ÄÉ |‚ÄÉ ‚ÄÉ‚ÄÇ

IL: GeoDa Press.

Anselin, L., & Rey, S. J. (2014). Modern spatial econometrics in practice: A guide to Geoda, Geodaspace and Pysal. Chicago, 

Assuncao, R., Neves, M. C., Camara, G., & Freitas, C. (2006). Efficient regionalization techniques for socio‚Äêeconomic geo‚Äê
graphical units using minimum spanning trees. International Journal of Geographical Information Science, 20, 797‚Äì811.
Atkinson, P. M., & Naser, D. K. (2010). A geostatistically weighted k‚ÄêNN classifier for remotely sensed imagery. Geographical 

Bailey, S. (2012). Principal component analysis with noisy and/or missing data. Astronomical Society of the Pacific, 124, 

Bair, E., Hastie, T., Paul, D., & Tibshirani, R. (2006). Prediction by supervised principal components. Journal of the American 

Basu, S., Davidson, I., & Wagstaff, K. (2008). Data constrained clustering: Advances in algorithms, theory, and applications. 

Analysis, 42, 204‚Äì225.

1015‚Äì1023.

Statistical Association, 101, 119‚Äì137.

London, UK: Chapman & Hall/CRC Press.

Bengio,  Y.  (2012).  Practical  recommendations  for  gradient‚Äêbased  training  of  deep  architectures.  In  G.  B.  Orr,  &  K.  R. 
M√ºller (Eds.), Neural networks: Tricks of the trade (Lecture Notes in Computer Science, Vol. 7700, pp. 437‚Äì478). Berlin, 
Germany: Springer.

Bergstra, J., Yamins, D., & Cox, D. (2013). Making a science of model search: Hyperparameter optimization in hundreds of 

dimensions for vision architectures. JMLR Workshop & Conference Proceedings, 28(1), 115‚Äì123.

Berry,  B.  (1966).  Essays  on  commodity  flows  and  the  spatial  structure  of  the  Indian  economy  (Research  Paper  No.  111). 

Chicago, IL: Department of Geography, University of Chicago.

Bishop, C. M. (2006). Pattern recognition and machine learning. Berlin, Germany: Springer.
Bourgault, G., Marcotte, D., & Legendre, P. (1992). The multivariate (co) variogram as a spatial weighting function in clas‚Äê

sification methods. Mathematical Geology, 24, 463‚Äì478.

Bro, R., & Smilde, K. A. (2014). Principal component analysis. Analytical Methods, 6, 2812.
Caeiro, S., Goovaerts, P., Painho, M., & Costa, M. H. (2003). Delineation of estuarine management areas using multivariate 

geostatistics: The case of Sado estuary. Environmental Science & Technology, 37, 4052‚Äì4059.

Chavent, M., Kuentz‚ÄêSimonet, V., Labenne, A., & Saracco, J. (2017). ClustGeo: An R package for hierarchical clustering 

with spatial constraints. Computational Statistics, 33(4), 1799‚Äì1822.

Chen,  H.‚ÄêL.,  &  Chang,  Y.‚ÄêI.  (2011).  All‚Äênearest‚Äêneighbors  finding  based  on  the  Hilbert  curve.  Expert  Systems  with 

Applications, 38, 7462‚Äì7475.

Davidson, I., & Basu, S. (2007). A survey of clustering with instance level constraints. Retrieved from https ://pdfs.seman ticsc 

holar.org/3066/0a8af 6327d e8751 054e6 0650f 72c1f 72bbe0.pdf

Davies,  T.,  Marshall,  J.,  &  Hazelton,  M.  (2017).  Tutorial  on  kernel  estimation  of  continuous  spatial  and  spatiotemporal 

relative risk with accompanying instruction in R. Statistics in Medicine, 37(7), 1191‚Äì1221.

de Bollivier, M., Dubois, G., Maignanb, M., & Kanevski, M. (1997). Multilayer perceptron with local constraint as an emerg‚Äê

ing method in spatial data analysis. Nuclear Instruments & Methods in Physics Research A, 389, 226‚Äì229.

Delchambre, L. (2015). Weighted principal component analysis: A weighted covariance eigendecomposition approach. 

Monthly Notices of the Royal Astronomical Society, 446, 3545‚Äì3555.

Ding, C., & He, X. F. (2004). K‚Äênearest‚Äêneighbor consistency in data clustering: Incorporating local information into global 
optimization. In Proceedings of the 19th ACM Annual Symposium on Applied Computing (pp. 584‚Äì589). Nicosia, Cyprus: 
ACM.

Duque, J. C., Anselin, L., & Rey, S. J. (2012). The max‚Äêp‚Äêregions problem. Journal of Regional Science, 52(3), 397‚Äì419.
Duque, J. C., Ramos, R., & Surinach, J. (2007). Supervised regionalization methods: A survey. International Regional Science 

Review, 30, 195‚Äì220.

Dureja, J. S., Singh, R., & Bhatti, M. (2014). Optimizing flank wear and surface roughness during hard turning of AISI D3 

steel by Taguchi and RSM methods. Production & Manufacturing Research, 2(1), 767‚Äì783.

Esri. (2018). Documentation for ArcGIS. Retrieved from https ://doc.arcgis.com/en/
Ferligoj, A., & Batagelj, V. (1982). Clustering with relational constraint. Psychometrika, 47, 413‚Äì426.
Fisher, D. (1987). Knowledge acquisition via incremental conceptual clustering. Machine Learning, 2(2), 139‚Äì172.
Fuller, E. L. Jr, & Hemmerle, W. J. (1966). Robustness of the maximum‚Äêlikelihood estimation procedure in factor analysis. 

Gabriel,  K.  R.,  &  Zamir,  S.  (1979).  Lower  rank  approximation  of  matrices  by  least  squares  with  any  choice  of  weights. 

Getis, A., & Ord, J. K. (1992). The analysis of spatial association by use of distance statistics. Geographical Analysis, 24(3), 

Gilardi, N., & Bengio, S. (2000). Local machine learning models for spatial data analysis. Journal of Geographic Information 

Psychometrika, 31(2), 255‚Äì266.

Technometrics, 21(4), 489‚Äì498.

186‚Äì206.

& Decision Analysis, 4(1), 11‚Äì28.

Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep learning. Cambridge, MA: MIT Press.

GOVOROV et al.‚ÄÉ‚ÄÇ ‚ÄÉ| ‚ÄÉ1075

Gopal,  S.  (2017).  Artificial  neural  networks  in  geospatial  analysis.  In  D.  Richardson,  N.  Castree,  M.  F.  Goodchild,  A. 
Kobayashi, W. Liu, & R. A. Marston (Eds.), International Encyclopedia of Geography (pp. 1‚Äì7). New York, NY: John Wiley 
& Sons.

Govorov, M. O., & Malikov, B. N. (1986). Selected topics of the use of geological and economic maps. Geography & Natural 

Resources, 4, 165‚Äì167.

Govorov, M., Putrenko, V., & Gienko, G. (2016). Mining spatial patterns of distribution of uranium in surface and ground 
waters in Ukraine. In S. Faiz, & K. Mahmoudi (Eds.), Handbook of research on geographic information systems: Applications 
and advancements (pp. 520‚Äì546). Hershey, PA: IGI Global.

Guo,  D.  (2008).  Regionalization  with  dynamically  constrained  agglomerative  clustering  and  partitioning  (REDCAP). 

International Journal of Geographical Information Science, 22(7), 801‚Äì823.

Guo, D., & Wang, H. (2011). Automatic region building for spatial analysis. Transactions in GIS, 15(s1), 29‚Äì45.
Guo, R., Ahn, M., & Zhu, H. (2014). Spatially weighted principal component analysis for imaging classification. Journal of 

Computational & Graphical Statistics, 24(1), 274‚Äì296.

Guttman, L. (1953). Image theory for the structure of quantitative variates. Psychometrika, 18(4), 277‚Äì296.
Halkidi, M., Batistakis, Y., & Vazirgiannis, M. (2001). On clustering validation techniques. Journal of Intelligent Information 

Systems, 17(2&3), 107‚Äì145.

Harman, H. H. (1976). Modern factor analysis (3rd ed.). Chicago, IL: University of Chicago Press.
Harris,  P.,  Brunsdon,  C.,  &  Charlton,  M.  (2011).  Geographically  weighted  principal  components  analysis.  International 

Journal of Geographical Information Science, 25(10), 1717‚Äì1736.

Haykin, S. (2008). Neural networks and learning machines (3rd ed.). New York, NY: Pearson/Prentice Hall.
He, W., Ling, H., Zhang, Z., & Gong, C. (2018). Multi‚Äêobjective spatially constrained clustering for regionalization with 

particle swarm optimization. International Journal of Geographical Information Science, 32(4), 827‚Äì846.

Hennig, C., & Liao, T. F. (2013). How to find an appropriate clustering for mixed type variables with application to so‚Äê
cioeconomic  stratification  (with  discussion).  Journal  of  the  Royal  Statistical  Society:  Series  C  (Applied  Statistics),  62, 
309‚Äì369.

Hinton, G. E., Osindero, S., & Teh, Y.‚ÄêW. (2006). A fast learning algorithm for deep belief nets. Neural Computation, 18(7), 

Hu, T., & Sung, S. (2005). Data fusion in radial basis function networks for spatial regression. Neural Processing Letters, 

1527‚Äì1554.

21(2), 81‚Äì93.

IBM. (2018). IBM SPSS Statistics: Neural network help resources. Retrieved from https ://www.ibm.com/us‚Äêen/marke tplac 

e/spss‚Äêneural‚Äênetwo rks/resou rces

Ioffe,  S.  (2017).  Batch  renormalization:  Towards  reducing  minibatch  dependence  in  batch‚Äênormalized  models.  In 

Proceedings of the 31st Annual Conference on Neural Information Processing Systems. Long Beach, CA.

Ioffe, S., & Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate 

shift. In Proceedings of the 32nd International Conference on Machine Learning (pp. 448‚Äì456). Lille, France.

Jia, Z., Zhou, S., Su, Q., Yi, H., & Wang, J. (2018). Comparison study on the estimation of the spatial distribution of regional 
soil metal(loid)s pollution based on kriging interpolation and BP neural network. International Journal of Environmental 
Research & Public Health, 15, 34.

Johnston, R. J. (1970). Grouping and regionalizing: Some methodological and technical observations. Economic Geography, 

46, 293‚Äì305.

Jolliffe, I. T. (2002). Principal component analysis (2nd ed.). New York, NY: Springer.
Jolliffe,  I.  T.,  &  Cadima,  J.  (2016).  Principal  component  analysis:  A  review  and  recent  developments.  Philosophical 

Transactions, Series A, Mathematical, Physical, & Engineering Sciences, 374, 2015.0202.

Joreskog, K. G. (1977). Factor analysis by least squares and maximum‚Äêlikelihood methods. In K. Enslein, A. Ralston, & H. 

S. Wilf (Eds.), Statistical methods for digital computers (pp. 125‚Äì153). New York, NY: John Wiley & Sons.

Kanevski, M. (Ed.). (2008). Advanced mapping of environmental data: Geostatistics, machine learning, and Bayesian maximum 

entropy. London, UK: ISTE and John Wiley & Sons.

Kanevski,  M.,  Arutyunyan,  R.,  Bolshov,  L.,  Demyanov,  V.,  &  Maignan,  M.  (1996).  Artificial  neural  networks  and  spatial 

estimations of Chernobyl fallout. Geoinformatics, 7(1&2), 5‚Äì11.

Kanevski, M., Pozdnoukhov, A., & Timonin, V. (2008). Machine learning algorithms for environmental spatial data: Theory, 

applications and software. Lausanne, Switzerland: EPFL Press.

Kaufman, L., & Rousseeuw, P. J. (1990). Finding groups in data: An introduction to cluster analysis. Hoboken, NJ: John Wiley 

& Sons.

Kirovgeologija.  (2004).  Prirodni  ta  antropogenni  dzherela  formuvannja  radioaktivnosti  prirodnih  vod  Ukra—óni  ta  radiacijnij 
zahist naselennja. Kiev, Ukraine: Derzhkomprirodresursiv Ukra—óni, Departament geologichno—ó sluzhbi, Kazenne pid‚Äê
pri—îmstvo ‚ÄúKirovgeologija.‚Äù

Kohonen, T. (2001). Self‚Äêorganizing maps (3rd ed.). Berlin, Germany: Springer.

GOVOROV et al.1076‚ÄÉ |‚ÄÉ ‚ÄÉ‚ÄÇ

Graphics, 10(4), 459‚Äì470.

Koren,  Y.,  &  Carmel,  L.  (2004).  Robust  linear  dimensionality  reduction.  IEEE  Transactions  on  Visualization  &  Computer 

LeCun, Y. A., Bottou, L., Orr, G. B., & M√ºller, K.‚ÄêR. (2012). Efficient backprop. In G. Montavon, G. B. Orr, & K.‚ÄêR. M√ºller 
(Eds.), Neural networks: Tricks of the trade (Lecture Notes in Computer Science, Vol. 7700, pp. 9‚Äì48). Berlin, Germany: 
Springer.

Lee, S., An, H., Yu, J., & Oh, J. (2014). Creating an advanced backpropagation neural network toolbox within GIS software. 

Legendre, P. (1987). Constrained clustering. In P. Legendre, & L. Legendre (Eds.), Developments in numerical ecology (pp. 

Environmental Earth Sciences, 72, 3111‚Äì3128.

289‚Äì307). Berlin, Germany: Springer.

Legendre,  P.,  Ellingsen,  K.,  Bjornbom,  E.,  &  Casgrain,  P.  (2002).  Acoustic  seabed  classification:  Improved  statistical 

method. Canadian Journal of Fisheries and Aquatic Sciences, 59(7), 1085‚Äì1089.

Leung, H., Hennessey, G., & Drosopoulos, A. (2000). Signal detection using the radial basis function coupled map lattice. 

IEEE Transactions on Neural Networks, 11(5), 1133‚Äì1151.

Li, M., Zang, S., Zhang, B., Li, S., & Wu, C. (2014). A review of remote sensing image classification techniques: The role of 

spatio‚Äêcontextual information. European Journal of Remote Sensing, 47, 389‚Äì411.

LISA LAB. (2015). Deep learning tutorial. Retrieved from http://deepl earni ng.net/tutor ial/deepl earni ng.pdf
Lu, D., & Weng, Q. (2007). A survey of image classification methods and techniques for improving classification perfor‚Äê

mance. International Journal of Remote Sensing, 28(5), 823‚Äì870.

Maier, H., Jain, A., Dandy, G., & Sudheer, K. (2010). Methods used for the development of neural networks for the pre‚Äê
diction of water resource variables in river systems: Current status and future directions. Environmental Modelling & 
Software, 25, 891‚Äì909.

Makarenko, M. M. (2000). Ocinka prirodnih i tehnogennih faktoriv zabrudnen' pidzemnih i poverhnevih vod prirodnimi 
radionuklidami  navkolo  uranovih  rodovishh  Ukra—óni.  Informacijnij  bjuleten'  pro  stan  geologichnogo  seredovishha 
Ukra—óni, 102‚Äì111, Ki—óv.

Masters,  D.,  &  Luschi,  C.  (2018).  Revisiting  small  batch  training  for  deep  neural  networks.  ArXiv  preprint,  arxiv.org/

abs/1804.07612.

Evolution, 5, 771‚Äì779.

Miele,  V.,  Picard,  F.,  &  Dray,  S.  (2014).  Spatially  constrained  clustering  of  ecological  networks.  Methods in Ecology and 

Montavon, G., Orr, G., & M√ºller, K.‚ÄêR. (Eds.) (2012). Neural networks: Tricks of the trade (2nd ed.) (Lecture Notes in Computer 

Science, Vol. 7700). Berlin, Germany: Springer.

Moon, B., Jagadish, H. V., Faloutsos, C., & Saltz, J. H. (2001). Analysis of the clustering properties of the Hilbert space‚Äêfill‚Äê

ing curve. IEEE Transactions on Knowledge Data Engineering, 13(1), 124‚Äì141.

Moran, P. A. P. (1950). Notes on continuous stochastic phenomena. Biometrika, 37, 17‚Äì23.
Oja, E. (1982). Simplified neuron model as a principal component analyzer. Journal of Mathematical Biology, 15(3), 267‚Äì273.
Oliver, M. A., & Webster, R. (1989). A geostatistical basis for spatial weighting in multivariate classification. Mathematical 

Openshaw, S. (1977). A geographical solution to scale and aggregation problems in region‚Äêbuilding, partitioning and spa‚Äê

tial modeling. Transactions of the Institute of British Geographers, 2, 459‚Äì472.

Openshaw,  S.,  &  Rao,  L.  (1995).  Algorithms  for  reengineering  1991  census  geography.  Environment  &  Planning  A,  27, 

Geology, 21(1), 15‚Äì35.

425‚Äì446.

Pag√®s, J. (2014). Multiple factor analysis by example using R. London, UK: Chapman & Hall/CRC.
Ripley, B. D. (2012). Pattern recognition and neural networks (2nd ed.). Cambridge, UK: Cambridge University Press.
Rosenblatt,  F.  (1958).  The  perceptron:  A  probabilistic  model  for  information  storage  and  organization  in  the  brain. 

Rousseeuw, P. J. (1987). Silhouettes: A graphical aid to the interpretation and validation of cluster analysis. Journal of 

Psychological Review, 65(6), 386‚Äì408.

Computational & Applied Mathematics, 20, 53‚Äì65.

Sagan, H. (1994). Space‚Äêfilling curves. Berlin, Germany: Springer.
Salah,  M.  (2017).  A  survey  of  modern  classification  techniques  in  remote  sensing  for  improved  image  classification. 

Journal of Geomatics, 11(1), 118‚Äì138.

Santibanez,  S.,  Kloft,  M.,  &  Lakes,  T.  (2015).  Performance  analysis  of  some  machine  learning  algorithms  for  regres‚Äê
sion  under  varying  spatial  autocorrelation.  In  Proceedings  of  the  18th  AGILE  International  Conference  on  Geographic 
Information Science. Lisbon, Portugal: AGILE.

Shi, X. (2010). Selection of bandwidth type and adjustment side in kernel density estimation over inhomogeneous back‚Äê

grounds. International Journal of Geographical Information Science, 24(5), 643‚Äì660.

Silva, M., Horta, I., Leal, V., & Oliveira, V. (2017). A spatially‚Äêexplicit methodological framework based on neural networks 

to assess the effect of urban form on energy demand. Applied Energy, 202, 386‚Äì398.

GOVOROV et al.‚ÄÉ‚ÄÇ ‚ÄÉ| ‚ÄÉ1077

Simbahan, G., & Dobermann, A. (2006). An algorithm for spatially constrained classification of categorical and continuous 

Stroyny, A. L., & Rowe, D. B. (2002). A re‚Äêexamination of some popular latent factor estimation methods (Working Paper). 

Su, T.‚ÄêC. (2016). A filter‚Äêbased post‚Äêprocessing technique for improving homogeneity of pixel‚Äêwise classification data. 

soil properties. Geoderma, 136(3&4), 504‚Äì523.

Milwaukee, WI: Marquette University.

European Journal of Remote Sensing, 49, 531‚Äì552.

smoothing. Journal of Soil Science, 23, 222‚Äì234.

Webster,  R.,  &  Burrough,  P.  (1972).  Computer‚Äêbased  soil  mapping  of  small  areas  from  sample  data:  II.  Classification 

Wu, T., & He, K. (2018). Group normalization. arXiv preprint, arXiv:1803.08494.
Xu, P., & Tirthapura, S. (2014). Optimality of clustering properties of space‚Äêfilling curves. ACM Transactions on Database 

Systems, 39(2), 1‚Äì27.

How to cite this article: Govorov M, Beconytƒó G, Gienko G, Putrenko V. Spatially constrained regionalization 

with multilayer perceptron. Transactions in GIS. 2019;23:1048‚Äì1077. https ://doi.org/10.1111/tgis.12557 

GOVOROV et al.