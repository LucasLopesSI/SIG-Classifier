GeoInformatica 10:1, 37–53, 2006
# 2006 Springer Science + Business Media, Inc. Manufactured in The Netherlands.

Detecting Roads in Stabilized Video with the
Spatio-Temporal Structure Tensor

ROBERT PLESS
Department of Computer Science and Engineering, Washington University, One Brookings Dr., St. Louis,
MO, USA
E-mail: pless@cs.wustl.edu

Received 22 March 2005; Accepted 29 June 2005

Abstract

Video provides strong cues for automatic road extraction that are not available in static aerial images. In
video from a static camera, or stabilized (or geo-referenced) aerial video data, motion patterns within a
scene enable function attribution of scene regions. A Broad,^ for example, may be deﬁned as a path of
consistent motionVa deﬁnition which is valid in a large and diverse set of environments. The spatio-
temporal structure tensor ﬁeld is an ideal representation of the image derivative distribution at each pixel
because it can be updated in real time as video is acquired. An eigen-decomposition of the structure tensor
encodes both the local scene motion and the variability in the motion. Additionally, the structure tensor ﬁeld
can be factored into motion components, allowing explicit determination of trafﬁc patterns in intersections.
Example results of a real time system are shown for an urban scene with both well-traveled and infrequently
traveled roads, indicating that both can be discovered simultaneously. The method is ideal in urban trafﬁc
scenes, which are the most difﬁcult to analyze using static imagery.

Key words:

statistical video processing, stabilized video, aerial video

1.

Introduction

Automatically populating databases with current information about road networks is
important in the automatic acquisition and update of geographic information systems
(GIS). Both civic planning and tactical response to emergency situations require such
data to reﬂect current conditions. The extraction of roads from image data has led to
signiﬁcant scientiﬁc inquiry within the Computer Vision community developing tools
for scale-invariant detection of features amidst signiﬁcant and highly varied
background clutter. The complexity of this problem requires image analysis systems
to include signiﬁcant semantic modeling, allowing context based reasoning to be used
in image areas with ambiguous image data. Hinz states:

is clear that] detailed semantic modeling, contextual reasoning, and self
[It
diagnosis . . . must be integral parts of an extraction system to attain reasonably
good results over a variety of scenes [9]

While this assertion may be valid for extracting roads from single images, we argue
here that the ambiguities may be mitigated in the analysis of video data from a scene.

38

PLESS

Video data is particularly beneﬁcial for urban scenes, where roads tend to be more
difﬁcult to identify from a single image but there is a high trafﬁc volume and
therefore consistent motion cues.

Historically, several problems have limited the use of video data in photo-
grammetryVthe relatively low resolution of video and the massive and highly
redundant form of the data set. These problems have been ameliorated with the wider
availability of megapixel video cameras and algorithmic advances that allow real time
stabilization (registering a video to an internally consistent coordinate system or geo-
registration) and anomaly detection as tools for extracting efﬁcient representations of
the data. For the remainder of this paper we will assume that the video has been
stabilized, so that motion within the video is caused by (1) objects moving in the
scene, (2) the background motion of ﬁxed objects in the scene (trees, water motion),
or, in the case of aerial video, (3) residual (unstabilized) motions of static objects that
are signiﬁcantly above the ground plane and thus not stabilized.

This work is inspired by recent work in video surveillanceVanomaly detection
algorithms that are effective at modeling consistent background motions (eg., trees
waving in the wind, water waves, or consistent trafﬁc patterns) in order to trigger an
alarm or to save data when an unusual event occurs or an object moves through the
scene in an unusual manner [15]. The construction of these models, which are
intended to capture the typical behavior of a scene, turns out to be an ideal pre-
processing and video-data summarization step in terms of identifying roads in a
scene. This approach is compelling for a number of reasons:

(cid:1) The anomaly detection method is based on capturing the joint distribution of
spatio-temporal image derivatives instead of continuously tracking objects and
modeling typical trajectories. Therefore the data can be generated by many short
time sequences (at least pairs of images) and does not require continuous imaging
of the same area. This gives ﬂexibility in the data capturing process and allows road
extraction data analysis to be piggybacked on data captured for other purposes
(such as aerial surveillance).

(cid:1) The processing of the video data gives, for each pixel or small image region, the
best ﬁtting motion direction, and a measure of how consistent the image derivatives
are with a single motion direction. This serves as a pre-processing step that may
provide features to support many annotation tasks and can be integrated with tools
such as snakes, condensation, and particle ﬁltering.

(cid:1) It is also possible to automatically discover patterns of motion within the scene that
indicate, for example, different ﬂow patterns of trafﬁc through a trafﬁc intersection.
(cid:1) This method is effective at seeding static image analysis methods. Detecting roads
based on consistent motion patterns is highly effective, but only for roads with
visible trafﬁc. The parameters of the roads detected in this manner (the image size,
color, typical curvature, etc) can be used to seed image based methods with
parameters speciﬁc to the given data set, in order to detect the remaining roads in
the scene.

(cid:1) Finally, the representation may be augmented to capture additional information
depending on the length of time a scene is observed, including the volume or
frequency of travel along the road and the distribution of vehicle speeds.

ROAD DETECTION FROM STABILIZED VIDEO

39

The following section attempts to place this work in the context of recent
approaches to road extraction. Section 3 introduces the real-time approach to spatio-
temporal image processing and techniques to maintain a representation of the motion
distributions at each pixel. Section 4 gives implementation details for using this
representation for road detection and a demonstration in an urban scene. Section 5
illustrates a natural extension to the technique to discover motion patterns which
allows classiﬁcation of trafﬁc intersections, and a short discussion follows.

2. Background

Many of the systems for road extraction can be categorized in terms of their (1) front
end sensors, (2) initial data ﬁltering and analysis at the pixel or local level, and (3)
methods to deﬁne extended paths on the basis of initial image data. Here we present
a sparse survey of recent literature on road extraction methods as a means of putting
our proposed approach into context. Although our approach is deﬁned explicitly in
subsequent sections, for comparison purposes,
it would be categorized in this
framework as using (1) aerial video and (2) extended spatiotemporal ﬁltering. We
are explicitly agnostic about the third component, and emphasize that the front end
processing we propose can be used within existing multiresolution, active testing, or
snake based models for detecting roads.

Geman and Jedanyk discuss road extraction from satellite imagery [7]. They argue
that immediately classifying pixels as Broad^ or Bbackground^ is infeasible because
the local region surrounding a road pixel and a background pixel may appear identical
(even in multi-spectral LandSat imagery). Thus they propose a particular brightness
invariant local operator and use an active testing approach that follows the road
appearance and path by minimizing an entropy measure. Additional methods improve
road detection by integrating larger local windows of image appearance. Exemplars
for this approach advocate multiscale analysis for the extraction of road network from
multi-spectral imagery [22], and using snakes as a method of ﬁnding long regions that
are straight or curve slowly [12], or the combination of multi-spectral imaging and a
selforganizing road map which preferentially converge to smooth road paths [4]. A
stochastic representation of road appearance based on snakes has been proposed to
take advantage of multiple images for shape optimization and change detection [1].
Focusing more on the data analysis at the pixel level, Porikli proposes a set of line-
ﬁlters that measures both how likely a particular pixel is a road, as well as the
direction of the possible road at that pixel. As the algorithm progresses, the ends of
currently detected roads can be extended to areas that have a very low likelihood of
being a road, as long as they have the correct orientation [16]. Related methods deﬁne
the road as a probabilistic contour, and use color and gradient information to extend
contours across occlusions or shadows [2].

Synthetic aperture radar (SAR) has been considered as a front end sensor to
simplify the process of road extraction. Tupin considers the problem of road
extraction in urban areas, and proposes a 2-step algorithm that extracts line features
from the speckle radar image and subsequently uses a Markov random ﬁeld to impose
contextual knowledge to cluster the detected segments into roads [19]. Wessel argues

40

PLESS

that road extraction from SAR imagery is effective for highways where there are no
scattering objects (signs, or bridges) that interfere with the road, but are ineffective in
industrial areas (which tend to always have scattering objects) or for secondary roads
(which tend to have insufﬁcient signal return).

Returning to aerial imagery, the papers most closely related to this approach
concentrate on road extraction in urban environments. Hinz points out that most work
focuses on the easier problem of rural road extraction, and existing work on urban
scenes make assumptions about
the grid structure of many city streets [5], or
combines height models and high-resolution imagery to extract streets through
residential areas [17]. To be effective in more general situations, a system is proposed
that incorporates a great deal of detailed knowledge about roads and their context,
uses explicit formulated scale-dependent models of road appearance, and continually
performs hypothesis testing to ensure that the local context information is appropriate
[9].

Finally, from the computer vision community, work on trafﬁc monitoring using a
BForest of sensors,^ is effective at creating trajectories of objects tracked through an
environment [8], [18]. While this could form the basis for an approach similar to ours,
they do not consider the problem of road extraction, and their method requires
continuous long term surveillance to build trajectories, instead of capturing and
integrating short term motion cues.

A technical issue directly related to our approach (otherwise independent of road
extraction) is that we require the input video sequence to be stabilized, so that
collecting statistics of spatio-temporal ﬁlter responses over many frames at a single
pixel gives motion information about the same scene point. Numerous algorithms for
this process have been proposed using either the tracking of feature points [23], or
based directly on spatiotemporal ﬁlter responses [3], [14]. We adapt the method used
in [14] which involves, for each frame, computing spatio-temporal ﬁlters at a sparse
set of image points, and solving for a general linear transformation (the image
warping homography) that minimizes the change from the previous frame. The
sequence of the warped images becomes the stabilized video used as input to the
algorithm described below. Alternatively, video data that is tagged with very accurate
knowledge of the 3D position and orientation of the camera in each frame may permit
warping and stabilization without additional image processing.

In summary, to our knowledge, no one has directly considered the question of using
aerial video imagery in the detection of roads. Recent advances in computational
power and algorithmic maturity make the use of video data feasible. Using video
imagery to deﬁne roads based upon motion patterns is most effective in urban
environmentsVa domain that remains particularly challenging for both image and
SAR based analysis.

3. Motion features: Spatio-temporal structure tensor

The approach is based upon spatiotemporal image analysis. This approach explicitly
avoids ﬁnding or tracking image features. Instead, the video is considered to be a 3D
function I(x,y,t), deﬁning the image intensity as it varies in space (across the image)

ROAD DETECTION FROM STABILIZED VIDEO

41

and time. The fundamental atoms of the image processing are the value of this
function and the response to spatio-temporal ﬁlters (such as derivative ﬁlters),
measured at each pixel in each frame. Unlike interest points or features, these
measurements are deﬁned at every pixel in every frame of the video sequence.
Appropriately designed ﬁlters may give robust measurements to form a basis for
further processing. Optimality criteria and algorithms for creating derivative and
blurring ﬁlters of a particular size have been developed by [6], and lead to
signiﬁcantly better results than estimating derivatives by applying Sobel ﬁlters to
raw images. For these reasons, spatio-temporal image processing provides an ideal
ﬁrst step for streaming video processing applications.

Space and time derivative ﬁlters are particularly meaningful in the context of
analyzing motion on the image. Considering a speciﬁc pixel and time (x,y,t), we can
deﬁne Ix(x, y, t) to be the derivative of the image intensity as you move in the
x-direction of the image. Iy(x, y, t), and It(x, y, t) are deﬁned similarly. At a given
pixel at a given time, and the optic ﬂow constraint equation gives a relationship
between Ix, Iy, and It, and the optic ﬂow, (the 2d motion at that part of the image) [10]:

Ixu þ Iyv þ It ¼ 0:

This classical equation in computer vision holds true for smoothly varying images,
when the motion (the magnitude of the bu, vÀ vector) is relatively small, and the only
reason that the intensity at a pixel changes is because of local motion in the image.
This gives just one equation with two unknowns (u, v) so it is not possible to directly
solve for the optic ﬂow. Many optic ﬂow algorithms therefore assume that the optic
ﬂow is constant over a small region of the image, and use the (Ix, Iy, It) values from
neighboring pixels to provide additional constraints. This assumption does not hold
true at the boundaries of objects, leading to consistent errors in the optic ﬂow
solution.

The advantage of using stabilized video for scenes with consistent motion patterns
is that instead of combining data from spatially extended region of the image, we can
instead combine equations through time. This allows one to compute the optic ﬂow at
a single pixel location without any spatial smoothing. Figure 1 shows one frame of a
video sequence of a trafﬁc intersection, and the ﬂow ﬁeld that best ﬁts the data for
each pixel over time. The key to this method is that the distribution of the spatio-
temporal intensity derivatives observed at a pixel (simply the distribution, and not, for
instance the time sequence) encodes several important parameters of the underlying
variation at each pixel.

In this section we introduce our approach to representing the distribution of image
derivatives captured at each pixel. We use the spatio-temporal structure tensor ﬁeld
(the covariance matrix of the space and time image derivatives accumulated through
time at each pixel), which has dual beneﬁts: ﬁrst, the parameters are efﬁcient to
update and maintain within a real-time application, and second, the set of parameters
for the entire image efﬁciently summarizes and encodes features of interest to GIS
applications. Furthermore, the structure tensor ﬁeld admits a natural method of
parsing the scene into multiple motion components, which are common, for example,
at intersections where trafﬁc lights enforce varied trafﬁc patterns.

42

PLESS

Figure 1. Flow estimated from 3-D structure tensor for 10 minutes of video of an intersection. Multiple
motions at the middle of the intersection cause a circular pattern in the estimated ﬂow ﬁeld. The coﬁdence
decreases in the parts of the middle of the intersection where the angle between the two major motions is
larger. Inside the intersection, the two Bquadrants^ which see motion with directions differing by more than
90- have lower conﬁdence that the other two quadrants.

3.1. Representing the distribution of derivatives

In this section we introduce the spatio-temporal structure tensor. We denote the
spatio-temporal image derivatives by the column vector lI, so that:

(cid:1)
rI p!; t

(cid:2) ¼ Ix p!; t
(cid:1)
(cid:1)

(cid:1)
(cid:2); Iy p!; t

(cid:1)
(cid:2); It p!; t

(cid:2)

(cid:2)>;

(cid:2) at pixel p! and time
are the spatio-temporal derivatives of the image intensity I p!; t
t. The covariance matrix of a particular spatial temporal measurement lI can be
written as lIlIB. We deﬁne the structure tensor (cid:1) at pixel p! to be the average of
the covariance matrices of all derivative measurements observed at that pixel:

(cid:1)

(cid:1) p!(cid:1)

(cid:2) ¼

(cid:1)
rI p!; t

(cid:1)
(cid:2)rI p!; t

(cid:2) ¼

1
f

f
X

t¼1

!

 Xf

1
f

I 2
x

IxIy

IxIt

1
Xf
1
Xf
1

Xf
1
Xf
1
Xf
1

IxIy

I 2
y

IyIt

Xf
1
Xf
1
Xf
1

IxIt

IyIt

I 2
t

where f is the number of frames in the sequence and we omit p!; t, in the matrix
shown above and hereafter for the sake of clarity. Except as described in Section 3.1.
Here we model these distributions independently at each pixel, although we recognize
that in real scenes there may be signiﬁcant correlations and in Section 5 we explicitly
reason about these correlations. To focus on scene motion, the measurements are
(cid:2) that come
ﬁltered based on their It value, only considering measurements rI p!; t
from variation in the scene. We choose to use a simple threshold, and incorporate into
the model all measurements for which |It| > 2 (a change of at least two out of 256
gray levels between consecutive frames).

(cid:1)

ROAD DETECTION FROM STABILIZED VIDEO

43

In real-time applications, batch computation over the entire sequence is not feasible
and the structure tensor must be estimated online. Assuming the distribution is
stationary, the structure tensor (cid:1)t at time t can be estimated exactly as a weighted
combination of the structure tensor at time t j 1, and the covariance matrix of the
current measurements, lIlIB:

(cid:1)t ¼

ð

Þ

n (cid:2) 1
n

(cid:1)t(cid:2)1 þ

rIrI B:

1
n

The structure tensor has a number of interesting properties that are exposed through
computation of its eigenvalues and eigenvectors. In particular, suppose that (cid:1) has
to smallest) (cid:1)1, (cid:1)2, (cid:1)3, and corresponding
eigenvalues (sorted from largest
1; v!
eigenvectors

(cid:2). The following properties hold:

2; v!

v!

(cid:1)

3

(cid:1) The vector v!

3 is a homogeneous representation of the total least squares solution

[11, 20, 21], for the optic ﬂow. The 2-d ﬂow vector ( fx, fy) can be written:

(cid:1)

fx; fy

(cid:2) ¼

(cid:3)

v3 1ð Þ
v3 3ð Þ

;

v3 2ð Þ
v3 3ð Þ

(cid:4)

(cid:1) If, for all the data at that pixel, the set of image intensity derivatives exactly ﬁts

some particular optic ﬂow, then (cid:1)3 is zero.

(cid:1) If, for all the data at that pixel, the image gradient is in exactly the same direction,

then (cid:1)2 is zero. (This is the manifestation of the classical aperture problem).

(cid:1) The consistency value C ¼ 1 (cid:2) (cid:1)3
(cid:1)2

, is a an indicator of how consistent the image
gradients are with the best ﬁtting optic ﬂow, with 1 indicating perfect ﬁt, and 0
indicating that many measurements do not ﬁt this optic ﬂow.

(cid:1) The speciﬁcity value S ¼ (cid:1)2
(cid:1)1

, varies from 0 to 1, and is an indicator of how well
speciﬁed the optic ﬂow vector is. When this number is close to 0, the image
derivative data could ﬁt a family of optic ﬂow vectors with relatively low error,
when this ratio is closer to 1, then the best ﬁtting optic ﬂow is better localized.

These are properties of the spatiotemporal structure tensor accumulated through
time at each pixel. Considering the entire image, we can use these properties to deﬁne
useful features at each pixel (x, y). Our claim is that these variables are an effective
summary of information contained in a video sequence, and that the analysis of the
following scalar, vector, and tensor ﬁelds is an effective method for extracting road
features from stabilized video:

(cid:1) (cid:1) (x, y), the tensor ﬁeld consisting of the structure tensor at each pixel,
(cid:1) bfx (x, y), fy (x, y)À, the best ﬁtting optic ﬂow ﬁeld,
(cid:1) s (x, y), the speciﬁcity of the optic ﬂow solution, and
(cid:1) c (x, y), the consistency of the optic ﬂow solution.

The next section gives implementation details that indicate how to use these variables
to detect roads.

44

PLESS

4.

Implementation details

A formal evaluation is limited by the lack of standardized test data and the absence of
other algorithms which compute GIS features from stabilized aerial video. To address
this problem in the future, the code and data sets presented in the following section
are publicly available.1 Here we completely deﬁne the approach taken for an example
data set, indicate the results, point out limitations and indicate areas of potential
future research.

4.1. Methods

We have found that the method is quite robust to various implementation choices,
but for concreteness we describe here the exact choices used in the results.
Consecutive pairs of images from the video sequence are decompressed to create
2D arrays of intensity values. The image is convolved with a discrete 11 by 11
ﬁlter approximating a Gaussian with standard deviation of three pixels to create a
blurred image. The Ix and Iy values are computed with appropriately oriented Sobel
ﬁlters convolved with the blurred image. The It value is estimated as the difference
between pixel values in consecutive frames. This (Ix, Iy, It) measurement
is
maintained for every pixel in the image, but is ignored at pixels whose distance
from the boundary is less than 6 pixels, as the results of the convolution ﬁlters at
these points depends upon assumptions about pixel values outside the image. At
2,
each pixel, a 3 (cid:3) 3 covariance matrix is maintained by storing 7 parameters, ((cid:1)Ix
2, (cid:1)IxIy, (cid:1)IxIt, (cid:1)IyIt, n). To isolate the effects of image motion from intensity
(cid:1)Iy
gradients that exist in the static image, these sums and the value of n are only updated
when |It| > 2. The number n records the number of measurements that have been used
in each of the sums. We emphasize that the above sums are taken through time, and
these parameters are recorded separately for each pixel, so some pixels may have
more measurements that deﬁne their covariance matrix than others.

2, (cid:1)It

4.2. Results

SC(cid:1)I 2
t ;

The original image and several of the results of the preprocessing methods described
above are shown for a 451 frame stabilized aerial video. This video is taken at
approximately 3 frames per second. The video was shot from an aerial platform and
georegistered. Two frames of the geo-registered video are shown at the top of Figure
2, the black areas at the bottom corners arise because these images are warped to the
coordinate system of the reference frame and these areas were outside the image. For
each pixel, a score is calculated to measure how likely that pixel is to come from a
road. This score function is:

which is the intensity variance at that pixel, modulated by the previously deﬁned
scores that measure how well the optic ﬂow solution ﬁts the observed data (C) and

ROAD DETECTION FROM STABILIZED VIDEO

45

Figure 2. The top row shows frames 1 and 250 of a 451 frame stabilized aerial video (approximately 2:30
minutes long, 3 frames per second). The black in the corners are areas in this geo-registered frame that are
not captured in these images, these areas are in view for the majority of the sequence. The bottom right
shows the amount of image variation modulated by the motion consistencyVa measure of how much of the
image variation is caused by consistent motion (as would be the case for a road). These scores are overlayed
onto an original image in the bottom left.

how unique that solution is (S). This score is thresholded (threshold value set by
hand), and overlayed on top of the original image in the bottom left of Figure 2.

Road are detected in image regions where cars are visible moving during the video
input, including regions where image based detection systems would fail, such as the
upper of the two curved roads in the middle of the image. This justiﬁes the earlier
assertion that this system is ideal in urban areas, where the image based cues are less
clear but where the frequency of trafﬁc is sufﬁcient that motion cues are often available.
However, the motion cues provide more information than simply a measure of
whether the pixel lies on a road. The best ﬁtting solution for the optic ﬂow also gives
the direction of motion at each pixel. The components of the motion vectors are
shown as the top row of Figure 3. There is signiﬁcant noise in this motion ﬁeld
because of substantial image noise and the fact that for some roads the data included
few moving vehicles. A longer image sequence would provide more data and make
ﬂow ﬁelds that are well constrained and largely consistent. The method would
continue to fail in regions that contain multiple different motion directions or where
the optic ﬂow constraint equations fail. The next section introduces a post-processing
step to make the ﬂow ﬁeld analysis feasible with shorter video sequences.

46

PLESS

Figure 3. The top row show the x and y components of the best ﬁtting optic ﬂow vectors for the pixels
designated as roads in Figure 2. The ﬂow ﬁelds are poorly deﬁned, in part because of noisy data, and in part
because there were few cars that move along some roads. These (poor) ﬂow estimates were used to deﬁne
the directional blurring ﬁlters that combine the image intensity measurements from nearby pixels (forward
and backwards in the direction of motion). Using the covariance matrix data from other locations along the
motion direction gives signiﬁcantly better optic ﬂow measurements (bottom row). In these images, black is
negative and white is positive, relative to the origin in the top left corner of the image.

4.3. Post-processing

In the presence of noise within the ﬂow ﬁelds, it is necessary to combine information
between nearby pixels. Typically, combining information between pixels leads to
blurring of the image and a loss of ﬁdelity of the image features. However, the ﬂow
ﬁeld extracted gives a best ﬁtting direction of travel at each pixel. We use this as a
direction in which we can combine data without blurring featuresVthat is, we use the
noisy estimate of the motion as a cue to help combine data along the roads, rather
than across roads.

In particular, we adapt previous work on smoothing ﬂow ﬁelds, called motion
oriented averaging [13], but instead of averaging optic ﬂow estimates, we compute
the weighted average of nearby covariance matrices. This process ﬁrst deﬁnes a local
weighting function at each pixel by computing a normalized direction vector bdx, dyÀ.
A spatial covariance matrix M is created with this vector as the principle Eigenvector,
in order to deﬁne a Gaussian weighting function w which is oriented so that larger

ROAD DETECTION FROM STABILIZED VIDEO

47

values lie along the direction of motion. More speciﬁcally, we follow the following
algorithm:

Algorithm:

1. Compute a normalized vector in the direction of motion

(cid:5)
dx; dy

(cid:6) ¼

(cid:5)

q

(cid:6)

Þ; fy x; yð
Þ

fx x; yð
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
Þ þ f 2
f 2
y x; yð
x x; yð
Þ

2. Deﬁne weights with higher weight along orientation of motion

T ¼

"

30dx

30dy

#

(cid:2)3dy

3dx

M ¼ T BT

w a; bð

Þ ¼ e(cid:2) a;bh

i

B

M (cid:2)1 a;bh

i

3. Compute weighted average of nearby structure tensors
X

X

w a; bð

Þ(cid:1) x þ a; b þ y
Þ

ð

b(cid:1)(cid:1) x; yð

Þ ¼

a¼(cid:2)5...5

b¼(cid:2)5...5

This process serves to reduce noise in the estimates of the structure tensor without
average across different directions of trafﬁc. The bottom row of Figure 3 depicts the
ﬂow ﬁeld components derived from the smoothed covariance matrix ﬁeld (the third
eigenvector of the matrices b(cid:1)(cid:1) x; yð
Þ): These ﬁelds are signiﬁcantly cleaner than the
ﬂow ﬁeld computed from the covariance matrices without smoothing and show
clearly deﬁned directions of travel for all roads which had motion. Furthermore, for
large roads with signiﬁcant motion cues, there is a clearly deﬁned separation between
different directions of travel. The motion patterns at the intersections, however,
cannot be cleanly described with a single model. The following section works to
consider the analysis of these regions with multiple motion patterns.

4.4. Analysis of results and algorithmic efﬁciency

While we believe that the analysis of stabilized aerial video will be increasingly
important in the future, there is currently a paucity of publicly available sample
imagery for testing of this approach. However, in this section we attempt to identify
the limitations of our approach with respect to various parameters of the input video.
This approach uses motion to detect road locations. Therefore the video duration
must be long enough so that some vehicle is seen moving along each road in the scene.
Furthermore, the distance a car travels per frame must be less than half the size of the
ﬁlters (described at the beginning of Section 3) used to estimate the spatio-temporal
image derivatives, otherwise the ﬁlter responses are not correlated with object motion.
Additionally, apparent scene motion not due to vehicles is mis-categorized as roads (for
example, residual motion for poorly registered aerial video or specular reﬂections off of
drainage canals often lead to apparent motion).

48

PLESS

Figure 4.
Isolating the top of the video sequence, we show the score function from Equation 1, as
computed when using 4, 20, 50, 100, 200, 300, or 450 consecutive frames of an input sequence taken at 2
frames per second. Notice the dotted appearance of the top horizontal road after just a few frames, and the
decrease in the score function for the left-most vertical road.

Our implementation of the algorithm as described through Section 4, continuously
updating the structure tensor and computing the scoring function (Equation 1), runs in
real time (30 fps for a 512 by 512 image size) on commodity hardware (a 2.0 GHz
Pentium PC). The post-processing step of Section 4.3 takes several seconds but is only
done once per video, and the running time depends only on the size of the image, not the
length of the video. This efﬁciency implies that there is no algorithmic limit to the
maximum length of the input video.

Figure 4 shows the effect of using progressively more frames of video. For densely
travelled urban scenes, even 4 frames of video is sufﬁcient to detect consistent
motions patterns along some roads. Also notice that the score function may decrease
over time. In the ﬁgure, a car passes along vertical road on the left side of the ﬁgure
within the ﬁrst 20 frames, but over the subsequent hundreds of frames, other image
variation (largely due to clouds) decreases the road detection score because the image
derivative distribution is no longer consistent with just one motion direction. The
initial dotted appearance of the top road is due to individual vehicles that move too
far between frames to see continuous motionVas more vehicles pass through the
same region of road this is ﬁlled in. The dotted appearance of the curved road in the
bottom middle of the scene is due to partial occlusion from trees that obscure the ve-
hicles at some locations.

5. Motion patterns: Mixtures of structure tensor ﬁelds

The last section labels pixels based on the local distribution of spatio-temporal
intensity derivatives. In this section we consider the case that there are multiple global

ROAD DETECTION FROM STABILIZED VIDEO

49

motion patterns in a scene (such as in a trafﬁc intersection). Factoring the scene into
coherent motion patterns allows more accurate motion estimates in areas such as
trafﬁc intersections. This can be achieved by considering the image derivatives from
all pixels in a given frame as a single measurement vector and using adaptive mixture
models to ﬁnd a small number of models that account for all the data [24].

The structure tensor, as described in the last section, is the covariance matrix, (cid:1), of
the measurements, lI, at each pixel. Assuming independence between pixels (a false
assumption, but one which makes the mathematics tractable), then the structure tensor
ﬁeld may be considered as a single joint Gaussian, Nglobal, over the entire image. If (cid:1)i
is the structure tensor at the i-th pixel, the covariance matrix of the global distribution
is the block-diagonal matrix:

e(cid:1)(cid:1)global ¼

(cid:1)1

0

B
B
B
B
B
B
B
@

0

0

(cid:1)2

.

. .

1

C
C
C
C
C
C
C
A

(cid:1)p

This deﬁnes a distribution over the set of all image derivative measurements in the
image at a particular frame. This measurement can be written as erI, the concatenation
of the gradient vector at each individual pixel: erI ¼ (cid:1)I 1ð Þ
; . . .(cid:2): If the
motions visible in the scene come from multiple motion patterns, then the image
derivative measurements, erI, do not derive from just one global model, e(cid:1)(cid:1) , but rather
from several. We solve for these several motions models using the adaptive mixtures
framework, an online method which maintains several global motion models and
updates the models with new data by an amount proportional to the likelihood that the
new data comes from each model2. Given a model, N global, then the likelihood of
observing global image derivatives, erI, is:

; I 1ð Þ
t

; I 1ð Þ
y

; I 2ð Þ
x

x

(cid:9)

(cid:8)
P erI N global
(cid:8)

(cid:10)

¼ k exp (cid:2)

erI B

e(cid:1)(cid:1)(cid:2)1

global erI

(cid:4)

(cid:3)

1
2

where k is a normalizing constant. Because e(cid:1)(cid:1) is block diagonal, this can be rewritten
as:

(cid:9)

(cid:8)
P erI N global
(cid:8)

(cid:10)

¼

Y

P rIi N i 0; (cid:1)i
j

ð

ð

Þ

Þ;

i

which allows this computation to be efﬁcient. The adaptive mixture model deﬁnes the
distribution of measurements as the sum of a collection of Gaussian distributions:

(cid:9)
w1N 1 0; e(cid:1)(cid:1)1

(cid:10)

þ (cid:4) (cid:4) (cid:4) þ wM N M 0; (cid:1)M

ð

Þ

where M is the number of models. The adaptive mixture model update equations (as
used, for example, in Stauffer and Grimson [18]) allow this model to be automatically
acquired. Here it is applied to a very high-dimensional distribution (with a special

50

PLESS

block diagonal structure). Updating the mixture model online requires ﬁrst calculating
the likelihoods:

(cid:9)

(cid:10)

P N i erI

¼

(cid:8)
(cid:8)
(cid:8)

j

(cid:10)

(cid:9)
wiP erI N i
M
X

(cid:9)
wjP erI N j

(cid:8)
(cid:8)

;

(cid:10)

j¼1

and then updating each ﬁelds as:

e(cid:1)(cid:1)i;t ¼ 1 (cid:2) (cid:2)i
ð

Þe(cid:1)(cid:1)i;t(cid:2)1 þ (cid:2)i erI erI B

with a weighting factor (cid:2)i ¼ P(cid:1)N ijerI(cid:2), which is proportional to the probability that
the correct model. The complete update of the adaptive mixture model requires
N i
that the weights of the components be adjusted. The weights wi can be updated as wi,t
= (1 j (cid:2)i)wi,t j 1 + (cid:2)i. It is important to note that each component of the mixture
model retains the block diagonal, so that the constraints on the derivative measure-
ments at each pixel are independent, except in their contribution to determining the
likelihoods that each model ﬁts the global data.

Figure 5 shows the results of a real time system applying this adaptive mixture
model to a 10 minute video scene dominated by an intersection. The four components

Figure 5. Coherent motion ﬁelds [24] serve to automatically parse complicated intersection ﬂow ﬁelds
into their components trafﬁc patterns. Four mixture components are estimated by applying adaptive mixture
models to the distribution of global image derivative measurements. This serves as a cue that may support
high ﬁdelity annotation of types of intersections.

ROAD DETECTION FROM STABILIZED VIDEO

51

of the mixture model deﬁne distributions over the global image derivative measure-
ments. Decomposing each mixture model into its block diagonal components, and
solving for the best ﬁtting optic ﬂow of each component, gives a visualization of the
component ﬂow ﬁelds within this scene. Notice that these ﬂow ﬁelds indicate that this
intersection has one trafﬁc directions whose light cycle is Bstraight in both directions
then a left turn only^ N 1; N 4
Þ, and the other direction of travel is Bgo straight and
turn left then (in the reverse direction) go straight and turn left^ N 2; N 3
Þ: This
decomposition derives from video data alone, and does not require any synchroni-
zation with the trafﬁc light.

ð

ð

6. Discussion

This paper has presented a robust algorithm for the preprocessing of stabilized video,
marking pixels whose intensity variation is consistent with motions along a road.
Furthermore, the direction of travel can be accurately identiﬁed, and varied patterns
of motion may possible help characterize intersections. The motion cues are stronger
for regions with signiﬁcant trafﬁc, and therefore most useful in urban settings. These
settings are very challenging for current approaches based on the analysis of static
images because of the typical complexity of the background.

6.1. Future work

including joining motion based
Many avenues are open for future research,
algorithms with more standard appearance based algorithms, as well as ﬁtting the
motion cues into larger systems that output symbolic representations of roads rather
then pixel scores. Concrete future directions include:

(cid:1) Integrate appearance or motion models speciﬁc to intersections, parking lots, and

other features of interest.

(cid:1) Use the motion based analysis of motion to bootstrap appearance based road modeling.
For example, in Figure 2, not all roads in the scene are discovered because some roads
did not have cars pass along them during the input video. However, statistical ap-
pearance models of the roads identiﬁed through motion cues would give a scene-
speciﬁc road appearance model to ﬁnd those roads that were missed.

(cid:1) Adapt snake based road following algorithms (such as Laptev et al. [12]) to
incorporate the ﬂow ﬁeld direction in the energy function minimized by the snake.
(cid:1) Design algorithms that close the loop between region interpretation and anomaly

detection and detect unusual events such as cars stopping in unusual places.

Acknowledgment

This work was supported under NSF grant IIS-0413291.

PLESS

52

Notes

References

1. http://www.cse.wustl.edu/~pless/videoGIS.html
2. For clarity, this presentation ignores special case issues in initializing the mixture model and identifying
data that comes from none of the current models. We use exactly the approaches deﬁned (for another
application) in Stauffer and Grimson [18].

1. P. Agouris, A. Stefanidis, and S. Gyftakis. BDifferential snakes for change detection in road segments,’’

Photogrammetric Engineering & Remote Sensing, Vol. 67(12):1391Y1399, 2001.

2. M. Bicego, S. Dalﬁni, G. Vernazza, and V. Murino. BAutomatic road extraction from aerial images by
probabilistic contour tracking,’’ in Proceedings of IEEE International Conference on Image Processing
(ICIP), vol. III, pp. 585Y588, 2003.

3. X.-T. Dai, L. Lu, and G. Hager. BRealtime video mosaicing with adaptive parameterized warping,’’ in
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, volume (Demo
Program), 2001.

4. P. Doucette, P. Agouris, A. Stefanidis, and M. Musavi. BSelf-organized clustering for road extraction in
classiﬁed imagery,’’ ISPRS Journal of Photogrammetry and Remote Sensing, Vol. 55(56):347Y358,
2001.

5. A. Faber and W. Forstner. BDetection of dominant orthogonal structures in small scale imagery,’’

International Archives of Photogrammetry and Remote Sensing, 33(Part B3/1):274Y281, 2000.

6. H. Farid and E.P. Simoncelli. BOptimally rotationequivariant directional derivative kernels,’’ in

Computer Analysis of Images and Patterns (CAIP), pp. 207Y214, 1997.

7. D. Geman, and B. Jedynak. BAn active testing model for tracking roads in satellite images,’’ IEEE

Transactions on Pattern Analysis and Machine Intelligence, Vol. 18(1):1Y14, 1996.

8. W.E.L. Grimson, C. Stauffer, R. Romano, and L. Lee. BUsing adaptive tracking to classify and monitor
the IEEE Conference on Computer Vision and Pattern

activities in a site,’’ in Proceedings of
Recognition, pp. 22Y27, 1998.

9. S. Hinz, and A. Baumgartner. BAutomatic extraction of urban road networks from multiview aerial

imagery.’’ ISPRS Journal of Photogrammetry and Remote Sensing, 53:83Y98, 2003.

10. B.K.P. Horn. BRobot Vision.’’ McGraw Hill: New York, 1986.
11. S. Van Huffel and J. Vandewalle. BThe Total Least Squares Problem: Computational Aspects and

Analysis,’’ Society for Industrial and Applied Mathematics, Philadelphia, 1991.

12. I. Laptev, H. Mayer, T. Lindeberg, W. Eckstein, C. Steger, and A. Baumgartner. BAutomatic extraction
of roads from aerial images based on scale space and snakes,’’ Machine Vision and Applications, Vol.
12(1):23Y31, 2000.

13. H.H. Nagel. BExtending the Foriented smoothness constraint_ into the temporal domain and the
estimation of derivatives of optical ﬂow,’’ in Proceedings of the First European Conference on
Computer Vision, Springer: Berlin Heidelberg New York, Inc., pp. 139Y148, 1990.

14. R. Pless, T. Brodsky, and Y. Aloimonos. BDetecting independent motion: The statistics of temporal
continuity,’’ IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. 22(8):68Y73, 2000.
15. R. Pless, J. Larson, S. Siebers, and B. Westover. BEvaluation of local models of dynamic
backgrounds,’’ in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 73Y78, 2003.

16. F.M. Porikli. BRoad extraction by pointwise gaussian models,’’ in SPIE AeroSense Technologies and

Systems for Defense and Security, vol. 5093, pp. 758Y764, 2003.

17. K. Price. BUrban street grid description and veriﬁcation,’’ in IEEE Workshop on Applications of

Computer Vision, pp. 148Y154, 2000.

18. C. Stauffer, and W.E.L. Grimson. BAdaptive background mixture models for realtime tracking,’’ in
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 246Y252, 1999.
19. F. Tupin, H. Maitre, J.F. Mangin, J.M. Nicolas, and E. Pechersky. BDetection of linear features in SAR
images: Application to the road network extraction,’’ IEEE Transactions on Geoscience and Remote
Sensing, Vol. 36(2):434Y453, Mar. 1998.

ROAD DETECTION FROM STABILIZED VIDEO

53

20. S. Wang, Y. Markandey, and A. Reid. BTotal least squares ﬁtting spatiotemporal derivatives to smooth
optical ﬂow ﬁelds,’’ in Proc. of the SPIE: Signal and Data Processing of Small Targets, vol. 1698,
pp. 42Y55. SPIE, 1992.

21. J. Weber, and J. Malik. BRobust computation of optical ﬂow in a multiscale differential framework,’’

International Journal of Computer Vision, Vol. 14:67Y81, 1995.

22. C. Wiedemann, C. Heipke, H. Mayer, and S. Hinz. BAutomatic extraction and evaluation of road
networks from moms-2p imagery,’’ International Archives of Photogrammetry and Remote Sensing,
Vol. 32(3): 285Y291, 1998.

23. L. Wixson. BDetecting salient motion by accumulating directionallyconsistent ﬂow,’’ IEEE Trans-

actions on Pattern Analysis and Machine Intelligence, Vol. 22(8):774Y780, 2000.

24. J. Wright, and R. Pless. BAnalysis of persistent motion patterns using the 3d structure tensor,’’ in

Proceedings of the IEEE Workshop on Motion and Video Computing, pp. 14Y19, 2005.

is currently an Assistant Professor of Computer Science and Assistant Director of the
Robert Pless
Center for Security Technologies at Washington Univerisity in St. Louis. A native of Baltimore, Maryland,
he graduated from Cornell University with a Bachelor of Science and Computer Science in 1994, and
received a Ph.D. in Computer Science from the University of Maryland in 2000. His ﬁeld of research is
Computer Vision, with a concentration in extreme camera geometries, panoramic vision, surveillance, and
manifold learning, and he served as chairman of the 2003 IEEE International workshop on Omni-directional
Vision and Camera Networks (Omnivis _03).

