GeoInformatica
https://doi.org/10.1007/s10707-020-00399-7

Parallel discriminative subspace for city target
detection from high dimension images

Yipeng Zhang 1 & Yiming Zhang 2 & Bo Du 1 & Chao Zhang 1 & Xiaoyang Guo 1 &
Weiping Tu 1

Received: 28 February 2019 / Revised: 11 December 2019
Accepted: 26 January 2020

# Springer Science+Business Media, LLC, part of Springer Nature 2020

Abstract
City Target Detection is an enduring problem that intrigues the researchers all over the
world. The great success of existing Target Detection algorithm appears in ubiquitous
scenarios: Pedestrian Detection, Vehicle Tracking, etc. However, as for the city target
detection in the remote sensing, we are facing with two inevitable problems: Complex
Environment and Massive Information. The complicated environment encumbers the
accurate extraction of the target profile, and the huge amount of information turns it into a
heavy workload to get the final outcome for the conventional CPU- compiler architecture.
In this paper, we propose a binary hypothesis framework based on adaptive dictionary
and discriminative subspace for hyperspectral city target detection (BHADDS). Further-
more, we have also implemented it on other hardware platform alongside with CPU, such
as FPGA. FPGA is a low-power portable and programmble SoC, and also the protocol
model for potential massive production of the SoC chipset. Our eventual aim is heading
for the high-performance processor with strong instant processing ability for remote
sensing. In the final part of the paper, we have given a comprehensive performance
comparison over the different platforms and summarized their applicable scenarios.

Keywords Target detection . Remote sensing . Parallel computing . FPGA

Yipeng Zhang and Yiming Zhang contributed equally to this work.

* Bo Du

gunspace@163.com

* Weiping Tu

tuweiping@whu.edu.cn

Extended author information available on the last page of the article

GeoInformatica

1 Introduction

Recent years have witnessed the development of hyperspectral remote sensing (HSIs) with
super high spectral resolution. Unlike the natural images that only covers the human visual
spectral range, hyperspectral images owns hundreds or even thousands of narrow consecutive
bands [2], which reflects the abundance of complicated information in each pixel [19, 51, 55].
Target detection in HSI [18] can be interpreted as a binary classification problem to discriminate
whether a specific target texture is presented in a single pixel, according to its unique spectral
signature [7, 52]. It has been widely applied in both civil and mining fields, such as detecting rare
minerals in resources investigation, identifying ancient artifacts,and so on [27]. Given the available
target spectral signature, which is obtained from the hyperspectral images or a standard spectral
library [53], many classic algorithms [30] have been proposed and have shown great success in
target detection [29, 31], such as orthogonal subspace projection (OSP) [10, 34], which achieves
the elimination of undesired background signatures by projecting each pixel’s spectral vector onto
a subspace orthogonal to the undesired signatures [3]; matched subspace detector (MSD) [36, 42],
which assumes that both hypotheses obey gaussian distribution with the same scaled identity
covariance matrix and only differ on their means; the adaptive coherence/cosine detector (ACE)
[28], which estimates the noise covariance structure using sample covariance matrices; constrained
energy minimization (CEM) [4, 12], which maximizes the response of the target spectral signature
while suppressing the resonance of the unknown background signatures [50].

Many algorithms are proven to be successful in the elimination of the dataset complexity,
such as the non-negative matrix factorization, dictionary learning, sparse coding, and so on,
however, there is lack of theoretic model to define the precise generalization bounds. Thus, we
are unable to get the most featured vector to represent the original matrix with minimum loss.
This work [20] publicizes in great detail about the their managements to tight the generalization
bounds from the view of K-dimensional vectors, giving us theoretic support and motivation to
explore a more precise closed form solution with even minor residuals. Besides, concerning that
there occasionally happens to have contaminated or inaccurate labels, some works [21] are set
up to augment the correctness of the labels by reweighting the importance of corresponding
feature, depending on how convincing is the dataset according to probabilistic distribution.

Sparse representation is assured to be an efficient approach for the model reconstruction [11,
26]: an unknown signal can be expressed as a sparse combination of atoms in the over-completed
dictionary with tolerable residuals [48, 49]. Thus, the substance to be determined can be interpreted
as the mixture of materials which are already known. However, with the extensive growth in the
spectral bands, there is tremendous data redundancy in the hyperspectral spectrum. Therefore, the
dimension of the dataset should be reduced and then transferred to the target domain to get the
eigenvector. Thus, we can get the sparse representation of the input signal and analyze the
components contained in the pixel. The algorithms based on sparsity model have made much
great progress, such as sparse representation for target detection (STD) [47], which sparsely
represents a test image by a few training samples [23] and directly employs the approximation
residuals to perform the detection; simultaneous joint sparsity model [6] for target detection
(SSTD) [54], which incorporates the interpixel correlation [56] within the sparsity model, assum-
ing that neighboring pixels [24] usually share similar materials; a new sparse representation-based
binary hypothesis (SRBBH) model [25] for hyperspectral target detection, which relies on the
binary hypothesis model of an unknown sample induced by sparse representation; the joint sparse
representation [5] and multitask learning (JSR- MTL) method [56], which integrates the multitask
learning technique with the sparse representation model for the HSIs [13].

GeoInformatica

Nevertheless, there is a presumption that all atoms in the over-completed dictionary are
fully purified endmembers [14, 16] and there is an unique signature for each input in the
spanned subspace. There has been a growing concern over the obtained over-completed
dictionary, which could be probably useful for the training dataset, whereas that’s another
story for the test dataset, because the shadow and distortion of the color might make the
dictionary not applicable for the current scenario [35]. Owing to the variations in the
atmospheric conditions, location, surrounding materials, multiple scattering and reflection,
and other factors, a single image pixel spectrum may be blended with other materials.
Constrained by the spatial resolution of the sensor, the diffuse reflection of target surface will
also increase the difficulty in the target detection [37]. Consequently, an isolated pixel can
demonstrate a totally opposite property to the pixels in the vicinity, especially when the target
is tiny enough [41, 46]. In the case, sparse representation may not be feasible. Therefore, the
model we propose utilizes all the neighbored background atoms to participate in the construc-
tion of a test background pixel together under the null hypothesis, and use all target and
regional background atoms [40] to approximate a test target signature under the alternative
hypothesis. And the recovery process implicitly leads to a compromise between the two
hypotheses. The detection decision can be made by comparing the reconstruction residuals
under the different hypotheses [45]. Many works [43] focus on how to ameliorate this situation
by introducing the sel^adaptive mechanism, which suggests that dictionary can adjust its value
to accommodate the characteristics of the input image accordingly [33]. Our work has also
designed a similar sel^adaptive mechanism for the over-completed dictionary and we propose
a totally brand new binary-decision strategy compatible with our structure called BHAADS.
Aside from our theoretic innovation, we are also trying to carry out the deployment on the
specifically orientated hardware to accelerate the computation and reduce the energy con-
sumption. The experiment has shown that the FPGA and CPU are competent for different
scenarios. The rest of this paper is organized as follows. Section 2 will provide a formal
problem definition and talk about some basis knowledge from the scope of mathematics.
Section 3 will introduce our attempts for the parallel structure on a variety of platform, such as
CPU and FPGA. Subsequently, the performances on CPU and FPGA are compared in
Section 4. Finally, in the last section, the conclusion part is demonstrated.

2 Target detector based on collaborative representation

2.1 Problem definition

As it is mentioned in the introduction section, target detection is inherently a matter of
binary decision problem to determine whether a single pixel belongs to a certain class.
Since a single pixel in the hyperspectral images may contain hundreds of bands, which
suggests that it owns hundreds of features, we bring in the concept of Bayesian theorem to
help us to build the model. Given a pixel represented by a vector with a length of N,
X = [x1, x2, x3, ···, xn], we aim to put this vector on a binary decision, whether or not it
belongs to a part of the target. We can use strict mathematical model to give the
quantitative analysis, and Bayesian Classification is hereby the most appropriate model
to illustrate the point. The Input is the X = [x1, x2, x3, ···, xn], which could be the initial or
downsampled pixel information if the vector has too much redundancy. The Output is an
assemble of binary decision C = {y0, y1}, and all the outputs are mapped to the two

decisions. Our aim is to find the maximum posterior probability to decide which decision
is more reasonable. The whole process can be described as the following eq. [8]:

P yij x
ð

Þ ¼ max P y0j x
f

ð

Þ; P y1j x
ð

g
Þ

Since each band in the hyperspectral images is independent and does not interfere with each
other, independent variables in the bayes theorem obey the following rule in the posterior
probability:

Which P(x|yi) is often solved by a gaussian distribution function

P yi xjð

Þ ¼

P x yijð

ÞP yið Þ

P xð Þ

g x; η; σ
ð

Þ ¼

p

Þ2
− x−η
ð
2σ2

e

1ﬃﬃﬃﬃﬃﬃ
2π

σ

GeoInformatica

ð1Þ

ð2Þ

ð3Þ

This equation happens to be able to tackle our problem, since our demand can be rewritten in

. By this time, we have fully clarified why L2 norm is widely

(cid:3)
Þ ¼ g xk; η

(cid:4)

this:P xk yij
ð
adopted for the classification in practice.

yi

; σyi

However, without giving priority to all of the features, we have to treat every feature
equally and calculate the euclidean distance along the vector one by one. It arouses two
significant problem, one is the possible over-fitting and the other one is the data redundancy.
Equal importance of each feature will only induce the boundary of the classification vibrating
along the potential edge of different class, and this is so called over-fitting.Another motivation
driving us to reduce the dimension of the input is the demand for eliminating the workload for
the computing platform. The computing resources are always exposed to be finite in front of
the massive information processing, thus we need to reasonably distribute the resources [1].
There exists strong motivation driving us to build a highly discriminative subspace, which can
conduce to get the principle component of dataset and simplify the binary classification.

2.2 Discriminative subspace construction

Principal Component Analysis (PCA) [15, 44], a classic and representative method to extract the
essential information, can build a highly discriminative subspace with the largest variance in
distribution. After the projection onto our proposed subspace, the principle components [9] and the
domain structure distribution will maintain, while redundant information will be removed. How-
ever, as for the target detection, it neglect the similarity in the same classes and the dissimilarity
between different classes (the target and background). So we are aiming to ameliorate the
performance of PCA by building a highly discriminative space feasible for target detection [57].
In addition, PCA is an unsupervised technique to preserve desired and critical information.
However, the pending coefficients and eigenvalues are determinant factors on the algorithm’s
performance, which need our continuous elaboration to get the delicate performance since the
vanilla PCA is far from perfect for the accurate detection. In order to compensate the biased
projection matrices obtained by PCA, we impose a number of labeled training samples along
with those unlabeled ones on fine-tuning a more elegant projection. The new projection is
based on PCA while possessing more discriminative information and a centralized distribution
of targeted texture. The labeled target training samples T are usually the central pixels of the

GeoInformatica

targets, which are deemed to be pure, and the labeled background samples B are chosen from
the typical background pixels from the global scene. The pseudo code of the PCA can be
summarized as follows, and almost all the PCA algorithms follow this path of rule. It is
demonstrated in the pseudo code Algorithm 1 [22].

As aforementioned, PCA serves as the foundation of our proposed theory. The subspace built
by PCA is considered as a baseline for the established discriminative space. For every input
data, the distance between its projection onto the new discriminative subspace and that obtained
in PCA-subspace should be minimized to preserve the common implicit information provided
by the labeled and unlabeled training data. The expression PTym represents the spectral vector
projected onto PCA space. Our intention is to fit the implicit eigenvector of the training samples
QTym on our proposed subspace into PTym and it can be written in the following formula,

min
Q

∑
m∈ T þBþU

ð

Þ

(cid:5)
(cid:5)
PT ym

−QT ym

(cid:5)
(cid:5)
2

In the detection of City Target, our aim is to separate the target samples from the background
ones as many as possible. As shown in Fig. 1, there are target pixels and background pixels,
denoted by red solid circles and yellow solid circles respectively. In the original space, target
samples are more easily contaminated with several background samples, especially those
background samples in the neighborhood. For the purpose of getting more valuable informa-
tion for HSI target detection, we aim to learn a highly discriminative subspace, where the
samples belonging to the same class are projected as closet as possible and those belonging to
different classes are separated as far as possible, which can be seen in Fig. 1. Hence, in the
discriminative subspace, for every target training sample yi, its Euclidean distances to the other
target samples yt(t ∈ T) are expected to be as minimal as possible, while the intraclass distances
to the other background samples yb(b ∈ B) should be as large as possible. Namely, we are
inclined to minimize the following formula to get an ideal projection matrix,

C ¼ min

Q

∑
t∈T

(cid:5)
(cid:5)
QT yi

−QT yt

(cid:5)
(cid:5)−μ ∑
b∈B

(cid:5)
(cid:5)
QT yi

(cid:5)
(cid:5)

−QT yb

ð4Þ

ð5Þ

GeoInformatica

Fig. 1 The above diagram means samples in the original feature space and the below one shows samples in the
new discriminative subspace. Red solid circles and yellow solid circles denote target pixels and background
pixels respectively

Q is the new projection matrix and μ is a scalable parameter. The formula (4) will be
enclosed as a constraint term into PCA to accommodate the target detection in complex
environment. The objective function can be
(cid:5)
(cid:5)
PT ym

−QT ym

(cid:5)
(cid:5)
2 þ ςC

ð6Þ

min
Q

∑
m∈ T þBþU

ð

Þ

ς controls the penalty of C, and we aim to pursue a tradeoff between the global optimum and a
sparse formation of C.

The final matrix yielded by Eq. (5) results in a dense matrix with numerous nonzero
elements. During the proceeding of gradient descent, we swerve to avoid the negative
influence of irremissible random noise by bringing in the l1-norm regulation for the suppres-
sion. Hence, a sparse constraint to restrict the number of nonzero entries is adopted in the
projection matrix Q. The number of nonzero elements in Q can be characterized by the l0-norm
of the matrix Q. However, it turns out to be an NP-hard problem and the norm of Q, namely,
least absolution shrinkage and selection operator (lasso), is adopted as a relaxation of the l0-
norm formulation. Besides, recent research demonstrates the elastic net penalty, a promising
regularization and variable selection method, which combines the l1-norm and l2-norm of the
projection matrix together, enjoys a similar sparsity of representation as to lasso, but outper-
forms lasso. So it is introduced into the objective function and we can find an appropriate
sparse projection matrix:

(cid:5)
(cid:5)

min
Q

∑
m∈ TþBþU

ð

Þ

(cid:5)
(cid:5)

PT ym

−QT ym

2 þ ςC þ θ Qk k1 þ κ Qk k2

2

ð7Þ

The projection matrix Q can be finally achieved by solving the above problem.

GeoInformatica

2.3 Binary hypothesis based on adaptive dictionary and discriminative subspace

As mentioned above, the new discriminative subspace is regarded as a very promising
approach for hyperspectral target detection and the detection process is going to be fully
discussed in this section. In the common practice, we should not only consider the scattering,
reflection, location, atmospheric conditions, but also the random distribution of land covers,
the spectra of all.

background signatures in surrounding area, which may contaminate spectrum of the central

background pixel.

As a result, in the discriminative subspace, a background sample x in the image can be
, which are also

represented by the combination of background samples, Ab ¼ abif
the columns of the background dictionary:

gi¼1;2;⋯;N b

x≈α1αb1 þ α2αb2 þ ⋯ þ αN b

αbN b ¼ ab1ab2⋯abN b

½

(cid:2) α1α2⋯αN b
½

(cid:2) ¼ Abα

ð8Þ

Then, x can be reconstructed by solving the following problem:

bα ¼ arg min Abα−x

k

k2subject to αk k2

≤ε

This formula guarantees to minimize the reconstruction error ||— x||2 while maintaining ||q||2
trivial enough. Following the regularized least square method, the object function is expressed
as:

arg min Abα−x
k

k2 þ λ αk k2

where A is a scalable parameter. By scaling this parameter, we can build a non-convex
problem to a convex one, which is solvable from the mathematics. Besides, it can also control
the penalty of the norm of the weight vector a, and get a sparse weight vector corresponding to
the atoms in dictionary. The solution of formula (10) can be drawn:

(cid:6)
bα ¼ AT

(cid:7)−1

b Ab þ λI

AT
b x

In order to utilize the correlation of the central point with the surrounding pixels, the weight of
every background dictionary atom is preprocessed according to its similarity to the test pixel.
When determining the coefficient, our approach is based on a premise that the weight belonged
to those pixels which are far from the test pixels, should be largely suppressed, and pixels
highly similar to the central pixel should be given a larger weight.

Hence, it is necessary to impose an additional term, a diagonal regularization matrix on the

objective function:

ð9Þ

ð10Þ

ð11Þ

ð12Þ

2

4

Ψ ¼

x−ab1
k

k2

⋯

0

3

5

0

x−abi

k

k2

The diagonal elements are reversely proportional to the similarity between the test pixel and
every background training sample around, which controls the weight estimation. A larger
element stands for a major difference to the test pixel and vice versa. At this moment, the Eq.
(12) will be improved as:

GeoInformatica

ð13Þ

ð14Þ

This can be worked out as follows:

arg min Abα−x
k

k2 þ λ ψαk

k2

(cid:6)
bα ¼ AT

b Ab þ λψT ψ

AT
b x

(cid:7)−1

Obviously, it is a new way to reconstruct the signal and get n close-formed solution
with less computational complexity. As for a target pixel, as long as the target
is
portrayed in the test sample, specific to a full-target pixel, it will be proven almost
impossible to find similar spectral signatures in background dictionary and more likely
to be approximated by the union dictionary A (the union of background dictionary and
target dictionary). Each column of the union dictionary represents a target or back-
ground training sample. Besides, due to the external environmental factors and the
random distribution of land cover, each pixel spectrum can be expressed by all target
training samples At ¼ atif gi¼1;2;⋯;N t
and surrounding background training samples
around Ab ¼ abif

together, namely,

gi¼1;2;⋯;N b

x≈α1ab1 þ α2ab2 þ ⋯ þ αN b abN b þ β

½

(cid:2) α1α2⋯αN b
¼ ab1ab2⋯abN b
½
¼ Abα þ Atβ ¼ A
¼ Aγ

(cid:10) (cid:11)
α
β

1at1 þ β
(cid:2)T þ at1at2⋯atN t

2at2 þ ⋯ þ β
(cid:8)
(cid:2) β

β

½

1

2

N t atN t
(cid:9)
⋯β

N t

T

Since the weight matrices are still unconfirmed and should be determined by the compound
combination of atoms in the dictionary, the coefficients can be settled down by the following
formula:

(cid:6)

bγ ¼ AT A þ λθT θ

AT x

(cid:7)−1

t is noted that we manage to utilize the union dictionary atoms to represent target test pixel,
while independently approximate the background by using the background dictionary. Thus, a
new binary hypothesis model can be proposed on this basis. An unknown test sample can be
chosen from the competing hypotheses:

H 0 : PT y ¼ Abα; target
H 1 : PT y ¼ Abα þ Atβ ¼ Aγ; target

absent

absent

where α is the coefficient whose entries reflect the abundance of the training samples in Ab,
and γis the prefix coefficient whose entries represent the abundance of the training samples in
A. α and γ can be acquired by engaging the aforementioned method:

ð15Þ

ð16Þ

ð17Þ

ð18Þ

(cid:6)

H 0 : bα ¼ AT
b Ab þ λψT ψ
(cid:6)
H 1 : bγ ¼ AT A þ λθT θ

(cid:7)−1
(cid:7)−1

(cid:6)
(cid:7)
; t arget
b PT y
AT
(cid:7)
(cid:6)
; t arget
AT PT y

absent

absent

After the reconstruction of the target profile, the vector a and 7 implicitly indicate a compro-
mise between the null hypothesis represented by the background subspace and the competing
hypothesis represented by the union subspace. Then, the type of the test signal observation can
be determined by comparing the reconstruction residuals. That is to say,

GeoInformatica

(cid:5)
(cid:6)
(cid:5)
(cid:5)

(cid:7)

(cid:4)
(cid:3)
− Abbα

(cid:5)
(cid:5)
(cid:5)
2

(cid:5)
(cid:6)
(cid:5)
− PT y
(cid:5)

(cid:7)

(cid:3) (cid:4)
− Abγ

(cid:5)
(cid:5)
(cid:5)
2

(cid:7)

(cid:5)
(cid:6)
(cid:5)

PT y

where

D xð Þ ¼ PT y
(cid:5)
(cid:5)
2 is the reconstruction residuals corresponding to the background
(cid:5)
(cid:7)
(cid:5)
− Abγð
2 is the residual corresponding the union dictionary. If (19)
dictionary, while
Þ
is greater thee the given threshold,it means that the pixel can be approximated more accurately
using the union dictionary and x is labeled as a target at this moment. Otherwise, the test pixel
is considered as background in default.

− Abbαð
Þ
(cid:5)
(cid:6)
(cid:5)
PT y

ð19Þ

2.4 Principles of adaptive dictionary

In the newly built discriminative subspace, the construction of dictionaries and A should entail
further discussion in this section. A general dictionary built via the training samples in the
global image scene is proven to be excessive and hinders the instant recovery of the coefficient
vector. Therefore, an adaptive dictionary is employed to flexibly customize a regional back-
ground dictionary for the local statistics [32].

The number of the target training samples is finite and the target dictionary is therefore
constructed upon some of the purer target pixels spread across the whole image. However, the
backup samples for a background dictionary are absolutely sufficient comparatively. Here, a
dual concentric window structure is employed to construct a local-oriented dictionary in the
newly built subspace [17]. The window separates the local area around pixel x into a small
inner window region (IWR) and a larger outer window region (OWR) as shown in Fig. 2. The
IWR prohibits the possible existence of target elements from the background dictionary, and
the OWR is to build a model of the local background. We propose a concept of Local
Background Dictionary here: Only pixels in the outer region are counted as endmembers to
build the atoms in background dictionary.

The dictionary is adjusted in order to accommodate the local statistics in terms of the
surrounding spatial information. It can be inferred that if the test pixel belongs to the same
category as the background, there is a perfect match in the background dictionary. In other
words, during the detection, an adaptive dictionary in the discriminative subspace is construct-
ed to approximate the test pixel. According to the aforementioned descriptions, the entire
BHADDS model for hyperspectral target detection is illustrated in Fig. 3.

3 Parallel methodologies on software-hardware Codesign of BHAADS

In this section, our ambitions reach even further: section II introduces some heuristic methods
to ameliorate the conventional approaches, covering the Dimensional Reduction, Dictionary
Construction, Dual Concentric Windows Detection. Our amelioration indeed improves the
performance of the target detection. However, the improvement is at an expenditure of extra
computational workload. The instruction-level optimization is confronted with a bottleneck
subject to both the physical characteristic of CMOS and architecture. Thus, the research on the
target detection are no longer restricted to the proposal of novel algorithm, but also the new
architecture and material. In this paper, we are going to redefine the systematic architecture
from the bottom layer to ameliorate the performance.

Besides, it also provides an embedded solution for the computation-intensive task, since the
highly integrated front-end processor is indispensible in the real-time processing, especially for the

GeoInformatica

Fig. 2 Dual concentric windows and the window sizes

aerial image photographing. In order to increase the throughput of the overall system, the algorithm
is usually packaged as independent IP core as the peripheral component adhered to the core
processing unit.

We can use a diagrammatic sketch to illustrate the complete structure of our proposed
methodology. In the central part, the CPU not only undertakes the task of coordinating

Fig. 3 Dual concentric windows and the window sizes

GeoInformatica

the whole system, but also is in charge of the Dimension Reduction and the Update of
Weight Matrices. In our work, we have proposed an innovative idea, by adding an
independent IP to the periphery of the CPU, which is specially oriented for the fast
target detection. The independent IP shares the task with the CPU, because the IP cores
can be designed in parallel organization. With the increase of the number of IP cores, the
new structure will greatly benefits from the high level of parallelism. Each processing
engine can deal with a concentrically rectangular region of data. Thus, multiple PEs can
simultaneously deal with several pixels and their neighbouring region. In this section, we
use a software and hardware codesign methodology to ameliorate the target detection
performance and speed. As Fig.4 depicts, a complete target detection consists of the
following components: IP1 for Dimension Reduction, CPU, Main Memory, Exterior
Memory to store the dictionary of atoms, CPU, and IP2 for the Target Detection. We will
list the function of each component:

IP1 This intellectual property is mainly oriented for dimensional reduction. With the advent of
the advanced photographing technology, a hyperspectral image could contain hundreds of
bands. The redundancy hinders the efficient calculation of the algorithm. Since the dimen-
sional reduction is an task-intensive process, the operations involves many complicated
function, like the global normalization and mirror duplication. Our algorithm does not cover
this part into the attempt of the parallelism.

CPU This CPU can be a universe processing unit. Universe central unit implies that
there is no specific regulations on the types of the CPU, either Intel and AMD can
work. This part should at least have the functions like instruction scheduling and basic
arithmetic ability. In a word, this component is in charge of the coordination of the
whole system.

Main memory The main memory here is in charge of the two goals: the primary goal is the
scheduling of instructions, and the secondary aim is the instructional parallelism, which means
the data and instruction’s settlement on the memory is ready for the concurrent execution.

Exterior memory This memory stores the spectral signature of the substances. The subspace
we used in the target detection is spanned from these vectors, and we can get the estimation of
an unknown spectral input by using the combination of the spectral signature.

Fig. 4 Systematic architecture of our proposed methodology

GeoInformatica

IP2 This intellectual property is the core unit in our design. We carry out the parallel
optimization on both soft IP core and hard IP core. In the following section, we will give
our comprehensive comparison on both of them.

3.1 Parallel deployment of the target detection after dimensional reduction
(software)

Hyperspectral image has provided us with abundant source of information, whereas the extraction of
the feature points from the complex environment is not an easy task. We are aiming at designing a
specific circuit oriented for the Target Detection in the remote sensing. Transfer Learning based on
the sparse coding solves this problem by building an over-complete dictionary which contains the
atoms of spectral signature of objects feature. In the Fig. 4, not all parts of the system are suitable for
the parallel optimization. Since some IPs involve the computation of the global variable and the
algorithm need the frequent access to the memory. Hereby we only elaborate the IP2 part for the
parallel deployment [39]. Since the original program consumes too many time intervals, the program
needs to be optimized in order to improve the performance of the code. Before optimization, code
execution result needs to be carefully analyzed to find out the key codes that need to be optimized,
and then optimizations launch, since indiscriminate optimization is also a kind of waste. By profiling
the original code in Matlab, a detailed information of the time consumption about the original code
will be displayed accordingly. According to the performance analysis results, it can be seen that there
are three parts which consume the major time.

1. Extraction of the sliding window Conventional methodology to bound the ROI [38]
section is seemingly quite straightforward: typically there is a large registers array to tempo-
rarily hold the values and then pass them along to the processing engine. However, it
introduces a new problem with the rocketing loops, since in each iteration we have also
enclosed many operations to complete a complicated function, like matrix transposition and
euclidean distance to the target vector. A single iteration may not be worth of being particular
about, but with the loops accumulated, this process becomes lengthy and tedious, which has
adverse effects on the realtime processing. There is an interesting phenomenon in our
experiment: our sliding window is hollow in the middle and it takes too much time in deciding
which value is valid for computing. So here we propose a totally brand-new method to address
this problem. In order to deal with the hollow square sliding window, we cr reshape the sliding
kernel and use the slicing technique to erase the unnecessary vector.

2. Trick of computing the diagonal matrix According to the analysis of this part of the code,
it can be found that when calculating variable λΓ ' Γ, where Γ represents diagonal matrices, the
process of calculating λΓ ' Γ has computational redundancy and can be simplified even further.
Since variable Γ ' Γ is a diagonal matrix, that is, all the elements except diagonal elements are
0, then F is actually the square of diagonal elements. Since variable F is calculated by diag(v '
v),we can immediately square variable and diagonalize it directly. This simple move can
greatly enhance the calculation speed, from the initial 31 s, jumping to approximately 6 s,
which is very impressive.

3. Amelioration for computation of matrix inversion Matrix Inversion is an inevitable
step in the Linear Regression Problem when we try to figure out the least square distance
between the source and target. However, it is an imperative task to find an efficient and

GeoInformatica

effective way to tackle the high-dimensional linear equations. There exists three ap-
proaches to get the solution of Linear Regression Problem with the inverse command,
such as inv.(), pinv(), backslash Our intuition may tell us that inv.() should first pop up in
our mind, and it is indeed complied with most cases. Actually, inv.() is a compromise
between the performance and fitness: inv.() tries to accomodate a general need for the
matrix inversion, and it uses LU decomposition at the input. It then utilizes the result to
rebuild a group of linear equations to get the inversion of matrices. However, it creates
an extra group of linear equations in advance in order to get the final result, which
suggests the there is a waste of time in getting the final solution. Fortunately, the
backslash is hereby recommended for the quick solution of linear equations with even
less residual error, since this method utilizes the method of Gaussian Elimination,
without giving the implicit inversion result. The final results show that the calculation
time of matrix dictionary atoms are optimized down to 20+ seconds compared with the
original 40+ seconds, achieving a better performance with an increasing rate approxi-
mately 50%. It contends that backslash is more preferable to the inv.() in the calculation
of linear equations.

The above statements have summarized the maneuvers adopted to ameliorate the perfor-
mance for the binary decision, and it is inferred that the flexible use of optimization approach
can make a total difference. Especially, we have adopted an unusual structure in our model.

3.2 Parallel deployment of the target detection after dimensional reduction
(hardware)

In the previous section, we have briefly introduced the parallel acceleration from the scope of
the software IP core. A complete architecture of our proposed way can be shown in the Fig. 5.
The pipeline structure undertakes a task of optimization of data dependency and instruction
rescheduling. Typically, the pipeline structure is often built by a set of buffers. The datasets
queue in line and wait for the handshaking signal to inform the readiness to go to the PE. The
completion of a single PE has paved the way for the massive use of multiple PE system. So
here we will introduce what is inside in a single PE, there are some key components in PE.
The main body of a complete PE in our proposed way consists of two large scale inverse
modules, which greatly differ from the ordinary inverse function. The HLS has provided us
with inverse solution for the small scale matrices, however, an effective inverse method for

Fig. 5 Parallel Performance on CPUs

GeoInformatica

large matrices are expected to solve the problems in remote sensing. In our experiment, we
deploy an ameliorated Gaussian Elimination Process on the programmable SoC. In the next
part, we will further discuss the advantage of the deployment on the FPGA SoC instead of the
CPU. The FPGA has a low requirement on the power, which suggests that this equipment can
be driven by low power plant. Besides, the customizable property makes FPGA flexible to deal
with many problems.

4 Experiment part

4.1 Experimental settings

The Hyperspectral Digital Imagery Collection Experiment (HYDICE) is a classic dataset
collected by airborne imaging spectrometer, which is widely recognized as an authorita-
tive baseline for the evaluation of novel algorithm. As for HYDICE, the target dictionary
is built upon the spectral signatures of real target spread across the global map, whereas
the construction of background dictionary is a regional operation by using the aforemen-
tioned dual concentric rectangular structure. After our a great many trials, the optimal
sizes of OWR and IWR are set as 19 × 19 and 5 × 5 in BHADDS, and the channel depth
is Nb = 336.

4.2 Accuracy demonstration

Our proposed framework is compared with some of the classic algorithms, including the local
constrained energy minimization (Local CEM), the local adaptive coherence estimator (Local
ACE). Some state of the art methodologies related to the sparse representation are also
involved for the comparison experiment. For every algorithm, the spectral signature selected
to build the target dictionary is the same: the center pixel is always deemed to be the purest part
of the target. The pixels located in the OWR are utilized to build the background convariance
matrices for Local CEM and Local ACE, whereas they are used for the construction of
background dictionary for STD. SRBBH and our proposed framework.

There are some important indices that have to be clarified in advance to help us evaluate the

performance of algorithm:

Receiver operating characteristic (ROC) ROC curve is a classic quantitative evaluation
standard, which demonstrates the relationship of the false alarm rate v.s probability of detection
based on the ground truth. An ideal algorithm is expected to have the highest detection rate
while holding a lowest false alarm rate.

Area under curves (AUC) The sole ROC evaluation standard is not objective and may be
misleading in cases, since the distances between the ROC curves sometimes are so narrow that
there lack of a fine-grained assessment system. Hence the area under the ROC curve (encircled
by the ROC curve and the false alarm rate axis) will work and the largest one is regarded as the
best setting.

After we introduce some important evaluation indices of the target detection, the
actual performance comparison will definitely gives us intuitive perception. We can
notice that our algorithm has great advantage under both ROC and AUC standards

GeoInformatica

(see Fig. 6), which fully demonstrate the superiority of our proposed way and our
success of achieving a high detection rate with minor false alarm rate.

4.3 Parallelize with Matlab

Considering the massive information that are needed to deal with, we are always
considering an approach to speed up the computation. This is because our typical
algorithm design is based on the Matlab, a powerful mathematica ltool, but is only
backed up by CPU and finite GPU support. Thus, it lacks the instant processing ability to
handle the dataset. There is a tremendous cost in the storage system, because the aircraft

Fig. 6 Detection performance for HYDICE data set (a) ROC (b) AUC

GeoInformatica

has to store all the photographs and transmit the graphs back to the center only when the
communication signal is available. The delay in the transmission process inhibits the
prompt feed-back from the datacenter. Therefore, there is a huge demand for the specific
computer-architecture that are able to instantly analyze the information. Atthe early
stage, we are considering how to improve the CPU’s performance on the aircraft. Then,
we reprise the parallel structure on the CPU and we can notice that the optimized CPU’s
structure is extremely compatible for the massive information sceanarios. However, our
work will not cease at here. The fatal drawback of the CPU or the GPU is that the on-
chip structure is fixed and the optimization is limited on the instructional level. We
should also consider the Register-Transfer-Level (RTL) design, especially for the
computation-intensive work, how to redefine the on-board structure is a intriguing
problem nowadays. An Remote-Sensing Oriented ASIC is therefore emerging as the
state-of-the-art trend in the current research. Therefore, the research on the FPGA is
urgent and we are heading for it in the subsequent research.In this section, we will
demonstrate the flowchart of our propose structure.Our comparison includes three as-
pects: Dimension Reduction, Diction Atoms Self-Adjustment, Binary Categorical Pre-
diction. The pseudo codeAlgorithm2gives us the most compatible optimization for us
after our numerous trials.The following Fig. 7 demonstrates the comparison among the
original, code-optimized, and CPU-Parallel scenario. It could be observed that the
original Fig. 7: Parallel Performance on CPUs code has occupied huge amount of time
slots and make it a tedious process to wait for the final outcome. The purple stride
depicts the time in fetching the Region-of-Interest (ROI) into the memory. If this
operation does not consider the data dependency and reconfigurability in the design,
which always exists in our daily design, it would extend the time consuming. However,
in our design, we eliminate the inter-loop dependency and manage to unroll the loops,
therefore, the loops for the window sliding can be executed concurrently. Moreover, we
extend the parallel optimization onto the other stages in the same programme. Thus we
can see that the time latency of the programme are ameliorated greatly with more
improvements being deployed.

4.4 Parallelize with FPGA HLS

In this section, we will explain the experiments and discuss the results. We use Vivado HLS tool to
implement our design. Simulations are carried out on Virtex-7 VC707 FPGA, and clock period is set
to 10 ns. The key problem existed in BHADDS algorithm is how to calculate the inversion of a large
matrix. In order to achieve parallelization, we are seeking a way on how to split the sequential
operations into a concurrent architecture, which means that the inversion process needs to meet the
parallel requirement. For the sake of reducing executive time, we parallelize the algorithm with
compiling directives such as pipeline, unroll, to execute as many independent and concurrent
operations as possible simultaneously. Following this principle, we test several methods to calculate
the inversion of the matrix, and finally choose the gaussian elimination method for the BHADDS
algorithm because it has stable performance hardware implementation.

The gaussian elimination method can be expressed as follows: Given a matrix A, we
transform the matrices [E1, E2, ⋯, Ek]T into a unit matrix I through a series of basic row
operations, and then transform I through the same row operation sequence to the reverse of A.
Therefore, in order to calculate the inverse matrix, the specific implementation steps are as
follows:

GeoInformatica

Fig. 7 Parallel Performance on CPUs

1.We merge the original matrix with an identity one which is on the righthand side. The
original part is aimed to be transformed to an identity matrix by rolling the rows with scaled
additions and substractions.

2.The left matrix, as is indicated in step 1, is moving towards a triangular matrix, then
identity matrix, by rolling the rows across the entire matrix (including the matrix on the right),
with a scaled row additions. The rightside identity matrix follows the same operation rules, and
the final output is the inverse matrix.

3.While iterating the optimization of the results obtained from the above step 2, we keep

updating the right matrix until a convergence to the restraint condition we set up.

4.If the determinant of the main matrix is zero, then there is no inverse matrix and the whole

program halts and reports there is an exceptional interruption back to the client end.

Table 1 Performance comparison pre and post optimization

Metrics

Min Latency
Max Latency
BRAM
DSP48E
FF
LUT

Initial with Single PE

Parallel with Multiple PEs

115,117,826
1,274,641,603
7388
141
25,566
34,198

94,679,468
1,382,736,055
7372
199
52,029
79,012

GeoInformatica

As we can see from Table 1, after applying the parallel directives to our method for
computation of the inversion, min latency decreases about 20,438,358 clock cycle, approxi-
mately 204.38 ms in this case, whereas the max latency has risen a little bit. It indeed indicates
the parallel deployment can accelerate the computation in some cases, but it may not always
mean the same for every circumstance, since our algorithm involves many matrices inversion,
whereas matrices inversion do not seem capable of the parallel deployment on FPGA. The
actual performance of matrices inversion on FPGA highly depends on the matrices structure,
which suggests the inversion of the matrices is a long term puzzle on the FPGA, especially the
large scale matrices. It is noted that our attempt for the parallelism of the binary detection on
SoC also utilizes more FPGA resources, since both PEs and controller unit are included in the
design. FPGA has a great advantage over the conventional CPU-compiler architecture for its
low power consumption, and the implementation on the FPGA has also provided a possible
solution for the ASIC design. We still have a lot of things to do in order to integrate the target
detection algorithm on the SoC, since the optimization is not an easy task for the large scale
matrices. But in the foreseeable future, the miniaturization of integrated system on SoC is
going to be a very promising direction which is worth of our common endeavour.

5 Conclusion

In this work, we propose a novel algorithm to build a highly discriminative subspace for the
target detection and also seek for the parallel deployments of discriminative subspace on both
soft and hard IP cores. BHAADS algorithm has a very impressive accuracy compared to other
classic algorithm and the optimization on the software end has achieved the goal of fast target
detection on hyperspectal images after dimension reduction on the most universal platform
—CPU. Moreover, we have also discussed about the formation on hardware, such as FPGA.
We treat the single sliding window as an independent PE, and finish the design using the High
Level Synthesis. The implementation on the FPGA has a great advantage in the low power
scenarios, and we can then have progressive amelioration on the basis of this work in the
future. With the thorough optimization on the FPGA, we may even launch the integrated ASIC
as the peripheral part on SoC to specifically deal with the task with target detection on
hyperspectral images.

Acknowledgement This work was supported in part by the National Key R & D Program of China under Grant
2018YFA060550 and the National Natural Science Foundation of China under Grants 41871243, the Natural
Science Foundation of Hubei Province under Grants 2018CFA050, and the Science and Technology Major
Project of Hubei Province (Next-Generation AI Technologies) under Grant 2019AEA170.

References

1. An L, Wang W, Shang S, Li Q, Zhang X (2018) Efficient task assignment in spatial crowdsourcing with

worker and task privacy protection. Geoinformatica 22(3):1–28

2. Chang CI (2002) Target signature-constrained mixed pixel classification for hyperspectral imagery. IEEE

3. Chang CI (2005) Orthogonal subspace projection (osp) revisited: a comprehensive study and analysis. IEEE

Trans Geosci Remote Sens 40(5):1065–1081

Trans Geosci Remote Sens 43(3):502–518

GeoInformatica

4. Chang CI (2000) Liu: generalized constrained energy minimization approach to subpixel target detection for

multispectral imagery. Opt Eng 39(5):1275–1282

5. Chen Y, Nasrabadi NM, Tran TD (2011) Simultaneous joint sparsity model for target detection in
hyperspectral imagery. IEEE Geosci Remote Sens Lett 8(4):676–680. https://doi.org/10.1109
/LGRS.2010.2099640

6. Chen Y, Nasrabadi NM, Tran TD (2011) Sparse representation for target detection in hyperspectral imagery.

IEEE J Sel Top Signal Process 5(3):629–640

7. Cheng G, Han J (2016) A survey on object detection in optical remote sensing images. Isprs J Photogramm

Remote Sens 117:11–28

8. csguo: https://ww.cnblogs.com/csguo/p/7804355.html (2017)
9. Dong Y, Du B, Zhang L, Zhang L (2017) Dimensionality reduction and classification of hyperspectral
images using ensemble discriminative local metric learning. IEEE Trans Geosci Remote Sens 55(5):2509–
2524

10. Du B, Zhang L (2011) Random-selection-based anomaly detector for hyperspectral imagery. IEEE Trans

Geosci Remote Sens 49(5):1578–1589

11. Du B, Zhang Y (2016) Beyond the sparsity-based target detector: a hybrid sparsity and statistics-based

detector for hyperspectral images. IEEE Trans Image Process 25(11):5345–5357

12. Du Q, Ren H, Chang CI (2003) A comparative study for orthogonal subspace projection and constrained

energy minimization. IEEE Tans. Geosci Rem Sens 41(6):1525–1529

13. Han J, Zhang D, Cheng G, Guo L, Ren J (2015) Object detection in optical remote sensing images based on
weakly supervised learning and high-level feature learning. IEEE Trans Geosci Remote Sens 53(6):3325–
3337

14. Harsanyi JC, Chang CI (1994) Hyperspectralimage classiHcationand dimensionality reduction: An orthog-

onal subspace projection approach. IEEE Trans Geosci Remote Sens 320:779–785

15. Jr F, Mersereau RM (2005) On the impact of pea dimension reduction for hyperspectral detection of difficult

targets. IEEE Geosci Rem Sens Lett 2(2):192–195

16. Keshava N, Mustard JF (2002) Spectral unmixing. IEEE Signal Process Mag 19(1):44–57
17. Kwon H, Der SZ, Nasrabadi NM (2003) Dual-window-based anomaly detection for hyperspectral imagery.

In: Automatic Target Recognition XIII. Int Soc Opt Photonics 5094:148–159

18. Landgrebe D (2002) Hyperspectral image data analysis. IEEE Signal Process Mag 19(1):17–28
19. Li W, Tramel EW, Prasad S, Fowler JE (2014) Nearest regularized subspace for hyperspectral classification.

IEEE Trans Geosci Remote Sens 52(1):477–489

20. Liu T, Tao D (2016) Classification with noisy labels by importance reweighting. IEEE Trans Pattern Anal

21. Liu T, Tao D, Xu D (2016) Dimensionality-dependent generalization bounds for k- dimensional coding

Mach Intell 38(3):447–461

schemes. Neural Comput 28(10):1

Trans Geosci Remote Sens 54(1):373–385

Cybern 47(4):884–897

22. LLLiuye: https://www.cnblogs.com/11iuye/p/9156763.html (2018)
23. Lu T, Li S (2016) Spectral—spatial adaptive sparse representation for hyperspectral image denoising. IEEE

24. Lu X, Yuan Y, Zheng X (2017) Joint dictionary learning for multispectral change detection. IEEE Trans

25. Luo F, Hong H, Ma Z, Liu J (2016) Semisupervised sparse manifold discriminative analysis for feature

extraction of hyperspectral images. IEEE Trans Geosci Rem Sens 54(10):6197–6211

26. Ma B, Du Q (2013) Hyperspectral target detection with sparseness constraint. In: 2013 IEEE International

Geoscience and Remote Sensing Symposium-IGARSS, pp. 1059— 1062. IEEE

27. Manolakis (2009) Is there a best hyperspectral detection algorithm? In: Algorithms and technologies for
multispectral, hyperspectral, and ultraspectral imagery XV, vol. 7334, p. 733402. International Society for
Optics and Photonics

28. Manolakis D, Marden D, Shaw GA (2003) Hyperspectral image processing for automatic target detection

applications

29. Manolakis DG, Shaw G (2002) Detection algorithms for hyperspectral imaging applications, sig proc mag

ieee. Signal Process Mag IEEE 19(1):29–43

30. Matteoli (2010) A tutorial overview of anomaly detection in hyperspectral images. IEEE Aerosp Electron

Syst Mag 25(7):5–28

Signal Process Mag 31(1):34–44

31. Nasrabadi NM (2014) Hyperspectral target detection: An overview of current and future challenges. IEEE

32. Niu Y, Wang B (2017) Extracting target spectrum for hyperspectral target detection: An adaptive weighted
learning method using a self-completed background dictionary. IEEE Trans Geosci Remote Sens 55(3):
1604–1617

GeoInformatica

33. Pati, Y.C. (1993) Orthogonal matching pursuit: Recursive function approximation with applications to
wavelet decomposition. In: Proceedings of 27th Asilomar conference on signals, systems and computers,
pp. 40—44. IEEE

34. Qian D, Ren H, Chang CI (2003) A comparative study for orthogonal subspace projection and constrained

energy minimization. IEEE Trans Geosci Rem Sens 41(6):1525–1529

35. Qian D, Wei W, May D, Younan NH (2010) Noise-adjusted principal component analysis for buried

radioactive target detection and classification. IEEE Trans Nucl Sci 57(6):3760–3767

36. Scharf LL, Friedlander B (1994) Matched subspace detectors. IEEE Trans Signal Process 42(8):2146–2277
37. Schweizer SM, Moura JM (2001) Efficient detection in hyperspectral imagery. IEEE Trans Image Process

38. Shang S, Chen L, Jensen CS, Wen JR, Kalnis P (2017) Searching trajectories by regions of interest. IEEE

10(4):584–597

Trans Knowl Data Eng 29(7):1549–1562

networks. Vldb J 27(3):395–420

39. Shang S, Chen L, Wei Z, Jensen CS, Kai Z, Kalnis P (2018) Parallel trajectory similarity joins in spatial

40. Shang S, Chen L, Wei Z, Jensen CS, Kain is P (2016) Collective travel planning in spatial networks. IEEE

Transactions on Knowledge Data Engineering 28(5):1132–1146

41. Shang S, Chen L, Zheng K, Jensen CS, Wei Z, Kalnis P (2019) Parallel trajectory-to- location join. IEEE

Trans Knowl Data Eng 31(6):1194–1207. https://doi.org/10.1109/TKDE.2018.28Ja05

42. Shang S, Ding R, Kai Z, Jensen CS, Kalnis P, Zhou X (2014) Personalized trajectory matching in spatial

networks. VLDB J 23(3):449–468

43. Stefanou MS, Kerekes JP (2010) Image-derived prediction of spectral image utility for target detection

applications. IEEE Trans Geosci Remote Sens 48(4):1827–1833

44. Sun, J., Jiang, Y., Zeng, S.(2005) A study of pea image fusion techniques on remote sensing. In:

International Conference on Space Information Technology

45. Sun, X., Li, N., Zhao, H.J. (2008) Performance evaluation for hyperspectral target detection algorithms. In:
Seventh International Symposium on Instrumentation and Control Technology: Sensors and Instruments,
Computer Simulation, and Artificial Intelligence, vol. 7127, p. 712725. International Society for Optics and
Photonics

46. Tropp JA, Gilbert AC (2007) Signal recovery from random measurements via orthogonal matching pursuit.

IEEE Trans Inf Theory 53(12):4655–4666

47. Wang T, Du B, Zhang L (2013) A kernel-based target-constrained interference-minimized filter for

hyperspectral sub-pixel target detection. IEEE J Sel Top Appl Earth Obs Rem Sens 6(2):626–637

48. Xu, J., Han, J. (2016) Robust and sparse fuzzy k-means clustering. In: IJCAI, pp. 2224—2230
49. Yang S, Shi Z (2016) Hyperspectral image target detection improvement based on total variation. IEEE

Trans Image Process 25(5):2249–2258

50. Yang X, Wu Z, Fu X, Zhan T, Wei Z (2016) A target detection method based on low-rank regularized least

squares model for hyperspectral images. IEEE Geosc Rem Sens Lett 13(8):1129–1133

51. Yuan Y, Zheng X, Lu X (2015) Spectral—spatial kernel regularized for hyperspectral image denoising.

IEEE Trans Geosci Remote Sens 53(7):3815–3832

52. Yue X, Xi M, Chen B, Gao M, He Y, Xu J (2019) A revocable group signatures scheme to provide privacy-

53. Zhang L, Du B, Zhong Y (2010) Hybrid detectors based on selective endmembers. IEEE Trans Geosci

preserving authentications. Mob Netw Appl

Remote Sens 48(6):2633–2646

54. Zhang Y, Du B, Zhang L (2015) A sparse representation-based binary hypothesis model for target detection

in hyperspectral images. IEEE Trans Geosci Remote Sens 53(3):1346–1354

55. Zhang Y, Du B, Zhang Y, Zhang L (2017) Spatially adaptive sparse representation for target detection in

hyperspectral images. IEEE Geosci Remote Sens Lett 14(11):1923–1927

56. Zhang Y, Ke W (2017) Independent encoding joint sparse representation and multitask learning for

hyperspectral target detection. IEEE Geosci Remote Sens Lett 14(11):1933–1937

57. Zou H, Hastie T (2005) Regularization and variable selection via the elastic net. J Royal Stat Soc: Series B

(statistical methodology) 67(2):301–320

Publisher’s note
institutional affiliations.

Springer Nature remains neutral with regard to jurisdictional claims in published maps and

GeoInformatica

Yipeng Zhang is now a PhD student in School of Computer Science, Wuhan University. His research interests
include machine learning and its application.

Yiming Zhang is now a graduate student in LIESMARS, Wuhan University. Her research interests include blind
source separation and image processing.

GeoInformatica

Bo Du (M’10–SM’15) received the B.S. degree and the Ph.D. degree in Photogrammetry and Remote Sensing
from State Key Lab of Information Engineering in Surveying, Mapping and Remote Sensing, Wuhan University,
Wuhan, China, in 2005, and in 2010, respectively.

He is currently a professor with the School of Computer, Wuhan University, Wuhan, China. He has more
than 40 research papers published in the IEEE Transactions on Geoscience and Remote Sensing (TGRS), IEEE
Transactions on image processing (TIP), IEEE Journal of Selected Topics in Earth Observations and Applied
Remote Sensing (JSTARS), and IEEE Geoscience and Remote Sensing Letters (GRSL), etc. Five of them are
ESI hot papers or highly cited papers. His major research interests include pattern recognition, hyperspectral
image processing, and signal processing.

He is currently a senior member of IEEE. He received the best reviewer awards from IEEE GRSS for his
service to IEEE Journal of Selected Topics in Earth Observations and Applied Remote Sensing (JSTARS) in
2011 and ACM rising star awards for his academic progress in 2015. He was the Session Chair for both
International Geoscience And Remote Sensing Symposium (IGARSS) 2016 and the 4th IEEE GRSS Workshop
on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS). He also serves as a
reviewer of 20 Science Citation Index (SCI) magazines including IEEE TGRS, TIP, JSTARS, and GRSL.

Chao Zhang is now a graduate student in School of Computer Science, Wuhan University. His research interests
include machine learning and its application.

GeoInformatica

Xiaoyang Guo is now a graduate student in School of Computer Science, Wuhan University. His research
interests include machine learning and its application.

Weiping Tu received the B.S. and Ph.D. degrees from Wuhan University, Wuhan, China,. She is currently an
associate professor with the School of Computer, Wuhan University Her research interests include pattern
recognition, image processing.

Yipeng Zhang 1 & Yiming Zhang 2 & Bo Du 1 & Chao Zhang 1 & Xiaoyang Guo 1 & Weiping Tu 1

GeoInformatica

Affiliations

Yipeng Zhang
zyp91@whu.edu.cn

Yiming Zhang
15827636571@163.com

Chao Zhang
darcyzhang001@foxmail.com

Xiaoyang Guo
627191278@qq.com

1 National Engineering Research Center for Multimedia Software , School of Computer Science and Institute

of Artificial Intelligence, Wuhan University, Wuhan, China

2

State Key Laboratory of Information Engineering in Surveying, Mapping, and Remote Sensing, Wuhan
University, Wuhan, China

