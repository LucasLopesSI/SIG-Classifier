Transactions in GIS, 2012, 16(2): 233–248

Research Article

Earth Observation Scientiﬁc Workﬂows in
a Distributed Computing Environment

Terence L van Zyl
Earth Observation Science and IT
Council for Scientiﬁc and Industrial
Research
South Africa

Graeme McFerren
Earth Observation Science and IT
Council for Scientiﬁc and Industrial
Research
South Africa

Anwar Vahed
Earth Observation Science and IT
Council for Scientiﬁc and Industrial
Research
South Africa

Derek Hohls
Earth Observation Science and IT
Council for Scientiﬁc and Industrial
Research
South Africa

Abstract
Geospatially Enabled Scientiﬁc Workﬂows offer a promising toolset to help research-
ers in the earth observation domain with many aspects of the scientiﬁc process. One
such aspect is that of access to distributed earth observation data and computing
resources. Earth observation research often utilizes large datasets requiring extensive
CPU and memory resources in their processing. These resource intensive processes
can be chained; the sequence of processes (and their provenance) makes up a
scientiﬁc workﬂow. Despite the exponential growth in capacity of desktop comput-
ers, their resources are often insufﬁcient for the scientiﬁc workﬂow processing tasks
at hand. By integrating distributed computing capabilities into a geospatially enabled
scientiﬁc workﬂow environment, it is possible to provide researchers with a mecha-
nism to overcome the limitations of the desktop computer. Most of the effort on
extending scientiﬁc workﬂows with distributed computing capabilities has focused
on the web services approach, as exempliﬁed by the OGC’s Web Processing Service
and by GRID computing. The approach to leveraging distributed computing
resources described in this article uses instead remote objects via RPyC and the
dynamic properties of the Python programming language. The Vistrails environment
has been extended to allow for geospatial processing through the EO4Vistrails
package (http://code.google.com/p/eo4vistrails/). In order to allow these geospatial

Address for correspondence: Terence L van Zyl, Earth Observation Science and IT, Council for
Scientiﬁc and Industrial Research, P.O. Box 395, Pretoria 0001, South Africa. E-mail: tvzyl@
csir.co.za

© 2012 Blackwell Publishing Ltd
doi: 10.1111/j.1467-9671.2012.01317.x

234

T L van Zyl, A Vahed, G McFerren and D Hohls

processes to be seamlessly executed on distributed resources such as cloud comput-
ing nodes, the Vistrails environment has been extended with both multi-tasking
capabilities and distributed processing capabilities. The multi-tasking capabilities are
required in order to allow Vistrails to run side-by-side processes, a capability it does
not currently have. The distributed processing capabilities are achieved through the
use of remote objects and mobile code through RPyC.

1 Introduction

The last few decades has seen a legitimization of in-silico (Information Communication
Technologies enabled) science alongside more commonly known and accepted scientiﬁc
methods. Certain noteworthy characteristics of in-silico scientiﬁc endeavour are impor-
tant to identify. First, scientists have access to vast and increasing datasets, generated by
simulations, physics experiments, sensor networks and remote sensing instruments.
Second, modern computers have allowed scientists the latitude to develop complex,
long-running and often complex models and simulations which often produce enormous
datasets, such as is common in climate models. Thirdly, interconnected networks like the
Internet have enhanced opportunities for scientiﬁc collaboration, necessitating an
increased focus on data and model provenance, and have also resulted in more network-
centric or distributed deployments of data, processing, storage and metadata services and
resources. Fourthly,
specialized high-performance computing environments have
emerged to support these trends. The large data requirements and the need to perform
complex calculations on these data in distributed multi-core environments have made
scientists turn to the concept of scientiﬁc workﬂows (Asanovic et al. 2006, Barker and
Van Hemert 2008, Gil et al. 2007, Ludascher et al. 2006, Meglicki, 2001).

Research indicates that scientists can be shielded to an extent from the complexities
of these high performance environments through the mechanism of scientiﬁc workﬂows,
which automate complex processes and provide integrated access to datasets often
characterized by their large sizes and distributed locations (Gil et al. 2007, Gibson et al.
2007, Ludascher et al. 2006).

Current approaches to distributed scientiﬁc computing in the geospatial domain
commonly rely on the use of Open Geospatial Consortium (OGC) web services (Open
Geospatial Consortium 2010), Open Data Access Protocol (OPeNDAP) and, less com-
monly, GRID computing (Clematis et al. 2003; Foster et al. 2001, 2003). A major
limitation of these web services approaches is that deployments are static, cannot easily
be modiﬁed and require redeployment to enable minor changes. This may be acceptable
if the workﬂows and algorithms to be run upon these web services are predeﬁned, which
is usually the case. It is not entirely clear how a scientist would deploy his or her own
evolving experiments to these infrastructures yet still exploit the potentially large-scale
computing power and extensive data handling capabilities available at these resources
(Deelman and Chervenak 2008). Additonally it has been shown that the GRASS GIS
platform (http://grass.fbk.eu/) can take advantage of various of these approaches, includ-
ing OGC web services in the cloud, GRID and Message Passing Interface (MPI), to
achieve distributed and parallel processing within a GIS (Baranski 2008, Jin et al. 2011).
In a scientiﬁc workﬂow computing environment, scientists may introduce entirely
new algorithms and continuously make minor alterations to reﬁne them. In addition, the
pipelines connecting algorithms are continuously being altered as the scientist explores

© 2012 Blackwell Publishing Ltd
Transactions in GIS, 2012, 16(2)

Earth Observation Scientiﬁc Workﬂows

235

alternate mechanisms for proving or disproving their hypotheses. These algorithms and
pipelines of processes may work upon large data sets that should not be moved around,
but rather remain upon the specialized hardware and software infrastructures that
support them. In the case of geospatial-temporal processing, moving the data away from
the infrastructures that support indexing and rapid data retrieval, such as a spatial
database, to the infrastructure containing a process is not always as viable as moving the
process to the data. This suggests functional requirements to support mobile code in a
high performance geospatial-temporal processing environment (Barker and Van Hemert
2008, Deelman and Chervenak 2008).

Although GRID computing does promise much of the above, the approach can be
complicated, requiring the deployment of complex software infrastructure and expert
knowledge to use the GRID resources. In this article we describe an approach that is
lighter, more intuitive, and requires less specialized expertise on the part of the end user.
It is true that the approach is somewhat less sophisticated with respect to aspects such as
automated scheduling and load balancing; however we expect that this challenge will be
overcome in time (Foster et al. 2001, 2003).

The work described in this article couples the scientiﬁc workﬂow value proposition
to the use of remote objects and provision of mobile code support enabled by the
dynamic nature of the Python programming language. This article describes the broader
architecture and design of the work, some lessons learnt, and discussion of various
strengths and weaknesses of this particular approach. These lessons and discussions
include, for example, discovering limitations of inter-process communication and the
implications of the use of the C and C++ programming languages that underpin the
performance in many Free and Open Source Software for GIS (FOSS4G) software tools.
The remainder of the article proceeds as follows: in Section 2 we present some
background on geospatially enabled scientiﬁc workﬂows, remote objects and mobile
code; Section 3 looks at the design decisions made in building the prototype of our
system; Section 4 presents some results of using the prototype within the earth observa-
tion domain together with lessons learnt and Section 6 discusses future work and
research directions.

2 Background

In order to discuss the approach to distributed geospatial processing taken within this
article, the concepts of geospatially enabled scientiﬁc workﬂows, remote objects and
mobile code need to be explored. In the remainder of this section we expand on these
ideas and provide background information.

2.1 Geospatially Enabled Scientiﬁc Workﬂows

A toolset is required that supports: (1) the iterative and fast-paced nature of in-silico
science; (2) the imperative for researchers to share their results with the broader scientiﬁc
community; and (3) the tasks of verifying, interrogating and building upon the results of
others in the community. Scientiﬁc workﬂow software provides such a collaborative
environment, one that allows for proper provenance, reproducibility, extensibility,
knowledge sharing and automation of the scientiﬁc process.

© 2012 Blackwell Publishing Ltd
Transactions in GIS, 2012, 16(2)

236

T L van Zyl, A Vahed, G McFerren and D Hohls

Scientiﬁc workﬂows and workﬂow environments are used as means for modelling
and enacting scientiﬁc experiments. They share and leverage many features and tech-
niques of mainstream business workﬂows and workﬂow environments, but also offer
different or enhanced functionality. Conversely, scientiﬁc workﬂows promise to become
an important area of research within workﬂow and process automation, possibly leading
to the development of the next generation of problem-solving and decision-support
environments. Scientiﬁc workﬂow environments have capabilities for handling data or
transacting intense computation, more human interaction, and a large array of methods
and tools to support scientiﬁc activities (e.g. visualization, data transformation). Further,
they focus strongly on workﬂow provenance, capturing rich details of in-silico experi-
ments, and place an emphasis on re-execution and comparison (Barker and Van Hemert
2008, Deelman and Gil 2006, Deelman and Chervenak 2008, Howe et al. 2008).

This article uses the concept of geospatially enabled scientiﬁc workﬂows: a concept
that is tangential to that of geospatial workﬂows and expanded upon in previous work
by the authors. We subscribe to the following deﬁnition of a geospatially enabled
scientiﬁc workﬂow (McFerren et al. 2012):

In our conception of geospatially enabled scientiﬁc workﬂow environments,
geospatial functionalities and data are intermingled with a variety of other sets of
tools, functionalities and data, from, for example, the numerical modelling,
computational intelligence, high performance computing and statistical domains.

EO4Vistrails (http://code.google.com/p/eo4vistrails/), a FOSS4G, is an attempt to
support this concept of geospatially enabled scientiﬁc workﬂows. The EO4Vistrails
package was developed as part of the demonstration of distributed computing in the
research presented in this article. The EO4Vistrails package shown in Figure 1 extends
the capabilities of the Vistrails (http://www.vistrails.org) scientiﬁc workﬂow environment
(Callahan et al. 2006) to support geospatial functionalities and data-sources such as
OGC web services (Open Geospatial Consortium 2010). In EO4Vistrails, geospatial
functionality is supplied by the FOSS4G platforms QGIS (http://www.qgis.org/) Appli-
cation Program Interface (API) (OSGEO 2010), PostGIS (http://postgis.refractions.net/)
(Refractions Research 2010) and certain OGC services are being intertwined with a
variety of other toolsets, functionalities and data, for example, the Open Source numeri-
cal analysis library NetworkX (http://networkx.lanl.gov/), computational intelligence
libraries and statistical programs such as R (http://cran.r-project.org/) and PySAL (http://
code.google.com/p/pysal/). EO4Vistrails connects these toolsets with distributed and
high performance computing environments.

In Vistrails the basic unit of workﬂow composition is the Vistrails module. A
Vistrails module has a set of input ports and output ports. An input port can either take
its value from the output port of some other Vistrails module or may be set to a constant
value. When a Vistrails module is executed by the workﬂow engine the values from the
input ports are read, some processing is performed using those inputs and any results are
placed on the output ports for the next module. By connecting the input ports and output
ports of Vistrails modules in this way a pipeline is formed – this is a workﬂow (Callahan
et al. 2006).

Developers specialize the basic Vistrails module to provide various additional
modules with speciﬁc processing capabilities. All ports are typed and only input ports
and output ports of the same specialized type may be connected.

© 2012 Blackwell Publishing Ltd
Transactions in GIS, 2012, 16(2)

Earth Observation Scientiﬁc Workﬂows

237

Figure 1 The set of current geospatial modules provided by the EO4Vistrails package

2.2 Remote Methods and Mobile Code

As the use of parallel computing expands, architectures such as cloud computing and
standards such as Message Passing Interface (MPI) encourage scientists to construct
complex distributed solutions that span networks (Zinn et al. 2010). A valuable addition

© 2012 Blackwell Publishing Ltd
Transactions in GIS, 2012, 16(2)

238

T L van Zyl, A Vahed, G McFerren and D Hohls

to geospatially enabled scientiﬁc workﬂow environments is a simple, concise notation
that allows for easy parallelization of computing tasks and supports the composition of
large numbers of parallel computations aimed at parameter exploration.

Relational Database Management Systems have long demonstrated the usefulness of
mobile code. Structured Query Language (SQL) requests and commands are often just
concise code snippets sent to a remote data management system where the statements are
executed, processing occurs, changes are made to the data source and/or results are
returned. In this context, scientiﬁc computing and relational database computing have
similar characteristics – notably the need to perform operations on a dataset at the data
source, this being more efﬁcient than retrieving the entire (often large) dataset and
performing those operations locally.

The Python language, with its dynamic nature, simpliﬁes the use of mobile code and
allows for some elegant and sophisticated solutions in which mobile code can be sent to
a remote data source for execution. Another technology is that of remote objects
supported by remote method invocation (RMI). This technique allows developers to
instantiate an instance of a object on a remote compute node and interact with it as if it
were a local instance. Our research involved the evaluation of a number of remote objects
and parallel process execution frameworks for Python, including Pyro, Python multipro-
cessing, Parallel Python and IPython. Eventually RPyC (http://rpyc.sourceforge.net/) was
chosen due to its ﬂexibility, stability and support for both mobile code and remote
objects.

RPyC provides remote objects capabilities via remote method calls, remote proce-
dure calls and support for mobile code execution on a compute node. We use the term
compute node to refer to a computing resource along with any peripheral network and
data handling capabilities that allows remote access to a full Python interpreter via
RPyC. It is thus conceptually correct for a number of compute nodes to be running
on a single physical machine, a high performance cluster or a cloud computing
infrastructure.

3 Design

In previous work we introduced the concept of geospatially enabled scientiﬁc workﬂow
environments where geospatial tools, functionalities and data are used in conjunction
with other research tools, functionalities and data, from, for example, the visualization,
numerical modelling and simulation, computational intelligence, high performance com-
puting and statistical domains (McFerren et al. 2010). We noted too, the key role
FOSS4G tools provide in making such environments possible. We note that FOSS4G
tools provide possibilities for geospatially enabling scientiﬁc workﬂows and allow oppor-
tunities for experimentation, which is vital to the scientiﬁc workﬂow process. The low
cost and variety of FOSS4G tools available ofers functionality and ﬂexibility for accom-
plishing scientiﬁc inquiry (McFerren et al. 2010). Our previous work experimented with
combinations of scientiﬁc workﬂow environments and geospatial tools; here we discuss
the advances and results accrued from developing and utilizing software on a single
scientiﬁc workﬂows platform.

The remainder of the section brieﬂy describes the architecture and design of the
chosen platform, introducing the EO4Vistrails extension, before unpacking the various

© 2012 Blackwell Publishing Ltd
Transactions in GIS, 2012, 16(2)

Earth Observation Scientiﬁc Workﬂows

239

geospatial and high performance computing components that it delivers. We then brieﬂy
discuss some implementation experiences, before addressing some challenges we expe-
rienced and future research directions.

3.1 Architecture

EO4Vistrails is an extension to the scientiﬁc workﬂow, visualization and data provenance
system Vistrails (Callahan et al. 2006) that provides functionality for working with
geospatial libraries, data stores, data types and services. EO4Vistrails recognizes several
important points about geospatio-temporal data:

• Earth observation and geospatial data are often large
• Data and processing capabilities are often found behind complex web service

• It is not always advisable to move these data around; sometimes it is more pragmatic

to move the analysis/processing code around

• GIS tools are not necessarily the key tools used by researchers in their spatio-temporal

interfaces

analytic work

• The time dimension is increasingly important to researchers working with Earth

• Researchers wish to focus on their research, not web service protocols or geospatial

Observation and geospatial data

data transformations

To support these points, EO4Vistrails provides web service aware components, distrib-
uted and multi-process aware helper utilities, and access to scientiﬁc data types and
libraries. The view of Earth Observation (EO) or geospatial data drops in importance
somewhat, with the focus on shaping such data for use in numerical modelling, simula-
tion and statistical tools in addition to the primary geo-information aware tools.
EO4Vistrails attempts to provide transparent geospatial data transformations, where
possible.

includes

In fact, Vistrails

Vistrails itself is a PyQT-based application. This has two notable efects. Firstly,
modules and extensions of Vistrails are built in Python and can harness much of the
large array of scientiﬁc computing and geospatial
libraries that expose a Python
API.
such as Matplotlib (http://matplotlib.
sourceforge.net/) and VTK (http://www.vtk.org/) in its base conﬁguration, while user-
space packages for Numpy and SciPy (http://numpy.scipy.org/) are also provided. Sec-
ondly, Vistrails utilizes the graphical user interface and non-gui functionality provided
by the rich, underlying QT set of libraries – this assists in rapid development of any
GUI components necessary in a Vistrails extension. This use of QT allows for ready
mappings between Vistrails and the QT/PyQT based QGIS (and potentially the
reverse).

libraries

Vistrails/EO4Vistrails is a desktop-based application, but uses various Python librar-
ies for accessing Internet-based data and services. Vistrails’ core, and extensions to it,
consist of packages of modules. Functionality is contained in these modules; scientiﬁc
workﬂows are constructed by pipelining various modules. Modules are connected via
strongly typed input and output ports. Modules are not linked until runtime, but there
is opportunity at conﬁguration time to parametrize modules and inspect metadata about
potential module input.

© 2012 Blackwell Publishing Ltd
Transactions in GIS, 2012, 16(2)

240

T L van Zyl, A Vahed, G McFerren and D Hohls

3.2 Functionalities

The functionalities provided by EO4Vistrails can broadly be broken into a number of
categories. Table 1 shows a list of the modules provided.

3.3 High Performance and Distributed Computing Utilities

In order to allow for the execution of Vistrails modules on remote compute nodes a
decorator is applied to a standard Vistrails module. The decorator is a contractual
agreement by the developer indicating the module is now capable of executing on remote
compute nodes, i.e. it is thread and multiprocessor safe. In addition, the decorator acts
as a python Mixin (Wikipedia 2011) that upon execution of the Vistrails module by the
workﬂow engine, replaces the module in Vistrails on the user’s desktop with a proxy and
moves all execution code to the remote compute nodes. This proxy Vistrails module is
wired to the Vistrails module on a remote compute node. In other words: remote
objecting occurs. The result is that the user of the Vistrails system is unable to diferentiate
between a locally executing Vistrails module and a remotely executing Vistrails module
as shown in Figure 2. In fact, if there are no remote compute node capabilities available
the module will run on the local machine as if it were a normal Vistrails Module.

In the instance where the user wishes to execute a code snippet such as a new algorithm
in his or her workﬂow, the system uses mobile code techniques to transfer the given code
to the remote compute node as shown in Figure 3; any python code can be executed in this
manner. All relevant variables from the current local context are also transferred to the
remote context. The remote compute node is now able to execute the mobile code in its
own context. Upon completion of execution on the remote compute node, relevant results
are copied back into the local context. It should be noted that the use of remote objects in
this research involves mobile code, since the objects need not have existed on the remote
compute node prior to execution, but are transferred as needed.

The result is that the entire process of remote code execution is abstracted away from
the user. This allows the user to focus on the scientiﬁc workﬂow construction and
evaluation and less on the technology that enables it. Long running, complex modules
can be deployed dynamically to distributed high performance computing infrastructure
where they have access to large amounts of RAM, CPU cycles and high performance data
access, simply by selecting the appropriate destination from a drop down combobox as
shown in Figure 4.

Where possible, the appropriate library dependencies and Vistrails dependencies are
also remoted to the executing remote compute node. Effectively, the compute nodes need
have very little knowledge of the workﬂows that will be executed on them. This creates
a level of independence between the user and the infrastructure being used. In our
research, we seamlessly integrated remote execution of PostGIS queries, Numpy, Net-
workX and PySAL on distributed computing resources to perform spatio-temporal
processing of multi-billion row datasets. (signiﬁcantly reducing processing time and
allowing careful re-inspection of the process steps to check for correctness).

4 Results

The system was primarily tested in the wildﬁre research domain using, amongst others, the
workﬂow shown in Figure 5. A series of 500 m burned area observations from the MODIS

© 2012 Blackwell Publishing Ltd
Transactions in GIS, 2012, 16(2)

Earth Observation Scientiﬁc Workﬂows

241

Table 1 Modules provided by EO4Vistrails package

Category

Name

Description

Data Access

Vector Layer
GML String
GeoJSON String
Raster Layer
Map Layer
Temporal Vector Layer
WKT String
OGC SOS OGC WCS OGC WFS

Stores feature data as a QGIS vector layer
A GML data model
A GeoJSON data model
A generic coverage/raster data model via QGIS
Storing raster and vector data (in QGIS)
Spatial temporal vector layer
A well known text data model
Provides an OGC Sensor Observation Service

client

NetCDF Cube Reader
Area Of Interest Deﬁner Line Of

Provides an OGC Web Coverage Service client
Provides an OGC Web Feature Service client

Connected Components

Use QGIS to write a vector layer Adding spatial

Interest Deﬁner Point Of
Interest Deﬁner

EPSG Code
OGR Transform
QGIS Layer Writer
Weight Matrix
Graph

Octave Script
R Script
PovRay Script RPyC C Code
RPyC Python Script
RPyC Node
Fork
Basic Returning Query
Copy From File To Table Copy
From Table To File Feature
Returning Query
Non Returning Query
Numpy Returning Query

PostGIS Session
Reproject PostGIS Table

RasterLang Script
Save Array to Raster File
Array As Layer Layer As Array
QGIS Map Canvas

Helpers

PySAL
NetworkX

Scripting

PostGIS

Raster Lang
Visualisation

© 2012 Blackwell Publishing Ltd
Transactions in GIS, 2012, 16(2)

A NetCDF data model
Deﬁnes a Spatial area of interest
Deﬁnes a Spatial line of interest, e.g. a transect
Deﬁnes a Spatial point of interest
Select an EPSG code for map projection

purposes GDAL/OGR to perform transforms
on vector data

analysis functions

Adding graph and network analysis
Returns Connected Components
Matlab like scripting in octave
R scripting Ray tracing
Remote C compilation and execution
Remote Python scripting
Deﬁne a remote node to execute code
Fork the above process running multi-threaded

Provides a PostGIS query via psycopg2
Copy a ﬁle into a PostGIS table Copy a PostGIS

table into a ﬁle

Return QGIS Vector layer from database
Run a non-returning query e.g. create a table

Return a Numpy array from a database query

QGIS

Reproject a table and update spatial metadata
Load a shapeﬁle into a PostGIS table
Raster manipulation and analysis
Save a Numpy array to a raster ﬁle
Convert an array to a raster layer Convert a

raster layer to an array

Display raster and vector layers

RPyC

Shapeﬁle Loader

Provides a PostGIS client via psycopg2 and

242

T L van Zyl, A Vahed, G McFerren and D Hohls

Figure 2 A Vistrails module capable of executing on a remote compute node looks and
behaves just like any other Vistrails module as shown in the ﬁgure. This makes the
learning required in order to use remote execution minimal

instrument for a four year period, across four scenes, derived from two algorithms (>200
individual scenes) needed to be roughly clustered (via spatial and temporal adjacency) into
presumed individual ﬁre events for the given time period. These events then needed to be
associated with weather data at a coarser resolution, and the resultant data exported to a
collection of CSV ﬁles for use in a downstream statistical analysis. The approach taken to
this problem involved the development of a graph data structure from a set of spatial and
temporal queries and the use of an algorithm to extract all the sub-graphs, where each
sub-graph would be one ﬁre event. PostGIS, PostGIS WKT Raster and NetworkX were the
primary tools used in the exercise, alongside the functionality provided by Vistrails.
Figure 6 shows the provenance of this workﬂow as provided by Vistrails.

Given the large spatial and temporal extent of the data and the number of spatial
tests that needed to be executed, the RAM requirements and the parallel nature of the
problem necessitated the use of some high performance computing capabilities. The
problem was initially solved on the desktop in EO4Vistrails using a small subset of the
data since executing on the full dataset failed with insufcient RAM (>4.5 GB) for a 32bit
processor. Then the relevant modules were tasked to execute on remote compute nodes
and the workﬂow was re-parameterized and re-executed, without altering or specializing
the logic of the process. Results were returned acceptably after a moderately long running
time ((cid:2)2 Hours). Current ﬁre-related research involves spatio-temporal processing of
large datasets to extract statistics for several vegetation indices from multi-year satellite
based observations of the southern African region (>1,000,000 km2).

© 2012 Blackwell Publishing Ltd
Transactions in GIS, 2012, 16(2)

Earth Observation Scientiﬁc Workﬂows

243

Figure 3 Example of a code snippet to be executed on a remote compute node. The
code is moved as is to the remote node and executed

Figure 4 Selection of the destination remote compute node from a drop down selec-
tion box. The above examples show three possibilities, local execution in the main
process, remote execution on a given RPyC node using the IP address and local execu-
tion using multi-cores in own process

Other problems that have been tested using remote compute nodes include various
raster operations such as bulk Normalized Difference Vegetation Index (NDVI) calcula-
tions. The overall result is that the architecture and design of the system supported the
requirements as laid out in Section 1. In the following section we describe some of the
lessons learnt.

5 Discussion and Lessons Learned

The overall result was mixed, with some success and in other places much difﬁculty. Here
we outline and discuss some of the lessons learned.

© 2012 Blackwell Publishing Ltd
Transactions in GIS, 2012, 16(2)

244

T L van Zyl, A Vahed, G McFerren and D Hohls

FIRE DATABASE
(PostGisSession)

RpyC Node

Fetch self-unioned feature ids
(Basic Returning Query)

Generate Neighbourhoods Graph
(Graph)

Find connected neighbours
(connected_components)

RPyC Code

Prep db for indiv fires
(Non Returning Query)

Insert indiv fires
(Non Returning Query)

ExecuteInOrder

spatial merge final fires
(Non Returning Query)

ExecuteInOrder

Figure 5 The workﬂow connects to PostGIS to retrieve the MODIS data as pixels
(>500 MB). NetworkX modules convert the pixels into a adjacency graph both spatially
and temporally (>2 GB). The connected components of graph are single ﬁre events
which are combined

© 2012 Blackwell Publishing Ltd
Transactions in GIS, 2012, 16(2)

Earth Observation Scientiﬁc Workﬂows

245

Figure 6 E04Vistrails allows the user to shift between a local mode executing on his/her
own machine and a distributed mode executing on a remote machine with little to no
additional efort. The Vistrails provenance framework allows the user to compare the
results of these runs

Modules should be stateless and atomic. This is especially important when modules
need to be placed on remote nodes and when parallelization is required. This style of
componentization is common in scientiﬁc workﬂow environments and encouraged by the
community at large. However, this has a signiﬁcant impact on performance, as the module
needs to complete its operations on all the data before it can pass control to the next
Vistrails module. A more efﬁcient mechanism may be for the current module to perform
some processing and write back intermediate results for the next Vistrails module in the
processing chain to work on concurrently, though this possible solution remains an open
research question. This limitation can be overcome to some extent by allowing for multiple
concurrent executions of the same workﬂow each tackling a diferent piece of the same
problem.

Passing of URIs is preferable to passing of data in many cases. A module should read
the data from the URI (which may be local). After processing, the module should write
the results back to a ﬁle. This procedure has signiﬁcant beneﬁt since the storing of
intermediate results allows for a form of caching such that workﬂows can be resumed
from the last unchanged module. Additionally, this provides for persistence of interme-
diate results. Passing URIs around is far cheaper than moving data and allows for a level
of ﬂexibility where, for example, if a module is run from different locations, there is no
effect on the code.

C and C++ code underlying Python libraries is not easily remoted in the same way
that Python code is. In these scenarios the libraries must be present on the remote
compute node.

C and C++ data structures such as Numpy arrays and the GDAL and OGR data
translators cannot be used via Python remote objects. These issues can be overcome by
following the recommendation to write all intermediate results to disk or some shared
high performance data space.

© 2012 Blackwell Publishing Ltd
Transactions in GIS, 2012, 16(2)

246

T L van Zyl, A Vahed, G McFerren and D Hohls

Visualization requires special precautions especially in cases where the interaction
with the visualization is dynamic such as found in a GIS. In cases of less interactive
visualization, the images or videos may be generated and transferred to the user’s desktop
from the remote nodes.

Vistrails modules accessing data from a Vistrails module on another compute node
can be prohibitively slow, even when the compute nodes reside on the same machine.
This is due to inter-process communication. This problem has been overcome on a single
physical device by placing all data structures in shared memory. However, when working
across physical devices, the correct solution is the intermediate writing of data to a fast
shared data store such as a Storage Area Network (SAN) or Network-Attached Storage
(NAS). In the case where two processes will run sequentially on the same data, the two
Vistrails modules can be run in the same compute node one after each other. Certain
naturally parallel processes are not bound by the same problem.

Although the RPyC approach taken tries to protect the end user from the underlying
technology and the details of where processes are running as much as possible, this is not
always easy when Vistrails modules make use of local resources such as temporary ﬁles.
However this detail in most cases becomes a challenge for the Vistrails module developers
as opposed to the end user.

Allowing users to execute arbitrary mobile code on private infrastructure may seem
like a major security issue. Providing read only access to a data source may be one
solution, which would not hinder the scientiﬁc process much, since most scientiﬁc
processes typically transform a copy of the source data rather than the source data itself.

6 Conclusions

In conclusion, the use of remote objects and mobile code coupled with geospatially
enabled scientiﬁc workﬂows can provide a complete and viable alternative to the Web
Service based approaches. However these two approaches are not juxtaposed. In fact
when remote objects and mobile code are combined with the web services based
approach, the beneﬁts of both technologies may be exploited to achieve better results.
Our future work will focus on how this might be accomplished. We expect that an
increased uptake of scientiﬁc workﬂows and access to distributed commodity computing
resources will see mobile code approaches becoming mainstream. The architecture and
design outlined here is a candidate solution for enabling these resources in the geospatial
scientiﬁc computing environment.

This article discusses advances in the development of a software environment with
capabilities for accessing, utilizing and incorporating geospatial data and processes into
the scientiﬁc workﬂows of researchers interested in incorporating earth observation and
geospatial analysis into their work. In previous work, we introduced the concept of
geospatially enabled scientiﬁc workﬂows (McFerren et al. 2010); this article shares
progress made, insights gained and future research avenues exposed in the process of
implementing EO4Vistrails, a geospatial extension to the open-source VisTrails scientiﬁc
workﬂow, provenance management, data exploration and visualization system. FOSS4G
software and toolsets are heavily used in EO4Vistrails to provide spatial database, OGC
Web Service, geospatial analysis, data transformation and visualization capabilities. We
identify the reasons for the suitability and usefulness of spatial databases in scientiﬁc
workﬂows, in the context of wildﬁre research, noting, for example, that (spatial) SQL

© 2012 Blackwell Publishing Ltd
Transactions in GIS, 2012, 16(2)

Earth Observation Scientiﬁc Workﬂows

247

provides a clear provenance mechanism and a means to distribute processing tasks.
Regarding OGC web services, we report that certain services are readily implemented into
scientiﬁc workﬂows, notably the WFS and WCS, since the kinds of data they work with are
well supported by many tools and applications. Other services are more difﬁcult to
incorporate: the SOS outputs data that are difﬁcult to translate into formats acceptable to
most scientiﬁc software; the WPS is very generic, increasing the challenge of developing
workﬂow modules that can handle dynamically the variability of inputs and outputs that
can be expected for a WPS. We highlight how the QGIS PyQT API provides a good
integration point for accessing data transformation and visualization functionality, and
hint at the possibilities of using Vistrails to provide a “ModelBuilder” like environment for
QGIS. We describe the relatively seamless ﬂow of spatial data from the geospatial
environment into more conventional scientiﬁc data analysis tools (e.g. Numpy, graph
theory tools) and the return of results to the geospatial environment where further spatial
processing and visualization can be performed. This last area is where most further
research avenues lie.

References

Asanovic K, Bodik R, Catanzaro B C, Gebis J J, Husbands P, Keutzer K, Patterson D A, Plishker
W L, Shalf J, Williams S W, and Yelick K A 2006 The Landscape of Parallel Computing
Research: A View from Berkeley. Berkeley, CA, University of California Berkeley, Department
of Electrical Engineering and Computer Science Technical Report No. 2006–183

Baranski B 2008 Grid computing enabled web processing service. In Proceedings of the Sixth

Geographic Information Days, Muenster, Germany

Barker A and Van Hemert J 2008 Scientiﬁc workﬂow: A survey and research directions. In
Wyrzykowski R, Dongarra J, Karczewski K, and Wasniewski J (eds) Parallel Processing and
Applied Mathematics. Berlin, Springer Lecture Notes in Computer Science Vol. 4967: 746–53
Callahan S P, Freire J, Santos E, Scheidegger C E, Silva C T, and Vo H T 2006 Vistrails:
Visualization meets data management. In Proceedings of the ACM SIGMOD Conference on
Management of Data, Chicago, Illinois

Clematis A, Mineter M, and Marciano R 2003 High performance computing with geographical

data. Parallel Computing 29: 1275–79

Deelman E and Chervenak A 2008 Data management challenges of data-intensive scientiﬁc work-
ﬂows. In Proceedings of the Eighth IEEE International Symposium on Cluster Computing and
the Grid (CCGRID ’08), Lyon, France: 687–92

Deelman E and Gil Y 2006 Managing large-scale scientiﬁc workﬂows in distributed environments:
Experiences and challenges. In Proceedings of the Second IEEE International Conference on
e-Science and Grid Computing (e-Science ’06), Amsterdam, The Netherlands

Foster I, Kesselman C, and Tuecke S 2001 The anatomy of the grid: Enabling scalable virtual

organizations. Journal of High Performance Computing Applications 15: 200–22

Foster I, Kesselman C, Nick J M, and Tuecke S 2003 The physiology of the grid. In Berman F, Fox
G, and Hey A J G (eds) Grid Computing: Making the Global Infrastructure a Reality. New
York, John Wiley and Sons: 217–50

Gibson A, Gamble M, Wolstencroft K, Oinn T, and Goble C 2007 The data playground: An
intuitive workﬂow speciﬁcation environment. In Proceedings of the Third IEEE International
Conference on e-Science and Grid Computing, Bangalore, India: 59–68

Gil Y, Deelman E, Ellisman M, Fahringer T, Fox G, Gannon D, Goble C, Livny M, Moreau L,
and Myers J 2007 Examining the challenges of scientiﬁc workﬂows. Computer 40(12): 24–
32

Howe B, Lawson P, Bellinger R, Anderson E, Santos E, Freire J, Scheidegger C, Baptista A, and Silva
C 2008 End-to-end e-science: Integrating workﬂow, query, visualization, and provenance at an
ocean observatory. In Proceedings of the Fourth International IEEE Conference on e-Science
(e-Science ’08), Indianapolis, Indiana: 127–34

© 2012 Blackwell Publishing Ltd
Transactions in GIS, 2012, 16(2)

248

T L van Zyl, A Vahed, G McFerren and D Hohls

Jin H, Jespersen D, Mehrotra P, Biswas R, Huang L, and Chapman B 2011 High performance
computing using mpi and openmp on multi-core parallel systems. Parallel Computing 37:
562–75

Ludascher B, Bowers S, McPhillips T, and Podhorszki N 2006 Scientiﬁc workﬂows: More e-science
mileage from cyberinfrastructure. In Proceedings of the Second IEEE International Confer-
ence on e-Science and Grid Computing (e-Science ’06), Amsterdam, The Netherlands
McFerren G, van Zyl T, and Vahed A 2010 FOSS geospatial libraries in scientiﬁc workﬂow
environments: Experiences and directions. In Proceedings of FOSS4G 2010, Barcelona, Spain
McFerren G, van Zyl T, and Vahed A 2012 FOSS geospatial libraries in scientiﬁc workﬂow

environments: Experiences and directions. Applied Geomatics 4: in press

Meglicki Z 2001 Advanced Scientiﬁc Computing. Computational Science Books (E-book; available

at http://beige.ucs.indiana.edu/B673/B673.pdf)

Open Geospatial Consortium 2010 OGC standards and speciﬁcations. WWW document, http://

Open Source Geospatial Foundation 2010 Quantum geographic information system. WWW docu-

www.opengeospatial.org/standards

ment, http://qgis.osgeo.org

Refractions Research 2010 Postgis: Support for geographic objects to the postgresql object-

relational database. WWW document, http://postgis.refractions.net
Wikipedia 2011 Mixin. WWW document, http://en.wikipedia.org/wiki/Mixin
Zinn D, Hart Q, Ludascher B, and Simmhan Y 2010 Streaming satellite data to cloud workﬂows
for on-demand computing of environmental data products. In Proceedings of the Fifth Work-
shop on Workﬂows in Support of Large-Scale Science (WORKS), New Orleans, Louisiana:
1–8

© 2012 Blackwell Publishing Ltd
Transactions in GIS, 2012, 16(2)

