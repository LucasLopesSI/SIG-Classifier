International Journal of Geographical Information
Science

ISSN: 1365-8816 (Print) 1362-3087 (Online) Journal homepage: http://www.tandfonline.com/loi/tgis20

Wavelets applied to simplification of digital terrain
models

Jan T. Bj⊘rke & Stein Nilsen

To cite this article: Jan T. Bj⊘rke & Stein Nilsen (2003) Wavelets applied to simplification
of digital terrain models, International Journal of Geographical Information Science, 17:7,
601-621, DOI: 10.1080/1365881031000135500

To link to this article:  http://dx.doi.org/10.1080/1365881031000135500

Published online: 19 May 2010.

Submit your article to this journal 

Article views: 252

View related articles 

Citing articles: 23 View citing articles 

Full Terms & Conditions of access and use can be found at
http://www.tandfonline.com/action/journalInformation?journalCode=tgis20

Download by: [The University Of Melbourne Libraries]

Date: 29 March 2016, At: 07:48

GIS 100589

. .   , 2003
. 17, . 7, 601–621

Research Article

Wavelets applied to simpliﬁcation of digital terrain models

JAN T. BJØRKE and STEIN NILSEN
Norwegian Defence Research Establishment, P.O. Box 115, N-3191 Horten,
Norway
e-mail: jan-terje.bjorke@ﬃ.no and stein.nilsen@ﬃ.no

(Received 12 July 2002; accepted 20 February 2003)

Abstract. Wavelet thresholding is used to generate simpliﬁed terrain models.
This non-linear ﬁltering technique is adaptive in the sense that the large wavelet
coeﬃcients in areas of high relief are kept, whereas all coeﬃcients smaller than a
threshold value, representing smooth areas, are thrown away. The degree of detail
in the simpliﬁed model is controlled by the number of wavelet coeﬃcients retained.
Two diﬀerent areas are analysed using this method. The experiments show the
smoothing eﬀect of the thresholding, and how the main terrain features survive
as the threshold value increases. At large threshold values the method introduces
blocking artifacts in the model.

1.

Introduction
Today the automated techniques for measuring the relief of the earth can be used
to construct high-resolution digital terrain models (DTMs). For example, Wever and
Lindenberger (1999) argued that laser-scanning in 1999 could cover land areas with
ground resolution better than 1.5 m at a reasonable cost. Modern multibeam echo
sounders can measure the seaﬂoor with metre or even cm resolution in dedicated
areas. The new generation of DTMs, therefore, will contain detailed information
about the structure of the earth’s topography. In the broad range of DTM applica-
tions the need for detailed information is diﬀerent, and the applications will beneﬁt
from eﬃcient methods for DTM simpliﬁcation. For example, distracting undulations
introduced by random measurement errors or unnecessary terrain details can by
DTM simpliﬁcation be smoothed out in cartographic products like perspective
images or relief maps.

According to the cartographic terminology, simpliﬁcation can be regarded as a
part of cartographic generalization (Morrison 1974, McMaster and Shea 1992).
Cartographic generalization, in addition to simpliﬁcation, deals with processes such
as exaggeration, displacement, enhancement, amalgamation, collapse etc. Weibel
(1995) divides cartographic generalization into model generalization and graphics-
oriented generalization. The objectives of model generalization are controlled data
reduction and the derivation of databases at multiple levels of accuracy and reso-
lution. Simpliﬁcation, according to the terminology of the present paper, is related
to model generalization, i.e. the goal of simpliﬁcation is controlled data reduction.

International Journal of Geographical Information Science
ISSN 1365-8816 print/ISSN 1362-3087 online © 2003 Taylor & Francis Ltd
http://www.tandf.co.uk/journals
DOI: 10.1080/1365881031000135500

Downloaded by [The University Of Melbourne Libraries] at 07:48 29 March 2016 602

J. T . Bjørke and S. Nilsen

We will restrict the paper to the investigation of a new method for the simpliﬁca-
tion of DTMs, i.e. a wavelet based method. Wavelets are not much used in GIS
despite their many attractive properties in terrain modelling (Bjørke and Nilsen
2002). The present paper will show how the small relief features of a terrain model
can be smoothed out and how the main structure of the model will survive as
coeﬃcients are removed from its wavelet representation.

The method we have chosen for our investigations is a biorthogonal 2D wavelet
transform which reproduces polynomials of degree two. This wavelet has the property
of conserving average values of the data at diﬀerent scales, meaning that the average
height of an approximating surface over a grid block will be the same as the average
height of the original data over that same block.

In the literature there are described several techniques for surface simpliﬁcation.
Jones (1997), for example, mentions the n-point method, local polynomials, Fourier
series, and triangulated irregular networks (TINs). Adaptive TINs (Heller 1990) are
widely used for the simpliﬁcation of terrain models. By the introduction of bandwidth
parameters or threshold values, TINs can eliminate unwanted detail. During the last
decade there has been substantial development of adaptive and hierarchical TIN-
methods, (Park et al. 2001, Dæhlen and Fimland 1999, Misund 1996, De Floriani
and Puppo 1995, Midtbø 1994). Compared to TINs, the wavelet method shows
stronger numerical stability and lower time complexity in the generation of
generalized surface models (Bjørke and Nilsen 2002).

The structure of the paper is as follows: in the next section we present the wavelet
method. Then we present our case studies and discuss the results. Two areas in
Norway with diﬀerent topographies are selected for our case study; one with high
relief and the other with low relief. We conclude the paper with some remarks on
future work.

2. Wavelets

The wavelet method provides us with a new research area in digital cartography
and GIS. It seems strange that the wavelet method has not been used more in these
ﬁelds, because already at the beginning of the 1990s cartography was used as an
example of a situation where there is a clear need for a multi-scale treatment;
important information is present on a large number of scales (Meyer 1993). The
same holds for generation, analysis and use of digital terrain models. Since terrain
contains variations on many scales, and diﬀerent uses of terrain models require
diﬀerent accuracy and resolution, a multi-scale representation is preferable.

The concepts of scale and resolution have a clear meaning in wavelet theory, and
a wavelet representation is a multi-scale or multi-resolution representation. This
suggests that wavelets at least are a good candidate for representation of terrain
data. Wavelets also have other properties that make them suitable for representing
terrain models: the algorithms are fast and stable, and there is no need for complicated
data structures; the representation is eﬃcient, making it possible to obtain large
compression ratios; and error estimates are easy to calculate. Some of these issues
have already been explored in the geoscience literature: Bjørke and Nilsen (2002)
show how wavelets can be used to reduce the amount of computer storage space
required to represent DTMs; Bruun and Nilsen (2001, 2003) have developed a quad-
tree wavelet algorithm that makes it possible to handle huge data sets; Balboa and
Lope´z (2000) investigate the application of wavelets in line generalization.

Downloaded by [The University Of Melbourne Libraries] at 07:48 29 March 2016 Wavelets applied to simpliﬁcation of DT Ms

603

2.1. Modelling terrain with wavelets

Several issues need to be addressed when deciding which wavelet to use for
modelling terrain. For instance, if we are building a global model we might want to
use wavelets based on a spherical triangulation of the Earth instead of the usual
‘quadrilateral’ 2D wavelets based on tensor products of 1D wavelets (Schro¨ der and
Sweldens 1995). A drawback with this is the more complex data structure that is
needed. Another problem is that there are very few results on the accuracy of wavelet
approximations based on semi-regular triangulations.

The regularity of the data set also plays a role in the choice of which wavelet to
use. For smooth data, such as solutions of some diﬀerential equations, a better
performance can be achieved by using very regular wavelets. The regularity, or
smoothness, of a wavelet is related to the length of the wavelet ﬁlters; smooth
wavelets have long ﬁlters. If the correlation length of the data is small, wavelets with
shorter ﬁlters perform just as well.

For terrain data, our working hypothesis is that the height of one cell can be
reasonably well predicted using only the eight neighbouring cells. Consequently, we
use wavelet ﬁlters that can only ‘see’ as far as the nearest neighbours.

The wavelets we have chosen for our investigations are known as biorthogonal
average interpolating wavelets. They have the property of conserving average values
of the data at diﬀerent scales. This means that the average height of an approximating
surface over a grid block will be the same as the average height of the original data
over that same block.

As an introduction, we start by working through the Haar transform, which is
the simplest example of a wavelet transform and also an average interpolating
wavelet. We next modify the Haar transform in order to obtain wavelets that are
smoother, and have more potential for data compression. It turns out there is a
whole family of average interpolating wavelets. Most of this material is covered in
greater detail in Daubechies (1992). The theory of average interpolating wavelets is
treated in Donoho (1993).

With large data sets, memory or processing requirements make it necessary to
divide the data into smaller blocks and process each block individually. We thus
need wavelet transforms adapted to ﬁnite domains. Here we follow the method
presented in Sweldens and Schro¨ der (1996).

After a description of how to construct wavelets in several dimensions, we brieﬂy
explain the process of wavelet thresholding. We end the wavelet section with some
suggestions on the implementation of the method.

2.2. T he Haar transform

The fundamental idea in wavelet analysis is to use a pair of ﬁlters to split a data
sequence into two parts: a low frequency component (drift, trend), and a high
frequency component (ﬂuctuation). The ﬂuctuation component is stored, and the
trend is split into new trend and ﬂuctuation sequences using the same ﬁlters. The
process is repeated until some coarsest scale is reached.

J

Let f

be a piecewise constant function deﬁned on I=[0, 1), where the intervals

have length 2−J, JµN ﬁxed. We can represent f
by the sequence
={s
(2J−1)}
J
J
over intervals of length 2−J, see ﬁgure 1.
The set of piecewise constant functions on intervals of length 2−j, jµN is a

of average values of f

J
(2J−2), s

(1), ..., s

(0), s

s

J

J

J

J

Downloaded by [The University Of Melbourne Libraries] at 07:48 29 March 2016 604

J. T . Bjørke and S. Nilsen

Figure 1. Sequences S

(k) and S

(k).

2

1

. The function f

deﬁned above is thus in

J

j

s

J

j−1

vector space. We call this vector space V
V

. Note that V
We deﬁne two new sequences s

for all j.
and d
by
J−1
(2k+1)+s
J
(2k+1)−s

is a subspace of V
j
J−1
(k)=D(s
J
(k)=D(s
J
for k=0, 1, ..., 2J−1−1. The sequence s
J−1
intervals of length 2−J+1, which represents a function in V
onto the space V
from V
a projection of s
projection is represented by the sequence d
onto a space called W
projection from V
information between V
J−1
as a sum of an element in V

. Every element in V
and an element in W

J−1
d
J−1

(2k))

(2k))

J

J

J

J

J

and V
J−1

J
J−1

f

J

contains the average values of
J−1

over
. Equation (1) describes
. The information lost under this
. This sequence is the result of a
, which contains the diﬀerence in
can be represented uniquely
. We write

J−1
J−1
J−1

V

=V

+W

J−1
is the trend at this scale, and the component in W

J−1

The component in V
is the
ﬂuctuation. By adding and subtracting equations (1) and (2) we get back the sequence
s
J−1
J
and d

, so the transform is invertible, and all the information in s

is contained in s

J−1

J−1

J

J

.

This process can clearly be repeated. At each level, we decompose the trend at
that level into a trend and a ﬂuctuation at the next coarser level. The trend at level
j+1 lies in the space V
, and the ﬂuctuation
j+1
at level j is in the space W

. We thus get a decomposition of every space V

, the trend at level j lies in the space V

j

,

J−1

j+1

The projections are given by

j

V

j+1

=V

+W

j

j

s

j
d

(k)=D(s
(k)=D(s

j

j+1
j+1

(2k+1)+s
(2k+1)−s

(2k))

(2k))

j+1
j+1

(1)

(2)

(3)

(4)

As above, the transform is invertible, so if we know the components of a signal
. The decomposition process is

, we can ﬁnd the components in V

and W

in V
stopped at some coarsest scale V

j

j

. We have

0
+W

=W

V

J

J−1

+ ··· +W

+W

+V

1

0

0

J−2

j+1

The transform is depicted graphically in ﬁgure 2. Instead of storing all the ﬁne

Downloaded by [The University Of Melbourne Libraries] at 07:48 29 March 2016 Wavelets applied to simpliﬁcation of DT Ms

605

Figure 2. Decomposition sequence.

scale averages s
averages s

.

J

0

, we store the ﬂuctuations d

, j=J−1, ..., 0 and the coarse scale

j

The total number of original samples, or averages, is n=2J. At each level there
( · ). It takes one addition and one multiplication to
, giving 2j+1 operations at each level. There are J
j−1

are 2j scaling coeﬃcients s
j
calculate each of s
and d
j−1
levels, so the total number of operations is

2j+1<2J+2=4n

∑J
j=1

The Haar transform of a vector with n elements thus takes O(n) operations.

For the inverse transform we obtain the sequence s

by adding and subtracting
j+1
, and this holds for every level. The cost of the inverse

the coeﬃcients of s
transform is therefore also O(n).

and d

j

j

We see that the Haar transform has many of the properties we look for in a
terrain representation. The transform is certainly fast, O(n) is even better than the
Fast Fourier Transform. The algorithm is easy, and does not require complicated
data structures to implement. We have also achieved a hierarchical representation
of the data. Starting from a coarse scale representation s
in V
, we successively add
information on ﬁner and ﬁner scales; ﬁrst we add d
and V
, which gives us s
, then
we add d
, and so on, until we reach our original data set
. It is of course our hope that the wavelet representation is also more eﬃcient than
s
J
the original representation, i.e. that a few terms in the wavelet expansion can capture
more of the essentials of the data set than the same number of terms in the original
basis. Although the Haar transform is a good illustration of the wavelet idea, it does
not yield the most eﬃcient representations. In the next section it is shown how the
Haar transform can be modiﬁed to give much better approximations without giving
up too many of its properties.

, which gives us s

and V

1

0

0

0

1

1

2

2

2.3. Average interpolating wavelets

Let I

I
j+1,2k
j+1,2k+1
some function f over the intervals I

=[k2−j(k+1)2−j). The left and right halves of this interval are given by
(k), j ﬁxed, consist of average values of
. If we neglect the ﬂuctuation coeﬃcients d
,

. Let the sequence s

j,k
and I

j

j,k

j

Downloaded by [The University Of Melbourne Libraries] at 07:48 29 March 2016 606

J. T . Bjørke and S. Nilsen

and do one level of the inverse Haar transform on s

, we get

j

(2k)=s
s
j+1
(2k+1)=s

(k)

(k)

j

j

s

j+1

j

j+1

j+1

, and s

(2k) and s

(k) is the average value over the interval
By the properties of the Haar transform, s
(2k+1) are the averages over I
I
and I
, respect-
j+1,2k
j+1,2k+1
j,k
ively. We can interpret the equations in the following way: we have used the average
on the next ﬁner level j+1.
on level j to predict the average values s
values s
j+1
j
Here this prediction is constant; the average values over I
and I
will
. The wavelet coeﬃcients at this
both be equal to the average over their union I
scale are thus a measure of how much the data deviates from a constant, and our
prediction is only correct in areas where the data is constant. Since most terrain
data contain variations, we will have to store a lot of wavelet coeﬃcients at each
scale. In general, wavelet coeﬃcients represent the diﬀerence between exact and
predicted values. If we can predict the features of the data better, the wavelet
coeﬃcients will be smaller, and we will get a better approximation of the data using
only a few terms in the expansion.

j+1,2k+1

j+1,2k

j,k

to predict the averages over the subblocks I

Elevation data is in general not constant, but it is not completely random either.
The measured value at one point depends on the values at neighbouring points. We
can exploit this correlation by using more of the information available on scale j to
predict the averages on scale j+1. In the Haar transform we only used the block
I
, but we are free
j,k
to use the rest of the blocks at scale j, namely ..., I
in the
j,k+1
prediction. We will consider the case where we take two neighbouring blocks into
account. A more general theory can be found in Sweldens and Schro¨ der (1996).
Including more blocks in the prediction will give rise to other and smoother average
interpolating wavelets.

j+1,2k+1
, I
, I
j,k

j+1,2k

j,k−1

and I

, ...,

j

and I

which constitute the two halves of I

. We want to predict the average value of

(k−1), s
j
, I
j,k−1
j,k
j+1,2k+1

(k+1) be the average values of some function f over the
(k) and s
Let s
j
f over
and I
intervals I
j,k+1
I
. There is a unique
j+1,2k
(k+1).
(k) and s
second-degree polynomial that interpolates the averages s
We let the predicted average values over I
be the averages of this
polynomial over the given intervals. This prediction will not only be exact when the
data is constant, but also when the data is linear and even quadratic.
The new algorithm looks very much like the Haar transform. If s
j+1

are the exact
average values on scale j+1, we can ﬁnd the exact averages on the next coarser
scale j by

j
j+1,2k+1

j,k
(k−1), s

j+1,2k

and I

j

j

s

(k)=D(s

(2k)+s

(2k+1))

j+1
This is exactly the same equation as in the Haar transform. If we let s˜
s˜
j+1
expression for the wavelet coeﬃcients,

(2k) and
(2k+1) denote the predicted averages, we get the following modiﬁcation in the

j+1

j+1

j

(5)

d

j

(k)=D(s

(2k+1)−s

(2k))−D(s˜

(2k+1)−s˜

j+1

j+1

j+1

(2k))

j+1

(6)

The process thus starts with a Haar transform. Once we have calculated the averages
at the next level, s
, we have all the information needed for the prediction. We then
subtract the predicted values from the Haar wavelet coeﬃcients, and obtain a new
set of wavelet coeﬃcients to be stored at this level. Note that we only need the

j

Downloaded by [The University Of Melbourne Libraries] at 07:48 29 March 2016 Wavelets applied to simpliﬁcation of DT Ms

607

diﬀerence between the predicted values over I
. The inverse
j+1,2k
j+1,2k+1
transform is just as easy. We use the averages s
to predict the averages on the next
level. This information is added to the wavelet coeﬃcients before an inverse Haar
transform is carried out.

and I

j

The implementation of wavelet transforms through the use of prediction operators
is known as lifting. The Lifting Scheme is a general tool for constructing wavelets.
Common wavelets such as Daubechies biorthogonal and orthogonal wavelets
(Daubechies 1992), can be factored into lifting steps, but the procedure also makes
it easy to construct new wavelets on curves, surfaces, irregular data sets and ﬁnite
domains. A number of lifted wavelets are described in Sweldens and Schro¨ der (1996).
The average interpolating wavelet transform is slightly slower than the Haar
transform due to the extra prediction operator, but it is still an O(n) transform.
Implementation is relatively straightforward. The only problem is how to predict
near the boundaries of the domain. Since the prediction operator requires neigh-
bouring blocks on both sides to calculate average values on the next scale, we need
to treat the blocks near the boundaries separately. The solution is to construct a
second-degree polynomial that interpolates the averages over the boundary block
and two of its neighbours on one side, and calculate the averages of this polynomial
over the subblocks of the boundary block.

What we have constructed is a version of the so-called Daubechies 1, 3 biortho-
gonal wavelet transform adapted to a ﬁnite domain (Daubechies 1992, p. 272). The
functions are not smooth, but they are continuous, and will reproduce exactly
polynomials of degree two and less.

2.4. Wavelet transforms in 2D

In the two previous sections we saw how to construct wavelet transforms for
one-dimensional data, but since elevation data is two-dimensional we need to extend
the transform to handle matrix data. Wavelets in several dimensions can be con-
structed in a number of diﬀerent ways, but the simplest is via tensor products.

The two-dimensional wavelet transform of a matrix is calculated by ﬁrst doing
one-dimensional wavelet transforms along the rows, and then along the columns of
the matrix. Note that the transform has to be carried out level by level; one level
horizontally, then one level vertically, next level horizontally, and so on. Doing a
complete horizontal wavelet transform followed by a complete vertical transform is
possible, but this will not give rise to a multi-resolution model.

When transforming the rows of the matrix, the coeﬃcients can be stored in the
same way as in ﬁgure 2, with low pass coeﬃcients to the left, and high pass coeﬃcients
to the right. During the transformation of each column, low pass coeﬃcients can be
stored in the upper part of the matrix, and high pass coeﬃcients in the lower part.
This results in a partition of the matrix, where the low pass/low pass coeﬃcients are
located in the upper left-hand corner of the matrix. These are the only coeﬃcients
that will be transformed further. The rest of
the matrix contains wavelet
coeﬃcients that are to be stored.

This 2D transform will inherit all the properties of the one-dimensional trans-
form; fast algorithm, easy implementation, hierarchical representation, and good
approximation properties. The prediction operator will be exact for biquadratic data.

2.5. Wavelet projection and thresholding

One of the largest beneﬁts of using a wavelet representation is the possibility of
eﬃciently constructing good approximations to data sets. This normally requires

Downloaded by [The University Of Melbourne Libraries] at 07:48 29 March 2016 608

J. T . Bjørke and S. Nilsen

that some of the original information be thrown away. Both wavelet projection and
thresholding are methods for prioritizing and selecting wavelet coeﬃcients.

Wavelet projection is a linear approximation method. Which coeﬃcients are
important is determined a priori, i.e. before you look at the given data set. A typical
application of projection is in the construction of coarse scale approximations. This
is done by removing wavelet coeﬃcients level by level
from the expansion; all
coeﬃcients below a certain level are kept, and all the coeﬃcients on ﬁner levels are
set to zero.

Thresholding, on the other hand, is a non-linear method. Here coeﬃcients are
prioritized after the transform is carried out. The user has to determine a threshold
value, and all coeﬃcients larger than this value are kept, the rest are discarded.

Let us for a moment go back and look at the function f

deﬁned in §2.2. The L 2

J

norm of this function is deﬁned by

(7)

(8)

d f

d2
L2

J

=P

| f

J

−1
(t)|2 dt= ∑2J
k=0

[0,1]

2−J|s

(k)|2

J

By the properties of the Haar transform, we also have
−1
−1
(k)|2= ∑2j0
∑2j
k=0
k=0

(k)|2+ ∑J−1
j=j

−1
∑2J
k=0

2−J|s

2−j

|s

J

0

j

2−j|d

(k)|2

j

0
Note: since our implementation of the Haar transform is not orthonormal, but
merely orthogonal, we have to include the scaling 2−j in the formula. For a two-
dimensional transform the scaling factor is 2−2j. The number of levels in the transform
is given by L =J−j

0

.

We deﬁne a threshold value t, and select coeﬃcients based on equation (8). A
(k)|2>t2, otherwise it is shrunk to zero. This

coeﬃcient d
procedure is known as hard thresholding.

(k) is kept as it is if 2−j|d

j

j

0

For the average interpolating wavelets, the formula in equation (8) does not hold
exactly. Nevertheless, the right-hand side deﬁnes an equivalent norm, and this
formula can still be used as a basis for the thresholding procedure.

The L 2 norm is easy to calculate, but it is not necessarily appropriate for a given
application. In terrain modelling the average and the max errors might be just as
important. One very important feature of wavelet expansions is that membership in
diﬀerent function spaces can be determined from the asymptotic behaviour of wavelet
coeﬃcients, and norms in these spaces are calculated as weighted sums of coeﬃcients.
Among the spaces where norms can be calculated from the size of wavelet coeﬃcients
are the Besov spaces Ba
(Lp). This set includes the Sobolev and L p spaces. Besov
p
spaces are used for measuring smoothness of functions, and can thus be used for
measuring errors where one has some a priori knowledge of the smoothness of the
data. For a description of Besov spaces, see DeVore et al. (1992). The theory of
thresholding and shrinking in Besov spaces can be found in Donoho (1995) and
Chambolle et al. (1998).

2.6. Implementation

We conclude this wavelet introduction with a short pseudocode for the imple-

mentation of the wavelet transform.

The input to the one-dimensional transform is the vector s_( j+1)[m]. For
simplicity we assume this vector has 2j+1 elements. The output is a vector s_( j)[k]
of low pass coeﬃcients, and a vector d_( j)[k] of high pass coeﬃcients, each having

Downloaded by [The University Of Melbourne Libraries] at 07:48 29 March 2016 Wavelets applied to simpliﬁcation of DT Ms

609

2j-elements. The vector d_tmp_( j)[k] is a temporary storing space for Haar wavelet
coeﬃcients. These coeﬃcients are modiﬁed by subtracting half the diﬀerence between
the values predicted over the subblocks. The value is found by evaluating the function
Predict_diﬀerence( j, k). The implementation of equations (5) and (6) is as follows:

for (k=0; k<=2ˆj−1; k++){

s_( j)[k]=0.51(s_( j+1) [2k+1]+s_( j+1)[2k]);
d_tmp_( j)[k]=0.51(s_( j+1)[2k+1]−s_( j+1)[2k]);
d_( j)[k]=d_tmp_( j)[k]−Predict_difference( j, k);

}

For the wavelets we have used in this study, the predicted average values
over I
,
j,k+1
j+1,2k+1
i.e. they depend on s_( j)[k−1], s_( j)[k] and s_( j)[k+1]. The value of the
Predict_diﬀerence( j, k) function is given by:

depend on the averages over I

j+1,2k

and I

and I

j,k−1

, I

j,k

Predict_difference( j, k)=0.1251(s_( j)[k+1]−s_( j)[k−1]); # for 0<k<2ˆj−1
=−0.1251s_( j)[k+2]+0.51s_( j)[k+1]−0.3751s_( j)[k]; # for k=0

=0.3751s_( j)[k]−0.51s_( j)[k−1]+0.1251s_( j)[k−2]; # for k=2ˆj−1

The input to the 2D transform is a matrix of average values, or low pass/low

pass coeﬃcients.

matrix=ﬁne scale average values;
for each level of transform {

for each row of the matrix {

}
for each column of the matrix {

do one level of one-dimensional wavelet transform;

do one level of one-dimensional wavelet transform;

}
store wavelet coeﬃcients;
matrix=low pass/low pass coeﬃcients;

}
threshold wavelet coeﬃcients;

The inverse transform is carried out by working backwards through the forward
transform. Starting from coarse scale averages and the coarsest scale wavelet coeﬃ-
cients, the averages at the next ﬁner scale are found by doing one level of inversion
of the columns, and then of the rows. The one-dimensional inversion consists of
adding predicted values to the wavelet coeﬃcients, and then carrying out an inverse
Haar transform.

3. Methodology

The methodology applied in the present paper is comparative, i.e. we have
computed series of simpliﬁed models and compared them to the original model by
diﬀerent indexes and visualizations. The evaluations cover surface attributes such as
the vertical distance to the reference model, slope and shape deviation, and possible
systematic shifts in the x, y and z axes.

Downloaded by [The University Of Melbourne Libraries] at 07:48 29 March 2016 610

J. T . Bjørke and S. Nilsen

3.1. Construction of the simpliﬁed models

The method we have chosen for selection of wavelet coeﬃcients is thresholding.
We prefer this non-linear adaptive method over linear projection. As will be
shown, for a given number of coeﬃcients thresholding gives a signiﬁcantly better
approximation than level-by-level projection.

We have computed the generalized models from several diﬀerent threshold values,
giving us sequences of successively thinner data sets. The distance between the
original surface and the generalized surface, in terms of standard deviation, is com-
puted as:

s=S1

n

∑n
i=1

( fˆ (u

)− f (u

i

))2,

i

(9)

where fˆ ( · ) and f ( · ) are the height values of the original model and the generalized
model, respectively, at the n sample points u
. Since our original samples are block
averages, this measure can be obtained from the L 2 error deﬁned in equation (7) by
factoring out the length/area dependence of the norm. The standard deviation gives
information about the vertical distance between the two surfaces, but since we cannot
expect one single index to oﬀer information about all the complex properties of the
shape of a surface, we have selected several methods for the description of the surface
properties. These methods are described in the forthcoming sections.

i

3.2. Contour plots and relief maps

Contour plots oﬀer visual information about the shape of the simpliﬁed surfaces.
How small details can be seen in contour plots is limited by the equidistance. Due
to the limitation of the contour maps and the ability of relief maps to illustrate small
surface structures, we will also present series of relief maps. Relief maps cannot give
information about the value of the height diﬀerences.

3.3. Computation of valley lines

We have supplemented the contour plots with valley lines. Valley lines can be
computed from diﬀerent procedures (Skidmore 1990). The method we apply com-
putes the valleys by a simple moving-window algorithm. The cell with the maximum
elevation in a two-by-two moving window is ﬂagged Any unﬂagged cells remaining
after the algorithm has passed over the DEM represent the valleys.

The procedure considered is very simple and other algorithms for drainage line
computation can give more realistic results. Since the goal of our analysis is to
illustrate how the number of small terrain features disappears as the DTM becomes
more and more simpliﬁed, the drainage procedure selected is satisfactory for our
purpose.

3.4. Correlation analysis

Correlation analysis of the generalized models can be used to determine whether
we have systematic shifts in x, y or h. The algorithm we will apply computes the
shifts in x, y and h which minimize s, i.e. minimizes the vertical distance between
the original model and the generalized model. Since the algorithm computes s
relative to the mean of the two surfaces, s from this procedure is approximately
half the value of s computed from the comparison of the original model and the
generalized model as deﬁned by equation (9).

Downloaded by [The University Of Melbourne Libraries] at 07:48 29 March 2016 Wavelets applied to simpliﬁcation of DT Ms

611

4. Case study
4.1. Description of case study area

Two areas in Norway were selected for our case study. One area is from the
central part of southern Norway whereas the other is from the west coast. The two
areas, termed East and West, represent terrain with diﬀerent topographies; one with
high relief and the other with low relief. East and West are represented as grid
models of 256×256 height points with a 100 m resolution in the horizontal plane.
The accuracy of the height values in terms of standard deviation is assumed to lie
in the interval ±5 m to ±15 m (Statens Kartverk 1998). Table 1 shows some meas-
ures for the two areas. The table gives the minimum, maximum, mean and standard
deviation of the height values.

4.2. Results

Throughout the forthcoming presentation of our experiment the name of the
diﬀerent terrain models is composed of two ﬁelds. The ﬁrst ﬁeld identiﬁes the test
area whereas the second is the number of wavelet coeﬃcients used for the construction
of the model. The diﬀerent DTMs are generated from the wavelet threshold technique
and the accuracy of the diﬀerent models is evaluated by the methods described in
the previous sections. The range of the number of wavelet coeﬃcients for the models
is 1024 to 65 536 coeﬃcients, i.e. from 1.6% to 100% of the coeﬃcients. When all the
coeﬃcients are used, the model constructed from the wavelets is identical to the
original model.

Table 2 shows the relation between the number of wavelet coeﬃcients and the
standard deviation computed from equation (9). As seen from the table the accuracy
of the model depends on the relief of the surface, i.e. the West area with the very
steep relief requires more coeﬃcients than the East area to construct a model with
a certain accuracy. The experiment also shows that the projection method requires
more coeﬃcients than the threshold method to create models with a certain accuracy.

Table 1. The minimum and the maximum height value in the test area, their mean and
standard deviation. The values are in metres.

Region

East
West

min(H)

max(H)

mean(H)

Std(H)

60
0

702
1580

242
563

175
377

Table 2. The vertical distance between the original model and the generalized models
expressed as standard deviation. The results for both the projection and the threshold
method are presented. The projection method selects coeﬃcients on a certain level of
decomposition whereas the threshold method selects the coeﬃcients which give the
most signiﬁcant contribution to the terrain model.

West

East

# coeﬀs.

16 384
4096
1024

projection
s

threshold
s

projection
s

threshold
s

16.24
31.81
70.95

8.05
22.30
52.10

4.53
7.79
14.08

2.34
5.52
9.90

Downloaded by [The University Of Melbourne Libraries] at 07:48 29 March 2016 612

J. T . Bjørke and S. Nilsen

West

Original  terrain model

East

25000

20000

15000

10000

5000

25000

20000

15000

10000

5000

25000

20000

15000

10000

5000

2500 0

2000 0

1500 0

1000 0

500 0

25000

20000

15000

10000

5000

25000

20000

15000

10000

5000

5000

1000 0

1 5000

20000

25000

5000

10000

15000

20000

25000

Thresholding 1.5 % of the coefficients

5000

10000

15000

20000

25000

5000

10000

15000

20000

25000

Projection 1.5 % of the coefficients

5000

10000

15000

20000

25000

5000

10000

15000

20000

25000

Figure 3. Comparison of the projection and the threshold methods. With a certain number
of coeﬃcients the threshold technique gives a more precise representation of the terrain
than the projection method, i.e. in the threshold case we have more terrain details and
less visible block eﬀects than in the projection case.

Downloaded by [The University Of Melbourne Libraries] at 07:48 29 March 2016 Wavelets applied to simpliﬁcation of DT Ms

613

Figure 4. The contour lines and the valleys of area East computed for the threshold based
models. The contour interval is 250 m.

Since the projection method does not evaluate the contribution of the coeﬃcients to
the accuracy of the terrain model, but selects coeﬃcients on a certain level of
decomposition, the adaptive method is expected to manage the coeﬃcients more
eﬀectively than the projection method. The adaptive method selects the coeﬃcients
which give the most signiﬁcant contribution to the terrain description.

The relief maps in ﬁgure 3 compare the projection and the threshold methods.

Downloaded by [The University Of Melbourne Libraries] at 07:48 29 March 2016 614

J. T . Bjørke and S. Nilsen

Figure 5. The contour lines and the valleys of area West computed for the threshold based
models. The contour interval is 250 m.

From the maps we can see how the threshold technique gives a more detailed
representation of the terrain than the projection method.

Figures 4 and 5 show the contour lines and the valleys for diﬀerent levels of
simpliﬁcation. The models are generated from the threshold technique. The vertical
distance between the contour lines is 250 m. Since the map scale in the ﬁgures
considered is rather small, a window of the models is zoomed; as shown in ﬁgures 6

Downloaded by [The University Of Melbourne Libraries] at 07:48 29 March 2016 Wavelets applied to simpliﬁcation of DT Ms

615

Figure 6. Zoom into the contour lines and the valleys of area East. The contour interval is
250 m. The A marks a contour line whose shape is sensitive to small changes in the
model; see the explanation in the text.

and 7. The main impression from the contour plots is that the diﬀerent levels of data
reduction do not change the shape of the contour lines to any signiﬁcant extent, but
sometimes it may happen that minor modiﬁcations of the model can introduce
considerable changes of the shape of some of the contour levels as demonstrated in
ﬁgure 6; see the contour lines marked by an A. Since the contour lines are computed
for certain height values, they cannot give precise information about the shape of
the terrain between them. Therefore, a small change of the model can substantially
change certain contour levels.

Downloaded by [The University Of Melbourne Libraries] at 07:48 29 March 2016 616

J. T . Bjørke and S. Nilsen

Figure 7. Zoom into the contour lines and the valleys of area West. The contour interval
is 250 m.

The valley lines clearly show how the number of valleys is reduced from one
level of generalization to the next. Since the threshold technique enables us to smooth
the less signiﬁcant details of the surface, the adaptive wavelet model gradually adds
smaller and smaller bends to the model.

The relief maps in ﬁgures 8 and 9 illustrate how the amount of detail relates to

Downloaded by [The University Of Melbourne Libraries] at 07:48 29 March 2016 Wavelets applied to simpliﬁcation of DT Ms

617

Original terrain model

Thresholding 25 % of the coefficients

5000

10000

15000

20000

25000

5000

10000

15000

20000

25000

Thresholdin  g 6 % of    the  coef ficients

Thresholding 1.5 % of the coefficients

25000

20000

15000

10000

5000

25000

20000

15000

10000

5000

25000

20000

15000

10000

5000

25000

20000

15000

10000

5000

5000

10000

15000

20000

25000

5000

10000

15000

20000

25000

Figure 8. Relief maps for area East. The maps illustrate how the threshold based technique
simpliﬁes the terrain model, i.e. how the amount of detail disappears as the number
of coeﬃcients decreases. The block eﬀect is clearly visible in the 1.5% map.

the number of coeﬃcients. For large threshold values the block eﬀect is clearly visible
in the relief maps. This eﬀect is less pronounced in the contour maps.

The result of the correlation analysis is presented in table 3. The shift in the three
co-ordinate axis varies from 0 m to 1.31 m in x, y and from 0 m to 0.17 m in h. To
evaluate the inﬂuence of these shifts we can compare s before and after the correla-
tion. Since the two values are always very similar, the shifts have a minor eﬀect on
the accuracy of the model. On the other side, the maximum shift is more pronounced
for the ﬂatter areas, i.e. in ﬂat regions small bends are more important for the
correlation than small bends in steep regions. This clearly demonstrates that the
purpose of the model is very important to the degree of simpliﬁcation in ﬂat regions,
e.g. terrain navigation algorithms can fail if very small features are removed from
the model.

When the terrain model contains features like sea and lake surfaces, artefacts can

Downloaded by [The University Of Melbourne Libraries] at 07:48 29 March 2016 618

J. T . Bjørke and S. Nilsen

Original terrain model

Thresholding 25 % of the coefficients

25000

20000

15000

10000

5000

25000

20000

15000

10000

5000

25000

20000

15000

10000

5000

25000

20000

15000

10000

5000

5000

10000

15000

20000

25000

5000

10000

15000

20000

25000

Thresholding 6 % of the coefficients

Thresholding1.5 % of the coefficients

5000

10000

15000

20000

25000

5000

10000

15000

20000

25000

Figure 9. Relief maps for area West. The maps illustrate how the threshold based technique
simpliﬁes the terrain model. The block eﬀect is clearly visible in the 1.5% map. Some
artefacts can be observed in the original model as parallel lines in the midle part of
the image. The lines have almost disappeared in the 25% image, i.e. they have been
smoothed out.

occur; see ﬁgure 10. These kinds of errors are commonly introduced by L 2 approxi-
mations, and they are related to the Gibbs phenomenon in Fourier analysis
(Florinsky 2002). Since the data contains a discontinuity in the derivative along the
coastline or edge of a lake, some oscillations must be expected in these areas. The
visual impact of such errors can be signiﬁcant even if the numerical error is small,
especially if the contour lines are placed exactly at, or slightly above, the water level.
The artifacts considered are clearly visible in the 1.5% relief map in ﬁgure 9. The
relief map of the original model in the ﬁgure considered illustrates that the artefacts
are not related to the quality of the original DTM.

In the case of lakes and seas, the artefacts can be avoided if the depth values of
the lake and sea ﬂoor are introduced in the model. If this kind of information is not

Downloaded by [The University Of Melbourne Libraries] at 07:48 29 March 2016 Wavelets applied to simpliﬁcation of DT Ms

619

Table 3. Correlation analysis; the models are correlated to the original model. The elements
dx, dy and dh are the shift values in the horizontal and the vertical directions. The
correlation is computed for the threshold based models. The values are in metres.

Terrain model
# coeﬀs.

east

west

16 384
4096
1024

16 384
4096
1024

Before
correlation
s

After
correlation
s

1.14
2.68
4.77

3.95
10.89
25.22

1.14
2.68
4.74

3.95
10.88
25.11

Shift (m)

dy

0.00
0.16
1.31

−0.02
−0.01
−0.10

dh

0.01
0.07
0.17

0.02
0.02
0.06

dx

0.00
0.02
0.11

0.00
0.00
0.24

Figure 10. Artefacts can occur when the relief changes from steep terrain to a ﬂat surface
like a lake or a sea. The artefacts can be masked by diﬀerent techniques. The artefacts
are also visible in the 1.5% map in ﬁgure 9. The map of the original DTM in ﬁgure 9
illustrates that the phenomenon considered is not related to artefacts in the original
model.

present, the areas considered can be masked, i.e. the condition of ﬂat surface is
introduced as an additional layer to the wavelet generated model. One should not
try to correct these errors by introducing more coeﬃcients in the expansion. Since
the exact height in these areas is known, such a procedure will only waste valuable
storage space.

5. Conclusions

The wavelet method provides us with a new set of tools for the simpliﬁcation of
DTMs. Eﬃcient methods for simpliﬁcation, i.e. controlled reduction of the amount
of detail, become more and more pressing as new equipment such as multibeam
echo sounders and airborne laser-scanners can measure high-resolution DTMs
covering large areas. The experiment shows how the adaptive properties of the
wavelet method can be utilized to eliminate unwanted detail and reduce the amount

Downloaded by [The University Of Melbourne Libraries] at 07:48 29 March 2016 620

J. T . Bjørke and S. Nilsen

of information to represent a DTM, i.e. the wavelet method can be used in model
generalization.

Although the case study covers two test areas only, we conclude the experiment
by stating the proposition that the wavelet method is an eﬃcient tool to eliminate
unwanted detail in digital terrain models. Artefacts, however, become visible when
the threshold passes a certain value. Therefore, the application of the method is
limited to a certain interval of the threshold. The maximum size of the threshold is
related to the steepness of the terrain and the quality requirements of the application.
The artefacts which can be generated on ﬂat surfaces like lakes and seas, can be
eliminated by diﬀerent techniques. The most natural way of avoiding this kind of
problem is to introduce depth values of the features considered.

The correlation analysis has demonstrated that the average interpolating wavelets
do not introduce signiﬁcant global shifts in the three principal directions, i.e. the
shifts were of minor size compared to the threshold value applied. The correlation
analysis also demonstrates that when the terrain model is to be used in applications
like terrain navigation, the threshold value should be lower in ﬂat terrain than in
steep terrain.

In cartographic generalization exaggeration of important features is sometimes
necessary. The wavelet method does not directly oﬀer this kind of procedure, but it
is a topic for further investigation to formulate a wavelet based algorithm which
detects features which are candidates for exaggeration, and rescales them.

References
B, J. L. G., and L , F. J. A., 2000, Frequency ﬁltering of linear features by means

of wavelets: a method and an example. Cartographic Journal, 37, 39–49.

Bø, J. T., and N, S., 2002, Eﬃcient representation of digital terrain models: compres-

sion and spatial decorrelation techniques. Computers & Geosciences, 28, 433–445.

B, B. T., and N, S., 2001, Multiscale representation of terrain models using average
interpolating wavelets. In Proceedings of ScanGIS’2001, edited by J. T. Bjørke and
H. Tveite (A˚ s: Department of Mapping Sciences, The Agricultural University of
Norway), pp. 33–44.

B, B. T., and N, S., 2003, Wavelet representation of large digital terrain models.

Computers & Geosciences, 29, 695–703.

C, A., D, R., L, N., and L, B., 1998, Nonlinear wavelet image
processing: Variational problems, compression and noise removal through wavelet
shrinkage. IEEE T ransactions on Image Processing, 7, 319–335.

D, I., 1992, T en L ectures on Wavelets (Philadelphia, Pennsylvania: Society for

Industrial and Applied Mathematics).

D F, L., and P, E., 1995, Hierarchical triangulation for multi-resolution surface

description. ACM T ransactions on Graphics, 14, 363–411.

D, R., J, B., and P, V., 1992, Compression of wavelet decompositions.

American Journal of Mathematics, 114, 737–785.

D, D., 1993, Smooth wavelet decompositions with blocky coeﬃcient kernels. In: Recent
Advances in Wavelet Analysis, edited by L. L. Schumaker and G. Webb (New York:
Academic Press), pp. 1–43.

D, D., 1995, Adapting to unknown smoothness via wavelet shrinkage. Journal of

American Statistical Association, 90, 1200–1224.

D, M., and F, M., 1999, Constructing hierarchical terrain models by edge
ordering over triangulations. In Proceedings of ScanGIS’99, edited by E. Stubkjær and
S. Hansen (Aalborg: Aalborg Universitetsforlag), pp. 25–38.

F, I. V., 2002, Errors of signal processing in digital terrain modelling. International

Journal of Geographical Information Science, 16, 475–501.

Downloaded by [The University Of Melbourne Libraries] at 07:48 29 March 2016 Wavelets applied to simpliﬁcation of DT Ms

621

H, M., 1990, Triangulation algorithms for adaptive terrain modelling. In Proceedings of
International

4th International Symposium on Spatial Data Handling (Zurich:
Geographical Union), pp. 163–174.

J, C. B., 1997, Geographical Information Systems and Computer Cartography (Cambridge:

Longman).

MM, R. B., and S, K. S., 1992, Generalization in Digital Cartography (Washington,

DC: Association of American Geographers).

M, Y., 1993, Wavelets: Algorithms and Applications (Philadelphia, Pennsylvania: Society

for Industrial and Applied Mathematics).

Mø, T., 1994, Removing points from a Delaunay triangulation. In Proceedings of the 6th
International Symposium on Spatial Data Handling, edited by T. C. Waugh and
International Geographical Union, Commission on
R. G. Healy (Edinburgh:
Geographic Information Systems), pp. 739–750.

M, G., 1996, Varioscale TIN based surfaces. In Advances in GIS Research II, Proceedings
of the 7th International Symposium on Spatial Data Handling., edited by M. J. Kraak
and M. Molenaar (London: Taylor & Francis), pp. 353–364.

M, J. L., 1974, A theoretical framework for cartographic generalization with emphasis
an the process of symbolization. International Yearbook of Cartography 1 (Kirschbaum:
Verlag), pp. 115–127.

P, D., C, H., and K, Y., 2001, A TIN compression method using Delaunay triangula-
tion. International Journal of Geographical Information Science, 15, 255–269.
S , P., and S, W., 1995, Spherical wavelets: eﬃciently representing functions
on a sphere. In 2000 Wavelets in the Geosciences, edited by R. Klees and R. Haagmans
(Berlin: Springer), pp. 72–130.

S, A. K., 1990, Terrain position as mapped from a gridded digital elevation model.

International Journal of Geographical Information Systems, 4, 33–49.

S K, 1998, FOU prosjekt digital terrengmodell. Research report on digital

terrain models (Hønefoss, Norway: Statens Kartverk).

S, W., and S , P., 1996, Building your own wavelets at home. In 2000
Wavelets in the Geosciences, edited by R. Klees and R. Haagmans (Berlin: Springer),
pp. 72–130.

W, R., 1995, Three essential building blocks for automated generalization. In GIS and
generalization: Methodology and practice, edited by J. C. Mu¨ ller, J. P. Lagrange and
R. Weibel (London: Taylor & Francis), pp. 56–69.

W, C., and L, J., 1999, Experiences of 10 years laser scanning. In Proceedings
of Photogrammetric Week’99, edited by D. Fritsch and R. Spiller (Heidelberg, Germany:
H. Wichmann Verlag), pp. 125–132.

Downloaded by [The University Of Melbourne Libraries] at 07:48 29 March 2016 