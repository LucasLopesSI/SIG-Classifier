16

ARTICLE

A cyclic self-learning Chinese word segmentation for the
geoscience domain
Qinjun Qiu, Zhong Xie, and Liang Wu

Abstract: Unlike English and other western languages, Chinese does not delimit words using white-spaces.
Chinese Word Segmentation (CWS) is the crucial first step towards natural language processing. However, for
the geoscience subject domain, the CWS problem remains unresolved with many challenges. Although traditional
methods can be used to process geoscience documents, they lack the domain knowledge for massive geoscience
documents. Considering the above challenges, this motivated us to build a segmenter specifically for the geosci-
ence domain. Currently, most of the state-of-the-art methods for Chinese word segmentation are based on super-
vised learning, whose features are mostly extracted from a local context. In this paper, we proposed a
framework for sequence learning by incorporating cyclic self-learning corpus training. Following this framework,
we build the GeoSegmenter based on the Bi-directional Long Short-Term Memory (Bi-LSTM) network model to per-
form Chinese word segmentation. It can gain a great advantage through iterations of the training data. Empirical
experimental results on geoscience documents and benchmark datasets showed that geological documents can be
identified, and it can also recognize the generic documents.

Key words: geoscience documents, cyclic self-learning, deep learning, Chinese word segmentation.

Résumé : Contrairement à l’anglais et aux autres langues occidentales, le chinois ne délimite pas les mots en uti-
lisant des espaces blancs. La segmentation de mots chinois (SMC) est la première étape cruciale vers le traitement
naturel du langage. Cependant, pour le domaine des géosciences, le problème de la SMC n’a pas encore été résolu
et de nombreux défis demeurent. Bien que les méthodes traditionnelles puissent être utilisées pour traiter des
documents géoscientifiques, elles manquent de connaissances sur le domaine pour les documents
géoscientifiques imposants. Compte tenu des défis ci-dessus, cela nous a motivés à construire un segmenteur
spécifiquement pour le domaine géoscientifique. Actuellement, la plupart des méthodes de segmentation de mots
chinois les plus récentes sont basées sur l’apprentissage supervisé, dont les caractéristiques sont principalement
extraites d’un contexte local. Dans cet article, nous avons proposé un cadre pour l’apprentissage des séquences
en intégrant l’autoapprentissage cyclique au sein du corpus d’apprentissage. En suivant ce cadre, nous construi-
sons le GeoSegmenteur basé sur le modèle de réseau bidirectionnel de la mémoire à long terme et à court terme
(Bi-LSTM : Bi-directional Long Short-Term Memory) pour effectuer la segmentation des mots chinois. Le segmen-
teur peut acquérir un grand avantage grâce à des itérations des données de formation. Des résultats
expérimentaux empiriques sur des documents géoscientifiques et des ensembles de données de référence ont
montré que les documents géologiques peuvent être identifiés et que le segmenteur peut également reconnaître
les documents génériques. [Traduit par la Rédaction]

Mots-clés : documents géoscientifiques, autoapprentissage cyclique, apprentissage en profondeur, segmentation de
mots chinois.

1. Introduction

With the explosive increase of geoscience texts, geo-
science documents have increased exponentially. Each
geoscience document contains different geological
themes, including rocks, minerals, and geological

structure. Furthermore, these documents, which consist
of large amounts of structured data and unstructured
data, are stored in different formats, including .doc,
.pdf, .jpg, .tiff, and spatial data files. For the structured
data, they are managed by relational or spatial

Received 20 April 2018. Accepted 14 June 2018.

Q. Qiu, Z. Xie, and L. Wu. Department of Information Engineering, China University of Geosciences, Wuhan 430074, China;
National Engineering Research Center of Geographic Information System, Wuhan 430074, China.

Corresponding author: Liang Wu (email: wuliang@cug.edu.cn).

Copyright remains with the author(s) or their institution(s). Permission for reuse (free in most cases) can be obtained from RightsLink.

Geomat. 72: 16–26 (2018) dx.doi.org/10.1139/geomat-2018-0007

Published at www.nrcresearchpress.com/geomat on 30 July 2018.

Geomatica Downloaded from www.nrcresearchpress.com by GLASGOW UNIVERSITY LIBRARY on 08/02/18For personal use only. 17

被查干楚鲁(qaganqulu)粗粒黑云母花岗岩(biotite granite)侵入，该套地层主要分布在格日吐

(Lattice vomiting) 防火站和敦德哈布其勒南山 (London Dehabuqile Nanshan)
It was invaded by Qaganqulu coarse grained biotite granite, the strata are mainly
distributed in Lattice vomiting fire station and London Dehabuqile Nanshan

被 查干楚鲁 粗粒 黑云母 花岗岩 侵入， 该套 地层 主要 分布在 格日吐 防火站 和 敦德哈布其勒

被查 干楚鲁 粗粒 黑云母 花岗岩 侵入， 该套 地层 主要 分布在 格日 吐 防火 站 和 敦德哈布 其

Table 1. Example of Chinese geoscience word segmentation.

Qiu et al.

Original text

English translation

The correct word segmentation

Segmentation made by a general

domain segmenter

南山

勒 南山

databases. However, large volumes of unstructured data
that include diverse types and fragmented information
are difficult to handle. In addition, the unstructured data
have more abundant information and have more poten-
tial value. How to automatically extract information
from geoscience documents has received increasingly
more attention in recent years.

The first step for automatically extracting potential
geoscience information from the vast amounts of geo-
science documents is developing a Chinese word seg-
mentation (CWS) system. Unlike English, Chinese has
consecutive characters that are not delimited by bounda-
ries between words. With the enormous amount of
information gathered currently, manual information
processing is far from sufficient, and the development
of fast automatic processes for information extraction
is becoming extremely important. Therefore, CWS is a
preliminary and important pre-process for natural
language processing (NLP) (Sun et al. 2012, 2013; Chen
et al. 2015a, 2015b), including POS tagging (Zheng et al.
2013), semantic analysis, question answering, abstractive
summarization (Rush et al. 2015), and machine transla-
tion (Sennrich et al. 2015).

Our research purpose in domain-specific CWS comes
down to two driving reasons. First, geoscience docu-
ments are rapidly and constantly increasing. Second,
the domain-general CWS has received much attention
and seen great advancement, especially with deep learn-
ing techniques, whereas there is much less concern for
domain-specific CWS (Huang et al. 2015).

Table 1 illustrates the CWS problem with an example
sentence from our experimental datasets based on the
domain-general segmenter. The original text has no
word boundaries at all. The manually annotated words
and English translation clearly demonstrate the succinct-
ness of the geoscience document. For example, the four
characters in 查干楚鲁 (qaganqulu) mean check/examine,
dry, clear, and Shandong, respectively. However, the four
characters form the place name that represented a
region. Furthermore, a character appearing in different
positions leads to different meanings and interpreta-
tion of words and phrases. For instance, the sentence
混合岩化作用 (migmatization) consists of six characters.
The generic segmenter may segment the sentence as
混合岩 化 作用 “migmatization”, which would lead to an
incorrect segmentation and produce a completely

different meaning (migmatite). If characters in a word
are treated individually rather than holistically, this
could lead to a completely different meaning. One way
to address this is to increase the vocabulary of the geosci-
ence domain. However, one obvious drawback of this
approach is that the available geoscience dictionaries
and thesauri in Chinese are scarce. Furthermore, even if
this is done based on dictionaries, these resources alone
are insufficient to form an accurate segmenter. The sim-
ple reason is that general terms only account for a part
of geoscience documents, and thus, it not designed for
identifying new words that are not in the corpus, espe-
cially specialized vocabulary.

The geoscience Chinese word segmentation (GCWS) is
full of challenges due to the following reasons: (i) the
limited generic word segmentation, (ii) the lack of large
amounts of supervised training data, and (iii) new geosci-
ence names and rules are constantly increasing. Some
previous works for CWS usually regard the CWS as a
sequence labeled problem by applying machine leering
methods such as conditional random fields (CRF), deep
learning (Mikolov et al. 2010; Krizhevsky et al. 2012;
Zaremba et al. 2014), and so forth.

Deep learning (DL) has become the mainstream
method for CWS. Deep learning is one of the most reli-
able sequence labeling methods, because it has demon-
strated high performance on different kinds of natural
language processing tasks. Long short-term memory
(LSTM) is can be used to process variable inputs, which
is detailed in Section 2. The LSTM (Hochreiter and
Schmidhuber 1997; Schuster and Paliwal 1997; Wang
et al. 2015) approach achieves great success in NLP tasks
such as sequence tagging. In this study, we construct a
model using a bi-directional LSTM (Bi-LSTM) for GCWS.
The Bi-LSTM can automatically learn features from
a dataset during the training process, which is an
end-to-end solution. Therefore, it can reduce the dataset
to design hand-crafted features and geoscience knowl-
edge resources.

Considering the above challenges, this motivated us to
develop a segmenter specifically for the geoscience
domain. In this paper, we proposed a framework by
incorporating the Bi-LSTM network model framework
based on a cyclic self-learning corpus training strategy
to perform CWS in two phases. In phase one, we build
a geoscience segmenter (GeoSegmenter) using the

Published by NRC Research Press

Geomatica Downloaded from www.nrcresearchpress.com by GLASGOW UNIVERSITY LIBRARY on 08/02/18For personal use only. 18

Geomat. Vol. 72, 2018

Fig. 1. Our framework. [Colour online.]

Bi-LSTM model. GeoSegmenter first identifies general
terms by using a generic baseline segmenter with generic
corpus. In phase two, it recognizes geoscience terms by
learning and applying cyclic corpus. Specifically, an in-
stance (observation) is treated as a character sequence
linked with a tag sequence that represents the posi-
tion of each character. Given the tag sequence, each
character sequence can then be segmented (broken
or grouped) into linguistically meaningful words.
Following the framework, the generic segmenter based
on the Bi-LSTM model is used to identify geoscience
terms from the first step. The second step model is
treated as a sequence labeling problem. Then, the cyclic
self-learning corpus training method is employed to
learn the sequence model.

Some primary experiments were conducted to validate
the performance of GeoSegmenter’s algorithms in
supporting CWS from geoscience documents. Compared
with all other baseline segmenters, the GeoSegmenter
achieves the best performance with a maximum perfor-
mance increase of 20%. Identical conclusions were
obtained in studies that showed that GeoSegmenter can
learn using a cyclic self-learning training strategy from
the generic annotated data and it can be seamlessly
extended to other domains.

The contributions of this paper can be summarized as
follows. First, we address the CWS problem for the geo-
science domain based on DL, which can capture domain
information, keep useful domain information, and avoid
a heavy dependence on the choice of handcrafted fea-
tures. For the domain-specific CWS, a theoretical cyclic
self-learning framework is proposed by employing
the DL model and methodology, which can easily be
extended to other subject domains. Third, despite CWS

being a specific case, our model can be easily generalized
and applied to the other sequence labeling tasks.

The remainder of the paper is organized as follows.
Section 2 describes the Bi-LSTM GeoSegmenter. Section 3
shows the experiments results. Section 4 discusses related
research. Section 5 concludes the paper and discusses
future work.

2. Materials and methods

To address the abovementioned problems and needs,
we propose a framework for sequence learning by incor-
porating cyclic self-learning training. The proposed
GCWS methodology based on DL allows for the simulta-
neous capturing of the dependency structures and the
distributions of both generic and specific data.

Figure 1 illustrates the proposed framework in sup-
porting GCWS that consists of two stages: the generic
segmentation and the domain-specific segmentation for
the geoscience domain. First, the domain-general testing
and training set are used to get the generic segmenter
with generic corpus. Then, geoscience documents are
segmented by the generic segmenter. Next, the seg-
mented corpus is trained repeatedly by manual interven-
tion. Finally, the trained model can be used to predict
input sentences. Section 3 explains these phases in detail.

2.1 Construction generic segmenter

We collect the corpora obtained from SIGHAN Bakeoff
(http://www.sighan.org), which are the main stream cor-
pora for Chinese language processing. We used the
Simplified Chinese versions of the training corpora cre-
ated by Microsoft Research (MSR) and Peking University
(PKU). The contents of the corpora are selected carefully,
and the domains are broadly representative such as poli-
tics, economics, culture, and so forth. Hence, the corpora

Published by NRC Research Press

Geomatica Downloaded from www.nrcresearchpress.com by GLASGOW UNIVERSITY LIBRARY on 08/02/18For personal use only. Qiu et al.

19

are sufficiently representative to evaluate the perfor-
mances of CWS. The training and the test data are used
to obtain a generic segmentation based on the Bi-LSTM
model. The results are used to segment the geoscience
sentences.

2.2 Construction GeoSegmenter

The generic segmenter in hand can be cursorily
applied to segment geoscience documents. However,
the results contain many mistakes, especially with geol-
ogy terms. Hence, a stronger algorithm is necessary to
address and correct the mistakes.

The main motivations behind the generic segmenter
method are domain dependence and language depend-
ence. To comply with these objectives, we first intro-
duce a circle self-learning framework and then
illustrate how to incorporate the framework into the
Bi-LSTM model.

2.2.1 Cyclic self-learning strategy

Let n be the number of characters in one sentence.
Given a sequence of characters x = {x1, x2, : : : , xn}, n is
the length of the sentence. A candidate label sequence
has the form y = {y1, y2, : : : , yn}. We use yn to denote these
ground truth labels, where yn ∈ {B, E, M, S}. {B, M, E}
represent the beginning, middle, and end of a multi-
character segmentation, respectively, and S represents
a single character segmentation. With the segmentation
labels generated from the segmentation produced by the
j ∈ {B, E, M, S}. We
baseline segmenter, each vector is ys
use ys

j to differentiate it from the ground truth yn.

To make full use of the raw corpus, the circle self-
learning strategy is introduced into the CWS, which fur-
ther adapts the traditional word segmentation model for
professional domains. For illustration, let x = 被查干楚鲁
粗粒黑云母花岗岩侵入, and s = {SBMMEBEBMEBMEBE}.
Therefore, the linguistically meaningful segmentation
is 被 查干楚鲁 粗粒 黑云母 花岗岩 侵入. If 查干楚鲁 is
segmented incorrectly, it will be removed from the
corpus. The final corpus is imported into the model for
cyclic training.
Given ys

j and yn, we can easily find different labels. For
different tags, manual intervention is taken to remove
the wrong segmentation. In a loop, the corpus after
intervention is finally obtained. The acquired corpus is
sent back to the network for re-training. The corpus is
trained repeatedly until it reaches convergence. Table 2
shows the cyclic self-learning training procedure.

2.2.2 Bi-LSTM model

Recurrent neural networks (RNNs) (Mikolov et al. 2010;
Mikolov et al. 2011) contain a high-dimensional hidden
state with non-linear dynamics, which encourages
RNNs to obtain the advantages of previous information.
In principle, RNNs have a powerful predictive ability.
They use a simple model that is able to compute the gra-
dients of the RNNs with back-propagation through time

Table 2. Algorithm 1 cyclic self-learning training
procedure.

end for

for i = 1 to n do

▹Generate Candidate Label
YS←Ø
YS = generic-segmenter-Procedure(c[1:n])

function DECODE(c1, c2, : : : cn, θ)
01: for i = 1 to m do
02: ▹Cycle self-training
03:
04:
05:
06:
07:
08: ▹Different Compare based on manual intervention
09:
10:
11:
12:
13:
14: end for
15: obtain the X, Y s

delete Xj.
update X, Y s

for j = 1 to n do

j and go to step 03

if YS ≠ Yj

end for

j

(BPTT) and can be applied to long duration flows without
sequence-based specialization. However, they fail to pro-
duce effective results due to the difficulty of BPTT and
the vanishing and exploding gradient problems, as dem-
onstrated in Bengio et al. 1994.

In recent years, LSTM approaches have achieved great
success in NLP tasks such as sequence tagging and
named entity recognition tasks. LSTM can be used to
process variable inputs and have a long-term memory,
as shown in Fig. 2. LSTM can address the vanishing and
exploding gradient difficulties by adding some extra
memory cells that are inherent for RNNs. The key idea
of proposing self-loops to yield paths are the main func-
tion of the LSTM model.

At every time step, the LSTM model has a memory cell
c that is encoding memory. Three “gates” named input
gate i, forget gate f, and output gate o control the behav-
ior of the cell. The operations on all gates are defined as
element-wise multiplications. Moreover, gates can be
used to scale the input value if the gate is a non-zero vec-
tor or omit the input if the gate is a zero vector. For the
output gate, the output is fed into the next time step
t+1 as the previous hidden state and the input of the
upper layer of neural network at current time step t.
The gates, cell, and output are updated as follows:

ð1Þ

ð2Þ

ð3Þ

ð4Þ

ð5Þ

it = δðW ixt + Uiht−1
f t = δðW f xt + Uf ht−1
Ct = f t ⊗ Ct−1
ot = δðW oxt + Uoht−1
ht = ot ⊗ tanhðCtÞ

+ biÞ

+ V ict−1
+ V f ct−1

+ bf Þ

+ V oct−1

+ boÞ

+ it ⊗ tanhðW cxt + Ucht−1

+ bcÞ

where δ and ⊗ are the logistic sigmoid function and
hyperbolic tangent function, respectively. it, ft, ot, and ct
are the input gate, forget gate, output gate, and the

Published by NRC Research Press

Geomatica Downloaded from www.nrcresearchpress.com by GLASGOW UNIVERSITY LIBRARY on 08/02/18For personal use only. 20

Geomat. Vol. 72, 2018

Fig. 2. Long short-term memory cell. The memory unit contains a cell c, which is controlled by three gates.

Fig. 3. A bidirectional LSTM network.

memory cell activation vector at time t, respectively.
ht represents the output of the LSTM unit at the time
step t. W, U, and V are the connection weights to be com-
puted during the training time.

In this study, we construct the bi-directional LSTM
(Graves and Schmidhuber 2005) instead of a single
forward network. Figure 3 shows a Bi-LSTM network
sequence tagging model. The Bi-LSTM neural network
is similar to the LSTM neural network because both of
them are constructed with LSTM units. Different from
the LSTM network, Bi-LSTM is able to capture informa-
tion from sequence dataset and maintain contextual
features from the past and future. Moreover, the
Bi-LSTM network has two parallel layers that are propa-
gating in two directions. The forward and backward
pass of each layer are conducted in similar ways to
regular neural networks, and these two layers memo-
rize the information of all sentences from both direc-
tions. For example, for each word xi in a given
sentence (x1, x2, : : : , xn), by applying Bi-LSTM to obtain
the representation lefti of the left context, we can also
gain the representation righti of the right context by
reversing the sentence. Associating the left and the
right context representations can produce the final rep-
resentation for a word, which is very effective for the
sequence labeling problem.

2.3 Segmentation output

For each Chinese sentence, to identify the segmenta-
tion locater, we construct the segmentation model based
i=1 mapped from Δ:XN→YN,
on the training data (xi, yi)n
where xi and yi denote the character and location vectors
in the ith sentence, respectively, and n is the number of
sentences. For instance, Δ({花,岗,岩}) = {B, M, E} indi-
cates that 花岗岩 is segmented as a word. Δ is a high-
dimensional function when the sizes of Chinese
documents are large. The parameter set of our model is θ.
To reduce the dimensionality, we first define a structured
marginal loss Δ(yi, ˆy) for the predicted tag sequence ˆy:

ð6Þ

Δðyi, ˆyÞ =

μ1

yi

ðtÞ

, ˆyðtÞ

Xn

n

o

t

where μ is a discount parameter. The loss is proportional
to the number of incorrectly segmented characters. For a
given training instance (xi, yi), the predicted tag sequence
ˆyi is the one with the highest score:

ð7Þ

ˆyi = arg max sðxi, y, θÞ

Given a training set Ω, the regularized objective func-
tion is the loss function J(θ) that includes an l2-norm
term:

Published by NRC Research Press

Geomatica Downloaded from www.nrcresearchpress.com by GLASGOW UNIVERSITY LIBRARY on 08/02/18For personal use only. Qiu et al.

ð8Þ

ð9Þ

JðθÞ = 1
jΩj

X

ðxi, yiÞ∈Ω

liðθÞ +

kθk2
2

λ

2

liðθÞ = maxð0, sðxi, ˆyi, θÞ + Δðyi, ˆyiÞ − sðxi, yi, θÞÞ

It is important to develop an efficient optimization
strategy to improve computational efficiency. Due to
the hinge loss, the objective function is not differentia-
ble, and we use a subgradient method that computes a
gradient-like direction.

With the proposed Bi-LSTM framework, GeoSegmenter
seamlessly integrates the generic CWS and domain
specific CWS. This has some unique advantages. First,
the Bi-LSTM framework can easily handle sparse data.
Second, with the construction of the cyclic corpus,
GeoSegmenter is flexible and easily adapted to other sub-
ject domains. Third, the GeoSegmenter is robust. Neural
networks can recognize new words through learning.
After all, it is impossible for a corpus to include every
possible word that a segmenter could encounter in geo-
science documents.

3. Experiments and discussion

We conducted five primary experiments to validate
the performance of the proposed algorithms in support-
ing CWS from geoscience documents. The first three
experiments were conducted to compare different seg-
menters’ performances, including the generic seg-
menter and the geoscience segmenter. The fourth
experiment was conducted to identify different neural
network models. The fifth experiment was conducted
to evaluate the precision and recall of the proposed algo-
rithm compared with other algorithms. The experimen-
tal setup, the experimental results, and the final
performance of the proposed CWS algorithm are sum-
marized and discussed in the following subsections.

3.1 Experimental setup

Datasets: the major experimental dataset consists of
43 geoscience documents. These records were collected
from the geoscience subject category of National
Geological Archives of China (http://www.ngac.cn/).
Following the same criteria used to generate benchmark
corpus from the SIGHAN CWS bakeoff, we manually seg-
ment these documents into words. We denote this data-
set as the GEO corpus. This resulted in 7.8M segments.
In addition, we use the first 90% sentences of the train-
ing data as the training set and the remaining 10% sen-
tences as the test set.

The benchmark corpus from the SIGHAN CWS bakeoff
consists of new documents with a wide range of topics.
We used the Simplified Chinese versions of the training
corpora created by Microsoft Research (MSR) and
Peking University (PKU). The resulting MSR and PKU cor-
pora have 12.2M and 5.63M segments, respectively.

Performance: five standard measures are used to evalu-
ate the segmenters’ performances. Precision denotes the

21

Table 3. Hyper parameters settings.

Character embedding size
Hidden units number
Initial learning rate
Margin loss discount
Regularization
Dropout rate on input layer
Maximum word length

d = 50
H = 50
α = 0.2
μ = 0.2
−6
λ = 10
P = 0.2
ω = 4

percentage of all predicted words that are true words as
labelled by human annotators. Recall denotes the
percentage of all true words that are correctly predicted.
F score evaluates the overall performance by calculating
F = 2PR/(P + R). The recall of OOV (ROOV) and recall of IV
(RIV) denote the percentage of OOV and in-vocabulary
(IV) words that are correctly segmented, respectively.
Given a learned model, ROOV indicates how well it can
be generalized to a new domain, whereas RIV suggests
its predictive power over the training data. All of these
measures are bounded between [0,1], with greater values
indicating higher accuracy.

3.2 Parameters adjustment

The hyper parameters of the neural network model
have significantly impacts on the performance. The
hyper parameters are shown in Table 3. In our research,
motivated by previous works (Collobert et al. 2011;
Chen et al. 2017), we set the character embedding size
of words to 50, and the number of hidden units is set to
be the same as the character embedding size. According
to the experimental results, the initial learning rate
and marginal loss discount both have been set at 0.2.
The maximum word length has been set at 4. The regu-
−6. We set the input layer of the model
larization is 10
with a dropout rate of 20%, which is a popular technique
for improving the performance of neural networks by
reducing overfitting.

3.3 Different segmenters performance

To evaluate the performance of GeoSegmenter, we
compare it with the performances of different segment-
ers on the dataset. Depending on whether resources
other than GEO corpus are used or not, they are classi-
fied as either a cross-domain segmenter or a closed-
domain segmenter.

GenSegmenter denotes the generic segmenter, and

GeoSegmenter denotes the geoscience segmenter.

Closed-domain segmenter: The first closed-domain
baseline implements the dictionary-based maximum-
match method for segmentation (denoted as MM). The
MM is a basic and representative method, which was also
used as a baseline in the SIGHAN competitions.

The second closed-domain baseline is the Bi-LSTM-based
segmenter built directly from the annotated data. It is
denoted as GESGEO.

Cross-domain segmenter: in the experiment, we test
the cross-domain segmenters. The first two are the

Published by NRC Research Press

Geomatica Downloaded from www.nrcresearchpress.com by GLASGOW UNIVERSITY LIBRARY on 08/02/18For personal use only. 22

Geomat. Vol. 72, 2018

Table 4. Segmentation performance of different segmenters.

Segmenter

Precision

Improvement (%)

Recall

Improvement (%)

F1 score

Improvement (%)

Baseline
MM
GenSegmenter
GESGEO
GESMSR
GESPKU
GeoSegmenter
GEOMM
GEOMSR
GEOPKU
GEOGEO

0.813

0.896
0.707
0.726

0.851
0.917
0.925
0.896

—

—
—
—

3.80
21.00
19.90
—

0.825

0.899
0.736
0.76

0.872
0.927
0.936
0.899

—

—
—
—

4.70
19.10
17.60
—

0.819

0.897
0.721
0.743

0.861
0.922
0.930
0.897

—

—
—
—

4.20
20.10
18.70
—

Note: The bold numbers represent the best results in their respective columns.

Bi-LSTM-based segmenters built from the MSR and the
PKU corpora using the same neural network model.
They are denoted as GEOMSR and GEOPKU, respectively.

By comparing the results, Table 4 shows the effective-
ness of the proposed domain adaptation method. The
GeoSegmenter outperformed its corresponding base-
lines by at most 21% with respect to the F score.

The segmenters built based on the same corpus (MM
and GESGEO) outperform those that built by the different
corpus (GESMSR and GESPKU). The results of segmenters
that used MM-based dictionaries were good. It showed
that MM is more accurate than the generic segmenters
(PKU and MSR), regardless of the fact that they used the
machine learning approach. However, this does not
mean that machine learning techniques are inferior to
segmenters that used MM-based dictionaries. It is
extremely helpful for the cross-domain segmenters
GESMSR and GESPKU, and their F scores increased by
15%–20%. Furthermore, due to the use of neural network
models that have strong learning power over training
data, the GeoSegmenter was able to learn the best adap-
tation model for each generic segmenter. As a result,
GEOPKU outperformed all other GeoSegmenter versions.
These results and findings suggest that the proposed
Bi-LSTM GeoSegmenter approach is indeed an effective
method to solve the domain-specific word segmentation
problem. General terms and geoscience terms are recog-
nized separately and compensate for each other.

3.4 New word detection

Existing generic segmenters can be used to process
geoscience documents; however, they lack the domain
specific knowledge. More specifically, new word detec-
tion is the key factor for the performance. Hence, for
evaluating the performance of the proposed GCWS
methodology, the new word detection experiments are
conducted using evaluation metrics. The recall of OOV
(ROOV) and recall of IV (RIV) are selected as the primary
evaluation metrics.

Table 5 shows the experimental results. When com-
paring the results of the ROOV rates and the RIV rates, it

Table 5. Performance of the GenSegmenter and
GeoSegmenter on recognizing OOV and IV terms.

Improvement
(%)

RIV

Improvement
(%)

Segmenter ROOV

Baseline
MM
GenSegmenter
GESMSR
GESPKU
GESGEO
GeoSegmenter
GEOMM
GEOMSR
GEOPKU
GEOGEO

0.058 —

0.417 —
0.405 —
0.553 —

2.2
0.08
33.3
0.75
0.721
31.6
0.558 0.5

0.918 —

0.823 —
0.776 —
0.936 —

0.926 0.8
0.914 9.1
0.938 16.2
0.938 0.2

Note: The bold numbers represent the best results in

their respective columns.

is very difficult to identify new words. For professional
domains, due to lack of domain knowledge, the neural
network model did not perform well. Nevertheless, our
approach does not depend on any predesigned features
due to the strong ability of the Bi-LSTM network in auto-
matic feature learning. The recall rates of OOV terms
increased dramatically when adaptation was performed,
resulting in increases of 33.3% and 31.6% in ROOV GESMSR
and GESPKU, respectively. These cases highlight the
capability of the GeoSegmenter in domain adaptation.
Based on the experimental results, the experimental
results suggest that the cyclic self-learning strategy can
improve the GCWS performance.

3.5 Overall performance

The cyclic self-learning strategy is important to deter-
mine the overall performance. To further evaluate the
effect of manual intervention with iterations, we con-
duct some experiments to validate the performance.

We test the proposed method based on the model
using cyclic self-learning on the cross-domain from the

Published by NRC Research Press

Geomatica Downloaded from www.nrcresearchpress.com by GLASGOW UNIVERSITY LIBRARY on 08/02/18For personal use only. Qiu et al.

Iteration

P

Table 6. Results of word segmentation based on cyclic
self-learning.

1
2
3
4
5
6
7
8
9
10
20

R

0.870
0.876
0.872
0.888
0.897
0.899
0.901
0.909
0.912
0.925
0.919

F

0.871
0.882
0.879
0.893
0.899
0.901
0.905
0.910
0.920
0.927
0.922

RIV

0.885
0.894
0.895
0.900
0.903
0.917
0.918
0.925
0.928
0.931
0.920

ROOV

0.785
0.796
0.802
0.833
0.841
0.846
0.851
0.860
0.873
0.881
0.880

0.872
0.888
0.886
0.898
0.901
0.903
0.909
0.912
0.928
0.930
0.926

MSR corpus. The effectiveness of the proposed segmenta-
tion results is shown in Table 6. The proposed method
has a high recall rate that increases with the number of
iterations. The GeoSegmenter outperformed the corre-
sponding baselines by at most 96.2% in F score. It shows
that with the continuous iteration of the corpus, the
strategy begins to work. After a certain number of itera-
tions, the F value fluctuates, thus indicating that the role
of cyclic learning begins to decrease. From the results,
we can see that the cyclic self-learning method has
greatly improved the segmentation in the geological
field, and the F value has reached 96.7%. With respect to
ROOV, the improvement is more obvious. The final result
reached 88.1%. This is not surprising because each itera-
tion updates the wrong segmentations.

Furthermore, due to the use of the cyclic learning
method, the GeoSegmenter was able to learn the best
adaptation model for each iteration. Cyclic self-learning,
due to the DL model, can learn professional word
formation rules in the corpus. The manual intervention
operation to guide network iterative training, the multi-
ple iterative training network, and DL through the self-
learning strategy improve segmentation performance. In
contrast, its competitors lose in their capability of identify-
ing non-geoscience terms, simply because either the GEO
or the PKU and the MSR corpora are too small to capture
every possible word that a segmenter could encounter.

These results and findings suggest that the proposed
two-step approach is indeed an effective approach to
solve the domain-specific word segmentation problem.
General terms and geoscience terms are recognized sep-
arately and compensate for each other.

As shown in Fig. 4, the cyclic self-learning method
impacted the three cross-domain segmenters, resulting
in GEOMSR, GEOPKU, and GEOGEO. This is because these
segmenters make full use of the GEO corpus and they
benefited from the large training set, as shown by their
steadily increasing curves. In particular, GeoSegmenters
learn well by cooperating with the cyclic self-learning
strategy, which can gain increased advantages with the

23

number of iterations in the training data. In other words,
although the baseline segmenter made more mistakes
with geoscience documents, the proposed strategy learns
how to correct these mistakes, and more corrected exam-
ples leads to better mistake recognition. In fact, the more
mistakes the baseline segmenter makes in the initial seg-
mentation, the more that GeoSegmenter can learn in the
adaptation procedure. This clearly shows the effective-
ness of our DL-based technique in domain specific CWS.
For GeoMM and GeoGeo, GeoGeo achieved greater per-
formance with the DL model. This advocates the DL tech-
nique over traditional dictionary-based approaches
because it can make better use of the training data.

3.6 Different neural network model

The neural network model contains many layers, and
the number of layers controls the sample learning. To
fine-tune the layers of the Bi-LSTM model, a total of four
controlled experiments are conducted with different
numbers of layers ranging from one layer to three layers.
The experimental results (in terms of average precision
and recall improvements) are shown in Table 7.

The experimental results show that as we stack more
Bi-LSTM layers, the performance gets a slight improve-
ment. The Bi-LSTM3 model achieves the best F scores of
0.94 and 0.92 on the dataset, respectively. However, com-
pared with the Bi-LSTM2 model, the performance is not
significantly enhanced. Based on the experimental
results, we can conclude that adding layers becomes
not as effective when the number of Bi-LSTM layers
exceeds three, which also takes quite a long time to
train. The result shows that the LSTM units become less
effective in higher level layers. Therefore, we believe that
there is no need to build very deep networks for
extracting contextual information.

3.7 Compared different algorithms

To further investigate our achievement, we compared
out results with other algorithms using the GEO corpus.
Specially, Huang (Huang et al. 2015) proposed a sta-
tistically learned Chinese word segmenter based on CRF
for the geoscience domain (SLC).

Table 8 shows the comparison results. It can be
found that our work outperforms the CRF as it ranks
first out of all methods in GEO. It improved by the
precision by 6% points (from 86.5% to 92.5%), the recall
by 5.8% points (from 85.4% to 91.2%), and the F1 by 7.1%
points (from 85.9% to 93.0%). When compared with
SLC, the method also achieves an average F1 score
increase of 2.1% points and the recall increased by
approximately 2.4% points. This could be caused
by the Bi-LSTM model having a stronger learning
ability than the SLC. This indicates that the proposed
approach, by dynamically adapting to the dependency
structures and distributions of generic and geoscience
data, can offer a promising way to reduce human effort
while achieving high performance.

Published by NRC Research Press

Geomatica Downloaded from www.nrcresearchpress.com by GLASGOW UNIVERSITY LIBRARY on 08/02/18For personal use only. 24

Geomat. Vol. 72, 2018

Fig. 4. Performance of the cross-domain segmenter with iteration times. [Colour online.]

Table 7. Performance of our models on test sets.

MSR + GEO

Model

P

R

LSTM
Bi-LSTM*
Bi-LSTM2*
Bi-LSTM3*

0.908
0.919
0.925
0.938

0.912
0.911
0.921
0.933

F

0.91
0.91
0.92
0.94

PKU + GEO

P

R

0.896
0.909
0.918
0.927

0.902
0.912
0.92
0.922

F

0.90
0.91
0.92
0.92

Note: Bi-LSTM2 means a bi-directional LSTM network
with two layers, and Bi-LSTM3 means a bi-directional LSTM
network with three layers. An asterisk (*) denotes that the
model is trained with dropout.

Table 8. Comparison of the results of our experiment with
others in GEO.

Algorithm

Precision

CRF
SLC
Our work

0.865
0.906
0.925

Recall

0.854
0.912
0.936

F1 score

0.859
0.909
0.930

4. Related work

CWS is an automated process that aims to segment
the text and produce meaningful sequences. Existing
CWS approaches can be classified into two primary cat-
egories: rule-based methods and machine learning (ML)
based methods (Gao et al. 2005).

Rule-based CWS methods rely on hand-crafted rules to
guide the segmentation of unstructured textual data
(Palmer 1997). The constructed rules represent domain
knowledge. This method can be applied to domain spe-
cific CWS; however, it lacks principled statistical infer-
ences and cannot achieve a high performance.

Different from the rule-based CWS methods, ML-based
CWS methods utilize ML algorithms to automate the seg-
mentation of the text (Xue 2003). A number of research
efforts have focused on developing ML-based CWS
methods to support segmentation tasks (Chao et al.
2015; Qiu et al. 2016; Shu et al. 2017). The ML techniques
(Collobert et al. 2011) have been used to reduce the
efforts of feature engineering (Qi et al. 2014) in extensive
investigations of CWS problems.

Among the ML-based CWS methods, the CRF and neu-
ral network model are the main stream approaches for
the CWS. The CRF model has shown good performances
on different kinds of natural language processing tasks
(Zhu et al. 2016; Wang et al. 2018). For example, Huang
et al. (2015) proposed a statistically learned Chinese word
segmenter for the geoscience domain based on CRF.
They unite the general terms and specific terms using
transforming model. The experiment results show that
segmenter can recognize both geoscience terms and gen-
eral terms. For the neural network model, Pei et al. (2014)
used a tensor neural network to achieve feature combi-
nations by capturing the complicated interactions
between tags and context characters. Chen et al. (2015a,
2015b) used a recurrent network structure to extract
more combined features to model complicated character
combinations. Chen et al. (2015a, 2015b) used an LSTM
model to capture long-range dependencies between
characters.

However, the reliability of CWS that can be achieved
using machine learning relies heavily on a large amount
of high-quality, manually segmented data. Hand-labeling
individual words is very time consuming and expensive.
Although a number of manually segmented datasets
have been constructed, it is not feasible to combine
them into other domains due to the incompatibility.

Published by NRC Research Press

Geomatica Downloaded from www.nrcresearchpress.com by GLASGOW UNIVERSITY LIBRARY on 08/02/18For personal use only. Qiu et al.

25

Thus, it is difficult to build a large-scale segmented cor-
pus, especially in the geoscience, and the resulting lack
of domain corpus is detrimental to the enhancement of
the accuracy of CWS.

5. Conclusion

The CWS is a preliminary and important task for
Chinese natural language processing. Domain-specific
CWS is especially important for mining and acquiring
knowledge from large documents. Simply put, it is diffi-
cult to acquire a large amount of high-quality, manually
segmented data. In this paper, we considered unstruc-
tured geoscience data that was mixed, variable, and
stable. We proposed a framework that incorporated the
Bi-LSTM network model and used the cyclic self-learning
training strategy to perform CWS. Following this frame-
work, it considers general terms and specific terms, and
combines them together. For the construction of corpus,
different from the traditional method, we use a cyclic
self-learning strategy to enrich the general domain cor-
pus and combine it with the word tagging Bi-LSTM
model based on the improved segmentation perfor-
mance and the effect of the segmentation model. The
experimental results show that the proposed method
can effectively enhance the segmentation ability of
unstructured geoscience documents and recognize a
large number of OOV words in geoscience.

Acknowledgements

We thank the anonymous reviewers for their valuable
comments. This study was financially supported by
the National Natural Science Foundation of China
(41671400), National key R&D program of China
(No. 2017YFC0602204), HuBei Natural Science Foundation
of China (2015CFA012), and the National Key Research
and Development Program (grant No. 2017YFB0503600).

References

Bengio, Y., Simard, P., and Frasconi, P. 1994. Learning long-term
dependencies with gradient descent is difficult. IEEE Trans.
Neural Netw. 5(2): 157–166. doi:10.1109/72.279181. PMID:18267787.
Chao, J., Li, Z., Chen, W., and Zhang, M. 2015. Exploiting hetero-
geneous annotations for weibo word segmentation and
pos tagging. Natural Language Processing and Chinese
Computing. Springer, Cham, Switzerland. pp. 495–506.

Chen, X., Qiu, X., Zhu, C., Liu, P., and Huang, X. 2015a. Long
short-term memory neural networks for Chinese word
segmentation. Proc. 2015 Conference on Empirical Methods
in Natural Language Processing. pp. 1197–1206.

Chen, X., Qiu, X., Zhu, C., and Huang, X. 2015b. Gated recursive
neural network for Chinese word segmentation. Proc. 53rd
Annual Meeting of the Association for Computational
Linguistics and the 7th International Joint Conference on
Natural Language Processing (Volume 1: Long Papers). Vol. 1,
pp. 1744–1753.

Chen, X., Shi, Z., Qiu, X., and Huang, X. 2017. Adversarial multi-
criteria learning for Chinese word segmentation. arXiv
preprint arXiv:1704.07556.

Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K.,
and Kuksa, P. 2011. Natural language processing (almost)
from scratch. J. Mach. Learn. Res. 12(Aug): 2493–2537.

Gao, J., Li, M., Huang, C.N., and Wu, A. 2005. Chinese word seg-
mentation and named entity recognition: a pragmatic
approach. Comput. Linguist. 31(4): 531–574. doi:10.1162/0891
20105775299177.

Graves, A., and Schmidhuber, J. 2005. Framewise phoneme clas-
sification with bidirectional LSTM and other neural network
architectures. Neural Networks, 18(5–6): 602–610. doi:10.1016/
j.neunet.2005.06.042. PMID:18220205.

Hochreiter, S., and Schmidhuber, J. 1997. Long short-term
memory. Neural Comput. 9(8): 1735–1780. doi:10.1162/neco.
1997.9.8.1735. PMID:9377276.

Huang, L., Du, Y., and Chen, G. 2015. GeoSegmenter: a sta-
tistically learned Chinese word segmenter for the geoscience
domain. Comput. Geosci. 76: 11–17. doi:10.1016/j.cageo.
2014.11.005.

Krizhevsky, A., Sutskever, I., and Hinton, G.E. 2012. Imagenet clas-
sification with deep convolutional neural networks. Advances
in Neural Information Processing Systems. pp. 1097–1105.
Mikolov, T., Karafiát, M., Burget, L., Černocký, J., and
Khudanpur, S. 2010. Recurrent neural network based lan-
guage model. Eleventh Annual Conference of
the
International Speech Communication Association.

Mikolov, T., Deoras, A., Povey, D., Burget, L., and Černocký, J.
2011. Strategies for training large scale neural network
language models. 2011 IEEE Workshop on Automatic
Speech Recognition and Understanding (ASRU). IEEE.
pp. 196–201.

Palmer, D.D. 1997. A trainable rule-based algorithm for word
segmentation. Proc. 35th Annual Meeting of the Association
for Computational Linguistics and Eighth Conference of the
European Chapter of the Association for Computational
Linguistics. Association for Computational Linguistics.
pp. 321–328.

Pei, W., Ge, T., and Chang, B. 2014. Max-margin tensor neural
network for Chinese word segmentation. Proc. 52nd Annual
Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers). Vol. 1, pp. 293–303.

Qi, Y., Das, S.G., Collobert, R., and Weston, J. 2014. Deep learn-
ing for character-based information extraction. European
Conference on Information Retrieval. Springer, Cham,
Switzerland. pp. 668–674.

Qiu, X., Qian, P., and Shi, Z. 2016. Overview of the NLPCC-
ICCPOL 2016 shared task: Chinese word segmentation for
micro-blog texts. Natural Language Understanding and
Intelligent Applications. Springer, Cham, Switzerland.
pp. 901–906.

Rush, A.M., Chopra, S., and Weston, J. 2015. A neural attention
model for abstractive sentence summarization. arXiv preprint
arXiv:1509.00685.

Schuster, M., and Paliwal, K.K. 1997. Bidirectional recurrent neu-
ral networks. IEEE Trans. Signal Process. 45(11): 2673–2681.
doi:10.1109/78.650093.

Sennrich, R., Haddow, B., and Birch, A. 2015. Neural machine
translation of rare words with subword units. arXiv preprint
arXiv:1508.07909.

Shu, X., Wang, J., Shen, X., and Qu, A. 2017. Word segmentation in
Chinese language processing. Stat. Interface, 10(2): 165–173.
doi:10.4310/SII.2017.v10.n2.a1.

Sun, X., Wang, H., and Li, W. 2012. Fast online training with
frequency-adaptive learning rates for Chinese word segmenta-
tion and new word detection. Proc. 50th Annual Meeting of
the Association for Computational Linguistics: Long
Papers-Volume 1. Association for Computational Linguistics.
pp. 253–262.

Published by NRC Research Press

Geomatica Downloaded from www.nrcresearchpress.com by GLASGOW UNIVERSITY LIBRARY on 08/02/18For personal use only. 26

Geomat. Vol. 72, 2018

Sun, X., Zhang, Y., Matsuzaki, T., Tsuruoka, Y., and Tsujii, J.I.
2013. Probabilistic Chinese word segmentation with non-
local information and stochastic training. Inf. Process.
Manage. 49(3): 626–636. doi:10.1016/j.ipm.2012.12.003.

Wang, C., Ma, X., Chen, J., and Chen, J. 2018. Information extrac-
tion and knowledge graph construction from geoscience
literature. Comput. Geosci. 112: 112–120. doi:10.1016/j.cageo.
2017.12.007.

Wang, P., Qian, Y., Soong, F.K., He, L., and Zhao, H. 2015.
A unified tagging solution: Bidirectional LSTM recurrent
neural network with word embedding. arXiv preprint
arXiv:1511.00215.

Xue, N. 2003. Chinese word segmentation as character
tagging. Int. J. Comput. Linguistics Chin. Language Process.
8(1): 29–48.

Zaremba, W., Sutskever, I., and Vinyals, O. 2014. Recurrent
neural network regularization. arXiv preprint arXiv:1409.2329.
Zheng, X., Chen, H., and Xu, T. 2013. Deep learning for Chinese
word segmentation and POS tagging. Proc. 2013 Conference
on Empirical Methods in Natural Language Processing.
pp. 647–657.

Zhu, Y., Liu, J., Xu, Y., Tian, H., and Ma, J. 2016. Chinese word
segmentation research based on Conditional Random Field.
Comput. Eng. Appl. 15: 019.

Published by NRC Research Press

Geomatica Downloaded from www.nrcresearchpress.com by GLASGOW UNIVERSITY LIBRARY on 08/02/18For personal use only. 