Geoinformatica
DOI 10.1007/s10707-015-0229-7

SPLZ: An efficient algorithm for single source shortest
path problem using compression method

Jingwei Sun1 · Guangzhong Sun1

Received: 25 April 2014 / Revised: 11 January 2015 / Accepted: 22 April 2015
© Springer Science+Business Media New York 2015

Abstract Efficient solution of the single source shortest path (SSSP) problem on road
networks is an important requirement for numerous real-world applications. This paper
introduces an algorithm for the SSSP problem using compression method. Owning to pre-
computing and storing all-pairs shortest path (APSP), the process of solving SSSP problem
is a simple lookup of a little data from precomputed APSP and decompression. APSP with-
out compression needs at least 1TB memory for a road network with one million vertices.
Our algorithm can compress such an APSP into several GB, and ensure a good performance
of decompression. In our experiment on a dataset about Northwest USA (with 1.2 mil-
lions vertices), our method can achieve about three orders of magnitude faster than Dijkstra
algorithm based on binary heap.

Keywords Shortest path · Compression · Road network

1 Introduction

The single source shortest path (SSSP) problem is a classic algorithm problem, and is also a
model for numerous real-world applications, such as navigation, facilities location, logistics
planning. Generally, given a graph G = (V , E), and a source vertex s, the goal of SSSP
problem is to find the shortest paths from s to all other vertices in the graph.

(cid:2) Guangzhong Sun

gzsun@ustc.edu.cn

Jingwei Sun
sunjw@mail.ustc.edu.cn

1

School of Computer Science and Technology, University of Science and Technology of China,
Hefei, China

Geoinformatica

Effective precomputation plays an important role of many efficient algorithms of SSSP
problem. This kind of algorithms contains two phases: precomputing some supporting
information offline, and computing the final results online. A straightforward precomputa-
tion method is precomputing all-pair shortest path (APSP) and storing it in memory. Then
the time complexity of SSSP problem online is O((cid:2)V (cid:2)) for a simple lookup and output,
However, space consumption for raw data of APSP needs at least O((cid:2)V (cid:2)2). There is no
sufficient memory to run this algorithm on an ordinary machine for a large-scale graph. For
example, a graph with one million vertices needs at least 1 TB memory. It is an extreme
situation that we only record the shortest path tree. An APSP includes O((cid:2)V (cid:2)) trees, and
each tree needs O((cid:2)V (cid:2)) bytes to record the parent of every vertex. In road networks, the
degree of a vertex is lower than 255, so recording the parent of a vertex needs 1 byte. At
this situation, the APSP of one million vertices takes about 1TB.

In this paper, we propose a compression method to reduce the space cost of precom-
putation effectively, and ensure a linear time complexity for decompressing. We call our
method shortest path with Lempel Ziv (SPLZ), which means it is a modification of LZ77
[33] algorithm. However, the offline time complexity is O((cid:2)V (cid:2)2), so SPLZ is not very suit-
able to work on a continental-sized graph, for example, the Europe road network with 18
million vertices. The number of vertices of our experiment road networks is from 300,000
to 1,200,000. Though they are not continental-sized, they may still be representations of
large-scale road network in real-world applications. SPLZ could work well at this scale.

In one of our experiment on road network of Northwest USA (about 1.2 million vertices),
SPLZ can compress the APSP of this graph (about 1.4 TB) into several GB. It is afford-
able for a high-end PC or an ordinary workstation. If memory is insufficient, we can store
the compressed APSP into extended memory. Our experiments show that using extended
memory can still achieve a good performance of decompressing.

There are three main contributions of our paper:

• We design an effective compression scheme for storing APSP data. With this method,
we can take full advantages of information generated by precomputation. When mem-
ory is not enough, we can store the compressed APSP into extended memory and still
keep a good performance of decompression.

• We develop a fast algorithm named SPLZ to solve SSSP problem. Our algorithm on
single core can achieve about three orders of magnitude faster than Dijkstra algorithm
based on binary heap. This performance is about two to three times as much as the time
cost of copying an array of length (cid:2)V (cid:2) using standard C library function memcpy().
SPLZ is simple to be implemented. SPLZ does not use complex data structures and
elaborate skills. In the offline phase, SPLZ uses Dijkstra (or PHAST) and LZ77 with
a little modification. In the online phase, the operation of SPLZ for solving SSSP is
copying an array of length O((cid:2)V (cid:2)).

•

The remainder of this paper is organized as follows: Section 2 describes related works.
Section 3 introduces the basic idea of SPLZ. Section 4 details the implement of SPLZ.
Section 5 reports the experimental results. Conclusion is made in Section 6.

2 Related works

SSSP problem has been widely researched. Dijkstra [18] algorithm is the most classic
method for SSSP problem. To improve the performance of Dijkstra, researchers have

Geoinformatica

adopted numerous type of priority queue. It has a series of modification like DIKB [17],
DIKBD [11], DIKH [13], DIKR [3]. They are also called label setting algorithm. Another
classic algorithm for SSSP problem is Bellman-Ford [7]. It is classified to another type of
algorithm called label correcting algorithm. Besides Bellman-Ford, label correcting algo-
rithm includes many others like PAPE [29], TWO-Q [28], THRESH [21], SLF [8]. They are
based on different label correcting strategies. Both label setting algorithm and label correct-
ing algorithm can be described by a unified framework [19]. These algorithms are designed
for general purpose, so they do not have preprocess phase and other optimization skills for
road network. These algorithms usually perform not so well for SSSP problem on large
scale road network.

Some algorithms accelerate solving the SSSP problem with parallelism. The clas-
sic parallel algorithms for SSSP problem contain parallel asynchronous label correcting
method [9] and (cid:2)-stepping [27]. Goldberg et al. [12] pointed out that traditional par-
allel algorithm for SSSP problem usually do not take full advantages of modern CPU
architecture, like multi-core, SSE. (cid:2)-stepping has less acceleration on large-scale road net-
work [25]. In 2011, Delling and Goldberg et al. developed PHAST [14], which is the
fastest algorithm at present. PHAST makes full use of SSE, multi-core, and is elaborately
designed to obtain a low cache missing. Its performance of GPU modification on large
scale road network is up to three orders of magnitude faster than Dijkstra on a high-
end CPU. Using PHAST instead of Dijkstra can reduce the time cost of offline phast
of SPLZ. Parallel technique is also used in SPLZ to accelerate the offline precomputing
phase.

Precomputing methods replace the time consumption online with time and space
consumption offline. It is a efficient approach for solving shortest path problem on
large-scale graph. Many high performance algorithm for point-to-point shortest path prob-
lem have a precomputing process, including Highway Hierarchies [30], Transit Node
Routing [5], ALT [22] et al. In 2008, Geisberger proposed Contraction Hierarchies[20]
algorithm. It is not only a good algorithm for calculating the answer of shortest path,
but also a efficient method for precomputing. Many outstanding algorithms like Tran-
sit Node Routing [4], PHAST and Hub-based labeling [1, 2] adopted it as precomputing
method. Hub-based labeling is the state-of-the-art point-to-point shortest path algorithm.
It takes more space than others to obtain the fastest online performance. Similarly,
SPLZ precomputes much more information (the total APSP) than these algorithms,
using more offline time and space consumption. Thus SPLZ can achieve high online
performance.

Some algorithms have good performance online, but space consumption of precomput-
ing is too huge to be loaded in memory. Compression methods are practical way to reduce
the space consumption. SILC [31], PCPD [32], and CPD [10] use compression method to
reduce space complexity of storing APSP. These methods are to solve point-to-point short-
est path problem on spatial networks (graphs where each node is labeled with coordinates).
They store APSP in the form of first move table. First move table can obtain high compres-
sion ratio by taking the advantage of path coherence [31]. Graph partitioning usually is used
for reducing online space in some algorithms, like Arc-flag [23], PCD [15] and CRP [26].
But for SILC et al, graph partitioning is used for clustering similar data in APSP. SPLZ has
a similar preprocess like SILC et al, but SPLZ is used for SSSP problem, not point-to-point,
so SPLZ adopts different compression and decompression method. Detailed difference will
be described in Section 3.1.

Geoinformatica

3 Basic SPLZ

3.1 Main idea

The main idea of SPLZ is to precompute APSP and then compress it for online lookup.
In SPLZ, APSP is stored in the form of shortest path tree(SPT). Here SPT is an array
of length (cid:2)V (cid:2), which records the last move of the shortest path from a source vertex to
all other vertices. For example, if we want to calculate the shortest path from vertex v
to other vertices, we will present the result by SP T (v), which is a tree with root v. If
SP T (v)[u] = w, it means that the predecessor of vertex u along the shortest path from v
to u is vertex w. Edge (w, u) is the last move from v to u, so SPT is also called last move
table. Traditional algorithm for SSSP problem like Dijkstra usually present the result in the
form of SPT too. If we store APSP straightforwardly, the space complexity is O((cid:2)V (cid:2)2), for
we need to store (cid:2)V (cid:2) SPTs.

Path coherence described in [31] reveals that “vertices contained in a coherent region
share the first segment of their shortest path from a fixed vertex”. For last move, path coher-
ence still makes sense with a bit change on description: vertices contained in a coherent
region share the last segment of their shortest path to a fixed vertex. Strictly, path coher-
ence only holds when the fixed vertex is sufficiently far away. In large-scale road network,
there always are numerous vertices which are sufficiently far away from each other, so path
coherence holds in most cases. A related experiment is introduced in Section 4.1 to show
how frequently it holds. Path coherence implies that the data among multiple SPTs contain
a large number of reduplicative sequences. This feature will lead to a high compression ratio
with LZ-family algorithm.

SILC [31], PCPD [32], and CPD [10] adopt first move table. First move table has the
similar feature like last move table, but it is suitable to solve point-to-point shortest path
problem. When querying the shortest path from vertex s to vertex t, first move table can
iteratively give the next vertex of the shortest path beginning from s. Our method is designed
for SSSP problem, so the result is a tree, not a single path. A vertex in a tree may have several
successors, thus the first move table, which is an array and can just store one successor of
a vertex on a shortest path, cannot fit our requirement. If we persist in using first move
table to record the tree, it need to be implement in the form of a data structure about tree,
which is more expensive than an array. We use last move table. A vertex in a tree only have
one predecessor, so last move table can be stored in the form of an array. That is also why
many classical algorithms for SSSP problem, like Dijkstra, use last move table to record the
shortest path tree. So SPLZ adopts last move table as the form of storage of result.

Generally, SPLZ contains 3 parts: (1)calculating the APSP, (2)compressing the APSP
offline, (3)decompressing the APSP online. We adopt PHAST algorithm to calculating the
APSP. Other existing methods, like Dijkstra, Bellman-Ford-Moore, are also feasible. Next
we focus on the compressing and decompressing method of SPLZ, which is a variant of
LZ77.

3.2 LZ77 algorithm

Let data be a string to be compressed, and i bytes has been compressed. DI CT SI ZE
is the size of dictionary. The compression procedure of LZ77 is as Algorithm 1. LZ77
keeps the dictionary, Dict, sliding with i increases. Longest match looks for the longest
common subsequence among dic + uncompressed data (begins from dict and may extend
into uncompressed data) and the prefix of uncompressed data, and then returns location

Geoinformatica

and length of the common subsequence. Finally the compressed data is an array of
(location, length) pairs.

Algorithm 1 The compression procedure of LZ77

len(data)) do
dict=data[i-DICT SIZE..i];
(location, length)=longest match(dict+data[i..end], prefix of data[i..end]);
output (location, length);
i=i+length;

1: while (i
2:
3:
4:
5:
6: end while

Let compressed data be an array of (location, length) pairs to be decompressed, and
assume i bytes has been decompressed. The decompression procedure of LZ77 is as Algo-
rithm 2. Decompression is much simpler than compression. It successively loads every
(location, length) in compressed data, and then looks up and outputs the corresponding
subsequence in dict. dict still needs to slide with i.

Algorithm 2 The decompression procedure of LZ77

1: for all (location, length) in compressed data do
2:
3:
4:
5: end for

dict=decompressed data[i-DICT SIZE..i];
output dict[location..location+length];
i=i+length;

LZ77 is a representation of regular compression methods without any domain knowl-
edge. Decompression speed of LZ family algorithm is fast, but decompressing some
particular data from compressed data is dependent on previous data because of the sliding
of dictionary. Other common LZ-family methods, like what are used in gzip, zlib and 7z, do
not have any superiority on retrieval speed compared with LZ77, because they need to not
only look up the dictionary to convert (location, length) to original data and slice the dic-
tionary, but also calculate some complex coding. Assuming that there are compressed data
contains n SPTs, from which we hope to decompress a particular SPT, we need decompress
n
2 SPTs on average. This process results in a large amount of redundant operations.

3.3 Fixed-dictionary compression

To avoid redundant operations, we fix the dictionary. This means the dictionary is a fixed-
size and fixed-location sequence in the front of the raw data. Data in the dictionary will
not be compressed, to achieve a faster decompressing speed. These modification result in
a lower compression ratio. We should point out that, SPLZ is not a compression algorithm
for general situation, but an algorithm for solving SSSP problem.

Let data be a set of SPT, and data[0], the first SPT in data, be the dictionary. I ndex[i]
is the starting position of i-th SPT in compressed data. In other words, index[i + 1] −
index[i] is equal to the compressed length of i-th SPT. Let len(compressed data) be the
total size of compressed data. Algorithm 3 describes the process of compressing. Firstly
SPLZ loads the first SPT in data as the dictionary and outputs it without compression.
Then SPLZ compresses remaining SPTs successively. For each SPT, SPLZ repeatedly finds
the longest common subsequence among dict and the prefix of this SPT, and compresses,
outputs and deletes the matched prefix, until this SPT is empty. There are many methods

Geoinformatica

for finding the longest match [6], we use a simple implement with a modification of KMP
[24]. Algorithm 4 is the process of decompressing the m-th SPT. dict is not compressed so
it can be loaded straightly. Next SPLZ locates the compressed m-th SPT in the compressed
data stream using index[m] and index[m + 1]. For each (location, length) pair, we can
look up the corresponding subsequence in dict to output the original data. The first line of
Algorithm 3 and Algorithm 4 is just an assignment to a pointer, without copying any real
data. In decompression process, the whole size of outputed data in line 4 is (cid:2)V (cid:2).

Algorithm 3 The compression procedure of SPLZ

1: dict=data[0];
2: output dict;
3: for all spt in data-data[0] do
4:
5:
6:
7:
8:
9:
10: end for

end while

index[number of spt in data]=len(compressed data);
while (spt!=NULL) do

(locationlength)=longest match(subsequence of dict, prefix of spt);
output (location, length);
spt=spt-matched prefix of spt;

Algorithm 4 The decompression procedure of SPLZ

1: dict=compressed data[0..sizeof(SPT)];
2: for i=index[m] to index[m+1] do
3:
4:
5: end for

(location, length)=compressed data[i];
output dict[location..location+length];

3.4 An example of SPLZ compression and decompression

Assume that there is a graph G as Fig. 1 shows (Table 1 is the adjacency list of G).
After calculating shortest path tree for each vertex, we get six SPTs shown in Table 2. If
SP T (Vi)[j ] = k, it means the precursory vertex of Vj on the shortest path from Vi to Vj is
Vk.

To obtain an effective compression, we convert Table 2 to another form. In Table 3,
SP T (Vi)[j ] = k means the precursory vertex of Vj on the shortest path from Vi to Vj is
the k-th adjacent vertex of Vj . We define SP T (Vi)[i] = 0.

Then, for example, we select SP T (V2) as the dictionary. Every SPT is compressed into
an array of 2-tuple (location, length). Note that sometimes a number in a SPT might not
exist in the dictionary. For example, in Table 3, SP T (V1)[2]=1, but “1” does not exist in
the dictionary. At this situation, we set length=0 and location=the number excluded in
dictionary.

To make a simple illustration, here we assume that each 2-tuple needs two bytes.

Therefore after compressing, the index array is: (0, 2, 8, 14, 20, 28, 38).

The total length of output is 44 bytes, for the length of dictionary is 6 bytes and the length
of compressed date is 38 bytes. The effectiveness of compression seems poor, because the
scale of graph in our example is too small.

When solving a SSSP problem online, for example, calculating the SP T (V1), the steps

are:

Geoinformatica

Fig. 1 Graph G

1. Find out that index[1] = 2 and index[2] − index[1] = 6. In other words, the length
of compressed SP T (V1) is 6 and its start location in whole compressed data is 2.
2. Convert every 2-tuple to original data by looking up the dictionary. For example, when
handling the 2-tuple (0, 2), we intercept the subsequence of dictionary, which begins at
0 and is of length 2.
All the conversion is: (0, 2) → 0 0; (1, 0) → 1; (0, 3) → 0 0 0.
This step can be easily parallelized.

3. Concatenate these subsequences to one array: (0 0 1 0 0 0). This array is SP T (V1).

4 Details of implementation

4.1 The key factor affecting the compression ratio

When we use fixed-dictionary compression, the compression ratio is mainly decided by
the similarity between the dictionary and data to be compressed. Let path-len(u, v) be the
number of edges along the shortest path between vertex u and vertex v. Similarity between
two SPTs has a negative correlation with path-len(u, v) between the source vertex of
the two SPTs. We use the proportion of common edges among two SPTs to measure the
similarity between two SPTs.

Figure 2 presents an experiment result, which is based on a northwest USA road net-
work with about 1.2 million vertices. . The less path-len(u, v) of two vertex u and v, the

Table 1 The adjacency list of G

Vertex

Adjacent vertices of Vi

V0
V1
V2
V3
V4
V5

V2
V2
V0
V2
V2
V3

V1
V5
V5
V4

V4

V5

Table 2 SPT of each vertex

SPT

Elements of SP T (Vi )

Geoinformatica

SP T (V0)
SP T (V1)
SP T (V2)
SP T (V3)
SP T (V4)
SP T (V5)

–

2

2

2

2

2

2

–

2

2

2

2

0

1

–

3

4

4

2

2

2

–

2

5

2

2

2

2

–

5

3

3

3

3

4

–

higer similarity between SP T (u) and SP T (v). This result is reasonable in real world. For
example, assume there are three locations A, B, and C. Both the distance of (A, C) and (B,
C) is 10 km. If the distance between A and B is one meter, we could guess that the short-
est path (A, C) and (B, C) are almost the same. When we choose SP T (u) as dictionary
and compress SP T (v), the impact of path-len(u, v) on the compression ratio is as Fig. 3
shows.

In addition, Fig. 2 also shows how frequently path coherence holds. When path-
len(u, v) is less than 100 (vertices contained in a coherent region), SP T (u) and SP T (v)
share over 93 % of their elements. An element in SP T (u) records the last move of the path
from u to a vertex. This experiment validates that path coherence holds in most cases on a
large-scale road network.

The result points out that the compression ratio decreases fast with increasing path-
len(u, v). To reduce the space consumption, it is necessary to limit the path-len(u, v)
between the dictionary and the SPT to be compressed.

4.2 Regions partition

If we choose only one SPT as the dictionary in a large scale graph, there always are many
vertices far away from the dictionary. By partitioning the graph into a series of regions with
smaller size, we can choose a SPT as dictionary and compress the rest SPT independently
for every region. We choose the SPT of a vertex which is closest to the geometrical center as
the dictionary, and call this vertex the root of this region. Partition ensures that in a region,
the path-len between vertex of dictionary and other vertices don’t exceed the diameter of
the region.

When partitioning the graph into numerous disjoint regions, we should keep the distance
of vertices among a region as close as possible. It can be handled as a clustering problem. We
use k-means, a simple but effective clustering method, to partition the graph. The simplest
attribute for clustering vertices in a road network is coordinate, and we adopt it. Actually,

Table 3 SPT of each vertex after
converting and compressing

SPT

Elements of SP T (Vi )

After compressing

SP T (V0)
SP T (V1)
SP T (V2)
SP T (V3)
SP T (V4)
SP T (V5)

0

0

0

0

0

0

0

0

0

0

0

0

0

1

0

2

3

2

0

0

0

0

0

1

0

0

0

0

0

1

0

0

0

0

1

0

(0,6)

(0,2) (1,0) (0,3)

dictionary

(0,2) (2,0) (0,3)

(0,2) (3,0) (0,2) (1,0)

(0,2) (2,0) (1,0) (1,0) (0,1)

Geoinformatica

Fig. 2 Relation between similarity and path-len

there are numerous methods to partition the graph without coordinate. Coordinate is an
unessential condition for SPLZ.

Due to that data in the dictionary will not be compressed and output in raw form, if the
number of regions is excessive, uncompressed data would occupy a large proportion in final
output. If the number of regions is small, we cannot ensure that the diameter of a region is
significantly less than the diameter of the total graph. Intuitively, to reach both small number
of regions and small size of every region, we assume that optimal number of regions is in
form of C ∗
(cid:2)V (cid:2), and choose a proper value of C by experiment. Section 5.2 compares
the impact of different parameter C.

√

4.3 Multi-step compression

Assume that u is a root of a region, and vertices of SPT(u) among its region is as Fig. 4
shows. In region showed in Fig. 4, all SPT except SP T (u) is compressed with dictionary
SP T (u). We call this process one-step compression. Path-len between most SPT in this
region and SP T (u) is 3 or 4.

We can reduce the path-len by multi-step compression. For example, let the grandparent
of each SPT be its dictionary. As for vertex u4, SP T (u4) will be compressed with dictionary
SP T (u2) and SP T (u2) will be compressed with dictionary SP T (u). When we decompress

Fig. 3 Relation between compression ratio and path-len

Geoinformatica

Fig. 4 Part of SPT(u) within a region

SP T (u4), we must decompress SP T (u2) at first. It is so-called two-step compression for
vertex u4. By applying similar operation to all vertex, their path-len is decreased to 1 or 2.
We call the path-len between a SPT and its dictionary len-to-dic. Figure 3 tell us
that shorter path-len leads to a higher compression ratio. But the reduction of space costs
brings higher time cost. If len-to-dic is d, the time to decompress SP T (u4) is (cid:6)path-
len(u, u4)/d(cid:7) times of one-step compression. By controlling len-to-dic, we can adjust
the point of balance between space costs of compression and time costs of decompression
online.

We define len-to-dic = ∞ when we use one-step compression.

4.4 Code of compressed data

The compressed data are array of (location, length). If location and length are fixed-
length integer, data compressed by method in Section 3.3 can be compressed one more time
by entropy coding. However, entropy coding has poor decompression speed. We adopt a
variable length coding to encode location and length. Though our coding method cannot
obtain the compression ratio as high as entropy coding, it has almost no negative effect on
decompression speed. The code is also prefix coding, so there is no ambiguity when we
decode it.

In the data stream of compressed data, location is presented by differential coding. We
just record the difference between every location and its predecessor except the first one,
because difference between two adjacent location usually smaller than their real value.
Differential coding might result in a shorter code. Value of length does not have such a
feature, so we record its real value.

Encoding method for length and location in detail is separately in Tables 4 and 5. In
the first line of Table 4, “uncompressed” means that some bytes does not appear in the
dictionary, so these bytes cannot be compressed. In our method, the value of such a byte
must be no more than 15. It is reasonable for a real-world road network, for number of

Geoinformatica

Table 4 Code for length

Range of value (in hex)

Code length

Code format(in binary)

[0x00, 0x0F] (uncompressed)

[0x00, 0x7F]

[0x0080, 0x3FFF]

[0x004000, 0x1FFFFF]

[0x00200000, 0x0FFFFFFF]

1

1

2

3

4

1111xxxx

0xxxxxxx

10xxxxxx xxxxxxxx

110xxxxx xxxxxxxx xxxxxxxx

1110xxxx xxxxxxxx xxxxxxxx xxxxxxxx

branch of real-world road usually smaller than 15. In the case of that degree of a vertex u is
more than 15, we can add a virtual vertex u(cid:9) to the graph. Let the distance between u and u(cid:9)
be zero and assign excess edges of u to u(cid:9).

5 Experiment

5.1 Experiment setup

Our experiment code is written in C++, and compiled by VC++ 2010. The program includes
two parts: precomputing offline and calculating the SSSP online. The experiments run on
a PC, with 3.4 GHz Intel i7-4770(4 cores), 24GB RAM. External memories include a 2TB
mechanical disk and a 256GB SSD. For parallelly precomputing, we use OpenMP. Data of
graph are downloaded from http://www.dis.uniroma1.it/∼challenge9, which are benchmarks
for the 9th DIMACS Implementation Challenge [16]. The data set we used is “Northwest
USA”(NW), with 1207945 vertices and 2840208 edges, and the type of graph is “Distance
graph”.

The source code of our experiments is released.1

5.2 Precomputing

Operation of precomputing consist of computing the APSP and compressing it. The target
of compressing is to reduce the space consumption of APSP. So the compression ratio is a
important feature for measuring the effectiveness of precomputing. The number of regions
has an impact on compression ratio. We choose different setting of parameter C for C ∗
√
(cid:2)V (cid:2) as the number of regions separately. For every parameter setting, the running time
of precomputing is about 13 hours. Table 6 shows the effect of number of regions on the
compression ratio.

The number of regions determine the average size of each region, and the size of a region
has effect on the path-len between vertices in a region. The less the path-len between ver-
tices, the higher the compression ratio. So it seems that more number of regions may lead to
higher compression ratio. However, Table 6 demonstrates that when the number of regions
is more than a certain value, the compression ratio falls down. It is owing to that, with the
number of regions increases, the proportion of dictionary increases. We select a vertex as
the representative vertex for each region. The SPT of the representative vertex is the dic-
tionary of that region. To ensure that the dictionary is available at the immediate time of

1https://github.com/asds25810/SPLZ

Range of value (in hex)

Code length

Code format(in binary)

Table 5 Code for location

[0x00, 0x3F]

[-0x00, -0x3F]

[0x0040, 0x1FFF]

[-0x0040, -0x1FFF]

[0x002000, 0x0FFFFF]

[-0x002000, -0x0FFFFF]

[0x00100000, 0x07FFFFFF]

[-0x00100000, -0x07FFFFFF]

1

1

2

2

3

3

4

4

Geoinformatica

00xxxxxx

01xxxxxx

100xxxxx xxxxxxxx

101xxxxx xxxxxxxx

1100xxxx xxxxxxxx xxxxxxxx

1101xxxx xxxxxxxx xxxxxxxx

11100xxx xxxxxxxx xxxxxxxx xxxxxxxx

11101xxx xxxxxxxx xxxxxxxx xxxxxxxx

decompressing, dictionary will not be compressed. Although more number of regions leads
to a higher compression ratio of single SPT, the total size of final data will increase because
the increased size of dictionaries.

If the dictionaries occupy a high proportion of the final output, we can consider to com-
press the dictionaries. But it will result in two problems. One is the extra time cost for
decompressing dictionary when decompressing data. Another one is that, actually, it is dif-
ficult to find a proper “dictionary” for compressing dictionaries, which intrinsically have
less data-redundancy. In other words, the compression ratio of compressing the dictionaries
is much lower than compressing SPTs.

√

We set 1 ∗

(cid:2)V (cid:2) as the number of regions for following experiment.

5.3 Multi-step compression

Table 6 shows the results of one-step compressing with different number of regions. Multi-
step compressing can achieve a higher compression ratio as shown in Fig. 5. SPLZ can
adjust the compression ratio by controlling the parameter len-to-dic, which was described
in Section 4.3. This capability of SPLZ makes it adaptable to different capacity of memory.
With the len-to-dic decreases, the compression ratio increases. It is due to the similarity
between the SPT to be compressed and its dictionary is higher when the parameter len-
to-dic decreasing. SPLZ achieves the highest compression ratio when len-to-dic=1. The
APSP of size about 1459 GB can be compressed to 2.87 GB. It is affordable for an ordinary
PC.

Compressed

Compression

Dictionary size

Proportion of

size[GB]

ratio

Table 6 Effect of number of regions on compression ratio

C

0.5

1

2

4

8

Raw size

[GB]

1459

1459

1459

1459

1459

16.65

13.29

11.75

12.21

15.87

88

110

124

119

92

[GB]

0.66

1.33

2.65

5.31

10.62

dictionary

4.0 %

10.0 %

22.5 %

43.5 %

66.9 %

Geoinformatica

Fig. 5 Effect of Len-to-dic on compression ratio

5.4 Online performance

After precomputing, we test the time costs of solving the SSSP problem for a particular
vertex. We load the compressed APSP in memory, and randomly generate a series of queries.
Each query input a vertex v, and request SP T (v) as output. Table 7 shows the average costs
of handling a query. The parameter len-to-dic evidently affects the performance. Higher
len-to-dic makes a lower space consumption. However, the reduced space consumption
is repayed by increasing time costs. Len-to-dic should be decided by the bottleneck of
different applications.

If the memory capacity is not enough to store the compressed APSP, we can consider
to store it to external memory. When SPLZ handles a query, it looks up the compressed
SPT from external memory, and then decompresses it and returns the result. Compared

Table 7 Time and space cost online

Methods

Len-to-dic

Time (μs)

Space(RAM)

Space(external)

SPLZ on memory

SPLZ on SSD

∞

16

∞

16

8

4

2

1

8

4

2

1

-

219

323

464

733

1265

2287

740

1176

1709

2556

4468

8002

7085

[GB]

13.29

9.37

6.32

4.44

3.41

2.87

1.33

1.33

1.33

1.33

1.33

1.33

1.33

-

[GB]

-

-

-

-

-

-

-

11.96

8.04

4.99

3.11

2.08

1.54

11.96

SPLZ on mechanical disk

∞

Dijkstra

217291

Table 8 Time costs of solving a
SSSP problem by SPLZ and
copying an array of length (cid:2)V (cid:2)

Methods

SPLZ

memcpy

for-loop assignment

Geoinformatica

Time costs(μs)

219

84

448

with RAM, external memory is cheaper and usually has higher capacity. The time costs
on external memory in Table 7 is the stable performance after handling a large number of
random queries. The latency of accessing mechanical disk is significantly higher than that of
memory. When len-to-dic is not ∞, the results on mechanical disk have obvious difference
in each experiment execution, and we do not find any regular pattern. So we only record
the result when len-to-dic is ∞. SSD performs better than mechanical disk. The time cost
on SSD is three to four times as much as that on memory. Note that the space costs only
consider the compressed APSP. Other consumptions are much less than them.

If the parent pointers need to be converted to the global id of each vertices in results,
another 887 μs is needed. This conversion is not always needed in our opinion. To represent
a path tree in parent pointer form, the local id (represented like Table 3) may be enough if
graph is stored in the form of adjacency list.

Although SPLZ needs about 13 GB space costs if len-to-dic = ∞, storing the com-
pressed APSP into disk or selecting a lower len-to-dic would make the space consumption
more practicable at the expense of lower quering.

The average time costs of Dijkstra algorithm based on binary heap is about 217 ms on
our experiment graph. In our experiment, the performance of SPLZ is almost three orders
of magnitude faster than Dijkstra based on binary heap, if let len-to-dic = ∞. We also
implement PHAST, which needs 27.4 ms in our experiment. These comparisons may be
influented by the details of how a people implements these algorithm. To fairly shows the
online performance of SPLZ, we try to find a lower bound of running time of SSSP problem,
and compare SPLZ with this lower bound.

5.5 Lower bound of SSSP problem

The time costs of solving SSSP problem has a natural lower bound. Whatever methods we
use to calculate the shortest path from a vertex to all vertices in a graph, we must fill the
result to an array of length (cid:2)V (cid:2) as output. So the natural lower bound is the time costs of
copying an array of length (cid:2)V (cid:2). We compares the time costs of SPLZ and array copying
in Table 8. We test two copying methods: memcpy and for-loop assignment. Memcpy()
is a standard function in C library, which is fully optimized. Considering many algorithm
successively output their result in a loop, we also test copying an array by for-loop(assigning
the elements one by one). The results in Table 8 shows that the performance of SPLZ is
close to the lower bound.

Table 9 Information about graph data

Name

FLA

COL

BAY

Description

Florida

Colorado

San Francisco Bay Area

Number of vertices

Number of edges

1,070,376

435,666

321,270

2,712,798

1,057,066

800,172

Geoinformatica

Table 10 The compression ratio and time cost

Name

FLA

COL

BAY

Raw size

[GB]

1146

190

103

[GB]

9.97

2.72

1.42

Compressed size

Compression ratio

Preprocess time
[h : m]

10:56

2:15

1:07

115

70

73

5.6 Experiments on other road networks

Up to now, we test SPLZ on only one graph, the Northwest USA road network. Here we
show the results of experiments on some other graph data. Table 9 introduces the related
information about these data. Like North west USA data, these data are also downloaded
from http://www.dis.uniroma1.it/∼challenge9. These experiments are used to show the per-
formance of SPLZ on different data. Here we set C = 1 and len-to-dic = ∞. Other
hardware and software configurations are the same to Section 5.1.

Table 10 shows the results of preprocess of SPLZ on these datasets. The scale of FLA
dataset is close to Northwest, so the performance of SPLZ is similar to what we have shown
above. SPLZ achieves less compression ratio on smaller graph, because path coherence
takes effects when vertices in a coherent region are sufficiently far away from a vertex,
while the average distance between vertices in smaller graph is closer. Table 11 is the
decompression performance on these datasets. The lower bound cost is also tested. Table 11
validates that SPLZ also works well on small-scale road networks.

Table 11 Time and space cost online on other datasets

Graph

Methods

Time (μs)

Space(RAM)

Space(external)

FLA

SPLZ on memory

COL

SPLZ on memory

SPLZ on mechanical disk

SPLZ on SSD

memcpy

for-loop assignment

SPLZ on mechanical disk

SPLZ on SSD

memcpy

for-loop assignment

SPLZ on mechanical disk

SPLZ on SSD

memcpy

for-loop assignment

BAY

SPLZ on memory

172

6920

657

76

459

98

6120

449

29

192

71

5767

371

21

140

[GB]

9.97

1.11

1.11

-

-

-

-

-

-

2.72

0.29

0.29

1.42

0.18

0.18

[GB]

8.86

8.86

-

-

-

-

-

-

-

-

-

2.43

2.43

1.24

1.24

Geoinformatica

6 Conclusion

In this paper, we presented SPLZ, an algorithm for solving single source shortest path prob-
lem on road network. SPLZ is about three orders of magnitude faster than Dijkstra based on
binary heap. Compared with the time costs of array copying, which is a natural lower bound
of SSSP problem, SPLZ shows a significantly high performance online. Even though SPLZ
consumes much memory yet, this problem can be solved by storing compressed data into
external memory or by adjusing the parameter len-to-dic.

Future research will focus on developing a more efficient preprocess method. In our
experiments, SPLZ can solve SSSP prolem on a road network with about 1.2 millions ver-
tices, and it is enough for many applications. But we should admit that, SPLZ still cannot
deal with a more large-scale road network, because of the huge time costs for precomput-
ing. We can make efforts to two points. One is to adopt an algorithm faster than sequential
PHAST to calculate APSP, and the other is to use a more efficient methods to find the
longest match while compressing the APSP.

Acknowledgments We would like to thank the reviewers for their valuable suggestions, and Shiyan Zhan
for the fruitful discussions. This work is supported by Natural Science Foundation of China (No. 61033009
and No. 61303047) and Anhui Provincial Natural Science Foundation (No. 1208085QF106).

References

1. Abraham I, Delling D, Goldberg AV, Werneck RF (2011) A hub-based labeling algorithm for shortest

paths in road networks. In: Experimental Algorithms, Springer, pp 230–241

2. Abraham I, Delling D, Fiat A, Goldberg AV, Werneck RF (2012) Hldb: Location-based services in
databases. In: Proceedings of the 20th International Conference on Advances in Geographic Information
Systems, ACM, New York, NY, USA, SIGSPATIAL’12, pp 339–348. doi:10.1145/2424321.2424365
3. Ahuja RK, Mehlhorn K, Orlin J, Tarjan RE (1990) Faster algorithms for the shortest path problem. J

4. Arz J, Luxen D, Sanders P (2013) Transit node routing reconsidered. In: Experimental Algorithms,

5. Bast H, Funke S, Sanders P, Schultes D (2007) Fast routing in road networks with transit nodes. Science

6. Bell T, Kulp D (1993) Longest-match string searching for ziv-lempel compression. Software: Practice

7. Bellman R (1956) On a routing problem. Tech rep, DTIC Document
8. Bertsekas DP (1993) A simple and fast label correcting algorithm for shortest paths. Networks

ACM (JACM) 37(2):213–223

Springer, pp 55–66

316(5824):566–566

and Experience 23(7):757–771

23(8):703–709

9. Bertsekas DP, Guerriero F, Musmanno R (1996) Parallel asynchronous label-correcting methods for

shortest paths. J Optim Theory Appl 88(2):297–320

10. Botea A, Baier JA, Harabor D, Hern´andez C (2013) Moving target search with compressed path

11. Cherkassky BV, Goldberg AV, Radzik T (1996) Shortest paths algorithms: Theory and experimental

databases. Proceedings of ICAPS-13

evaluation. Math Program 73(2):129–174

12. Cherkassky BV, Georgiadis L, Goldberg AV, Tarjan RE, Werneck RF (2009) Shortest-path feasibility

algorithms: An experimental evaluation. J Exp Algorithmics (JEA) 14:7

13. Cormen TH, Leiserson CE, Rivest RL, Stein C et al. (2001) Introduction to algorithms, vol 2. MIT press,

Cambridge

14. Delling D, Goldberg AV, Nowatzyk A, Werneck RF (2013a) Phast: Hardware-accelerated shortest path

trees. J Parallel Distrib Comput 73(7):940–952

15. Delling D, Goldberg AV, Pajor T, Werneck RF (2013b) Customizable route planning in road networks.

In: Sixth Annual Symposium on Combinatorial Search

Geoinformatica

16. Demetrescu C, Goldberg AV, Johnson DS (2009) The Shortest Path Problem: Ninth DIMACS Imple-

mentation Challenge, vol 74. American Mathematical Soc

17. Dial RB (1969) Algorithm 360: Shortest-path forest with topological ordering [h]. Commun ACM

18. Dijkstra EW (1959) A note on two problems in connexion with graphs. Numerische mathematik

19. Gallo G, Pallottino S (1986) Shortest path methods: A unifying approach. Netflow at Pisa, pp 38–64
20. Geisberger R, Sanders P, Schultes D, Delling D (2008) Contraction hierarchies: Faster and simpler

hierarchical routing in road networks. In: Experimental Algorithms, Springer, pp 319–333

21. Glover F, Klingman D, Phillips N (1985) A new polynomially bounded shortest path algorithm. Oper

12(11):632–633

1(1):269–271

Res 33(1):65–73

22. Goldberg AV, Harrelson C (2005) Computing the shortest path: A search meets graph theory. In: Pro-
ceedings of the sixteenth annual ACM-SIAM symposium on Discrete algorithms, Society for Industrial
and Applied Mathematics, pp 156–165

23. Hilger M, K¨ohler E, M¨ohring RH, Schilling H (2009) Fast point-to-point shortest path computations

with arc-flags. The Shortest Path Problem: Ninth DIMACS Implementation Challenge 74:41–72

24. Knuth DE, Morris JH Jr, Pratt VR (1977) Fast pattern matching in strings. SIAM J Comput 6(2):323–350
25. Madduri K, Bader DA, Berry JW, Crobak JR (2006) Parallel shortest path algorithms for solving large-

26. Maue J, Sanders P, Matijevic D (2010) Goal-directed shortest-path queries using precomputed cluster

distances. J Exp Algorithmics 14:2:3.2–2:3.27. doi:10.1145/1498698.1564502

27. Meyer U, Sanders P (2003) δ-stepping: a parallelizable shortest path algorithm. J Algorithms 49(1):114–

28. Pallottino S (1984) Shortest-path methods: Complexity, interrelations and new propositions. Networks

29. Pape U (1974) Implementation and efficiency of moore-algorithms for the shortest route problem. Math

Program 7(1):212–222

Esa 2005, Springer, pp 568–579

30. Sanders P, Schultes D (2005) Highway hierarchies hasten exact shortest path queries. In: Algorithms–

31. Sankaranarayanan J, Alborzi H, Samet H (2005) Efficient query processing on spatial networks. In:
Proceedings of the 13th annual ACM international workshop on Geographic information systems, ACM,
pp 200–209

32. Sankaranarayanan J, Samet H, Alborzi H (2009) Path oracles for spatial networks. Proc VLDB

33. Ziv J, Lempel A (1977) A universal algorithm for sequential data compression. IEEE Trans Inf Theory

Endowment 2(1):1210–1221

23(3):337–343

scale instances

152

14(2):257–267

Jingwei Sun is a graduate student in School of Computer Science and Technology, University of Science
and Technology of China(USTC). His research interest is combinatorial algorithms.

Geoinformatica

Dr. Guangzhong Sun is an associate professor in School of Computer Science and Technology, University
of Science and Technology of China (USTC). He is a member of National High Performance Computing
Centre (Hefei). He got his Ph.D. in computer science at USTC in 2005. He worked as a visiting researcher in
Microsoft Research Asia from October 2007 to August 2008 and from September 2010 to February 2011. He
has published more than 40 papers, including papers in reputed journals and major international conferences.
He is a member of ACM and IEEE. His research interests include pervasive computing, data processing,
parallel computing, and combinatorial algorithms.

