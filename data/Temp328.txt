This article was downloaded by: [University of North Dakota]
On: 11 September 2014, At: 22:03
Publisher: Taylor & Francis
Informa Ltd Registered in England and Wales Registered Number: 1072954 Registered
office: Mortimer House, 37-41 Mortimer Street, London W1T 3JH, UK

International Journal of Geographical
Information Science
Publication details, including instructions for authors and
subscription information:
http://www.tandfonline.com/loi/tgis20

Spatial data quality and beyond
Deren Li a , Jingxiong Zhang b & Huayi Wu a
a State Key Laboratory of Information Engineering in Surveying,
Mapping and Remote Sensing , Wuhan University , Wuhan , China
b School of Remote Sensing Information Engineering , Wuhan
University , Wuhan , China
Published online: 25 Sep 2012.

To cite this article: Deren Li , Jingxiong Zhang & Huayi Wu (2012) Spatial data quality and
beyond, International Journal of Geographical Information Science, 26:12, 2277-2290, DOI:
10.1080/13658816.2012.719625

To link to this article:  http://dx.doi.org/10.1080/13658816.2012.719625

PLEASE SCROLL DOWN FOR ARTICLE

Taylor & Francis makes every effort to ensure the accuracy of all the information (the
“Content”) contained in the publications on our platform. However, Taylor & Francis,
our agents, and our licensors make no representations or warranties whatsoever as to
the accuracy, completeness, or suitability for any purpose of the Content. Any opinions
and views expressed in this publication are the opinions and views of the authors,
and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content
should not be relied upon and should be independently verified with primary sources
of information. Taylor and Francis shall not be liable for any losses, actions, claims,
proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or
howsoever caused arising directly or indirectly in connection with, in relation to or arising
out of the use of the Content.

This article may be used for research, teaching, and private study purposes. Any
substantial or systematic reproduction, redistribution, reselling, loan, sub-licensing,
systematic supply, or distribution in any form to anyone is expressly forbidden. Terms &
Conditions of access and use can be found at http://www.tandfonline.com/page/terms-
and-conditions

International Journal of Geographical Information Science
Vol. 26, No. 12, December 2012, 2277–2290

Spatial data quality and beyond

Deren Lia, Jingxiong Zhangb* and Huayi Wua

aState Key Laboratory of Information Engineering in Surveying, Mapping and Remote Sensing,
Wuhan University, Wuhan, China; bSchool of Remote Sensing Information Engineering, Wuhan
University, Wuhan, China

(Received 25 October 2011; ﬁnal version received 06 August 2012)

Issues of accuracy, uncertainty, and spatial data quality have been on the top of most
GIScience research agendas around the world from the late 1980s. Ever since then,
growing research efforts have been directed toward uncertainty characterization in spa-
tial information, analysis, and applications, aiming for better understanding of spatial
uncertainty and thus improved methods and techniques for assessing and managing data
quality. Impressive progress has been made in various issues concerning data quality.
In addition, growing research on extensions to the conventional norms of data qual-
ity, such as the quality aspects of geospatial information services, has been observed.
Chinese researchers have contributed to this great cause by keeping abreast with the
developments abroad and striving for their own innovative work. This paper reviews the
past research on data quality-related issues and provides a perspective on future devel-
opments. These will be seen not only in continued research on theoretical and technical
issues concerning data quality, but also in developments of tools for quality assessment
and decision-making under uncertainty through geospatial information processing and
applications.

Keywords: spatial data quality; accuracy; error modeling; uncertainty; geostatistics;
validation; geographic information services

Introduction

1.
There is an increasing need for geospatial information in a fast changing world, as demon-
strated by widespread developments and applications of geographic information systems
(GIS). The quality of geospatial information is, however, not guaranteed, as informa-
tion gathering and processing suffer from various man–machine limitations and are thus
error-prone (Goodchild 1978, Goodchild and Gopal 1989, Goodchild and Jeansoulin 1998,
Wilkinson 2005, Congalton and Green 2008). On the one hand, the increasing availability,
exchange, and use of spatial data lead to large quantity of multi-source and heteroge-
neous spatial data with complex lineages, whose quality may be undocumented, mixed,
or unknown. On the other hand, there are user communities who may be aware of spatial
data quality issues but may not have at their disposal techniques and tools for data quality
assurance. This amounts to increasing distance between data users and producers, making
the situation particularly acute for today’s geographic information world. Thus, research
and developments need to be directed toward accuracy measures, error propagation, and

*Corresponding author. Email: jxzhang@whu.edu.cn

ISSN 1365-8816 print/ISSN 1362-3087 online
© 2012 Taylor & Francis
http://dx.doi.org/10.1080/13658816.2012.719625
http://www.tandfonline.com

Downloaded by [University of North Dakota] at 22:03 11 September 2014 2278

D. Li et al.

quality evaluation in geospatial information and analysis (Morrison 1988, Heuvelink 1998,
Zhang and Goodchild 2002, Goodchild 2008, Devillers et al. 2010).

There are several components of spatial data quality (Guptill and Morrison 1995). First,
lineage refers to the description of the source material from which the data were derived and
the methods of derivation and transformations involved in the production process. Second,
positional accuracy is the accuracy of coordinate values, for which a distinction is often
made between relative (within a single data set) and absolute (for multiple data sets) accu-
racy and between vertical and horizontal accuracy (vertical positional accuracy can also be
treated as attribute accuracy). Third, attribute accuracy is the accuracy of all attributes other
than the positional and temporal attributes of a data set. Fourth, temporal quality concerns
accuracy of time measurements, temporal validity, temporal consistency, last update, rate of
change, and temporal lapse. Other components of spatial data quality include logical con-
sistency, completeness, and semantic accuracy (see Guptill and Morrison 1995 for detail).
Clearly, description of errors in spatial data is important for spatial data quality. For this,
we need to understand the processes of geographic conceptualization and measurement.
First, consider the process of geographic conceptualization, which, as a fundamental theory
in GIS, concerns how the world should be conceptualized or how spatial data should be
modeled. There are basically two models for spatial data, discrete entities (or objects) and
ﬁelds, with the former perceiving the world as populated with discrete points, lines, and
areas, while in the latter the world is considered as a set of single-valued functions deﬁned
everywhere over space. In object models, an entity with an ID can be represented as a tuple
ID(x, Attri), where x and Attri stand for its position and attributes, respectively. As attribute
data are usually taken care of by specialists in speciﬁc application domains, our emphasis
is on positional data. Two kinds of variables are possible with ﬁelds: one is continuous
variables (measured at interval/ratio scales, such as elevation and snow depth) and the other
is categorical variables of nominal or ordinal nature (such as land cover and residential
areas of graded population density). For annotations, Z(x) stands for continuous ﬁelds,
while C(x) stands for categorical ﬁelds. We can organize our discussions about data quality
in objects and ﬁelds wherever necessary.

Second, the process of measurement from which data are derived involves various
errors, which may be of systematic, random, and gross types. Position-ﬁxing by land sur-
veying and GPS is crucial for positional data acquisition and geodetic control for detail
mapping. Errors in distance, angle, and time measurements contribute to inaccuracy in the
resultant coordinates of points surveyed. Photogrammetry has become a mainstream tech-
nology for topographic mapping based on reconstituted stereo-models built of image pairs
with sufﬁcient overlapping. Again, errors in image coordinate measurement, inadequacy
in photo control, and approximation in data reduction procedures result in errors in fea-
ture extracted and topographic data sampled from images. Remote sensing has become an
important technology for provision of geospatial information at local, regional, and global
scales, such as land cover and topographic information, which suffers from many sources
of uncertainty (Tucker et al. 2004). For instance, remotely sensed land cover area estimates
are subject to misclassiﬁcation errors, so is land cover change information, such as the total
area of change (Congalton and Green 2008).

Error implies the existence of the true value for a quality or quantity, at least in princi-
ple. However, in many occasions, this is an unattainable assumption, because it is often
impossible to determine the truth, although we may reach consensus about the likely
intervals wherein truth may fall into. Moreover, the so-called ground-truth data used for
validation of spatial data are often not perfect but degraded by error, as discussed in the
context of land cover change detection by Foody (2010). Thus, it is more sensible to use the

Downloaded by [University of North Dakota] at 22:03 11 September 2014 International Journal of Geographical Information Science

2279

term uncertainty than error, as the former implies relativity regarding the truth and will be
more suitable for conveying the meaning of vagueness (Burrough and Frank 1996, Zhang
and Goodchild 2002).

Description of spatial data quality is necessary for providing the potential user of a data
set with the necessary information to decide on the ﬁtness for use of a data set for his or her
particular application. The ﬁtness for use of a spatial data set can be assessed through risk
analysis given its quality, after initial checking of data content and accessibility (Agumya
and Hunter 2002). This can be taken care of by different approaches. The most rigorous
approach is to apply error propagation analysis to quantify how errors in input data sets
propagate into the outcomes of the analysis or decision-making processes (Florinsky 2002,
Heuvelink and Burrough 2002, Wechsler and Kroll 2006).

In this review paper, we will discuss the description, analysis, and management issues
with respect to errors and uncertainty in position, continuous ﬁelds, and categorical ﬁelds.
Accuracy metrics and assessment will be reviewed in terms of both global (non-spatial)
and local (spatial) perspectives. Approaches to error propagation will be differentiated into
analytical and simulation ones, with the former operating using the law of variance and
covariance propagation and the latter capitalizing on the ﬂexibility offered by geostatisti-
cally simulated realizations (Goovaerts 2001). This is followed by an overview of research
efforts and new developments in China on quality issues. We conclude by suggesting some
important topics for further research and development.

2. Description of spatial data quality: accuracy metrics and assessment

In general, accuracy metrics for spatial data include positional error descriptors for discrete
objects, error statistics for continuous ﬁelds, and classiﬁcation accuracy for categori-
cal ﬁelds. The accuracies of continuous ﬁelds are described with measures similar to
those used for positional data, for example, standard deviations and root mean squared
errors (RMSEs), while classiﬁcation accuracies of categorical ﬁelds are often described
by probabilistic measures, such as percent correctly classiﬁed (PCC) pixels and kappa
coefﬁcient of agreement. All these accuracy measures are summaries over the problem
domain and thus are called global, although classiﬁcation accuracies may be differenti-
ated by classes. Localized descriptors can capture the spatial variability in accuracies and
thus provide more information about the quality of the data sets concerned. These include
displacement vector ﬁelds for positional data (Zhang and Goodchild 2002), error surfaces
for interval/ratio ﬁelds (Carlisle 2005), and misclassiﬁcation probability (the inverse of
classiﬁcation accuracy) surfaces for categorical ﬁelds (Steele et al. 1998).

We will discuss errors for objects (position) and ﬁelds, with elevation and land cover
being taken frequently as examples for continuous and categorical ﬁelds, respectively.
Positional errors will be differentiated into those for points and lines (or polygon bound-
aries). Positional data may be speciﬁed by coordinates, addresses, or locality descriptions.
Thus, positional uncertainty should be discussed according to the type of positional
descriptions given. As will be shown, both probabilistic and fuzzy methods have been
used for uncertainty description in objects and ﬁelds.

Positional errors for points are described by error ellipses. We can obtain error ellipses
for position-ﬁxing after adjustment computation. In photogrammetric mapping and image
georeferencing (Tucker et al. 2004), similar error statistics may also be derived. Hughes
et al. (2006) investigated how the number and type (man-made vs. natural features) of
ground-control points (GCPs) and the order of the transformation polynomial affect the
spatial accuracy of a typical georectiﬁed aerial photograph.

Downloaded by [University of North Dakota] at 22:03 11 September 2014 2280

D. Li et al.

While positions are described by certain coordinate systems, there are often textual
descriptions on the locality information for specimens of geological and cultural objects.
Guo et al. (2008) proposed a method for georeferencing descriptive locality data, using an
uncertainty ﬁeld model to represent the distribution of a locality. There are also address-
based data for geographic information and analysis. The conversion of addresses into grid
coordinates is usually accomplished using TIGER (Topologically Integrated Geographic
Encoding and Referencing system)-type data sets and geocoding packages within a GIS
(Ratcliffe 2001, Zimmerman et al. 2007). The accuracy of geocoding relies on the
geocoding mechanism, as tested by Zimmerman et al. (2007) and Zandbergen (2011).

For lines, epsilon-error bands are often used as error descriptors. There has been
research on stochastic and probabilistic modeling of positional errors for lines. Positional
errors in map digitizing were analyzed using an autoregressive process consisting of trend
motion and random motion (Huang and Liu 1997). Leung and Yan (1998) proposed a posi-
tional error model for representing spatial features in vector form, which is a stochastic
model constructed to analyze point, line, and polygon errors within a uniﬁed framework.
Tveite and Langaas (1999) used buffers of increasing width around the lines to probabilis-
tically characterize positional accuracy of lines. The use of error ellipses and error bands
for positional uncertainty is limited, and these are replaced with models that provide full
probability distributions (Heuvelink et al. 2007, Zimmerman et al. 2007, De Bruin et al.
2008). Such models may be used to explore the propagation of positional uncertainty, as
shown in the next section.

Seo and O’Hara (2009) proposed a method to measure the correspondence between
line segments for assessing the geometric quality of spatial data. In their method, match-
ing is performed on rasterized line segments and their matching lengths and displacements
are measured. Experimental results showed that the line-based approach proposed is efﬁ-
cient to evaluate the positional quality of spatial data without requirements of topological
relationships among line features. Mozas and Ariza (2011) described a set of new met-
rics for evaluating the positional accuracy of lines in cartographic databases, which is
based on vertex displacements and their inﬂuence on adjacent segments. By using real
data sets of more than 180 km of roads and in comparison with the traditional point-based
methods for positional accuracy evaluation, their study conﬁrmed the viability of the pro-
posed vertex inﬂuence method for evaluating accuracy of positional cartographic products.
Kronenfeld (2011) proposed a polygonal modeling approach to represent boundary uncer-
tainty on area-class maps using a simple polygon tessellation with designated transition
zones, which can be conceptualized as duals of the epsilon bands. The transition zone-
based representations were found to be more ﬂexible than epsilon bands and allow for a
wide range of polygonal conﬁgurations, potentially useful for expert characterization of
areal units where gradation and/or boundary uncertainty are prevalent.

As mentioned earlier, there are also fuzzy approaches to uncertainty description in
spatial entities. Fonte and Lodwick (2004) deﬁned fuzzy geographic entities (FGEs) as
geographic entities with fuzzy spatial extent. The proposed fuzzy area operator gives more
information about the possible values of the area and enables the fuzziness in the spatial
extent of the entity to be propagated to the area. Dilo et al. (2007) proposed a system
of types and operators for handling vague spatial objects, with examples in coastal ero-
sion, forming a model for spatial data systems oriented to vague information processing.
Legleiter and Goodchild (2005) applied fuzzy logic for a more realistic representation
of in-stream habitat, which allows for continuous partial membership and gradual tran-
sitions among habitat types. Spatial patterns of classiﬁcation uncertainty can also be used
to identify areas of confusion, infer boundaries of variable width, and highlight areas of
increased habitat diversity.

Downloaded by [University of North Dakota] at 22:03 11 September 2014 International Journal of Geographical Information Science

2281

For errors in interval/ratio ﬁelds, such as terrain elevation, error quantiﬁcation is often
performed by testing a given data set against reference data. The Shuttle Radar Topography
Mission (SRTM) produced near-global 1(cid:2)(cid:2) and 3(cid:2)(cid:2) digital elevation models (DEMs), whose
quality was tested against the cartographically derived national elevation data set (NED)
by Guth (2006). The true resolution of 1(cid:2)(cid:2) SRTM DEMs was found to be no better than that
of 2(cid:2)(cid:2) SRTM DEMs. The quality of SRTM data was also investigated by Shortridge (2006),
with sub-meter accuracy elevation postings serving as ground-truth and land cover data
as covariates explaining spatial variation in accuracy. Error for the study site is substan-
tially less than the mission’s objective, but substantially more than that for the NED DEM
used in the test. There is signiﬁcant overestimation of actual elevations in the SRTM DEM,
especially in forested areas. Aguilar et al. (2007) proposed a non-parametric method for
accuracy evaluation. Tests using Monte Carlo simulation showed that about 64–128 check-
points were adequate for constructing an estimate of the global error of the DEM with 95%
conﬁdence.

Measures such as RMSEs summarize elevation errors in a DEM as a single value.
A more detailed description of DEM accuracy would allow better understanding of DEM
quality. Carlisle (2005) developed a new technique for creating a spatially distributed
model of DEM quality: an accuracy surface. The technique is based on the hypothesis
that the distribution and scale of elevation error within a DEM are at least partly related to
the morphometric characteristics of the terrain. Thus, regression models were developed to
deﬁne the relationship between DEM error and morphometric characters, generating stan-
dard deviation surfaces to represent DEM accuracy. However, accuracy surfaces in DEMs
do not provide spatial structural information themselves, which can be accommodated in
geostatistical error modeling (Kyriakidis et al. 1999, Holmes et al. 2000), as shown in the
next section.

The accuracy of a land cover classiﬁcation is the degree to which the map land cover
agrees with the reference land cover classiﬁcation (Stehman 2009). Error matrices are
useful tools for summarizing classiﬁcation accuracy, for which various measures are com-
monly used, such as PCC and kappa coefﬁcient of agreement (Congalton and Green 2008).
Accuracy assessment may be performed through map comparison, when a map layer can
be considered as the reference. But often, we can only obtain sampled reference data
for testing accuracy. Thus, sampling design should be considered carefully for accuracy
assessment (Mayaux et al. 2006, Stehman 2009).

Categorical map accuracy may vary over space, as shown by Kyriakidis and Dungan
(2001). Spatial variation in accuracy may be attributable to factors such as terrain, land-
scape complexity, and land use patterns. Steele et al. (1998) formulated a concept of
misclassiﬁcation probability and presented a method for estimating misclassiﬁcation prob-
abilities at training observation locations, which are then interpolated from the training
locations to a lattice of points via Kriging. They illustrated the method by Landsat TM
imagery, which was found to provide valuable information on the spatial distribution and
variation of map accuracy for both decision-makers and GIS analysts. van Oort et al. (2004)
developed logistic regression models to analyze variability in classiﬁcation accuracy, using
three additional explanatory variables in addition to land cover classes: heterogeneity in
the 3 × 3 window around a cell, the size of the patch, and the complexity of the landscape
in which a cell is located. It was found that per cell, the probability of correct classiﬁca-
tion was signiﬁcantly (α = 0.05) higher for cells with a less heterogeneous neighborhood,
for cells that are part of larger patches, and for cells in regions with a less heterogeneous
landscape. Spatial variability thus modeled led to a substantial improvement in the esti-
mation of the per-cell classiﬁcation accuracy. Foody (2005) tested a simple method for

Downloaded by [University of North Dakota] at 22:03 11 September 2014 2282

D. Li et al.

characterizing accuracy locally, which works by geographically constraining the data used
for accuracy assessment. Based on a crop classiﬁcation from Satellite pour l’Observation
de la Terre (SPOT) high-resolution visible (HRV) imagery, Foody (2005) estimated the
global accuracy of the classiﬁcation to be 84.0% but local accuracy to vary from 53.33%
to 100%. Moreover, per-class accuracy varied from 0% to 100% over the region, high-
lighting dangers in using a global measure of accuracy and hence the usefulness of local
accuracy assessment.

With the growing availability of remotely sensed images at increasingly high spatial
resolution, object-based methods for image analysis are developed, in which groups of
spatially adjacent pixels are classiﬁed as if they operated as a whole unit, raising issues
about how accuracy assessment should be conducted. Radoux et al. (2011) proposed an
object-based sampling strategy for accuracy assessment in object-based image classiﬁ-
cation, which, for a given conﬁdence interval, requires fewer sampling units than the
conventional methods, and is less likely to be inﬂuenced by positional errors, although
it often has a more complex response design.

Traditionally, the classes in area-class maps have been treated as crisp sets so that these
classes are assumed to be mutually exclusive and exhaustive. However, categorical maps
based on crisp sets suffer from limitations in the representations of continua of variation
found in most landscapes. Fuzzy sets allow more ﬂexibility for accuracy assessment and
area estimation, as demonstrated by Woodcock and Gopal (2000). Fuzzy kappa statistic
was also proposed, which accounts for the mean agreement in map comparison, which
depends not only on class histogram but also on spatial autocorrelation in class occurrences
(Hagen-Zanker 2009).

3. The ﬁtness for use: error propagation and error budget

As argued previously, error propagation analysis is the most scientiﬁc way to assess ﬁt-
ness for use. It can be done through variance and covariance propagation in an analytical
mode or through simulation. For the former, assume a linear function G = AZ, where A
is the matrix specifying the linear relationship between output vector G and input vector
Z. We can formulate error propagation as quantifying the variance and covariance in G,
CG, given variance and covariance matrix of Z, CZ, as CG = ACZAT, where superscript
T indicates a transpose. For the latter, error statistics concerning G is obtained by summa-
rizing realizations of G based on simulated Z ﬁelds through the geoprocessing operator A
in terms of mean and variance. This simulation-based approach offers advantages over the
analytical approach, as it can be easily implemented with respect to non-linear functions
(e.g. terrain aspect and curvature) and other complicated spatial analysis and modeling
(Van Niel et al. 2004), although stochastic simulation remains non-trivial. In this sec-
tion, we begin by reviewing analytical approaches to uncertainty propagation, followed by
simulation-based approaches. During the course, the full probability distribution approach
to positional uncertainty (Heuvelink et al. 2007) and geostatistical modeling of uncertainty
in ﬁeld variables (Zhang and Goodchild 2002) may be considered analogous if positional
errors are viewed as ﬁeld variables.

Variance and covariance propagation may be performed to assess the variance in area
estimates derived from polygonal data, given errors in positional data (Chrisman and
Yandell 1988). To calculate variance in a line’s length or a polygon’s areal extent based
on positional error at individual points constituting the line or polygon, we would require
knowledge of spatial correlation in positional errors, as discussed by Zhang and Goodchild
(2002). Error propagation of buffer analysis in a vector-based GIS is studied with the use

Downloaded by [University of North Dakota] at 22:03 11 September 2014 International Journal of Geographical Information Science

2283

of statistics and numerical analyses (Shi et al. 2003). Factors such as the error of commis-
sion, the error of omission, and the discrepancy area are proposed as the error indicators
for buffer analysis.

For quantifying error propagation in ﬁelds, we can pursue spatial statistical analysis.
Let S = {si} denote the n-dimensional column vector of (ﬁxed) ground-truth values; that
is, si is the true (univariate) value associated with location i. Let Z = {zi} denote the
column vector of corresponding observed values. Arbia et al. (2003) proposed a model,
Z = HS + ε, where H = {hi,j} is a ﬁxed n × n matrix with row sums equal to unity
and hi,j ≥ 0 for all i and j, and ε is an n-dimensional column vector of measurement
errors, which may assume normality. Using such models, Arbia et al. (2003) investigated
the effects of different types of source-map error on both the size and pattern of errors in
the resulting maps derived from linear combinations.

Error propagation modeling may also be pursued by stochastic simulation. Stochastic
simulation generates alternative, equal-probable maps based on a probabilistic model of
the input ﬁeld, including its distribution and autocorrelation structure (Heuvelink 1998,
Goovaerts 2001). These realizations emulate the differences or distributions that would
be observed from a measurement process known to be subject to error prescribed by the
model. Simulation-based error propagation may be performed to quantify uncertainty in
lines and areas based on positional errors in points (Zhang and Goodchild 2002, Heuvelink
et al. 2007), terrain derivatives based on errors in elevation ﬁelds (Holmes et al. 2000),
and change detection based on single-date misclassiﬁcation (Burnicki et al. 2007). Below,
we will review simulation approaches for modeling uncertainty in interval/ratio ﬁelds
and categorical ﬁelds. Uncertainty propagation in environmental modeling will also be
discussed.

Hunter and Goodchild (1997) proposed a general-purpose model of DEM errors in
which a spatially autoregressive random ﬁeld is added as a disturbance term to elevations.
Simulated error-contaminated DEMs facilitate slope and aspect errors characterization.
Holmes et al. (2000) used stochastic conditional simulation to generate multiple realiza-
tions of the DEM error surface that reproduce the error measurements at their original
locations and sample statistics such as the histogram and semivariogram. The differences
between these alternative error surfaces provide a model of uncertainty for the unknown
DEM error spatial distribution. Kyriakidis et al. (1999) proposed a geostatistical method-
ology for integrating the sparse elevation measurements (hard data) and the abundant
DEM-reported elevations (soft data) to model the unknown higher accuracy (reference)
elevation surface in a way that properly reﬂects the relative reliability of the two sources
of information. Both uncertainty at individual locations and joint uncertainty associated
with spatial features observed in the DEM can be modeled from realized DEMs, making
geostatistical simulation a valuable tool for decision-making based on DEM data prod-
ucts. Oksanen and Sarjakoski (2005) showed how random errors in a ﬁne toposcale DEM
are propagated to DEM-based surface derivatives, when the DEM error was modeled as a
second-order stationary Gaussian random process. Hebeler and Purves (2009) addressed
the impact of DEM uncertainty and elevation value error on derived products by using a
model of uncertainty including a stochastic component developed through correlation of
DEM error with local topography. Their study showed that the derivation of watersheds
and related statistics per watershed (e.g. hypsometry) vary signiﬁcantly as a result of the
introduced uncertainty, although global statistics for a range of topographic indices are
robust to introduced uncertainty.

All classiﬁcations of remote sensing data are subject to different kinds of errors, and
these errors can be carried over or propagated in subsequent geoprocessing (Congalton

Downloaded by [University of North Dakota] at 22:03 11 September 2014 2284

D. Li et al.

and Green 2008). Geostatistical simulation provides a good framework for uncertainty
characterization in land cover information. Methods for predicting and propagating mis-
classiﬁcation were developed on the basis of indicator samples and ancillary data, such as
spectrally derived posteriori probabilities (Kyriakidis and Dungan 2001). Stochastic sim-
ulation was proposed for generating multiple alternative realizations (maps) of the spatial
distribution of the higher accuracy class labels over the study area. The simulated alter-
native categorical maps can be used for assessing joint spatial accuracy, i.e. classiﬁcation
accuracy regarding the entire spatial features read from the thematic map, and serve as
input parameters to spatially explicit ecological models wherein spatial uncertainty charac-
terization is an integral element. For change detection and landscape dynamics, temporal
dependence between misclassiﬁcations across time could also be accommodated in sim-
ulation studies, as shown by Burnicki et al. (2007). Their results found that temporal
dependence of errors in land cover maps inﬂuences both the ability to detect a variety of
land cover changes and the level of error in change maps. Clearly, simulation can be used
to investigate patterns of error propagation with spatial and/or temporal interdependence,
although Burnicki et al. (2007) considered only two categories, i.e. forest versus non-
forest, in the simulated land cover maps. Cao et al. (2011) proposed a new probabilistic
method for modeling the posterior probability of class occurrences in stochastic simula-
tion of categorical ﬁelds, where transition probabilities rather than indicator covariances or
variograms are used as measures of spatial structure. The spatial structure of a reference
map, as quantiﬁed by transiograms, was found to be better reproduced than with other
existing approaches. The proposed method could be extended to spatio-temporal modeling
and aggregation of heterogeneous geospatial data, along with the methods to compute the
corresponding fusion weights for those cases.

However, indicator stochastic simulation does not produce replicable results for error
modeling as stochastic simulation does in the domain of interval/ratio ﬁelds. This deﬁ-
ciency originates from a ﬂaw in indicator-based simulation: realizations are drawn from
Kriged probability vectors of potentially arbitrary class orders, resulting in non-invariant
moments in simulated maps. To ﬁx this pathology, discriminant space models were pro-
posed (Goodchild et al. 2009). They work by generating equal-probable realizations of
discriminant variables Z, which are input to classiﬁers to obtain realized area-class maps
to drive uncertainty characterization in geoprocessing involving use of area-class maps.
These discriminant models may be useful to explore error propagation in the context of
land cover change detection and analysis.

Spatial data are increasingly incorporated in environmental modeling, ranging from
local hydrology to global change research. The formulation of these models is also subject
to uncertainty. It is thus important to understand how uncertainty in the model predic-
tions is inﬂuenced by uncertainty in the data or deﬁciency in the model (Van Niel et al.
2004). There is continuing research on quality assessment of spatial data and environmen-
tal models and accuracy in the modeling results (Heuvelink and Burrough 2002). Bishop
et al. (2006) examined how robust soil-terrain models are to uncertainty in the source
elevation data, conﬁrming that the model is not very sensitive to DEM errors. Heuvelink
et al. (2007) presented a statistical framework for representing and simulating uncertain
environmental variables, by which both positional uncertainty and attribute uncertainty are
handled.

In addition to error source identiﬁcation, error detection and measurement, and error
propagation, there are also error management and error reduction or elimination in the
hierarchy of needs for error handling, which are concerned with the inferences that may be
drawn from the results of error propagation (Veregin 1989). For this, we need to discuss

Downloaded by [University of North Dakota] at 22:03 11 September 2014 International Journal of Geographical Information Science

2285

error budget (Parysow et al. 2000), which seeks to rank input sources by their inﬂuence
on outputs of spatial analysis and modeling, with error quantiﬁed for each type of input
map. We can then reduce uncertainty in those inputs that have the largest inﬂuence on out-
puts by taking strategies such as more accurate measurement or higher spatial resolution
in sampling (Jager and King 2004). We can also explore the relationships between accu-
racy and cost to guide potential users in choosing between data products or deciding on
necessary expenditure on products, leading to sensible use of data quality information for
ﬁnancial cost counting concerning accuracy in data production and use, as demonstrated
in Januchowski et al. (2011).

4. Chinese perspectives

In research on spatial data quality, Chinese researchers have contributed mostly in
the following topics: positional error analysis, classiﬁcation accuracy assessment, fuzzy
methods for uncertainty description and analysis, and error propagation. These stud-
ies result in various methods for error analysis and modeling in geomatic engineering,
geographic science, landscape ecology, and other related ﬁelds, as reﬂected in steady pub-
lications of papers concerning spatial data quality in some academic journals in China,
such as Acta Geodaetica et Cartographica Sinica (xb.sinomaps.com), Geomatics and
Information Science of Wuhan University (http://www.wuj.whu.edu.cn/BJB-INFO/XINXI/
XINXIBAN/index.htm), and Journal of Remote Sensing (www.jors.cn). However, research
on spatial statistical methods for uncertainty analysis and modeling needs to be strength-
ened, while techniques and tools are developed and popularized to a wider community.
Below, we introduce some of the work by Chinese researchers on issues of spatial data
quality, in addition to those research efforts mentioned wherever relevant in the previous
sections, although a comprehensive review or thorough discussion is beyond the scope of
this paper.

Error analysis has been one of the research foci in geomatic engineering in China for a
long time. Errors in map digitizing were analyzed by many researchers, and error ellipses
and epsilon-error bands were often used to model positional errors in discrete points and
lines, respectively. Liao and Bai (2010) described a new method for error evaluation in
rasterization of vector data. Feature extraction from high-resolution imagery has received
lots of research efforts, for which error analysis will be a useful investment. Accuracy
assessment in DEMs and remote sensing thematic classiﬁcation has long been an impor-
tant topic for Chinese researchers, resulting in a large number of papers, as cited in Li
(2008). Research on validation in remotely sensed information will make greater progress,
as China has established its Earth observing and applications systems, in accordance with
the international efforts in harmonization and standardization of satellite data handling and
information exchange.

At present, China’s national spatial databases at 1:1,000,000, 1:250,000, and
1:50,000 scales, as part of its National Spatial Data Infrastructure, have been constructed.
Digital products, including digital orthophoto maps (DOMs), DEMs, digital line graphs
(DLGs), digital raster graphs (DRGs), place names, and land use, are made available.
The 1:10,000 scale spatial databases for individual provinces have been or are being con-
structed, while spatial databases at scales ranging from 1:500 to 1:2000 for large or medium
cities have been constructed (Li 2008). Increasing availability of digital map data at local,
regional, and national scale means that research on and investment into spatial data quality
are becoming more important for long-term developments of China’s geospatial informa-
tion industry. There is increasing funding from the government and the industry for new

Downloaded by [University of North Dakota] at 22:03 11 September 2014 2286

D. Li et al.

methodology and techniques with respect to quality assessment and decision-making in
the face of uncertainty. As will be shown below, recent developments in China are mostly
reﬂected in research directed toward geospatial information services (i.e. GI services) and
their qualities.

With developments of information and communication technology, more and more
geospatial applications rely on data, information, and geoprocessing services provided
through a distributed and networked environment (Friis-Christensen et al. 2007). This can
be seen not only in geospatial data acquisition by remote or in situ sensors but also in
new computing paradigms, with all these resources connected in real time to form a self-
organized web, a Geospatial Service Web (GSW) (Gong et al. 2009). These developments
have led to increasing quantity of spatial data with diversiﬁed lineages and cost-effective
information and geoprocessing services. Thus, we should attend to the issue of quality in
GI services.

Quality of a GI service concerns operational characteristics of a service, which deter-
mine the utility of the service in an application context (Onchaga 2005). Thus, it focuses
upon provision of the right information for the right person(s) at the right place and at the
right time (Wu et al. 2011). In other words, the quality of GI services lies in the delivery
of most useful information to those in needs within the shortest time and at minimized
costs. The quality of GI services is inﬂuenced by factors such as the level of understand-
ing of users’ requirements for spatial information, sophistication of semantic models in GI
services, quality of data and algorithms involved in a task, quality, speed, and integrity of
Internet communication, and quality of user interfaces, in particular, visualization of spa-
tial information services (Wu et al. 2011). Clearly, quality in GI services, as a key factor for
successful GI services, is broader in meaning and contains more interwoven components
in information chains than spatial data quality, while the latter continues to be important
on its own and provides a kind of assurance for the former.

5. Conclusion

Spatial data quality and related issues represent an important research cluster in GIScience,
which is led by Prof. Goodchild, and pursued by many of his colleagues and students
around the world. There have been remarkable developments in research on spatial data
quality. Research on issues of information service quality is also gaining momentum.
Chinese researchers have contributed to various important topics in quality of spatial data
and GI services by learning from and collaborating with researchers in the west.

To stimulate further research, we have brieﬂy highlighted some of the issues that need
to be further investigated with respect to quality in spatial data and related topics, admitting
that there are many other issues that need to be addressed. Research should be continued
on quality assessment, error propagation, and visualization in spatial data products char-
acterized by heterogeneous lineages, varied accuracies, and differing scales. For this, we
should further explore different schools concerning spatial data quality, such as probability
theory and fuzzy logic, as shown in the paper. More importantly, we should explore their
complementarity and integration for quality assessment.

In parallel to research, tools for quality assurance should be developed and imple-
mented in production and use of spatial data. We can address the issue of tools development
by taking a pragmatic approach. In short term, we should reinforce the gathering and
management of quality information for any data collected or distributed through stan-
dardization and training. Tools for quality description and metadata analysis can then be
developed, drawing upon well-established techniques for data quality description and error

Downloaded by [University of North Dakota] at 22:03 11 September 2014 International Journal of Geographical Information Science

2287

analysis, as reviewed in this paper. In longer term, tools should be developed for quality
assurance in the context of heterogeneous and multi-source data productions and distri-
butions. Looking further into the future, quality assurance in GI services should take an
integrative strategy so that scientiﬁc, technical, managerial, economic, and institutional
dimensions are factored in GI systematic developments and service provision.

Acknowledgments
The authors would like to extend their heart-felt gratitude to Prof. Michael Goodchild for his pioneer-
ing work on GIScience and dedication to advising, helping, and supporting Chinese researchers in
their exploration of the wonderful world of GIScience. Comments from anonymous reviewers were
received with thanks. This special issue’s Guest Editors have given the authors helpful advice about
orientation, structuring, and revision of the paper. The work is supported by the National Natural
Science Foundation of China (Grant Nos. 41071286 and 41171346).

References
Aguilar, F.J., Aguilar, M.A., and Agüera, F., 2007. Accuracy assessment of digital elevation models
using a non-parametric approach. International Journal of Geographical Information Science,
21, 667–686.

Agumya, A. and Hunter, G.J., 2002. Responding to the consequences of uncertainty in geographical

data. International Journal of Geographical Information Science, 16, 405–417.

Arbia, G., Grifﬁth, D.A., and Haining, R.P., 2003. Spatial error propagation when computing linear
combinations of spectral bands: the case of vegetation indices. Environmental and Ecological
Statistics, 10, 375–396.

Bishop, T.F.A., Minasny, B., and McBratney, A.B., 2006. Uncertainty analysis for soil-terrain

models. International Journal of Geographical Information Science, 20, 117–134.

Burnicki, A.C., Brown, D.G., and Goovaerts, P., 2007. Simulating error propagation in land-cover
change analysis: the implications of temporal dependence. Computers, Environment and Urban
Systems, 31, 282–302.

Burrough, P.A. and Frank, A.U., eds., 1996. Geographic objects with indeterminate boundaries.

London: Taylor & Francis.

Cao, G., Kyriakidis, P.C., and Goodchild, M.F., 2011. Combining spatial transition probabilities for
stochastic simulation of categorical ﬁelds. International Journal of Geographical Information
Science, 25, 1773–1791.

Carlisle, B.H., 2005. Modelling the spatial distribution of DEM error. Transactions in GIS, 9,

521–540.

Chrisman, N.R. and Yandell, B.S., 1988. Effects of point error on area calculations: a statistical

Congalton, R.G. and Green, K., 2008. Assessing the accuracy of remotely sensed data: principles

model. Surveying and Mapping, 48, 241–246.

and practices. Boca Raton, FL: CRC Press.

De Bruin, S., Heuvelink, G.B.M., and Brown, J.D., 2008. Propagation of positional measurement
errors to agricultural ﬁeld boundaries and associated costs. Computers and Electronics in
Agriculture, 63, 245–256.

Devillers, R., et al., 2010. Thirty years of research on spatial data quality: achievements, failures,
and opportunities. Transactions in GIS, 14, 387–400. doi: 10.1111/j.1467-9671.2010.01212.x.
Dilo, A., de By, R.A., and Stein, A., 2007. A system of types and operators for handling vague spatial

objects. International Journal of Geographical Information Science, 21, 397–426.

Florinsky, I.V., 2002. Errors of signal processing in digital terrain modelling. International Journal

of Geographical Information Science, 16, 475–501.

Fonte, C. and Lodwick, W.A., 2004. Areas of fuzzy geographical entities. International Journal of

Geographical Information Science, 18, 127–150.

Foody, G.M., 2005. Local characterization of thematic classiﬁcation accuracy through spatially
constrained confusion matrices. International Journal of Remote Sensing, 26, 1217–1228.
Foody, G.M., 2010. Assessing the accuracy of land cover change with imperfect ground reference

data. Remote Sensing of Environment, 114, 2271–2285.

Downloaded by [University of North Dakota] at 22:03 11 September 2014 2288

D. Li et al.

Friis-Christensen, A., et al., 2007. Designing service architectures for distributed geoprocessing:
challenges and future directions. Transactions in GIS, 11, 799–818. doi: 10.1111/j.1467-
9671.2007.01075.x.

Gong, J., et al., 2009. Geospatial service web. In: D. Li, J. Shan and J. Gong, eds. Geospatial

technology for earth observation. New York: Springer, 355–379.

Goodchild, M.F., 1978. Statistical aspects of the polygon overlay problem. In: G. Dutton, ed. Harvard

papers on geographic information systems. Vol. 6. Reading, MA: Addison-Wesley.

Goodchild, M.F., 2008. Spatial accuracy 2.0. In: J. Zhang and M. Goodchild, eds. Spatial uncertainty:
proceedings of the eighth international symposium on spatial accuracy assessment in natural
resources and environmental sciences, volume 1. Liverpool: World Academic Union, 1–7.
Goodchild, M.F. and Gopal, S., eds., 1989. Accuracy of spatial databases. London: Taylor & Francis.
Goodchild, M.F. and Jeansoulin, R., eds., 1998. Data quality in geographic information. Paris:

Éditions Hermes.

Goodchild, M.F., Zhang, J.X., and Kyriakidis, P., 2009. Discriminant models of uncertainty in

nominal ﬁelds. Transactions in GIS, 13, 7–23.

Goovaerts, P., 2001. Geostatistical modeling of uncertainty in soil science. Geoderma, 103, 3–26.
Guo, Q., Liu, Y., and Wieczorek, J., 2008. Georeferencing locality descriptions and computing
associated uncertainty using a probabilistic approach. International Journal of Geographical
Information Science, 22, 1067–1090.

Guptill, S.C. and Morrison, J.L., eds., 1995. Elements of spatial data quality. Tokyo: Elsevier

Guth, P.L., 2006. Geomorphometry from SRTM: comparison to NED. Photogrammetric Engineering

and Remote Sensing, 72, 269–277.

Hagen-Zanker, A., 2009. An improved fuzzy kappa statistic that accounts for spatial autocorrelation.

International Journal of Geographical Information Science, 23, 61–73.

Hebeler, F. and Purves, R.S., 2009. The inﬂuence of elevation uncertainty on derivation of

topographic indices. Geomorphology, 111, 4–16.

Heuvelink, G.B.M., 1998. Error propagation in environmental modelling with GIS. London: Taylor&

Science.

Francis.

Heuvelink, G.B.M., Brown, J.D., and van Loon, E.E., 2007. A probabilistic framework for represent-
ing and simulating uncertain environmental variables. International Journal of Geographical
Information Science, 21, 497–513.

Heuvelink, G.B.M. and Burrough, P.A., 2002. Developments in statistical approaches to spatial
uncertainty and its propagation. International Journal of Geographical Information Science, 16,
111–113.

Holmes, K.W., Chadwick, O.A., and Kyriakidis, P.C., 2000. Error in a USGS 30-meter digital
elevation model and its impact on terrain modeling. Journal of Hydrology, 233, 154–173.
Huang, Y. and Liu, W., 1997. Building the estimation model of digitizing error. Photogrammetric

Engineering and Remote Sensing, 63, 1203–1209.

Hughes, M.L., McDowell, P.F., and Marcus, W.A., 2006. Accuracy assessment of georectiﬁed aerial
photographs: implications for measuring lateral channel movement in a GIS. Geomorphology,
74, 1–16.

Hunter, G.J. and Goodchild, M.F., 1997. Modelling the uncertainty of slope and aspect estimates

derived from spatial databases. Geographical Analysis, 19, 35–49.

Jager, H.I. and King, A.W., 2004. Spatial uncertainty and ecological models. Ecosystems, 7,

841–847.

Januchowski, S.R., et al., 2011. Characterizing errors in digital elevation models and estimating
the ﬁnancial costs of accuracy. International Journal of Geographical Information Science, 24,
1327–1347.

Kronenfeld, B.J., 2011. Beyond the epsilon band: polygonal modeling of gradation/uncertainty in
area-class maps. International Journal of Geographical Information Science, 25, 1749–1771.
Kyriakidis, P.C. and Dungan, J.L., 2001. A geostatistical approach for mapping thematic classiﬁcation
accuracy and evaluating the impact of inaccurate spatial data on ecological model predictions.
Environmental and Ecological Statistics, 8, 311–330.

Kyriakidis, P.C., Shortridge, A.M., and Goodchild, M.F., 1999. Geostatistics for conﬂation and accu-
racy assessment of digital elevation models. International Journal of Geographical Information
Science, 13, 677–707.

Downloaded by [University of North Dakota] at 22:03 11 September 2014 International Journal of Geographical Information Science

2289

Legleiter, C. and Goodchild, M.F., 2005. Alternative representations of in-stream habitat: classi-
ﬁcation using remote sensing, hydraulic modeling, and fuzzy logic. International Journal of
Geographical Information Science, 19, 29–50.

Leung, Y. and Yan, J.P., 1998. A locational error model for spatial features. International Journal of

Geographical Information Science, 12, 607–620.

Li, D., 2008. Uncertainty in spatial information, analysis, and applications: Chinese perspectives.
In: Proceedings of the 8th international symposium on spatial accuracy assessment in natural
resources and environmental sciences, 25–27 June 2008, Shanghai, China, 1–7.

Liao, S.B. and Bai, Y., 2010. A new grid-cell-based method for error evaluation of vector-to-raster
conversion. Computational Geosciences, 14, 539–549. doi: 10.1007/s10596-009-9169-3.
Mayaux, P., et al., 2006. Validation of the global land cover 2000 map. IEEE Transactions on

Geoscience and Remote Sensing, 44, 1728–1739.

Morrison, J., 1988. The proposed standard for digital cartographic data. The American Cartographer,

15, 9–140.

Mozas, A.T. and Ariza, F.J., 2011. New method for positional quality control in cartography based on
lines. A comparative study of methodologies. International Journal of Geographical Information
Science, 25, 1681–1695.

Oksanen, J. and Sarjakoski, T., 2005. Error propagation of DEM-based surface derivatives.

Computers & Geoscience, 31, 1015–1027.

Onchaga, R., 2005. On quality of service and geo-service composition. In: Proceedings of AGILE

8th conference on geographic information science, Estoril, Portugal, 519–528.

Parysow, P., Gertner, G., and Westervelt, J., 2000. Efﬁcient approximation for building error budgets

for process models. Ecological Modelling, 135, 111–125.

Radoux, J., et al., 2011. Thematic accuracy assessment of geographic object-based image classiﬁca-

tion. International Journal of Geographical Information Science, 25, 895–911.

Ratcliffe, J.H., 2001. On the accuracy of TIGER-type geocoded address data in relation to cadastral
and census areal units. International Journal of Geographical Information Science, 15, 473–485.
Seo, S. and O’Hara, C.G., 2009. Quality assessment of linear data. International Journal of

Geographical Information Science, 23, 1503–1525.

Shi, W., Cheung, C., and Zhu, C., 2003. Modelling error propagation in vector-based buffer analysis.

International Journal of Geographical Information Science, 17, 251–271.

Shortridge, A.M., 2006. Shuttle radar topography mission elevation data error and its relationship to

land cover. Cartography and Geographic Information Science, 33, 65–75.

Steele, B.M., Winne, J.C., and Redmond, R.L., 1998. Estimation and mapping of misclassiﬁcation
probabilities for thematic land cover maps. Remote Sensing of Environment, 66, 192–202.
Stehman, S.V., 2009. Sampling designs for accuracy assessment of land cover. International Journal

of Remote Sensing, 30, 5243–5272.

Tucker, C.J., Grant, D.M., and Dykstra, J.D., 2004. NASA’s global orthorectiﬁed Landsat data set.

Photogrammetric Engineering and Remote Sensing, 70, 313–322.

Tveite, H. and Langaas, S., 1999. An accuracy assessment method for geographical line data sets
based on buffering. International Journal of Geographical Information Science, 13, 27–47.
Van Niel, K.P., Laffan, S.W., and Lees, B.G., 2004. Effects of error in the DEM on environmental
variables for predictive vegetation modeling. Journal of Vegetation Science, 15, 747–756.
van Oort, P.A.J., et al., 2004. Spatial variability in classiﬁcation accuracy of agricultural crops in the
Dutch national land-cover database. International Journal of Geographical Information Science,
18, 611–626.

Veregin, H., 1989. Error modelling for the map overlay operation. In: M.F. Goodchild and S. Gopal,

eds. Accuracy of spatial databases. London: Taylor & Francis, 3–18.

Wechsler, S. and Kroll, C., 2006. Quantifying DEM uncertainty and its effects on topographic

parameters. Photogrammetric Engineering and Remote Sensing, 72, 1081–1090.

Wilkinson, G.G., 2005. Results and implications of a study of ﬁfteen years of satellite image
classiﬁcation experiments. IEEE Transactions on Geoscience and Remote Sensing, 43, 433–440.
Woodcock, C.E. and Gopal, S., 2000. Fuzzy set theory and thematic maps: accuracy assessment and
area estimation. International Journal of Geographical Information Science, 14, 153–172.
Wu, H., et al., 2011. The theory and methods for quality in geographic information services. Wuhan:

Zandbergen, P.A., 2011. Inﬂuence of street reference data on geocoding quality. Geocarto

Wuhan University Press, 256 pp.

International, 26, 35–47.

Downloaded by [University of North Dakota] at 22:03 11 September 2014 2290

D. Li et al.

Zhang, J. and Goodchild, M.F., 2002. Uncertainty in geographical information. London and New

York: Taylor & Francis.

Zimmerman, D.L., et al., 2007. Modelling the probability distribution of positional errors incurred

by residential address geocoding. International Journal of Health Geographics, 6, 1.

Downloaded by [University of North Dakota] at 22:03 11 September 2014 