Geoinformatica (2006) 10: 399–422
DOI 10.1007/s10707-006-0340-x

Fast Cluster Polygonization and its Applications
in Data-Rich Environments

Ickjai Lee · Vladimir Estivill-Castro

Received: 6 January 2004 / Revised: 6 April 2005 /
Accepted: 27 September 2005
© Springer Science + Business Media, LLC 2006

Abstract We develop a linear time method for transforming clusters of 2D-point
data into area data while identifying the shape robustly. This method translates a data
layer into a space ﬁlling layer where shaped clusters are identiﬁed as the resulting
regions. The method is based on robustly identifying cluster boundaries in point
data using the Delaunay Diagram. The method can then be applied to modelling
point data, to displaying choropleth maps of point data without a reference map, to
identifying association rules in the spatial dimension for geographical data mining,
or to measuring a gap between clusters for cluster validity.

Keywords Cluster polygonization · Patterns and clusters · Delaunay diagram ·
Geographical data mining

1 Introduction

Analysts model real-world geographical data within Geographic Information Sys-
tems (GISs) using the McHarg’s multi-layer view of the world [32] where each
geographical layer captures something unique to it. The steady improvement into
even faster data gathering processes results in data-rich environments with many
layers that exceed the capability of human analysis [34], [39]. Thus, sophisticated
geographical data analysis tools become necessary for handling thousands of differ-
ent layers (or themes) that may contain millions of data points. One of the most

I. Lee (B)
School of Information Technology, James Cook University, Douglas Campus,
Townsville, QLD 4811, Australia
e-mail: ickjai.lee@jcu.edu.au

V. Estivill-Castro
School of Information and Communication Technology, Nathan and Logan Campuses,
Grifﬁth University, Brisbane, QLD 4111, Australia
e-mail: v.estivill-castro@grifﬁth.edu.au

400

Geoinformatica (2006) 10: 399–422

important summarising operations for analysis is to move point data into area data
while preserving its shape, so that overlaying analysis can consider matchings or
correlations [30], [46]. With the emergence of data mining, the summarisation task
is achieved with clustering or polygonization techniques [46]. Clustering [24], [25],
[26], [29], [31] is now one of the most popular and frequently used approaches
for ﬁnding undetected or unexpected patterns of concentrations residing in large
spatio-temporal databases. The analysis of clusters provides hypotheses for “where?”
and “when?”, and suggests leads into “why?” for post-clustering explorations. Thus,
clustering is seen as a starting point for a series of knowledge discovery processes
in massive spatio-temporal databases. While clustering techniques are becoming
mature, there is still a need for sophisticated post-clustering methods. Post-clustering
processes have attracted less attention than clustering itself despite their importance.
Identifying shapes (equivalently boundaries) of clusters is an intuitive way of
reasoning about clusters. If shapes of clusters of a set P of points match with a
particular feature data, then clusters exhibit a high association with the feature. For
example, if the shape of a set of geo-referenced points has a boundary that closely
resembles the shape of a river, one would suspect that the river has some impact on
the phenomena because occurrences appear much less on the other side of the river.
However, researchers have found it challenging to describe consistent and reliable
methods to extract the shape of a cloud of points. Determining the shape of a cloud
of points is an inference that can never provide an exact answer (as each observer
may have a different learning bias to extrapolate and generalise a boundary) and
thus, most approaches so far are application-dependent [36].

The Minimum Bounding Rectangle (MBR) is an intuitive and simple way of
approximating the shape of a cluster Ci ⊂ P. However, as a representation of the
dispersion of the phenomena in space, MBR is too crude to capture details of the
shape of Ci. The convex hull CH(Ci) of Ci is another simple method. Since CH(Ci) is
the smallest convex set containing Ci, it can be a natural representation. In addition, it
is unique for Ci (this is also the case with the MBR representation). Fast and unique
models for shape extraction are useful for data mining where parameter-tuning is
laborious and time consuming, but CH(Ci) is again too crude to capture details of
the shape of Ci unless the points are all arranged in a convex shape (and then, the
vertices of CH(Ci) coincide with Ci, which does not help much in summarising Ci).
The technique known as α-shape [13] overcomes the crudeness of CH(Ci) (this
is a generalisation of CH(Ci)). The value of the real number α controls the desired
level of detail. The set of α values leads to a family of shapes capturing crude to ﬁne
shapes of Ci. That is, it navigates intermediate shapes between the crudest (CH(Ci))
and the ﬁnest (points in Ci themselves).

Although users can explore a spectrum of α-shapes for a point dataset, several
problems arise when analysts determine spatial cluster boundaries with α-shapes in
data-rich environments.

1.

3.

It is difﬁcult to determine the best value for α that produces a shape neither too
crude nor too ﬁne [30].

2. Several trial-and-error steps are necessary for tuning the value of α. In data-rich

environments, this is laborious and time consuming.
If a study region R consists of l layers and each layer contains k clusters, then
we need to tune α for as many as lk times in order to ﬁnd the best shape of each

Geoinformatica (2006) 10: 399–422

401

cluster. In data-rich environments, l is typically a very large number. Thus, the
α-shape is not well suited for data-rich environments.

4. Since clusters in P require different values for α, the independent manipulation
of values of α for clusters belonging to the same layer not only increases time
requirements but taints exploratory analysis with human bias.

5. The α-shape of a cluster Ci ⊂ P is not affected by the distribution of points in
P\Ci. Clusters are truly the result of the peculiarities in the entire distribution
sampled by P. Thus, although a point in P\Ci does not belong to the cluster Ci,
it has some effect on the shape of Ci. Note that, shape is a matter of sets of points
and everything is related to everything else in geographical settings [41].

6. Further to the point above, and as we will illustrate, α-shapes are brittle to noise,

or to datasets where the points may be the result of a mixture.

In this paper, we propose an automatic boundary extraction process for clusters
in point data and extend it to cluster polygonization. In contrast to the α-shape
approach, our approach minimises the need for parameter-tuning. Instead, it derives
the shape of a cluster Ci from both the spread of points in itself and the spread of
points in P\Ci. The proposed argument-free approach is well-suited for data-rich
environments where parameter-tuning is expensive and difﬁcult. It approximates
shapes of clusters using the Delaunay Diagram. Note that, the Delaunay Diagram
has a number of geometrically and topologically robust characteristics and has been
applied to many GIS applications [36]. Once the Delaunay Diagram is constructed,
the boundary extraction process requires O(n) time. The algorithm can be used
as post-processing for Short–Long clustering [16] that is based on the Delaunay
Diagram as an underlying graph.

The remainder of this paper is organised as follows. Section 2 provides intuition
and details our boundary extraction and polygonization processes. We also provide
an analysis of the algorithm’s time complexity. In Section 3, we discuss experimental
results with both synthetic and real datasets. Section 4 illustrates the beneﬁts by
describing four applications of cluster polygonization. Finally, the last section draws
concluding remarks.

2 Extracting Cluster Boundaries

Why can humans so rapidly build a shape around a cloud of points? This not only
happens for the purpose of analysis, but many cultures associated clouds of stars in
the night sky with particular shapes, and grouped them into constellations. Figure 1a
shows a cloud of points which the vast majority of people would recognise as being
shaped like the letter A.1

In spatial data mining, shape ﬁnding is used for mining associations [17], [30].
Typically, clustering is used as the ﬁrst step to extract a proﬁle of the phenomena
in space. Although several spatial clustering methods have been proposed within

1We have chosen a cloud of points with the shape of the letter A to illustrate our discussion on shapes
with holes and with non-convex outer-boundary. Our techniques are aimed at building the shape of
clouds of points in GIS and not as techniques for pattern recognition in writing, nor to be applied to
raster images of characters for character recognition or image processing [7] (Although, we believe
they will still perform well on those settings.).

402

Geoinformatica (2006) 10: 399–422

Fig. 1 a A cloud of points. b Shapes suggested by the distribution of points in space

the data mining and the GIS disciplines, most peak-inference clustering methods are
based on global thresholds [14], [15, 27], [37], [43], [45]. As a result, these approaches
fail to detect some spatially interesting groups [16, See Section 2]. Peak-inference
clustering approaches are usually less effective in settings where regions can be
interpreted as clusters of different densities. To detect this, they demand careful and
laborious tuning of the global thresholds. It is not hard to illustrate this effect [16]
with DBSCAN [15]. The DBSCAN approach requires two global thresholds MinPts
and Eps. The value of Eps deﬁnes a spherical neighbourhood for each point.

Clustering methods by partitioning [35], [44] optimise some objective function
to ﬁnd spatial groups. Although these methods are suitable for facility location
problems, their results are convex clusters, so they are less informative for cluster
reasoning.

2.1 Short–Long Clustering

Recently, we proposed Short–Long clustering [16] to overcome drawbacks of tra-
ditional clustering methods. This approach detects possible boundaries of clusters,
and as such, it is able to identify various types of spatial concentrations. Clustering
for massive datasets should remain efﬁcient and effective while minimising the
number of user-supplied arguments for clustering. Efﬁcient clustering algorithms
that demand tuning of several user-supplied arguments for their best results remain
unsuitable for mining vast amounts of data. Finding best values for arguments is
expensive in terms of time efﬁciency since this necessitates several trial-and-error
steps or pre-processing. Short–Long clustering is a solid candidate for geographical
data mining and post-clustering analysis, since it needs O(n log n) time, produces
quality clusters and requires minimum tuning of only one parameter. For these
reasons, Short–Long clustering is used as a basis for our post-clustering analysis in
this paper. However, our method for polygonization extends in a straightforward
manner to other clustering approaches.

Short–Long clustering [16] provides a robust framework for analysing complex
spatial databases within GIS. It derives spatial concentrations exhibiting similar
characteristics by locating cluster boundaries where sharp changes in point density
occur. Since two sides of a cluster boundary have different densities (one is dense
while the other is sparse), sudden changes in point density are informative of cluster

Geoinformatica (2006) 10: 399–422

403

detection. Short–Long clustering locates sharp alterations using local neighbourhood
relationships and identiﬁes globally signiﬁcant cluster boundaries. The signiﬁcance is
determined by the consideration of all local neighbourhood relationships.

We refer the reader to the original description of Short–Long clustering [16];
however, we provide here an alternative motivation, more suitable for the purpose
of identifying boundaries. Consider for a moment the input set P of two dimensional
points, and let λ be a positive value. For each point pi ∈ P consider the function
f pi

( p) deﬁned as follows

( p) =

f pi

1, if (cid:4) pi − p(cid:4) ≤ λ;
0, otherwise.

(cid:2)

(cid:3)

( p). The function f pi

Now, consider the function F( p) =
( p) can be consid-
pi∈P f pi
ered a kernel function (regulated by λ) and the function F( p) can be regarded as
a histogram from the perspective of descriptive statistics. The shape of the cloud of
points is then much more apparent if we consider the view that F( p) provides when
we plot it. A way in which we can extract the shape of the set P of points is then by
considering the level curves of F( p). Figure 2a shows a view of the surface plot of
F( p) while Fig. 2b–c show the level curves. Naturally, if we chose λ too large, some
features of the shape would be lost, like the hole. If we chose λ too small (smaller
than the distance between any two points in P), then no shape would emerge at
all. Similarly, which level curve to select as the shape is delicate. A very high level
curve will just show the peaks and not the true shape, while a very low level curve
may include noise points that are not part of the area. Finally, we point out that we
( p), resulting in a slight different
may choose not to use the Euclidean distance in f pi
kernel, but very similar overall shape. All these parameters and options should be
automated for facilitating the analyst job.

The Delaunay Diagram of P, denoted by DD(P), (removing ambiguities from
the Delaunay Triangulation when co-circularity occurs) provides a unique structure
for capturing local neighbourhood relationships. We argue that in 2D the Delaunay
Diagram for the Euclidean metric succinctly represents all the proximity information
required to obtain the best guess for such parameters. In particular, its properties
regarding circum–circles of faces of the diagram are in close relationship with the
( p) would have in nearby points.
overlap (and thus height) that the function f pi

Fig. 2 a The level curves of the kernel-based histogram of the cloud of points in Fig. 1. b The proﬁle
in 3D of the kernel-based histogram. c The 2D view of the level curves in the histogram

404

Geoinformatica (2006) 10: 399–422

Delaunay edges in DD(P) connect local neighbors. Short–Long clustering attempts
to locate cluster boundaries in DD(P) to identify spatial aggregations.

Using local and global statistics Short–Long clustering classiﬁes each edge adja-
( pi)), long edge

cent to each pi into three groups: short edge (denoted by SEdgesk
( pi)).
(denoted by LEdgesk

( pi)) and other edge (denoted by OEdgesk

Delaunay edges in LEdgesk

( pi) represent exceptionally long edges in the
( pi) represent exceptionally
k-neighbourhood of pi while Delaunay edges in SEdgesk
short edges. Short–Long clustering removes exceptionally long Delaunay edges in
the 1-neighbourhood (LEdges1) to extract rough cluster boundaries. And then,
it eliminates some short Delaunay edges in the 1-neighbourhood (SEdges1) to
overcome the chain effect [23]. Short–Long clustering extends local neighbourhood
to the 2-neighbourhood to remove inconsistently long edges (LEdges2). Readers
need to refer to the original paper [16] for further details.

2.2 Cluster Polygonization Algorithm

Short–Long clustering delivers P with cluster identiﬁers, but this is not enough for
cluster polygonization analysis. We need to extract sets of cyclic lists of edges that
deﬁne polygons for clusters. To do this, we repeatedly merge triangles and remove
edges shared by triangles. The steps of the algorithm are shown in Fig. 3.

Step 1: After Short–Long clustering, we recuperate all intra-cluster edges by recon-
necting every edge ei, j = ( pi, pj) satisfying CC[ pi] = CC[ pj] (where CC[ p]
is the connected component of p). Figure 4b illustrates the result after
Short-Long clustering while Fig. 4c depicts the result of recuperation. The
recuperated subgraph approximates shapes of clusters. The process labeled
RecuperateDelaunayDiagram returns the recuperated graph (intraG).
Step 2: Since DD(P) is the underlying graph, faces after the recuperation are not
necessarily triangles. Figure 4c depicts an example (cid:6) p7, p8, p9, p10(cid:7). These
faces that are not triangles are temporarily triangulated (some edges are
added) to make our cluster polygonization algorithm simple. For instance,
either e7,9 or e8,10 can be added for the face (cid:6) p7, p8, p9, p10(cid:7). Note that, these
artiﬁcially added edges are removed in the later process (merging triangles);

Algorithm The Short-Long cluster polygonization procedure

Input: A subgraph G of DD(P) labeled with cluster identiﬁers;
Output: Polygons of clusters;
begin
Step 1) intraG ⇐ RecuperateDelaunayDiagram(G);
Step 2) triangulatedG ⇐ TriangulateFaces(intraG);
Step 3) b oundaryG ⇐ TheTriangleMerge(triangulatedG);
Step 4) ClusterPolygonization(b oundaryG);
end

Fig. 3 Steps in our Cluster Polygonization Algorithm

Geoinformatica (2006) 10: 399–422

405

Fig. 4 Cluster polygonization through cluster boundary extraction (||P|| = 10). a A set P of points
with DD(P). b Short–Long clustering results (two clusters). c Recuperating intra-cluster edges in
DD(P). d Triangulating faces resulting from co-circularity. e Extracting directed cluster boundaries.
f Polygons for the clusters

so they do not affect the shapes of clusters. Figure 4d depicts triangula-
tion of non-triangle faces. The process is labeled TriangulateFaces. It
takes intraG as an input graph and produces triangulatedG as an output.
It triangulates non-triangle faces in intraG resulting from co-circularity.
For example, for all non-triangle faces in intraG consisting of k points
( p1, p2, . . . , pk) where the j − i + 1 points ( pi, pi+1, . . . , pj) for 1 ≤ i ≤
j ≤ k have the same cluster identiﬁer, the triangulation adds edges ei,i+2,
ei,i+3,. . . , ei, j.

Step 3: The third step is labeled TheTriangleMerge and it takes triangulatedG as
an input graph and produces b oundaryG as an output graph made of lists of
directed edges. Because polygons may contain holes, a polygon may require
more than one cyclic list of directed edges. In our approach, the interior of
the polygon is always to the left of the directed edge. However, incomplete
dead ends [5] need also to be removed in the triangle merge process since
no areas are associated with them. Note that, the dead end e4,5 depicted in
Fig. 4d is removed and does not appear after the triangle merge process as
shown in Fig. 4e.
Since the Delaunay Triangulation is a planar subdivision into triangles, each
edge in the Delaunay Triangulation belongs to either one Delaunay triangle
(external Delaunay edge or hull edge [36]) or to two (internal Delaunay
edge [36]). Because triangulatedG is a subgraph of planar subdivision into
triangles, each edge in triangulatedG belongs to no triangles (dead end),

406

Geoinformatica (2006) 10: 399–422

Fig. 5 The two subcases of the
simple case in triangulatedG.
a Triangle in a cluster. b No
triangle

to one Delaunay triangle or at most to two. For each intra-cluster edge
ea,b = ( pa, pb ) ∈ triangulatedG, the TheTriangleMerge(triangulatedG)
proceeds as follows.

–

Simple case: the intra-cluster edge ea,b is an external Delaunay edge
or hull edge. That is, the edge can have at most one incident triangle.
Figure 5 illustrates this case.

Subcase: triangle in a cluster—the third node pc of the triangle
(cid:6) pa, pb , pc(cid:7) has the same cluster identiﬁer as nodes pa and pb .
Then, edge ea,b is labeled as a boundary edge and oriented (be-
cause triangulatedG is a planar embedding) such that pc is on its
left (the interior of the triangle) and the exterior is on its right.
Figure 5(a) illustrates this subcase.
Subcase: no triangle—the third node pc of the triangle (cid:6) pa, pb , pc(cid:7)
does not have the same cluster identiﬁer as pa and pb . Then, ea,b
is not labeled as a boundary edge (it will be removed). That is, ea,b
is not placed in the output of this process because it is a dead end.
Figure 5b illustrates this subcase.

– Complex case: the intra-cluster edge ea,b is an internal Delaunay edge.
That is, the edge can have two incident triangles (the most possible).
Figure 6 illustrates this complex case.

Subcase: two triangles in a cluster—the third nodes pc and pd
of triangles (cid:6) pa, pb , pc(cid:7) and (cid:6) pa, pb , pd(cid:7) are both in the same
connected component as pa and pb . Then, ea,b is removed (it will
not be in the output as a boundary edge since clearly it is inside
the quadrilateral (cid:6) pa, pc, pb , pd(cid:7) that is inside the cluster. Figure 6a
illustrates this subcase.

1.

2.

1.

Fig. 6 The subcases of the complex case in triangulatedG. a Illustration of the ﬁrst complex subcase,
two triangles in a cluster. Illustrations b and c are the second subcase: triangle in a cluster. d No
triangle

Geoinformatica (2006) 10: 399–422

407

2.

3.

Subcase: triangle in a cluster—one of the third nodes pc or pd is
in the cluster of pa and pb , but the other is not. Without loss of
generality, we assume pc is in the cluster of pa and pb (recall that
CC[ pa] = CC[ pb ]) while pd is not (i.e., CC[ pd] (cid:9)= CC[ pa]). Then,
edge ea,b is a boundary edge since the triangle (cid:6) pa, pb , pc(cid:7) is in the
cluster but the triangle (cid:6) pa, pb , pd(cid:7) is not. Then, ea,b is labeled as a
boundary edge and oriented in such a way that pc is on its left (the
interior) and pd is on its right (the exterior). Figure 6b–c illustrates
this subcase.
Subcase: no triangle—neither pc nor pd is in the same cluster as pa
and pb . Then, ea,b is removed because it is a dead end. Figure 6d
illustrates this subcase.

Step 4: Finally, polygons for clusters can be derived as depicted in Fig. 4f. Cluster
polygonization constructs a list of edges such that traversing the list of edges
corresponds to navigating along the boundary of a cluster. The list will be
circular, eventually returning to the same node. A polygon with holes will
be made of several of these lists.

2.3 Correctness and Time Complexity

Our description of the algorithm has provided proof of correctness for all steps
except for showing that cluster polygonization is always possible from the output of
the TheTriangleMerge. The following lemma proves that b oundaryG is Eulerian
and it is now a matter of traversing the graph within the planar embedding to extract
the circular (cyclic) lists of edges (equivalently nodes) that constitute polygons.

Lemma 2.3.1 For every node pa attached to an oriented edge e in b oundaryG, the
following holds.

– The degree of pa is even.
– The in-degree of pa equals the out-degree of pa.
–

It is possible to alternate the incoming edges and the outgoing edges either
clockwise or counterclockwise in the planar embedding representing the output
of the TheTriangleMerge process (b oundaryG).

Proof The output of the TheTriangleMerge process is equivalent to a union of
disjoint triangles (the triangles share edges but not their interiors). Note that, when
the TheTriangleMerge process removes intra-cluster edges, it performs the union
of two regions which are the union of triangles and thus the new region is also the
union of triangles. The point pa must belong to a triangle in the union because it is
attached to an oriented edge e. Let T be the sequence of triangles clockwise around
the point pa. This sequence T of triangles corresponds to a sequence of edges E.
These edges are all incident to pa with e oriented. Without loss of generality, assume e
is an incoming edge and that the sequence E = (cid:6)e, e1, . . . , ek(cid:7) is the sequence of edges
in the triangulation incident to pa when travelling clockwise around pa. Because e is
incoming, the triangle determined by (cid:6)e, pa, e1(cid:7) must be exterior. If e1 is oriented,
then it must be outgoing and the triangle (cid:6)e1, pa, e2(cid:7) must be interior. If e1 is not
oriented, it is because the next triangle (cid:6)e1, pa, e2(cid:7) is exterior. Continuing in this way,

408

Geoinformatica (2006) 10: 399–422

Fig. 7 Empirical measure-
ments of CPU seconds for
different step of polygon-
ization on large datasets

we see that the triangles in T alternate between a few that are interior and a few that
are exterior. In any case, when the triangles swap from interior to exterior, the edge
incident to pa is incoming and when the swap back the edge must be outgoing. But
the sequence of triangles around node pa is ﬁnite, so when completing the clockwise
(cid:10)(cid:11)
traversal we see that the lemma is satisﬁed.

This lemma completes the proof of correctness of our algorithm and suggests some
interesting theoretical questions. It is easy to see that the Delaunay Diagram of a
set of points does not need to be Eulerian. However, to the best of our knowledge
it remains open to characterize sets of points that lead to an Eulerian Delaunay
Diagram.

The linear time complexity of our algorithms is a consequence of the fact that,
for a set P of two dimensional points, the number of edges in DD(P) is linear
in P [36]. Thus, storing DD(P) requires linear space. To compute the connected
components of a graph is linear in the sum of both number of edges and num-
ber of nodes [33]. Thus, the two processes, RecuperateDelaunayDiagram and
TriangulateFaces, are bounded by O(n) time where n is the number of nodes
in DD(P). The TheTriangleMerge process tests if every intra-cluster edge e in
triangulatedG is a boundary edge. It performs constant work for each edge. Thus, this
is linear as well. Therefore, the cluster polygonization algorithm requires linear time
for Short–Long clustering. However, if the output of other clustering methods only
labels points with their cluster identiﬁers (and does not use the Delaunay Diagram
as an underlying proximity graph), then we need to compute DD(P), which requires
O(n log n) time.

We have developed a supporting application. The application is implemented in
the C++ programming language using LEDA.2 Our prototype supports visualisation

2LEDA stands for Library of Efﬁcient Data types and Algorithms and we used version 4.2. For
documentation and code see http://www.algorithmic-solutions.com/.

Geoinformatica (2006) 10: 399–422

409

of cluster boundaries, polygonized cluster regions, choropleth mapping and asso-
ciation mining. The application also has been enabled to log CPU timings of the
different steps in the polygonization process and thus we can validate experimentally
the time-complexity demonstrated in the analysis above. Figure 7 displays average
CPU seconds for datasets starting at 1,000 points through 128,000 points.

3 Performance Evaluation

3.1 A Comparisons with Crust and α-shapes

While the MBR and the Convex Hull are clearly too crude as representations of
a shape for a cloud of data points, there has been a signiﬁcant amount of research
in the Computational Geometry literature in recovering a shape from point (both
in 2D and in 3D) [2], [40]. Among the techniques proposed, the crust has gained
attention and it has also a strong intuition behind the notions of encoding proximity
in the Delaunay Diagram as well as the Voronoi tessellation. However, this technique
expects the data points to be all very close to the boundary of the objects (and not
a full sample of some phenomena). While in GIS applications the crust is seen as
a tool to manage fringe points and other applications [22] it is not effective for the
conversion of data point layers to area layers in the sense discussed here. Figure 8
shows our algorithm operating on the dataset discussed earlier which has the shape
of the letter A. We can see that noise points are eliminated and the hole in the shape
as well as the bay is clearly identiﬁed, but we can compare this with the results of
extracting the crust. Figure 9a shows the application of the crust algorithm to the
same dataset shaped as an A. Clearly, the crust fails on the original dataset where the
cluster is very dense. The crust is effective in Fig. 9b where all data points are known
to be near the boundary [3]. This is exactly what the theory of the crust indicates

Fig. 8 a The Delaunay Diagram. b Clusters obtained by Short–Long clustering. c The result of edge
recuperation. d The boundary. e The polygon

410

Geoinformatica (2006) 10: 399–422

Fig. 9 a The crust on the original data set. b The crust on only points that are near the boundary

(both in 2D and 3D) [4], [9], [10], [11], [12]. However, in the datasets we have in
mind, the actual question is which of the data points are in the boundary and which
are inside the cluster?

The alternative is to consider α-shapes. But as Fig. 10 shows, selecting the most
suitable value for α is challenging and it does not produce reliable results. Reducing
the value of α allows us to extract the boundary of the cluster, but we do not succeed
in obtaining the boundary of the hole. With the value α = 2 the external boundary
starts to degrade and the hole is not emerging. We have also compared our algorithm
with α shapes using other datasets used in the literature of spatial data mining. We
illustrate here with one of the datasets of the CHAMELEON’s benchmark suite [28].
Figure 11 shows the results of with α-shapes. Once again, robust values for α are not
easy to ﬁnd, and even those that appear as good values produce a large number of
unrelated edges to the apparent shape on the high density areas. Figure 12 shows
the results of our techniques in contrast with the level curves produced using a
kernel-based histogram discussed in the motivation of our work. We observe that our

Fig. 10 Alpha shapes for dataset in Fig. 1. a α = 56. b α = 25. c α = 12. d α = 4. e α = 2

Geoinformatica (2006) 10: 399–422

411

Fig. 11 Alpha shapes for a dataset in CHAMELEON’s benchmark suite [28]. a α = 156. b α = 40.
c α = 20. d α = 10. e Lines only for α = 10

methods are far superior in recovering a boundary of high density areas. Another
alternative to our approach is a grid approach as in STING+ [43]. Here we lay a
regular grid on the data and consider the number of points in a cell as a height (as
in a histogram) of as a gray-scale (as in a raster image). Unfortunately, ﬁnding the
best value for the granularity of the grid proves problematic, and even if that value
is found, the boundary is a set of axes-parallel lines. Moreover, the number of cells
grows quadratically as the grid becomes ﬁner. Using edge-detection algorithms (like
Sobel’s method) in the resulting gray-scale is also computationally expensive.

3.2 Results with Synthetic and Real Datasets

We now present results from experiments with synthetic datasets and real datasets
that illustrate the robustness of our approach. We use the Short–Long criterion with
default settings for all datasets, so that in all experiments, clusters whose size is less
than 1% of the total number of points are considered noise.

Figures 13 and 14 illustrate our approach applied to two datasets of the
CHAMELEON’s benchmark suite [28]. Cluster boundaries are successfully derived.
Figure 13c depicts boundaries for the ﬁrst dataset and Fig. 14c for the second set,

Fig. 12
d The polygons with our algorithm

a The level curves of a kernel-based histogram. b The clusters. c The oriented boundary.

412

Geoinformatica (2006) 10: 399–422

Fig. 13 Synthetic Dataset I (||P|| = 8,000). a Points with DD(P). b Resulting six clusters. c Cluster
boundaries. d Cluster regions

respectively. Boundaries reveal shapes of clusters more easily than human inspection
of the dataset or of the clustered data. Cluster regions depicted in Fig. 13d and 14d
reveal holes within clusters. Note that visual inspection is insufﬁcient to ﬁnd holes
in the regions of clusters, when presented simply as clusters (the (b) part of Figs. 13
and 14). However, the holes become visually apparent with polygonization (the (d)
part of Figs. 13 and 14). Figure 15 depicts a dataset containing many heterogeneous

Fig. 14
c Cluster boundaries. d Cluster regions

Synthetic Dataset II (||P|| = 8,000). a Points with DD(P). b Resulting eight clusters.

Geoinformatica (2006) 10: 399–422

413

Fig. 15 Synthetic Dataset III (||P|| = 8,000). a Points with DD(P). b Resulting 12 clusters. c Cluster
boundaries. d Cluster regions

clusters: small and large clusters, non-convex clusters, clusters with heterogeneous
densities, clusters inside clusters and clusters linked by multiple bridges. Cluster
boundaries and regions shown in Fig. 15c–d illustrate the robustness of our approach.
Real datasets representing geographical phenomena are more complex than
synthetic datasets, so it is more difﬁcult to extract cluster boundaries. The real
dataset displayed in Fig. 16 corresponds to locations of sexual offences occurred in
the year of 1997 around urban areas of Queensland in Australia. A large cluster
is spread around Brisbane, the capital city of Queensland. This large cluster in the
middle of the study region matches a cluster of universities and colleges. The other
crime concentrations are around the suburbs of Brisbane and match clusters in the
southern urban area of Brisbane where most of streets, highways and railways are
running across. The circle-like cluster with the hole in the bottom of the study
region matches with the suburb of Meadowbrook, and two small clusters also match
other concentrations (two clusters match Brisbane’s active and healthy precincts
such as Lota and Manly west, while the one below and to the right of the study
region matches with the coastal line and a tourist attraction area named Ormiston).
Although shapes of clusters are heterogeneous, irregular and complex, our approach
provides cluster boundaries for crime cluster regions and potential associations can
be derived as in spatial data mining. On the other hand, we attempted this exercise
using the commercial package ArcView 3.1 and ﬁve of its existing classiﬁcation
methods named EqualArea, EqualInterval, NaturalBreak, Quantile and Standard
Deviation with 1 StDev [8] to produce choropleth maps within the study region. In

414

Geoinformatica (2006) 10: 399–422

Fig. 16 Real Dataset representing sexual offences (||P|| = 1695). a Points with DD(P). b Result-
ing nine clusters. c Cluster boundaries. d Cluster regions

all these methods, boundaries depend on the political map or other thematic layer to
create the choropleth structure and not on the actual shape of the cluster. Moreover,
we found that choropleth mappings with EqualArea and Quantile do not show hot
spots properly, but instead spread all over the study region. The rest of these methods
(EqualInterval, NaturalBreak and standard deviation) seem to show high-density
regions, but fail to capture the large cluster to match the Brisbane city area. Thus,
these methods are unable to suggest possible spatial associations between thematic
layers.

4 Application

We discuss four settings for the application of our boundary ﬁnding algorithm. In
these four settings, better boundaries for the phenomena under study produce better

Geoinformatica (2006) 10: 399–422

415

analysis results. Also, analyses based on cluster boundaries and not just points extend
reasoning about points to clusters and regions. Thus, our algorithm goes beyond
the traditional approach that assumes one cluster to be independent of another
and enables to manipulate clusters on the basis of regions as opposed as just their
points (which occurs when, for example, distance between clusters is estimated
as distance between representative points). We will show that precise boundaries
enable sophisticated reasoning because richer relations are revealed. An example of
this would be the eight-shape cluster on the right of Synthetic Dataset III where our
results show the holes circle other denser clusters, but there is no overlap (something
that polygonization with CH or MBR would clearly be impossible and extremely
hard with alpha shapes).

4.1 Modeling Points with Clusters

Although modelling is a critical factor for spatial analysis, two widely adopted
modelling approaches, the raster and vector models, fail to uniquely deﬁne a
discrete relation is_neighbour ⊂ P × P for discrete map objects [21]. Point-to-area
transformations are possible solutions [18]. Now, points are regarded as neighbors if
their corresponding regions (areas) share common boundaries. Voronoi modelling
is one such solution. However, adjacency deﬁned by Voronoi modelling is local, so
neighbors are limited to a small number. Note that the expected number of neighbors
in Voronoi diagrams for points in (cid:12)2 is 6 [36]. In some spatial settings, we need large
scales of neighbouring. For instance, two points are considered as neighbors if they
exhibit similar characteristics even if their locally deﬁned “territories” (correspond-
ing areas resulting from point-to-area transformations) do not touch. Clustering
is a possible solution for this local neighbour limitation. Thus, points within the
same cluster are regarded as neighbors (since similar concentration occurs around
them). Regions that do not belong to any other cluster regions are assigned to a
default region (a noise region) for noise points. Note that the noise region may
be disconnected. That is, it may include several disconnected regions. Thus, every
location in space is assigned to one of cluster regions or alternatively to the noise
region. This constitutes a partition of the plane into regions. The resulting regions
by cluster polygonization are mutually exclusive and collectively exhaustive. That is,
cluster polygonization produces a spatial tessellation.

4.2 Choropleth Mapping with Cluster Boundaries

One of the difﬁculties of visualising point data is to manage privacy. Another
problem with point data is that theoretically, the probability of an event at a point
is zero. More intuitively, even a point is information about an area, but what
is the region to which the information applies? In order to resolve the privacy
problem or to make inferences on regions rather than locations, point data are often
aggregated, based on polygonal maps. For example, one way of visualising discrete
point data is to use choropleth maps. These maps display point data related to a
speciﬁc topic with respect to a boundary map such as political or administrative
area map. The display of a choropleth map ﬁlls polygons of the reference areas
with colors or grey tones according to densities (the number of points per unit
of area). One major ﬂaw of choropleth mapping is that its visual impression is

416

Geoinformatica (2006) 10: 399–422

heavily dependent on the base map under use. Dent [8] warns that distributions of
continuous geographical phenomena are not governed by political or administrative
subdivisions. Thus, choropleth mapping of points with political or administrative
layers is barely enlightening. Using the set of cluster regions of P as a basic area map
overcomes the problem of traditional choropleth map, since boundaries of cluster
regions are derived from the distribution of P. Thus, it minimizes artiﬁcial constraints
on choropleth mappings, which is important in exploratory spatial analysis [38].

Figure 17 displays choropleth maps for the datasets discussed in Section 3. Here,
the densest cluster is in black (RGB(0,0,0)) and the sparest cluster is in light grey
(RGB(200,200,200)). Intermediate clusters are coloured between the tones of grey
in proportion to their respective densities. Visual inspection of the original datasets
(see for example Fig. 13) is insufﬁcient to report relatively dense or sparse clusters.
In Fig. 13, one can hardly see any difference in density. The datasets seem too large
for such visual inspection. However, the choropleth map in Fig. 17a clearly indicates
that two clusters on left-hand side are relatively dense. A pattern that shows density
decreasing from left to right is now clearly visible. By contrast, Fig. 17b indicates
that density among clusters does not have a global pattern. The denser clusters of
synthetic Dataset II are randomly mixed with other clusters. This spread is not easily
recognised from Fig. 14, but the corresponding choropleth map is more informative.
Figure 17c shows that clusters are either high density or low density, and we can easily
ﬁnd three high concentrations. Figure 17d reveals that most densities of clusters are

Fig. 17 Choropleth maps with cluster regions. a Synthetic Dataset I. b Synthetic Dataset II.
c Synthetic Dataset III. d Real Dataset

Geoinformatica (2006) 10: 399–422

417

about the same although there are big discrepancies in the cluster sizes (area of the
regions they occupy).

4.3 Reasoning about Clusters

In Section 4.1 we argued that shaped areas are more informative than points, but
this is even more relevant for reasoning about regions (or clusters) since more spatial
relationships can be established. Clustering for data mining is to summarise the distri-
bution of P in order to suggest a manageable number of patterns of concentrations.
Here, clusters are representatives of the phenomena recorded with locations in P
and suggest interesting groups. Boundaries of clusters [30] and representatives of
clusters (medoids or means) [19] are possible candidates for reasoning about clusters.
However, these candidates are summaries. They constitute only partial information
about clusters, so we need special care when we use this information for cluster
reasoning.

Figure 18 illustrates the problem of using such partial information. Consider two
layers shown in Fig. 18a–b. Layer 1 displayed in Fig. 18a has a small cluster while
the Layer 2 depicted in Fig. 18b has two clusters, one small and one large. Black
dots represent medoids of clusters. Although the small cluster in Layer 1 has a high
association with the large cluster in Layer 2 (since the small cluster lies within the
large cluster), traditional approaches fail to detect this association. Cluster reasoning
using boundary matching [30] does not succeed in detecting this correlation, since
boundaries of the small cluster in Layer 1 do not match with those of the large cluster
in Layer 2. This is shown in Fig. 18c. Similarly, association analysis using medoids [19]
is unable to report this association, since the medoid of the small cluster in Layer 1
is closer to the medoid of the small cluster in Layer 2 than that of the large cluster in
Layer 2.

We are able to detect this relationship by transforming clusters to regions. Once
transformed, then we can use well founded region-based qualitative spatial reasoning
methods such as Region Connection Calculus (RCC) [6]. Note that a cluster is a set of
dimensionless points and there is no well studied reasoning method for dimensionless
points. RCC is based on a connection relation C(X, Y) that represents “region X
connects with region Y.” RCC consists of several families of binary topological
relations. One family RCC-5 uses a set of ﬁve pairwise base relations: DC(Discrete),
PC(Partial Overlap), EQ(Equal), PP(Proper Part) and PP−1(Inverse of PP). Using
RCC-5, now we can derive two topological relations from Fig. 18. These are as
follows:

1. DC(the small cluster in Layer 1, the small cluster in Layer 2)
2. PP(the small cluster in Layer 1, the large cluster in Layer 2)

Fig. 18 Reasoning about clusters. a Layer 1. b Layer 2. c Overlapping Layer 1 and Layer 2

418

Geoinformatica (2006) 10: 399–422

These relations clearly state that the small cluster in Layer 1 is more positively
associated with the large cluster in Layer 2 than the small one. The intuition of this
approach is that two clusters exhibit a high positive association if cluster 1 takes place
where cluster 2 occurs. Thus, cluster polygonization combined with RCC-5 provides
a robust framework for reasoning about clusters and detecting positive associations
among clusters.

4.4 Measuring Inter-cluster Distance

Measuring inter-cluster distance plays a critical role particularly in hierarchical
bottom-up (agglomerative) clustering and in cluster validity analysis. Agglomerative
clustering successively merges clusters until all clusters belong to the same cluster.
The merge is based on certain similarity measures. Some extreme points are typically
used to measure the closeness or remoteness. For instance, single-linkage clustering
employs two closest points, complete-linkage clustering uses two farthest points
while mean-linkage clustering works with two arithmetic means. Even though these
extreme point-based approaches are simple, inter-cluster distance is too sensitive
to the extremes. Some multiple point-based approaches [1], [23] have been pro-
posed to overcome this. Average-linkage clustering [1] requires a total involvement.
CURE [23] reduces this complexity by randomly choosing several representatives.
The total involvement may cause a computational overhead while the randomness
may mask hidden structures. Cluster polygonization is a middle ground of these
two approaches. It draws some representative border points based on geometrically
robust Delaunay Diagrams. Inter-cluster distance is measured as a sum of lengths of
Delaunay edges whose end points belong to two different clusters.

Cluster validity is another area where inter-cluster gap is of importance. It
is to quantitatively evaluate the goodness of a clustering algorithm. Here, com-
pactness and separation are two main categories proposed for evaluating cluster-
ing schemes [42]. In particular, separation is measured by inter-cluster distance.
Figure 19 shows a dataset exhibiting irregular margin (dataset taken from a cluster
validity analysis [20]). Inter-cluster distance varies from the shortest 0.3836 to the

Fig. 19 A dataset exhibiting irregular margin (||P|| = 1,453). a The dataset with its Delaunay
Diagram. b Two clusters detected by DBSCAN and Short–Long clustering. c Cluster boundaries.
d Some Delaunay edges representing inter-cluster edges

Geoinformatica (2006) 10: 399–422

419

longest 1.53637. Undoubtedly, these extreme values do not represent the separation
well, as shown in Fig. 19b. Cluster polygonization extracts border points and cluster
boundaries as shown in Fig. 19c. And then, it computes the inter-cluster distance
with Delaunay edges that stretch between the two clusters. Figure 19d depicts these
inter-cluster Delaunay edges. Inter-cluster distance is deﬁned as an average of these
inter-cluster Delaunay edges.

5 Final Remarks

Clustering methods are becoming more sophisticated and effective as they produce
high quality results without the need of users supplying good initial values. Post-
clustering processes that seek to identify positive associations are now in demand.
Detection of cluster boundaries is a natural choice for reasoning about clusters such
as matching boundaries with various feature data, for polygonizing clusters and for
mining associations.

We have described an automatic cluster boundary extraction method that is well
suited for data-rich environments because it avoids costly parameter-tuning. In truly
exploratory fashion, our methods ﬁnds shapes of clusters from the data, not by users
adequately supplying values for operational parameters. The automatic approach
derives the shape of a cluster not only from the distribution of points in the cluster,
but from that of points in different clusters. Thus, our approach are more informative
and avoids user bias. This is fundamental in exploratory spatial data analysis [38].
Our cluster boundary extraction allows cluster polygonization and we have described
four settings where this expands the potential for spatial analysis and spatial data
mining. Cluster polygonization is a new type of spatial tessellation that models
points with clusters and provides large-scale neighbouring. Choropleth mapping with
cluster regions as a base map exhibits unbiased visualisation. Further, intersecting
polygonized cluster regions enables us to ﬁnd positive associations among clusters.
Finally, cluster polygonization supports a robust measure of separation between
clusters, which can be used in agglomerative hierarchical clustering and cluster
validity.

References

1. M.S. Aldenderfer and R.K. Blashﬁeld. Cluster Analysis. Sage: Beverly Hills, 1984.
2. N. Amenta, S. Choi, and R. Kolluri. “The power crust,” in Proc. of the 6th ACM Symposium on

Solid Modeling and Applications, pp. 249–260, Ann Harbor, Michigan, 2001.

3. N. Amenta, S. Choi, and R.K. Kolluri. “The power crust, unions of balls, and the medial axis
transform,” Computational Geometry: Theory and Applications, Vol. 19(2–3):127–153, 2001.
4. N. Amenta, S. Choi, T.K. Dey, and N. Leekha. “A simple algorithms for homoeomorphic
surface reconstruction,” International Journal of Computational Geometry and Applications, Vol.
12:125–141, 2002.

5. P.A. Burrough. Principles of Geographical Information Systems for Land Resources Assess-

ment. Oxford University Press: New York, 1986.

6. A.G. Cohn, B. Bennett, J. Gooday, and N.M. Gotts. “Qualitative spatial representation and

reasoning with the region connection calculus,” GeoInformatica, Vol. 1(3):275–316, 1997.

7. L.F. da Costa. Shape Analysis and Classiﬁcation: Theory and Practice. CRC Press: Boca Raton,

FL, 2001.

8. B.D. Dent. Cartography: Thematic Map Design. 5th edition. WCB McGraw Hill: Boston, 1999.
9. T.K. Dey, J. Giesen, and S. Goswami. “Delaunay triangulations approximate anchor hulls,”
in Proc. of the 16th Annual ACM-SIAM Symposium on Discrete Algorithms, pp. 1028–1037,
Vancouver, British Columbia, January 23–25, 2005.

420

Geoinformatica (2006) 10: 399–422

10. T.K. Dey. “Sample based geometric modeling,” in Janardan, Smid and Dutta (Eds.), DIMACS

series in Discrete Mathematics and Theoretical Computer Science, Vol. 67, 2005.

11. T.K. Dey. “Curve and surface reconstruction,” in Goodman and O’ Rourke (Eds.), Chapter
in Handbook of Discrete and Computational Geometry, 2nd edition, CRC Press, Boca Raton,
FL 2004.

12. T.K. Dey, J. Giesen, and S. Goswami. “Shape segmentation and matching with ﬂow discretiza-
tion,” in F. Dehne, J.-R. Sack, and M. Smid (Eds.), Proc. Workshop Algorithms Data Strucutres
(WADS 03), LNCS 2748, pp. 25–36, Ottawa, Ontario, Canada, 2003.

13. H. Edelsbrunner, D. Kirkpatrick, and R. Seidel. “On the shape of a set of points in the plane,”

IEEE Transactions on Information Theory, Vol. 29(4):551–559, 1983.

14. C. Eldershaw and M. Hegland. “Cluster analysis using triangulation,” in B.J. Noye, M.D.
Teubner, and A.W. Gill (Eds.), Computational Techniques and Applications: CTAC97, World
Scientiﬁc: Singapore, pp. 201–208, 1997.

15. M. Ester, H.P. Kriegel, J. Sander and X. Xu. “A Density-Based Algorithm for Discovering
Clusters in Large Spatial Databases with Noise,” in E. Simoudis, J. Han, and U.M. Fayyad (Eds.),
Proc. of the 2nd International Conference on Knowledge Discovery and Data Mining, Portland,
Oregon, pp. 226–231, 1996.

16. V. Estivill-Castro and I. Lee. “Argument free clustering via boundary extraction for massive

point-data sets,” Computers, Environments and Urban Systems, Vol. 26(4):315–334, 2002.

17. V. Estivill-Castro and I. Lee. “Data mining techniques for autonomous exploration of large
volumes of geo-referenced crime data,” in David V. Pullar (Ed.), International Conference on
Geocomputation, 24–26, September, 2001, Brisbane, Australia, GeoCompuatation CD-ROM,
ISBN 1864995637, 2001.

18. V. Estivill-Castro, I. Lee, and A.T. Murray. “Criteria on proximity graphs for boundary extrac-
tion and spatial clustering,” in D. Cheung, Q. Li, and G. Williams (Eds.), Proc. of the 5th Paciﬁc-
Asia Conference on Knowledge Discovery and Data Mining, Hong Kong, China, pp. 348–357,
2001.

19. V. Estivill-Castro and A.T. Murray. “Discovering associations in spatial data—an efﬁcient
medoid based approach,” in X. Wu, K. Ramamohanarao, and K.B. Korb (Eds.), Proc. of the
2nd Paciﬁc-Asia Conference on Knowledge Discovery and Data Mining, Melbourne, Australia,
pp. 110–121, 1998.

20. V. Estivill-Castro and J. Yang. “Cluster validity using support vector machines,” in
Y. Kambayashi, M.K. Mohania, W. Wöß (Eds.), Proc. of the 3rd DaWak, LNCS 2737, pp. 244–
256, Springer: Berlin Heidelberg New York, 2003.

21. C.M. Gold. “Problems with handling spatial data - The Voronoi approach,” Canadian Institute

of Surveying and Mapping Journal, Vol. 45(1):65–80, 1991.

22. C. Gold and J. Snoeyink. “A one-step crust and skeleton extraction algorithm,” Algorithmica,

Vol. 30(2):144–163, 2001.

23. S. Guha, R. Rastogi, and K. Shim. “CURE: An efﬁcient clustering algorithm for large databases,”
in L.M. Haas and A. Tiwary (Eds.), Proc. of the ACM SIGMOD’98 International Conference on
Management of Data, Seattle, Washington, pp. 73–84, 1998.

24. J. Han, M. Kamber, and K.H. Tung. “Spatial clustering methods in data mining,” in H.J. Miller
and J. Han (Eds.), Geographic Data Mining and Knowledge Discovery, pp. 188–217, Cambridge
University Press: Cambridge, UK, 2001.

25. A.K. Jain and R.C. Dubes. Algorithms for Clustering Data. Prentice-Hall, 1988.
26. A.K. Jain, M.N. Murty, and P.J. Flynn. “Data clustering: A review,” ACM Computing Surveys,

Vol. 31(3):264–323, 1999.

27. I. Kang, T. Kim, and K. Li. “A spatial data mining method by Delaunay Triangulation,” in Proc.
of the 5th International Workshop on Advances in Geographic Information Systems, Las Vegas,
Nevada, pp. 35–39, 1997.

28. G. Karypis, E. Han, and V. Kumar. “CHAMELEON: A hierarchical clustering algorithm using
dynamic modeling,” IEEE Computer: Special Issue on Data Analysis and Mining, Vol. 32(8):68–
75, 1999.

29. L. Kaufman and P.J. Rousseuw. Finding Groups in Data: An Introduction to Cluster Analysis.

John Wiley: New York, 1990.

30. E.M. Knorr, R.T. Ng, and D.L. Shilvock. “Finding boundary shape matching relationships in
spatial data,” in M. Scholl and A. Voisard (Eds.), Proc. of the 5th International Symposium on
Spatial Databases, Berlin, Germany, pp. 29–46, 1997.

31. E. Kolatch. “Clustering algorithms for spatial databases: a survey,” in http://www.cs.umd.

edu/kolatch/papers/SpatialClustering.pdf, 2000.

Geoinformatica (2006) 10: 399–422

421

32. I.L. McHarg. Design with Nature. Natural History Press: New York, 1969.
33. K. Mehlhorn and S. Näher. LEDA A Platform for Combinatorial and Geometric Computing.

Cambridge University Press: Cambridge, UK, 1999.

34. H.J. Miller and J. Han. Geographic Data Mining and Knowledge Discovery: An Overview.

Cambridge University Press: Cambridge, UK, 2001.

35. R.T. Ng and J. Han. “Efﬁcient and effective clustering method for spatial data mining,” in J. B.
Bocca, M. Jarke, and C. Zaniolo (Eds.), Proc. of the 20th International Conference on Very Large
Data Bases, Santiago de Chile, Chile, pp. 144–155, 1994.

36. A. Okabe, B.N. Boots, K. Sugihara, and S.N. Chiu. Spatial Tessellations: Concepts and Applica-

tions of Voronoi Diagrams. 2nd edition, John Wiley: West Sussex, 2000.

37. S. Openshaw. “A Mark 1 geographical analysis machine for the automated analysis of point data

sets,” International Journal of Geographical Information Science, Vol. 1(4):335–358, 1987.

38. S. Openshaw. “Two exploratory space–time-attribute pattern analysers relevant to GIS,” in A.S.
Fotheringham and P.A. Rogerson (Eds.), Spatial Analysis and GIS, pp. 83–104, Taylor & Francis:
London, 1994.

39. S. Openshaw. “Geographical data mining: Key design issues,” in Proc. of the 4th International

Conference on Geocomputation, 1999.

40. W.L. Roque and D. Doering. “Constructing approximate Voronoi Diagrams from digital images
of generalized polygons and circular objects,” in The 11th International Conference in Central
Europe on Computer Graphics, Visualization and Computer Vision Formely Winter School of
Computer Graphics WSCG03 February 3-7 Plzen, Czech Republic. UNION agency-Science
Press, 2003.

41. W.R. Tobler. “A computer movie simulating urban growth in the Detroit region,” Economic

Geography, Vol. 46(2):234–240, 1970.

42. M. Vazirgiannis, M. Halkidi, and Y. Batistakis. “On clustering validation techniques,” Journal of

Intelligent Information Systems, Vol. 17(2–3):107– 145, 2001.

43. W. Wang, J. Yang, and R.R. Muntz. “STING+: An approach to active spatial data mining,” in
Proc. of the 15th International Conference on Data Engineering, pp. 116–125, IEEE Computer
Society Press, Los Alamitos, CA 1999.

44. B. Zhang, M. Hsu, and U. Dayal. “K-harmonic means—A spatial clustering algorithm with
boosting,” in J.F. Roddick and K. Hornsby (Eds.), Proc. of the International Workshop on
Temporal, Spatial and Spatio-Temporal Data Mining, Lyon, France, pp. 31–45, 2000.

45. T. Zhang, R. Ramakrishnan, and M. Livny. “BIRCH: An efﬁcient data clustering method for
very large databases,” in H.V. Jagadish and I.S. Mumick (Eds.), Proc. of the ACM SIGMOD’96
International Conference on Management of Data, Montreal, Canada, pp. 103–114, 1996.

46. X. Zhou, D. Truffe, and J. Han. “Efﬁcient polygon amalgamation methods for spatial OLAP
and spatial data mining,” in Advances in Spatial Databases, 6th International Symposium, SSD
LNCS 1651, pp.167–187, Springer, Berlin Heidelberg New York, 1999.

422

Geoinformatica (2006) 10: 399–422

Dr. Ickjai Lee received his Ph.D. degree in 2002 from the School of Electrical Engineering and
Computer Science, University of Newcastle, in Australia. After his Ph.D., he joined the Business
and Technology Laboratory in the University of Newcastle as a postdoctoral research fellow
working on four-legged robot soccer and intelligent agent projects. Dr. Lee has been a lecturer
in the School of Information Technology at James Cook University in Australia since 2003. His
research interests include clustering, multiple classiﬁers, spatial databases, geographical data mining,
knowledge representation and conceptual spaces.

Professor Vladimir Estivill-Castro obtained his Ph.D. in 1991 from the Department of Computer
Science, U. of Waterloo, in Canada. After a year as Assistant Professor at York U., Toronto,
Canada, Estivill-Castro joined consulting (in LANIA, an industry sponsored research laboratory) as
a project leader and senior-researcher. In 1996, he rejoined academic life at Queensland University of
Technology in Australia. He has demonstrated leadership in a research environment. He developed
the Machine Intelligence and Pattern Analysis Lab (Mi-PAL) at Grifﬁth University and engaged
many of the existing research staff in projects in the areas of robotics, image processing and
machine learning under the research agenda of the lab. His commitment to fostering a research
culture and his track record has resulted in an invitation to chair the program committee for the
Australian Computer Science Conference. Estivill-Castro is currently Professor at Grifﬁth University
where he has established multi-disciplinary research collaborations. His diverse research interests
include Security through Data Mining, Privacy, Machine Learning, Knowledge Discovery and Data
Mining, Spatial Databases and Geographical Information Systems, Computational Geometry and
Algorithms, Computer Vision and Robotics. He is internationally recognised for his work on Spatial
Data Mining, Clustering and Privacy in Data Mining. He is a member of the editorial board of the
Int. Journal of Data Warehousing and Mining (IJDWM).

