International Journal of Geographical Information
Science

ISSN: 1365-8816 (Print) 1362-3087 (Online) Journal homepage: http://www.tandfonline.com/loi/tgis20

An incremental construction algorithm for
Delaunay triangulation using the nearest-point
paradigm

Borut Žalik & Ivana Kolingerová

To cite this article: Borut Žalik & Ivana Kolingerová (2003) An incremental construction
algorithm for Delaunay triangulation using the nearest-point paradigm, International Journal of
Geographical Information Science, 17:2, 119-138, DOI: 10.1080/713811749

To link to this article:  http://dx.doi.org/10.1080/713811749

Published online: 03 Dec 2010.

Submit your article to this journal 

Article views: 104

View related articles 

Citing articles: 10 View citing articles 

Full Terms & Conditions of access and use can be found at
http://www.tandfonline.com/action/journalInformation?journalCode=tgis20

Download by: [University of Newcastle]

Date: 01 March 2016, At: 02:16

. .   , 2003
. 17, . 2, 119–138

Research Article

An incremental construction algorithm for Delaunay triangulation
using the nearest-point paradigm

BORUT Zˇ ALIK† and IVANA KOLINGEROVA´ ‡
†University of Maribor, Faculty of Electrical Engineering & Computer Sciences,
Department of Computer Science, Maribor, Slovenia; e-mail: zalik@uni-mb.si
‡University of West Bohemia, Faculty of Applied Sciences, Department of
Computer Science and Engineering, Plzen, Czech Republic

(Received 23 September 2000; accepted 20 May 2002)

Abstract. This paper introduces a new algorithm for constructing a 2D Delaunay
triangulation. It belongs to the class of incremental insertion algorithms, which are
known as less demanding from the implementation point of view. The most time
consuming step of the incremental insertion algorithms is locating the triangle
containing the next point to be inserted. In this paper, this task is transformed to
the nearest point problem, which is solved by a two-level uniform subdivision
acceleration technique. Dependencies on the distribution of the input points are
reduced using this technique. The algorithm is compared with other popular triangu-
lation algorithms: two variants of Guibas, Knuth, and Sharir’s incremental insertion
algorithm, two diﬀerent implementations of Mu¨cke’s algorithm, Fortune’s sweep-
line algorithm, and Lee and Schachter’s divide and conquer algorithm. The following
point distributions are used for tests: uniform, regular, Gaussian, points arranged in
clusters, and real data sets from a GIS database. Among all tested algorithms, the
divide and conquer approach turns out to be the best. The proposed algorithm is
the second fastest except for input points with highly non-uniform distribution. As
implementation of the algorithm is simple, it represents an attractive alternative to
other Delaunay triangulation algorithms used in practice.

1.

Introduction
In many engineering applications and especially in GIS, a triangulation of a
set of points in 2D represents a fundamental problem. Many diﬀerent triangu-
lations can be constructed on the given set of points, and among them Delaunay
triangulation is the most popular. There are two main reasons for that:

1. Delaunay triangulation is the optimal triangulation regarding the minimal
interior angle of all constructed triangles—a property, which assures a good
shape of triangles. A simple criterion called an empty circle property was
invented by Delaunay in 1934. This criterion minimizes the number of sliver
triangles (Aurenhammer 1991).

2. For Delaunay triangulation, a lot of algorithms exist (the most popular
approaches are considered brieﬂy in the next section) with an optimal expected
time complexity O(n log n), where n is the number of points being triangulated
(Su and Drysdale 1995).

International Journal of Geographical Information Science
ISSN 1365-8816 print/ISSN 1362-3087 online © 2003 Taylor & Francis Ltd
http://www.tandf.co.uk/journals
DOI: 10.1080/13658810210157813

Downloaded by [University of Newcastle] at 02:16 01 March 2016 120

B. Zˇ alik and I. Kolingerova´

Let us consider the most desired properties of an algorithm constructing Delaunay

triangulation.

$ The algorithm should be fast. The number of points being triangulated can be
huge especially in GIS applications. Therefore, the algorithm should not only
have a good theoretical time complexity, but also its actual run time should
be short.

$ The algorithm should not be memory demanding. The low memory requirements
are still among the most desired properties despite a low cost of the physical
memory today. Theoretical space complexity of all existing triangulation algo-
rithms does not exceed O(n), but, in practice, the actual memory requirements
diﬀer greatly.

$ The run-time of the algorithm should not depend substantially on the distribution
of the input data. In engineering practice, triangulated points are frequently
distributed non-uniformly. The algorithm should cope eﬃciently with expected
point distributions.

$ The implementation should not be too demanding. Complicated solutions lead
to a demanding implementation, which may represent a challenge even for
well-experienced programmers. Because of that, developers often choose a less
eﬃcient but easier to implement solution.

$ The algorithm must be robust. All geometric algorithms using ﬂoating-point
arithmetic tend to be unstable because of the ﬁnite precision of a digital
computer (Hoﬀmann 1989). The steps in the algorithm, which may produce
incorrect or inconsistent results under some conditions have to be implemented
with a special care (Shewchuk 1996).

In this paper, we are presenting a new algorithm for Delaunay triangulation
construction. Low run time of the proposed algorithm ranks it with the fastest
existing triangulation algorithms available today, except for a highly non-uniform
point distribution. Its memory requirements stay in acceptable limits, its implementa-
tion is not too demanding, and it is computationally stable. Although its run time
it copes eﬃciently with the distributions
depends on input point distributions,
met in practice. The paper is organized as follows: §2 gives an overview of previous
work. In §3, the algorithm is explained in detail. Section 4 gives good results and
comparisons. The paper is summarized in §5.

2. Previous work

Because of its importance in practice, Delaunay triangulation has been an active
research area for a long time. In this part we mention just the most popular
approaches classiﬁed into three main categories:

1. Divide and conquer algorithms. At ﬁrst, the input set of points is divided into
smaller sets (vertical strips, squared buckets of points), triangulated individu-
ally, and then merged into the ﬁnal triangulation. The main drawback of this
approach is the demanding implementation (see the solutions proposed by
Lee and Schachter (1980), Dwyer (1987), and Katajainen and Koppinen
(1988)).

2. Sweep-line algorithms. A popular sweep-line algorithm has been proposed by
Fortune (1987) and later outlined by de Berg et al. (1997). Actually, Fortune

Downloaded by [University of Newcastle] at 02:16 01 March 2016 Delaunay triangulation using the nearest-point paradigm

121

developed his algorithm to construct the dual graph of Delaunay triangula-
tion—a Voronoi diagram. The algorithm has to maintain two data structures:
queues of point events and of circle events. The implementation is again not
straightforward (Cˇ uk 1999).

3. Incremental algorithms. The algorithms from this group are the simplest to
implement and therefore the most frequently used, although they are not
worst-case optimal. Their worst case time complexity tends to be O(n2), but
expected time complexity is O(n log n). They can be classiﬁed further into two
categories:

$ Incremental search algorithms add one valid triangle to the current triangu-
lation at a time. The basic task of the algorithm is to search for a point,
which could be joined with the boundary vertices of the already constructed
triangulation while preserving the Delaunay criterion. For this task, diﬀerent
searching strategies have been proposed (Cignoni et al. 1992, Fang and
Piegl 1992, 1993).

$ Incremental insertion algorithms have a long tradition (Lawson 1977). They
consecutively insert points into the existing triangulation, and change it
after that to satisfy the Delaunay criterion. All algorithms employing this
approach perform the following tasks: ﬁrst, the triangle containing the next
point to be inserted is found. The triangle is split into three new triangles.
These triangles are checked recursively against the empty circle property
with their neighbouring triangles. If this property is violated, the common
edge of the two tested triangles is ﬂipped (see ﬁgure 1) until the Delaunay
criterion is assured using Lawson’s local optimisation procedure (Lawson
1977). This process is known as legalization.

Surprisingly, the most time demanding task in planar Delaunay triangulation is not
the legalisation but locating the triangle containing the point. For that, diﬀerent
solutions have been tried:

—Guibas and Stolﬁ (1985) start at a random edge in the current Delaunay
triangulation, and perform a walk around the edges in the direction of the
new point until the triangle is found. The idea has been later improved by
Mu¨ cke et al. (1996). They obtain the starting point for the walk by a random

Figure 1. Edge ﬂip at incremental insertion algorithms.

Downloaded by [University of Newcastle] at 02:16 01 March 2016 122

B. Zˇ alik and I. Kolingerova´

sampling. The sample consists of m<n randomly selected input points. The
nearest sample point is then used as the initial point for searching the
triangle. This algorithm achieves O(n5/4) to O(n4/3) time for uniformly distrib-
uted points. This walking method has also been used for a point location
in a polygonal approximation of terrains, as suggested by Garland and
Heckbert (1995).

—Sloan (1987) starts the search from the most recently located triangle and
walks in the direction of the inserted point. To accelerate the search, the
points are sorted in the way that the distance between a point and its
predecessor remains small. For this purpose, a uniform grid of cells is used.
Each point is associated with an index of the cell, which contains this point,
and then the points are sorted according to this index. According to the
theoretical estimate, the algorithm achieves O(n5/4) on average (for large n)
with sort, O(n3/2) without sort, and in the worst case O(n2). In practical use,
tests show about O(n1.1) time. Improvements to Sloan’s algorithm have been
suggested recently by Huang and Shin (1998) resulting in approximately
40% of CPU time reduction for randomly distributed points. They intro-
duced the following improvements: calculations in searching procedure are
replaced by comparisons, triangles related to a super-triangle are not tested
for Delaunay criterion, reduction of empty-circle-criterion has been achieved
by subdividing the domain into m×m bins (m is the closest integer of n0.25).
—Guibas, Knuth, and Sharir (1992) proposed probably the most popular
approach using a directed acyclic graph (DAG). Their algorithm is also
described by de Berg et al. (1997). The DAG contains all the triangles
already divided or deleted. Triangles of the current Delaunay triangulation
are stored in the leaves of the graph. This approach enables an eﬃcient
O(log n) time for locating the triangle containing the current point, but on
the other hand, it is also very memory demanding.

—Su and Drysdale (1995) used a uniform subdivision and showed that their
approach works almost in linear time when points are uniformly distributed.
Each cell of the uniform subdivision contains only one point—the one last
inserted in the cell. This point is then used as the starting point for walking
towards the next point to be inserted.

3. Triangulation algorithm

The algorithm presented in the following text belongs to the class of the

incremental insertion algorithms (see §2). It consists of the following steps:

(a) Initialization includes two steps:

$ Preparing the input data. To avoid the worst case of the incremental inser-
tion algorithms, the input points have to be inserted in a random order
(de Berg et al. 1997). A simple randomization function makes the algorithms
less sensitive to the insertion order of the input points.

$ Initialization of the algorithm. In this step, parameters of the searching data
structure are set, an artiﬁcial triangle (named also a super-triangle) embed-
ding the input points is created, and the ﬁrst point is inserted into the
initial triangulation. Details are given in §3.2.

Downloaded by [University of Newcastle] at 02:16 01 March 2016 Delaunay triangulation using the nearest-point paradigm

123

(b) Triangulation. The remaining points are incrementally inserted into the current

triangulation by the following tasks:

$ Find the triangle containing the next point to be inserted. Our algorithm is
based on the assumption that the nearest vertex to the inserted point
represents a good start for ﬁnding such a triangle. For that, a geometric
searching structure called two-level uniform subdivision is used. Details are
given in §3.1.

$ Preserving the Delaunay triangulation. In this step, the algorithm updates
the triangulation by splitting the triangle found in the ﬁrst step. After that,
by a recursive process of the legalisation, Delaunay criterion is reassured.
Details are given in §3.3.

(c) Finalization. Triangles deﬁned by the vertices of the artiﬁcial triangle are
eliminated, the triangulation is stored, and occupied memory is released.

In the next sections, details of the proposed algorithm are given.

3.1. Finding the triangle containing the next point to be inserted

i

Let us have a set S of n points p

, 0∏i<n in 2D and let us denote DT (S) a
Delaunay triangulation constructed on S. The points are usually called Delaunay
vertices (we will denote them by indices i, i=0, 1, ..., n−1), and edges connecting
these vertices Delaunay edges (denoted by pairs of indices i−j, i=0, 1, ..., n−1,
j=0, 1, ..., n−1, i≠j). Let DT (S
) be the Delaunay triangulation constructed on m
m
points, m<n, and let p
).
Let us assume that at each Delaunay vertex i, a set of triangles T
, incident on this
vertex, is available. To locate a triangle t e DT (S
, the following Lemma
is used:

, i=m+1<n be the next point to be inserted in the DT (S

) containing p

m

m

i

i

i

Lemma:

Let j, 0<j<i, i<n, be the nearest Delaunay vertex to the point p
the probability that one of the triangles t e T
denotes the second nearest vertex to p
then P(k)<P( j ).
contains p

. P( j ) denotes
. If k, 0<k<i, k≠i
i
and P(k) is the probability that t e T

contains p

k

j

i

i

i

i

i

i

The proof can be done easily using the properties of Voronoi diagrams (Preparata
and Shamos 1985). The Lemma tells that it is rational to start the search for the
triangle containing point p
at the nearest Delaunay vertex. If none of the triangles
incident to the vertex contains the tested point, the second nearest point is taken
is tested again using the surrounding triangles. This procedure continues
and point p
until the triangle containing point p
is found. As can be seen, the search for the
corresponding triangle is transformed to one of the classical problems of the computa-
tional geometry—the nearest point search. It has been addressed by many authors.
Bentley et al. (1980) assigned each point to a small cell of area C/n, where C is the
number of points in each cell. This approach is known as a uniform plane/space
subdivision—USP (Preparata and Shamos 1985). They have proved that the nearest
point problem can be solved within USP in the optimal expected-time O(1), if C%n.
Of course, if the distribution of the points is expected to be considerably non-
uniform, the performance of the USP gets worse. In such a case, an adaptive space
subdivision (also called hierarchical space subdivision) gives better results. A huge
collection of suitable data structures has been given by Samet (1989, 1990). The
maintenance of adaptive subdivisions requires O(log n). To improve the performance

Downloaded by [University of Newcastle] at 02:16 01 March 2016 124

B. Zˇ alik and I. Kolingerova´

of the USP for a non-uniform data distribution, combinations of the uniform and
the adaptive subdivision have also been proposed. For example, in the survey of
ray-generators for ray-tracing algorithms, Endl and Sommer (1994) mentioned the
possible combinations of hierarchical data structures (octrees) with eﬃcient algo-
rithms based on the uniform subdivision (Fujimoto et al. 1986, Sung 1991). However,
this approach (uniform subdivision at the ﬁrst level and quadtrees at the second)
seems not to be suitable for our triangulation algorithm. There are at least two
reasons for that:

$ Two completely diﬀerent data structures are used and therefore, two diﬀerent

sets of algorithms maintaining them are needed.

$ The nearest point cannot always be found eﬃciently in such a structure. The
problem occurs in searching the neighbouring cells. The tree structures located
in these cells have to be traversed from the top to the bottom before the leaves
storing the points would be reached. Of course, additional links could be
introduced between the leaves of neighbouring quadtrees. But, even with
these links it would be diﬃcult to jump directly between neighbouring cells
because there may not be a 1:1 relation between the neighbouring quadtrees
(the neighbouring cells would probably be divided into diﬀerent depths).
Clearly, the implementation of such data structure would be very demanding.

In our case, a simple extension to the standard uniform subdivision named a two-
level uniform subdivision (US2l) has been used. The idea is very simple, at ﬁrst the
uniform subdivision is established. As the points are inserted incrementally, some of
the cells become overpopulated, and these cells are divided uniformly once again.
There is an additional argument for using the variant of the uniform subdivision. In
some cases, also the next nearest vertices to the point p
are needed and it can be
implemented very eﬃciently using US2l.

i

Of course, one could suggest introducing even more levels. We have considered
this possibility, too. However, the results have not been encouraging because of
additional time spent for memory allocation for new levels. The cells at the lowest
levels are not entirely ﬁlled in the majority of cases (frequently, the lowest levels
contain only one point), and in this way the loss of CPU time is not compensated
by a faster solution to the nearest point problem.

3.2. Initialization of two-level uniform subdivision

To determine the size and the number of cells at the ﬁrst level of the uniform
subdivision, a heuristic similar to those proposed by Fang and Piegl (1993) and
Zˇ alik (1999) is used. It takes into account the number of input points n and the
bounding box (BoxXMin, BoxY Min, BoxXMax, BoxY Max) of these points.

NoOf CellsX

NoOf CellsY

v

2

=uratioXY √n
2ratioXYv
=u √n

1

1

ratioXY=

BoxXMax−BoxXMin
BoxY Max−BoxY Min

(1)

(2)

where

Downloaded by [University of Newcastle] at 02:16 01 March 2016 Delaunay triangulation using the nearest-point paradigm

125

Then the size of the cells is obtained by equation (3):

XSize

=

Y Size

=

BoxXMax−BoxXMin
NoOf CellsX
BoxY Max−BoxY Min
NoOf CellsY

1

1

1

1

If a cell contains too many points, we want to divide it into one more level. The
number of points when the cell should be subdivided into the second level is called
T hreshold (see equation 4) and was found experimentally.

T hreshold=

NoOf CellsX

1

4n
×NoOf CellsY

1

The cells at the ﬁrst level are subdivided into an equal number of cells in the x and
y directions denoted by SmallNoOf Cells. Again, by experiments SmallNoOf Cells is
set to 4 giving us 16 new cells at the second level. In general, when a larger number
of SmallNoOf Cells is used, the initialisation time predominates the speed-up of the
nearest point search, and for a smaller number of cells, the eﬀect of the second level
is reduced.

Beside initialization already described, only three functions are needed to support

our triangulation algorithm, and they are discussed brieﬂy below.

1. Inserting a point into two-level uniform subdivision. At each cell of the ﬁrst
level, an array of length T hreshold is allocated for the points (actually for their
indices) being inserted. This space is dynamically allocated when the ﬁrst point
falls in the cell. The cell at the ﬁrst level is determined using equation (5).

v
PosX=ux−BoxXMin
v
PosY=uy−BoxY Min

XSize1

Y Size1

(3)

(4)

(5)

i

2. The nearest vertex search. The nearest point search to point p

If this cell contains T hreshold points, the second level is introduced. Points at
the second level are stored in single-linked lists. The decision into which cell
should be inserted is made according to an equation similar to
the point p
into US2l is done in the constant time O(1).
equation (5). Inserting point p
in the uniform
subdivision is usually performed by the so-called spiral algorithm (Bentley
et al. 1980, Preparata and Shamos 1985, Fang and Piegl 1993). Another
possibility (usually not tied with the nearest point problem) is to use the idea
of the ﬂood-ﬁll algorithm (Foley et al. 1990) and it has been used in our
algorithm. Its implementation is much easier in the case of US2l than the
implementation of the spiral algorithm. The pseudocode is given below.

i

i

TheNearestVertex( p
//Function ﬁnds the nearest vertex v to point p

, v)

i

cell=FindTheCellAtTheFirstLevel( p
distance=2;
FloodFill( p

, cell, v, distance);

i

end TheNearestVertex;

i

i

); //equation (5) is used

Downloaded by [University of Newcastle] at 02:16 01 March 2016 126

B. Zˇ alik and I. Kolingerova´

FloodFill ( p
//Function ﬁnds the nearest vertex v to point p

, cell, v, distance)

i

having distance distance in

i

//US2l cell cell

begin

if cell is subdivided then

cell2=FindTheCellAtTheSecondLevel( p
, cell2, v, distance);
FloodFill( p

i

);

end
FindTheNearestVertexInCell(cell, p

, d, v);

i

//the Euclidean distance to each vertex in the cell
//is found; the distance and the vertex is returned

neighbour=SelectTheNextNon-VisitedCell(d, cell);

//the next non-visited cell from cell
//cell closer than distance d is found

if neighbour ( NILL then

FloodFill( p

, neighbour, v, distance);

i

i

end FloodFill

At the beginning when just a few points exist in US2l, the expected number
of empty cells visited is high, causing a lot of additional redundant calculations.
Because of that, we use the brute force search for the ﬁrst 500 points (this
number was determined by experiment).

3. The next nearest vertex search. All vertices being examined during the search
. The
for the nearest vertex are stored together with their distances to point p
point with the minimal distance is selected among them before the ﬂood-ﬁll
algorithm is activated. In this case, the inspection of the non-visited cells is
limited eﬃciently by the candidates for the next nearest vertex.

i

3.3. Constructing Delaunay triangulation

The algorithm for constructing Delaunay triangulation is described very brieﬂy
insertion triangulation
because it is in principle the same as other incremental
algorithms (details can be found in de Berg et al. 1997). To unify the implement-
ation, a big artiﬁcial triangle is ﬁrst created in such a way that all points from S are
embedded by it. The three artiﬁcial points p
are determined as
follows:

, and p

n+1

n+2

, p

n

=(kM, 0),

p

n

p

n+1

=(−kM, −kM),

=(0, kM)

p

n+2

(7)

where M is deﬁned by equation (8) and k is a small positive integer constant.

M=max{abs(BoxXMin), abs(BoxY Min), abs(BoxXMax), abs(BoxY Max)}

(8)

As suggested by de Berg et al. (1997), a suitable value for k=3. After the artiﬁcial
triangle has been created, the ﬁrst point from S is inserted. Figure 2 shows the
content of our data structure after the initialisation. A widely used data structure
for triangulations consists of storing, for each triangle, its three vertices and its three
adjacent triangles (Lawson 1977). Besides that, a list of pointers to triangles incident
to the considered vertex is stored (ﬁgure 2).

Downloaded by [University of Newcastle] at 02:16 01 March 2016 Delaunay triangulation using the nearest-point paradigm

127

Figure 2. Contents of the data structure after inserting the ﬁrst point into the triangulation.

All the points from S are now inserted in a random order as explained in the

pseudocode below:

Triangulate( p
begin

i

, DT (S))

i

2

1

, t

, t

FindTriangleContainingPoint( p

); // FloodFill algorithm is used for
// locating the nearest vertex, after that point-in-triangle test is executed;
// one or two triangles can be found
=NIL then // point p
i
, t
SplitTriangle(t

are obtained

1
, t

if t

, t

is strictly inside the triangle t
) // three new triangles t
, t
a
and t
2

is on the border edge shared by t
, t

else // point p

, t

2

1

1

b

b

a

c

c

i

SplitTwoTriangles(t

, t
c
SetAndUpdatePointersToNeighbours(t
// triangles to the split triangle t
// triangles are updated (see ﬁgure 2)

, t
d
, t
2
1
(and t
2

, t

, t

1

2

1

a

b

); // four new triangles are obtained
, t
// at neighbouring
a
), the pointers to the neighbouring

, t

, t

, t

);

b

d

c

RemoveTrianglePointersFromTheIncidentVerticesList(t

, t

);

// pointers

to

// triangle t

(and t

1

RemoveTrianglesFromDT(DT (S), t

2

) being split are removed
, t

); // triangle t

// from data structure

// data structure

1

2

a

b

c

d

InsertNewTrianglesToDT(DT (S), t

, t

, t

, t

); // new triangles are inserted into

1

2

(and t

) are removed

1

2

AppendTrianglePointersToTheIncidentVertices(i, a, b, c, d, t

); // new
// pointers to new triangles are inserted to the list of surrounding triangles
// at triangle vertices (ﬁgure 2)

, t

, t

, t

a

b

d

c

LegalizeNewTriangles(t
a

, t
b

, t
c

, t
d

); // the Delauney criterion is assured recursively

end

As seen from the pseudocode above, the last function is legalization of the newly
created triangles. The legalization executes the empty circle property, and swaps the

Downloaded by [University of Newcastle] at 02:16 01 March 2016 128

B. Zˇ alik and I. Kolingerova´

common edge of two tested triangles in the case when this test is violated as shown
in ﬁgure 1 (Lawson 1977). During the legalization, we have to consider the cases
when at least one vertex is an artiﬁcial one. The approach suggested by de Berg
et al. (1997) operates only with the indices of vertices. However, in this way, some
boundary triangles could be left out and the obtained triangulation is not complete.
The reason is that the artiﬁcial points are not inﬁnitely far, and they inﬂuence the
input points. If they were inﬁnitely far, the algorithm would become numerically
unstable. We suggest another, more stable solution in the continuation.

Let us have two triangles Dijk and Djil sharing edge i−j. The following cases

can occur:

$ None of vertices belongs to the artiﬁcial triangle. This is the most common case

and it is solved by the empty circle test.

$ Both vertices i and j belong to the artiﬁcial triangle (i.e. common edge is one of
the edges of the artiﬁcial triangle). This case is not possible because the edge
of the artiﬁcial triangle does not belong to more than one triangle.

$ One of vertices k, l belongs to the artiﬁcial triangle. Edge i−j is taken as a

correct edge and legalization is not performed.

$ One of vertices i, j belongs to the artiﬁcial triangle. This case causes problems
if it is not considered adequately. In ﬁgure 3, two diﬀerent sets of points are
given and only a few important triangles are shown. Let us consider an example
where two triangles Dinj and Djnk are tested for the Delaunay criterion (vertex
n is an artiﬁcial vertex). As seen in ﬁgure 3(a), and ﬁgure 3(b), the legalization
(i.e. edge ﬂipping) is needed, otherwise triangle Dikj would be missed in the
ﬁnal triangulation (i.e. one of the edges on the convex hull of S). The empty
circumcircle test should not be used here. For example, in ﬁgure 3(a), the
legalisation would be required, because vertex k is inside the circle deﬁned by
triangle Dinj. But, in ﬁgure 3(b), vertex k is outside the circle, and edge ﬂip
would not be executed. Because of that, a diﬀerent test is applied:

Let triangles Dinj and Djnk be triangles with one common artiﬁcial vertex
and let us denote that vertex by n. Let us construct a line through the triangle
non-artiﬁcial vertices (i and j for example). If vertex k is positioned on the
same half-plane as the artiﬁcial vertex, the legalisation has to be performed.

This test can be done easily by calculating vector products (equation 9).

vp

vp

1

2

=( p
=( p

−p
−p

)×( p
)×( p

j

j

i

i

−p
−p

)

)

j

j

n

k

(9)

1

and vp

have the same sign, vertices n and k are located at the same side
If vp
of the line deﬁned by vertices i and j, and the common edge has to be ﬂipped—
legalised. Otherwise, the legalization should not be performed.

2

4. Analysis of the algorithm

The ﬁrst part of this section estimates the time and space worst-case complexity
of the presented algorithm and in the second part, the presented algorithm is
compared with some other popular Delaunay triangulation algorithms using various
distributions of input points.

Downloaded by [University of Newcastle] at 02:16 01 March 2016 Delaunay triangulation using the nearest-point paradigm

129

Figure 3. Problems at the legalization of triangles determined by artiﬁcial vertices.

4.1. T he worst case time and space complexity

As explained in §2, the most time consuming part of

insertion
Delaunay triangulation algorithms represents a location of the triangle containing
the current point. As explained in previous sections, the nearest point paradigm is
employed for that task. To speed-up the nearest point (or the next nearest points)
location US2l searching structure is applied. The proposed algorithm would achieve
the worst case time complexity if all input points except one are located in one cell.
In this case, the time complexity reach O(n2).

incremental

: number of bytes required by a record storing a triangle, h

Let us consider the space complexity and let us introduce the following variables:
: number of bytes
: number of
: number of bytes for the list of vertices stored

h
t
required by a record pointing to the triangle incident on a vertex, h
bytes occupied by one US2l cell, h
in the second level.

L1

L2

p

t

(2(n+3)−2−3)=h

It is well known that a Delaunay triangulation consists of 2n−2−k triangles,
where n is the number of points being triangulated, and k is the number of points
determining the convex hull of those points (Preparata and Shamos 1985). Because
we added three artiﬁcial points, the total number of points is n+3. The artiﬁcial
points ensure that there are exactly three points on the convex hull, thus k=3.
(n)=
From that, the largest number of triangles, which needs to be stored is S
(2n+1). At each Delaunay vertex, there is a list of pointers to
h
t
the list of triangles incident on the vertex (see ﬁgure 2). Because each triangle is
(2n+1).
deﬁned by three vertices, space needed for storing these pointers is S
The space for the US2l searching structure at the ﬁrst level is always allocated. From
equation (1) it is clear that we need S
. If at each cell at least one point
is located, T hreshold memory location is allocated. Therefore, the total space taken
+T hreshold ). The second level
by the ﬁrst level is in the worst case S
is introduced according to the distribution of points. In the worst case, at each cell
(n)=(cid:1)NoOf SmallCells n.
of the ﬁrst level, NoOf SmallCells is allocated. Therefore, S
In the cells at the second level, the vertices are stored in a single-linked list and for
it S

n space is needed. The common space complexity is therefore:

L1
(n)=(cid:1)n(h

(n)=(cid:1)nh

(n)=3h

(n)=h

L1

L1

L1

L2

p

p

t

L

L2

S(n)=S

(n)+S

(n)+S

(n)+S

(n)+S

(n)=O(n)

p

L1

L2

L

t

(10)

Downloaded by [University of Newcastle] at 02:16 01 March 2016 130

B. Zˇ alik and I. Kolingerova´

4.2. Practical results

The time complexity of the algorithms is often analysed only from a purely
theoretical point of view. However, this analysis may not provide a realistic image
about the behaviour of the algorithm. For example, an algorithm can have worse
asymptotic time complexity than another one, but it can be more eﬃcient for real
data sets. Therefore, the actual run time of the proposed Delaunay triangulation
algorithm (US2l algorithm) is compared with the following algorithms:

1. The proposed algorithm, where instead of US2l data structure, the classical
uniform space partition (having only one level) has been used (denoted as
USP in the following discussion).

2. The incremental insertion algorithm using DAG proposed by Guibas et al.

(1992). Two variants of that algorithm are used:

$ The original Guibas, Knuth, and Sharir algorithm implemented by us where
the point-in-triangle test has been performed by barycentric coordinates—
i.e. a planar point is given by a linear combination of triangle vertices, if
any of the barycentric coordinates are negative, the point lies outside the
triangle (Farin and Hansferd 1998)
(in the following; the algorithm is
denoted as GKS-BC).

$ The improved version of Guibas, Knuth, and Sharir (1992) algorithm

proposed by Kolingerova´ and Zˇ alik (2002) (denoted as GKS-KZ).

3. The Shewchuk’s (2001) improved implementation of the walking algorithm
by Mu¨ cke et al. within his triangulation package T riangle (the algorithm is
denoted as GSM-S).

4. The Mu¨ cke et al. (1996) improvement of the Guibas and Stolﬁ (1985) walking

algorithm implemented by us (the algorithm is denoted as GS-M).

5. Fortune’s (1987) sweep-line algorithm implemented by Shewchuk in T riangle

(denoted as F-S).

6. Lee and Schachter’s (1980) divide and conquer algorithm implemented by

Shewchuk (1996, 2001) in T riangle (denoted as D-S).

Shewchuk’s (1996, 2001) set of triangulation algorithms is considered frequently
as the reference triangulation implementation today and, therefore, we use it too
(Devillers 1998, Boissonnat et al. 2000). We have implemented also the incremental
search algorithm proposed by Fang and Piegl (1993), but the algorithm turned out
to be very slow, and despite our intensive work, it remained numerically unstable.

4.2.1. CPU measurements

All measurements were made on a Dell Precision 410 computer with Pentium
III 700 MHz processor and 1GB RAM under Windows NT. For testing, both artiﬁcial
and real data sets were used. Let us consider ﬁrst artiﬁcial distributions. We used
uniform and Gaussian distributions, points placed in regular grids, and points
grouped in clusters (see ﬁgure 4). The spent CPU time is given in tables 1–4, and
the adequate graphic representations are shown in ﬁgure 5. In all cases, input/output
operations have been excluded.

Without a doubt, the Shewchuk’s implementation of the Lee and Schachter’s
(1987) divide and conquer approach could be proclaimed the fastest Delaunay
triangulation algorithm. Its performance lags behind only for the points arranged in
the regular grid. This algorithm is also the least sensible to the distribution of the

Downloaded by [University of Newcastle] at 02:16 01 March 2016 Delaunay triangulation using the nearest-point paradigm

131

Figure 4.

5000 points distributed (a) uniformly, (b) regularly, (c) Gaussian, (d) clustered.

Table 1. Spent CPU time (s) for uniformly distributed points.

Algorithm n

1000

5000

10 000

50 000

100 000

500 000

1 000 000

GKS-BC
GKS-KZ
GSM-S
GS-M
F-S
D-S
USP
US2l

GKS-BCS
GKS-KZ
GSM-S
GS-M
F-S
D-S
USP
US2l

0.05
0.03
0.02
0.04
0.01
0.01
0.02
0.03

0.05
0.02
0.02
0.04
0.01
0.01
0.02
0.03

0.29
0.18
0.17
0.26
0.11
0.09
0.11
0.10

0.31
0.18
0.17
0.26
0.12
0.10
0.11
0.10

0.63
0.39
0.39
0.68
0.25
0.18
0.22
0.21

0.64
0.39
0.39
0.68
0.25
0.20
0.23
0.22

3.79
2.28
3.00
6.10
1.40
0.98
1.24
1.21

3.89
2.33
3.04
6.01
1.43
1.03
1.28
1.22

8.13
4.94
7.20
15.34
3.00
2.01
2.59
2.42

8.36
5.04
7.43
15.15
3.06
2.13
2.60
2.58

47.83
28.89
57.46
125.36
18.07
11.30
14.51
12.89

46.52
27.57
57.07
122.70
18.24
11.52
22.14
18.46

103.99
63.74
144.07
312.66
38.56
23.46
36.62
32.46

97.34
57.39
143.38
305.35
41.09
23.57
69.53
54.95

Table 2. Spent CPU time (s) for points with Gauss distribution.

Algorithm n

1000

5000

10 000

50 000

100 000

500 000

1 000 000

input data. The proposed US2l algorithm is the fastest in the case of points arranged
in the regular grid. Obviously, its eﬃciency drops by increasing the non-uniformity
of the input points. For uniform distribution, the US2l algorithm is the second
fastest, at Gaussian the third fastest, and for points arranged in clusters, it landed at

Downloaded by [University of Newcastle] at 02:16 01 March 2016 132

B. Zˇ alik and I. Kolingerova´

Table 3. Spent CPU time (s) for points arranged in a grid.

Algorithm n

1000

5000

10 000

50 000

100 000

500 000

1 000 000

GKS-BC
GKS-KZ
GSM-S
GS-M
F-S
D-S
USP
US2l

GKS-BC
GKS-KZ
GSM-S
GS-M
F-S
D-S
USP
US2l

0.04
0.02
0.06
0.03
0.01
0.01
0.02
0.03

0.04
0.03
0.02
0.04
0.01
0.02
0.03
0.03

0.28
0.16
0.30
0.26
0.11
0.16
0.10
0.09

0.29
0.18
0.11
0.30
0.11
0.09
0.14
0.13

0.58
0.35
0.63
0.67
0.23
0.33
0.20
0.19

0.62
0.38
0.29
0.76
0.25
0.20
0.38
0.28

3.47
2.06
4.14
5.94
1.29
1.70
1.13
1.10

3.49
2.00
2.62
6.97
1.35
0.97
2.65
1.64

7.62
4.49
9.56
14.98
2.72
3.50
2.33
2.23

7.08
3.96
7.34
17.45
2.81
2.02
5.98
3.46

45.28
26.40
69.28
122.86
16.63
18.62
14.78
13.09

97.43
57.86
166.18
309.24
35.08
37.91
37.25
32.31

37.58
20.11
79.12
153.37
16.61
11.26
131.78
38.87

76.96
41.68
211.92
415.69
35.55
23.10
471.80
124.48

Table 4. Spent CPU time (s) for points arranged in clusters.

Algorithm n

1000

5000

10 000

50 000

100 000

500 000

1 000 000

Figure 5. Spent CPU time for artiﬁcially generated data sets.

Downloaded by [University of Newcastle] at 02:16 01 March 2016 Delaunay triangulation using the nearest-point paradigm

133

the ﬁfth place when the number of points becomes large enough (over 500 000 points).
According to the comparison, the use of the two-level uniform subdivision turns out
to be relatively eﬃcient. The most evident case is, when the points are concentrated
in clusters (see table 4 and the graph in ﬁgure 5(d )). It is also important to point out
that the use of the two-level uniform subdivision does not slow down the algorithm,
even if the points are arranged in a regular grid, which conﬁrms the correctness of
introduced heuristics.

The second type of data has been taken from a land cadastre, i.e. the boundary
stones of parcels have been sent to the algorithm’s input (see ﬁgure 6). Again, we
used all the triangulation algorithms mentioned at the beginning of this section. The
results are shown in table 5 and ﬁgure 7. As can be seen, the divide and conquer
algorithm (Lee and Schachter 1987, Shewchuk 2001) has again been the fastest,

Figure 6. The real point distributions: (a) 13 829 points; (b) 20 005 points; (c) 60 244 points;
(d) 70 433 points.

Table 5. Spent CPU time (s) for GIS point sets.

Algorithm n

4897

13 829

15 820

20 014

41 853

60 244

70 433

193 360

GKS-BC
GKS-KZ
GSM-S
GS-M
F-S
D-S
USP
US2l

0.28
0.17
0.17
0.27
0.14
0.08
0.11
0.09

0.90
0.55
0.64
1.24
0.39
0.25
0.36
0.34

1.08
0.66
0.70
1.33
0.39
0.31
0.38
0.36

1.44
0.84
0.94
1.77
0.55
0.37
0.66
0.59

3.13
1.88
2.41
4.81
1.17
0.84
1.03
1.00

4.58
2.78
3.78
7.59
1.80
1.30
1.47
1.49

5.63
3.40
4.77
9.81
2.09
1.45
1.80
1.57

16.48
9.72
6.72
34.16
5.97
3.95
8.92
5.17

Downloaded by [University of Newcastle] at 02:16 01 March 2016 134

B. Zˇ alik and I. Kolingerova´

Figure 7. Spent CPU time when GIS data are used.

while US2l algorithm landed in second place. Among the algorithms using the
incremental insertion approach, our algorithm is the fastest. This conﬁrms that US2l
algorithm performs acceptably well also for real point distributions met in practice.
As its implementation is not complicated, it could represent a reasonable choice in
a practical use.

Finally, we measured the actual memory requirements of all the algorithms. An
average value of consumed bytes per vertex gives table 6. The values represent the
total amount of memory being occupied by the data structure including input points
and the list of generated triangles, divided by n. As can be seen, the most memory
demanding are algorithms based on Guibas, Knuth, and Sharir algorithm with DAG
because of storing the history of generated triangles. The least memory is required
by the walking algorithm proposed by Mu¨ cke et al. (1996) (GS-M algorithm) because
it does not use any auxiliary data structure to speed up the location of a triangle
containing the next inserted point. The remaining algorithms use a comparable
amount of the memory. As can be seen, the number of bytes per vertex ranges from
280 to 330 bytes in the case of US2l algorithm. The lower bound is the same as in
the case of USP algorithm (see table 6), while the upper depends on the number

Table 6. Average memory requirements in bytes per vertex.

Algorithm

Number of bytes per vertex

GKS-BC
GKS-KZ
GSM-S
GS-M
F-S
D-S
USP
US2l

492
492
[268, 309]
72
340
[297, 327]
280
[280, 330]

Downloaded by [University of Newcastle] at 02:16 01 March 2016 Delaunay triangulation using the nearest-point paradigm

135

of overpopulated cells at the ﬁrst level. This conﬁrms that the point distribution
inﬂuences the memory requirement at US2l algorithm.

4.2.2. CPU neutral tests of US2l algorithm

As the main idea of the proposed algorithm is based on the point location, also
CPU neutral parameters of the algorithm with excluded triangulation process are
presented in this subsection.

One of the most important parameter of the algorithm is the number of vertices
being checked before the nearest one to the next inserted point is found. As expected
(see table 7), this parameter depends highly on the point distribution and the total
number of input points. The same parameter for GIS data sets is given in table 8
(column A). As mentioned before, for the ﬁrst 500 inserted points, the brute force
algorithm for the nearest point search is applied. Therefore, small sets of points are
excluded from this test. A high correlation between CPU measurements done in the
previous subsection, and the nearest point problem is obvious.

By measurement we determine that on average 4.5 triangles per vertex have
to be checked using the point-in-triangle test. As seen from table 9 and table 8
(column B), less triangles are checked for larger data sets. The number of tests is
almost constant for points arranged in grids (3.5 triangles per vertex), and depend
considerably on n for points with Gauss distribution (from 9.1 triangles per vertex
at 1000 triangulated vertices to 3.9 triangles per vertex for 1 000 000 points).

Table 10 and table 8 (column C) show how frequently the algorithm has to search
the next nearest vertex. On average, this has to be done in 11% of inserted vertices.
However, this percentage depends on the point distribution and the number of

Table 7. The average number of vertices being tested before the nearest one is found.

Point distribution/n

50 000

100 000

500 000

1 000 000

6.5
14.3
35.0
8.2

6.8
15.2
36.2
8.7

15.5
56.4
162.4
15.3

35.5
103.0
310.9
25.4

Table 8. The average number of vertices being tested before the nearest one is found for GIS
point sets.

A

not measured
not measured
not measured
13.5
16.1
14.4
17.8
23.0

B

4.31
4.88
4.96
5.13
4.48
4.41
4.36
4.31

C

0.11
0.21
0.21
0.24
0.13
0.12
0.10
0.10

Legend:
A=The average number of vertices being tested before the nearest one is found.
B=An average number of point-in-triangle tests.
C=Search of the next nearest vertex.

Uniform
Gauss
Cluster
Grid

N/parameter

4897
13 829
15 820
20 014
41 853
60 244
70 433
193 360

Downloaded by [University of Newcastle] at 02:16 01 March 2016 Uniform
Gauss
Cluster
Grid

Uniform
Gauss
Cluster
Grid

136

B. Zˇ alik and I. Kolingerova´

Table 9. An average number of point-in-triangle sets.

Point distribution/n

1000 5000 10 000

50 000

100 000 500 000 1 000 000

4.29 5.00
9.13 4.36
5.14 4.87
3.61 3.64

6.45
4.25
4.71
3.64

6.87
4.15
4.40
3.64

4.57
4.07
4.45
3.50

4.40
4.07
4.13
3.50

3.55
3.93
4.11
3.50

Table 10. The need of the next nearest vertex search per inserted vertex.

Point distribution/n

1000 5000 10 000

50 000

100 000 500 000 1 000 000

0.26 0.25
0.12 0.10
0.29 0.21
0.08 0.06

0.20
0.08
0.17
0.05

0.15
0.07
0.11
0.04

0.13
0.06
0.12
0.03

0.10
0.05
0.06
0.02

0.08
0.05
0.06
0.02

points. It is interesting that the uniform point distribution is the most demanding
concerning this parameter. Obviously, the proposed approach solves the next nearest
point search very eﬃciently.

5. Conclusions

This paper proposes a new algorithm for constructing 2D Delaunay triangulation.
It belongs to the class of incremental insertion algorithms. The most time consuming
task of this approach, location of the triangle containing the point to be inserted is
solved using the nearest point paradigm. To accelerate the nearest point location, a
two-level uniform subdivision is used. A heuristic is given for its creation. Various
point distributions and several reference triangulation algorithms are used to test
the algorithm. The analysis of the results shows that the best among the tested
Delaunay triangulators is Shewchuk’s (2001) implementation of Lee and Schachter’s
(1980) divide and conquer approach. However, it should be stressed that the imple-
mentation of this algorithm is considerably more demanding than the implementation
of an incremental algorithm. The run time of the proposed algorithm depends on
the distribution of the input data, and reaches in the worst case O(n2) when all of
the input points except one are located in one cell of the uniform space subdivision.
Such distributions are rarely seen in practice. As shown in the paper, the proposed
algorithm is the second fastest among the tested triangulation algorithms for real
point distributions (GIS data have been used) and the fastest among the algorithms
using the incremental insertion approach while for clustered data it is not particularly
successful. Because its implementation is not demanding, it is eﬃcient for the point
distributions seen in practice, its memory requirements stay in acceptable limits, and
because it easily solves the boundary cases, the proposed triangulation algorithm
using two-level uniform subdivision seems to be a good solution for engineering
(GIS) applications.

The proposed algorithm can be further improved with other algorithms for the
nearest point search. Very recently, Skala and Kucharˇ (2001) showed that an adequate
hash function can be constructed for this task. If the next nearest point was found
eﬃciently enough using this hash function, the proposed triangulation algorithm would
be further accelerated, and its implementation could be even more simpliﬁed.

Downloaded by [University of Newcastle] at 02:16 01 March 2016 Delaunay triangulation using the nearest-point paradigm

137

Acknowledgments

This work was supported by the Ministry of Education of The Czech Republic
and by the Ministry of Education of Slovenia—project VS 97 155 and project
ME 298.

References
A, F., 1991, Voronoi diagrams—a survey of a fundamental geometric data

structure. ACM Computing Surveys, 23, 345–405.

B, J. L., W, B. W., and A, C. Y., 1980, Optimal expected time algorithms
for the closest point problems. ACM T ransactions on Mathematical Software, 6,
563–580.

B, J. D., D, O., T, M., and Y, M., 2000, Triangulations in
CGAL. In Computational Geometry 2000 (Hong Kong: ACM Press), pp. 11–18.
C, P., M, C., and S, R., 1992, A merge-ﬁrst divide and conquer algorithm
for The Delaunay Triangulations, Internal Report C92/16, CNUCE-CNR, Pisa.
C , R., 1999, Construction of Voronoi diagrams using Fortune’s method: a look on an
implementation. In 3rd Central European Seminar on Computer Graphics CESCG
(Budmerice, Slovak Republic), pp. 27–36.

 B, M.,  K, M., O, M., and S, O., 1997, Computational

Geometry, Algorithms and Applications (Berlin: Springer-Verlag).

D, O., 1998, Improved incremental randomized Delaunay triangulation. In ACM

Symposium on Computer Geometry, 1998 (Minneapolis: ACM Press), pp. 106–115.

D, R. A., 1987, A faster divide-and-conquer algorithm for constructing Delaunay

triangulations. Algorithmica, 2, 137–151.

E, R., and S, M., 1994, Classiﬁcation of ray-generators in uniform subdivisions and

octrees for ray tracing. Computer Graphics Forum, 13, 3–19.

F, T.-P., and P, L., 1992, Algorithm for Delaunay triangulation and convex-hull

computation using a sparse matrix. Computer Aided Design, 24, 425–436.

F, T.-P., and P, L., 1993, Delaunay triangulation using a uniform grid. Computer &

F, G., and H, P., 1998, T he Geometry T oolbox for Graphics and Modelling

Graphics Applications, 13, 36–47.

(Massachussets: A. K. Peters).

F, J. D.,  D, A., F, S. K., and H, J. F., 1990, Computer Graphics—

Principles and Practice (Reading: Addison-Wesley).

F, S., 1987, A sweep-line algorithm for Voronoi diagrams. Algorithmica, 2, 153–174.
F, A., T, T., and I, K., 1986, ATRS: accelerated ray-tracing system. IEEE

Computer Graphics and Applications, 6, 16–26.

G, M., and H, P. S., 1995, Fast Polygonal Approximation of Terrains and
Height Fields, Technical Report CMU-CS-95-181, Carnegie Mellon University,
Pittsburgh, USA (accessible on http://www.cs.cmu.edu/~garland/scape)

G, L., and S, J., 1985, Primitives for the manipulation of general subdivisions and

the computation of Voronoi diagrams. ACM T ransactions on Graphics, 4, 75–123.

G, L., K, D., and S, M., 1992, Randomised incremental construction of

Delaunay and Voronoi diagrams. Algorithmica, 7, 381–413.

H, C. M., 1989, Geometric & Solid Modelling (San Mateo: Morgan Kaufmann).
H, C.-W., and S, T.-Y., 1998, Improvements on Sloan’s algorithm for constructing

Delaunay triangulations. Computers & Geosciences, 24, 193–196.

K, J., and K, M., 1988, Constructing Delaunay triangulations by merging

buckets in quadtree order. Fundamenta Informaticae, 11, 275–288.

K , I., and Z , B., 2002, Improvements to randomized incremental Delaunay

insertion (accepted in Computer & Graphics Journal ), 26, 477–490.

L, C. L., 1977, Software for C1 Surface Interpolation. In Mathematical Software III,

edited by J. R. Price (New York: Academic Press), pp. 161–194.

L, D. T., and S, B. J., 1980, Two algorithms for constructing a Delaunay
Triangulation. International Journal of Computer and Information Sciences, 9,
219–242.

Downloaded by [University of Newcastle] at 02:16 01 March 2016 138

Delaunay triangulation using the nearest-point paradigm

M , E. P., S, I., and Z, B., 1996, Fast-randomized point location without pre-
processing and two- and three-dimensional Delaunay triangulations. In Computational
Geometry ’96 (Philadelphia: ACM Press), pp. 274–283.

P, F. P., and S, M. I., 1985, Computational Geometry: An Introduction (New

York: Springer).

S, H., 1989, T he Design and Analysis of Spatial Data Structures (Reading: Addison Wesley).
S, H., 1990, Applications of Spatial Data Structures (Reading: Addison Wesley).
S, J. R., 1996, Triangle: Engineering a 2D Quality Mesh Generator and Delaunay
Triangulator. In: First Workshop on Applied Computational Geometry (Philadelphia:
Association of Computing Machinery), pp. 124–133.

S, J. R., 2001, A T wo-Dimensional Quality Mesh Generator and Delaunay T riangulator

http://www.cs.berkeley.edu/~jrs/index.html.

S, V., and K , M., 2001, The Hash function and the principle of duality. In Computer

Graphics International 2001 (Hong Kong: IEEE Computer Society), pp. 167–174.

S, S. W., 1987, A fast algorithm constructing Delaunay triangulations in the plane.

Advanced Engineering Software and Workstations, 9, 34–55.

S, P., and D, R. L. S., 1995, A comparison of sequential Delaunay triangulation
algorithms. In Proceedings of 11th Annual Symposium on Computational Geometry
(Vancouver: Association of Computing Machinery), pp. 61–70.

S, K., 1991, A DDA octree traversal algorithm for ray tracing. In EUROGRAPHICS’91:
Proceedings of the European Computer Graphics Conference and Exhibition (Vienna:
North Holland), pp. 73–85.

Z , B., 1999, A topology construction from line drawing using a uniform plane subdivision

technique. Computer Aided Design, 31, 335–338.

Downloaded by [University of Newcastle] at 02:16 01 March 2016 