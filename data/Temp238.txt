International Journal of Geographical Information
Science

ISSN: 1365-8816 (Print) 1362-3087 (Online) Journal homepage: http://www.tandfonline.com/loi/tgis20

Exploring maps by sounds: using parameter
mapping sonification to make digital elevation
models audible

Joram Schito & Sara Irina Fabrikant

To cite this article: Joram Schito & Sara Irina Fabrikant (2018): Exploring maps by sounds: using
parameter mapping sonification to make digital elevation models audible, International Journal of
Geographical Information Science, DOI: 10.1080/13658816.2017.1420192

To link to this article:  https://doi.org/10.1080/13658816.2017.1420192

Published online: 03 Jan 2018.

Submit your article to this journal 

View related articles 

View Crossmark data

Full Terms & Conditions of access and use can be found at
http://www.tandfonline.com/action/journalInformation?journalCode=tgis20

Download by: [University of New England]

Date: 03 January 2018, At: 16:59

INTERNATIONAL JOURNAL OF GEOGRAPHICAL INFORMATION SCIENCE, 2018
https://doi.org/10.1080/13658816.2017.1420192

ARTICLE

Exploring maps by sounds: using parameter mapping
soniﬁcation to make digital elevation models audible
Joram Schito a and Sara Irina Fabrikant

b

aInstitute of Cartography and Geoinformation, ETH Zurich, Zurich, Switzerland; bDepartment of Geography,
University of Zurich, Zurich, Switzerland

ARTICLE HISTORY
Received 1 September 2016
Accepted 18 December 2017

KEYWORDS
Auditory display; parameter
mapping soniﬁcation; spatial
data analysis; digital
elevation model; GIS

ABSTRACT
This study empirically investigates the potential of auditory displays for
spatial data exploration, as an additional means to broaden the acces-
sibility and dissemination of geographic information for a diverse body
of users. In a mixed factorial experiment, three parameter mapping
soniﬁcation methods are empirically evaluated to interactively explore
discrete and continuous digital elevation models by auditory means.
Contrasting prior soniﬁcation research, this study’s unique empirical
evidence suggests that participants can indeed successfully interpret
soniﬁed displays containing continuous spatial data. Speciﬁcally, the
auditory variable pitch leads to signiﬁcantly better response accuracy,
compared to the sound variable duration. Background and training
has a weak eﬀect on data interpretation performance with the audi-
tory display. The more immersive the experienced soundscape, the
better participants can interpret the soniﬁed terrain. These encoura-
ging empirical results indeed suggest that interactive auditory displays
might oﬀer additional means to disseminate spatial information, and
to increase the accessibility to spatial data, beyond the currently
dominant visual paradigm.

Introduction

Since the rise of multimedia desktop computing in the early 1990s, where digital
authoring systems allowed even nonexpert developers to creatively assemble a broad
range of digital data sources such as, text, charts, graphs, video and sound, GIScientists
have been engaged in exploiting the potential of multimodal map displays to access
and disseminate spatial data (Cartwright et al. 1999). In most geographic multimedia
systems (i.e. digital atlases etc.), the assembled information was, and still is, accessed
predominantly through visual means (Andrienko et al. 2013). The visual primacy of
communication has been argued successfully with the dominance of neurons in the
human brain associated with vision (McCormick et al. 1987). Whereas the amount of
spatiotemporal information available today increases exponentially, the already limited
human visual system’s capacity to process this information has remained largely the
same (Ware 2008). Jacobson (2010) argues that the invention of sound carriers, that is

CONTACT Sara Irina Fabrikant
This paper is based on the ﬁrst author’s unpublished Master’s thesis, completed under the direction of the second
author. The thesis received a distinction from the Faculty of Science of the University of Zurich.

sara.fabrikant@geo.uzh.ch

Supplemental data for this article can be accessed here.

© 2018 Informa UK Limited, trading as Taylor & Francis Group

Downloaded by [University of New England] at 16:59 03 January 2018 2

J. SCHITO AND S. I. FABRIKANT

the telephone, or the radio, help to inform, guide and shape human understanding
about places. Sound may therefore complement or even replace visual data encodings,
as to oﬄoad cognitive eﬀort especially in situations where the visual perception system
might already be overloaded, that is rapidly changing situations or in virtual reality or
augmented reality scenarios. One thus might wonder why the nonvisual human percep-
tual modalities have been underutilized for geographic information so far. This might
become even more critical today, as current spatiotemporal data displays are increas-
ingly used in mobile contexts. One’s sight might already be occupied with alerting
humans of appearing obstacles, and potentially suddenly emerging harmful contextual
situations, for example, while wayﬁnders are trying to navigate to a destination in a
rapidly changing environment. Not surprisingly then, successful mobile navigation
systems provide not only visual displays but are accompanied by textual turn-by-turn
instructions, coupled with real-time voice-overs. Also large-screen, 3D virtual display
systems increasingly employ sound and spatialized audio sources to enhance the
immersive experience of the projected 3D information (Théberge 2005, Ruotolo et al.
2013, Paterson and Conway 2014). Spatiotemporal data access with sound also supports
disadvantaged user groups such as the visually impaired or congenitally blind popula-
tion to beneﬁt from the spatial data revolution (Simonnet et al. 2009). In the following
related work sections, we highlight relevant research in the context of auditory data
representation that motivated our empirical study using parameter mapping soniﬁcation
(PMS) to explore and make sense of discrete and continuous spatial data.

Data soniﬁcation with auditory displays

The idea of making the inaudible hearable can be traced back to two inventions
(Schwartz 2003). The ﬁrst, since 1816, René Théophile Hyacinthe Laënnec’s stethoscope
ampliﬁed existing sound sources that would otherwise be inaudible for humans. The
second, the Geiger–Mueller counter, invented in 1908 by Hans Johann Wilhelm Geiger,
turns alpha particles that cannot be directly perceived by the human perceptual system
through electrical pulses into a synthetic sound source (Brazil and Fernström 2011).
Arguably, the Geiger counter systems could be called one of the earliest known auditory
displays.

Pollack and Ficks (1954) empirically investigated the potential of high dimensional
information transmission through sound and in doing so ﬁrst introduced the idea of
auditory displays. They asked participants to distinguish sounds manipulated through
eight sound variables (i.e. noise frequency and loudness, tone frequency and loudness,
the rate of alternation between noise and tone, the ‘on’-time fraction of the tone, the
total duration of presentation and the sound direction) and found that respondents’
error rates vary signiﬁcantly across sound variables. Early empirical soniﬁcation research
since the digital revolution by Yeung (1980), Bly (1982) and Williams et al. (1990) further
investigated the eﬀect of diﬀerent sound parameters to human perception. For example,
Yeung (1980) soniﬁed seven data dimensions of chemical compounds and tested
participants achieving high classiﬁcation accuracies (up to 98%). To examine the poten-
tial of sound to convey more information beyond visual means in exploratory data
analysis, Bly (1982) systematically compared the eﬀect of sound parameters in temporal,
multivariate and logarithmic datasets. He discovered that the combination of graphics

Downloaded by [University of New England] at 16:59 03 January 2018 INTERNATIONAL JOURNAL OF GEOGRAPHICAL INFORMATION SCIENCE

3

and sound yielded the highest interpretation accuracy. Similarly, Williams et al. (1990)
evaluated compound sound textures, so-called sound icons (i.e. by combining pitch,
attack rate, decay rate, volume and depth of frequency modulation), with multivariate
glyphs. They found that participants’ accuracy increases signiﬁcantly when graphic
symbols are explored jointly with sound icons. In this early epoch of empirical soniﬁca-
tion research, researchers focused primarily on testing the mappings of various dataset
dimensions to varying sound variables, including both discrete (e.g. timbre, harmonics,
damping and waveform) and continuous sound parameters (e.g. pitch, loudness, pan-
ning, duration and attack).

Since the ﬁrst International Conference on Auditory Display convened in 1992,
soniﬁcation established itself as an interdisciplinary research ﬁeld in its own right
(Kramer 1992), drawing upon knowledge from cognate disciplines including cognitive
science and human factors research and considering visually impaired or blind people.

Soniﬁcation in GIScience

Only a few GIScientists have experimented with using sound to explore and commu-
nicate spatial data, as early as the 1990s. For example, Krygier (1994) proposed nine
sound variables including location, loudness, pitch, register, timbre, duration, rate of
change, order and envelope to sonify spatial data. His theoretical framework was
inspired by Bertin’s fundamental visual variable system, already employed by cartogra-
phers for a long time to appropriately assign graphic symbols to depicted data char-
acteristics. Krygier (1994) contends that sound variables should only be used to
represent nominal or ordinal levels of information in spatial data. In contrast, Fisher
(1994) shows how sound variables can be used to represent uncertainty in land-use
classiﬁcation using the continuous sound variables pitch and loudness and in doing so is
able to provide compelling evidence for the use of sound variables for at least interval
level data. Lodha et al. (1999) soniﬁed a discrete raster map by means of parametric
auditory icons and reported improved understanding of the presented information in
Geographic Information System (GIS) displays in a user study, combining geographic
visualization with soniﬁcation for various spatial tasks, including estimates of raw data
values, local averaging and global comparisons. Using similar soniﬁcation methods, but
not supported with any user study, Brauen (2006) explored the use of sound to
represent election results in online web maps, whilst MacVeigh and Jacobson (2007)
soniﬁed land usage classes with four simultaneously rendered auditory icons.

Pushing the soniﬁcation boundary further, Heuten et al. (2006) introduced the head-
related transfer functions (HRTF) with auditory icon streams and hierarchical earcons to
represent geographic features including landmarks as sound areas within a sound room.
More recently, Geronazzo et al. (2016) demonstrated in two experiments how custo-
mized HRTF can be combined eﬀectively with a tactile mouse to help users orient
themselves acoustically and haptically by means of virtual anchor sounds located in
speciﬁc points of a soniﬁed map. Zhao (2006) developed iSonic, a speech-supported
PMS tool, to sonify categorized statistical data mainly through the variables pitch and
balance to facilitate interactive spatial data exploration for visually impaired and blind
data users. Her system allows interactive exploration of map content by means of a
touchpad and keyboard, supplemented with so-called Auditory Information Seeking

Downloaded by [University of New England] at 16:59 03 January 2018 4

J. SCHITO AND S. I. FABRIKANT

Actions (Zhao 2006). Extending this work, Delogu et al. (2010) empirically compared a
soniﬁed map with either tablet or keyboard interaction with four nonauditory tactile
map versions. Sighted and non-sighted participants were asked to compare cross-
modally which of the nonauditory maps corresponded best with the soniﬁed map
they had explored ﬁrst. Participants performed well with the soniﬁed map in all of the
four tactile map discrimination tasks. No signiﬁcant performance diﬀerences were found
between participants who used keyboard or tablet interaction modes, or between blind
or sighted participants.

More recently, soniﬁcation researchers explored various techniques to further
advance the soniﬁcation frontiers. For example, by using the raster scanning approach
as a framework for both image soniﬁcation and sound visualization, Yeo and Berger
(2008) proposed an algorithm to linearly map brightness value ranges in continuous
gray scale raster images to ranges of audio samples. Furthermore, Adhitya and
Kuuskankare (2012) developed a PMS library to sonify images along a vector path
deﬁned by a user. These soniﬁcation methods were found to provide a rapid overview
of a raster image.

Grond and Berger (2011) suggest the use of the sound variable duration for the
soniﬁcation of continuous spatial data models. Bearman and Lovett (2010) implemented
information, but without clear
Earcons based on triad notes to represent positional
evidence for improved data understanding. In a subsequent study, using piano notes
and bird calls to sonify distances, Bearman and Lovett (2010) found a signiﬁcant eﬀect in
favor of soniﬁcation, compared to visualization alone. Two similar approaches are also
noteworthy: On the one hand, Schiewe and Weninger (2012) evaluated the usability of
gamut-based and pitch-modiﬁed piano earcons for the extraction of qualitative informa-
tion in maps, but with no clear eﬃciency gains when combining soniﬁcation with
visualization. On the other hand, Josselin (2011) built a theoretical framework that
proposes the use of soundscapes or compound earcons. By coupling interactive map
displays with a musical synthesizer, a user is able to query an image using soniﬁed
summary statistics at cursor location. Furthermore, soniﬁcation research has successfully
moved on to mobile map display use. For example, Laakso and Sarjakoski (2010)
propose an interactive mobile hiking map with soniﬁed soundscapes that include
land-use classes. Finally, Josselin et al. (2016) implemented a QuantumGIS plug-in to
ﬁrst isolate geographic features depicted on an oﬃcial French topographic map by the
proportion of their dominant colors and then soniﬁed them by means of soundscapes.
In subsequent usability studies, they found that blind and visually impaired participants
were more successful at interpreting more complex soniﬁcation patterns, compared to
sighted participants, perhaps due to their compensatory capacities or due to previous
experience using sound to encode this information. This contradicts the ﬁndings of
Afonso et al. (2005) and Bujacz et al. (2011) who could not ﬁnd any signiﬁcant perfor-
mance diﬀerences between blind and sighted participants using auditory displays.

A key for successful interpretation of spatial representations is spatial metaphors,
which are based on image schema that are at the core of human cognition, such as
MORE-IS-UP (Lakoﬀ and Johnson 1980). The MORE-IS-UP image schema is also fre-
quently employed in ﬁlm to synch an action on screen to the movie’s soundtrack. This
ﬁlm technique is also called Mickey Mousing (Kalinak 1992). For example, a sequence of
rising notes is heard in synch when an actor walks up the stairs. Kuhn (1996) and others

Downloaded by [University of New England] at 16:59 03 January 2018 INTERNATIONAL JOURNAL OF GEOGRAPHICAL INFORMATION SCIENCE

5

have shown that spatial metaphors in GIS user interfaces help users make sense of
spatial and nonspatial data.

From this brief review, one can distill that auditory displays have already been
successfully implemented in various ways, mostly through mapping categorical data.
However, empirical evidence on its success for data exploration and communication,
especially for continuous spatial data,
is scarce and inconclusive at best. We thus
wondered whether one could push the soniﬁcation envelope further and provide
additional empirical evidence for the successful use of advanced parameter soniﬁcation
methods, speciﬁcally to explore continuous geographical datasets presented interac-
tively in auditory displays.

PMS for continuous spatial data

Similarly, to information visualization or spatialization, where nonspatial data dimen-
sions are mapped to a set of visual variables to construct abstract visuospatial data
displays, PMS assigns sound variables to multivariate data characteristics. As PMS oﬀers
great ﬂexibility in data transformation, input data of almost any kind can be mapped for
soniﬁcation, including speciﬁcally, continuous data models (Grond and Berger 2011). Of
course, such types of mapping ﬂexibility can also bear potential dangers. In order to
achieve usable solutions, not only technical or methodological aspects must be con-
sidered (e.g. signal processing etc.), but also informed decisions must be made related to
human sound perception,
including human information processing modes such as
thinking, tuning and listening (Grond and Berger 2011). As with information visualization
displays, the parameterization logic of a PMS must be learnt by a user for a particular
display, and thus the display is ideally complemented with an auditory legend to
increase usability (Grond and Berger 2011). Since the choice of appropriate sound
variables is essential for intuitive understanding of auditory displays, knowledge about
human acoustics including psychophysics is necessary in soniﬁcation research.

Acoustic principles for continuous auditory displays

By means of Fourier analysis, a sound can be decomposed into the continuous wave
components amplitude, frequency and phase (Campenhausen 1993, Goldstein 2002).
These three physical wave components are perceived as sound, from which diﬀerent
discrete and continuous sound variables used in music and sound engineering can be
derived (see Table 1). First, a wave’s amplitude corresponds to the sound variable
loudness, including the continuous musical variables panning, which is the distribution
of a sound between diﬀerent sources. Second, a wave’s frequency corresponds to the
sound variable pitch. Third, a wave’s phase incorporates two diﬀerent sound character-
istics: On the one hand, it reﬂects the sound variable duration and consequently its pace.
For example, consider a beeping sound with a sequence and duration of the beep and a
period of silence in between. On the other hand, a sound wave’s shape aﬀects the
sound’s timbre, which is typically characterized on a nominal level of information. A
wave’s envelope components translated into the sound variables attack, decay, sustain
and release are continuous. Lastly, the combination of amplitude, frequency, and phase
allows for the analysis of the more complex sound variables sound texture and spatial

Downloaded by [University of New England] at 16:59 03 January 2018 6

J. SCHITO AND S. I. FABRIKANT

Table 1. Relationship between wave components, sound variables and musical
variables.

Wave component
Amplitude

Sound variable
Loudness

Frequency

Pitch

Phase

Duration
Timbre

Amplitude, frequency and

phase combined

Texture
Location

Variables used in music and
sound engineering

Pan
Dynamics
Tone within a gamut
Cardinality within a gamut
Register
Order
Harmony
Pace
Rhythm
Meter
Clarity
Damping
Reverberation
Envelope (attack, decay, sustain, release)
Instrumentation
Distance
Angle

location, e.g. by decomposing a sound into its sources in space, or by locating a sound
source in the space. Thus, the continuous sound variables loudness, pitch and duration
can directly be mapped to continuous data, whereas the qualitative variable timbre
must be decomposed into its quantitative envelope components, to be useful for
continuous data soniﬁcation. While sound variables can be physically modeled and
measured, identical sound wave parameters might not be perceived the same way by
a human, due to environmental eﬀects (i.e. room acoustics etc.), and varying individual
diﬀerences in human sound perception.

In the following, we detail our own empirical study aimed at evaluating a novel,
interactive PMS by means of sound variables (Table 1), to explore digital elevation
models (DEMs) of various types. Through this study, we wish to provide additional
quantitative empirical evidence on the utility and usability of soniﬁcation of spatial
data for exploratory data analysis.

Experiment

The aims of this exploratory, empirical soniﬁcation study reported below are mani-
fold. On the one hand, we wish to replicate results with previously suggested sound
variables (e.g. those of Krygier 1994), speciﬁcally in the context of auditory displays
for continuous spatial data analysis (i.e. DEMs). We also aim at extending prior work
by investigating how diﬀerent soniﬁcation methods might inﬂuence spatial data
interpretation. As very little is known on how user characteristics (i.e. musicality,
geographic data analysis etc.) might inﬂuence performance with auditory displays,
we were also interested in evaluating the role of user background knowledge and
training for decision-making with soniﬁed data.

Downloaded by [University of New England] at 16:59 03 January 2018 INTERNATIONAL JOURNAL OF GEOGRAPHICAL INFORMATION SCIENCE

7

Experimental design

We chose a mixed factorial design for our study, consisting of the within-subject factor
terrain model type (three levels: abstract discrete, abstract continuous and realistic
continuous; Figure 4) discussed in the ‘Materials’ section, and the between-subject factor
soniﬁcation method (three levels: A, B, C; Table 3). We further assessed participants’
individual domain relevant background knowledge and training. The set of controlled
variables is summarized in Table 2.

The dependent variable response accuracy measured during the auditory display
portion of the experiment contains three components, as shown in Figure 1. For relation
a, a participants’ sound interpretation at response location in the auditory display is
compared to the modeled sound parameters at that location. Similarly, for relation c, a
participants’ sound interpretation is compared to the modeled sound parameters at the
correct target location. Finally, in relation b, the distance between correct target location
and a participants’ response location is assessed. Response accuracy is additionally
recorded in a series of closed-ended questions, related to the structure of the explored,
soniﬁed space, as well as in pre- and posttest questionnaires related to user background
and training.

Table 2. Controlled and independent variables of the experiment.

Controlled variables
Background knowledge

Technical ability
Musicality
Terrain interpretation

Within factors

Terrain model type

Complexity level

Abstract discrete
Abstract continuous
Realistic continuous

1
2
3

Between factors
Soniﬁcation method
A
B
C

Table 3. Mapped sound variables to the rendered dimensions of each group.
Soniﬁcation
method
A
B

Y-coordinate
Increasing pitch on right ear
Moving pan from full square wave to full

Increasing pitch on left ear
Moving balance from left to

X-coordinate

C

Moving balance from left to

Moving pan from full square wave to full

right ear

right ear

sine wave

sine wave

Altitude

Increasing pace
Increasing pitch

Increasing pitch and
acoustic elevation

proﬁle

Figure 1. Components of the dependent variable response accuracy.

Downloaded by [University of New England] at 16:59 03 January 2018 8

J. SCHITO AND S. I. FABRIKANT

Participants

In total, 61 participants were invited (26 females and 35 males) who were willing to
spend 90 min for a test session. The study was aimed at recruiting a participant pool
balanced by gender and familiar with the interpretation of spatial data. The participants
recruited also speciﬁcally spanned a broad range of ages, as hearing capacity is known
to deteriorate with age. Tested participants ranged from 18 to 56 years. Their participa-
tion was voluntary. All participants are sighted but were blindfolded during the display
exploration portion of the test.

Sound spatialization: the mapping of sound variables

The continuous acoustic variables pitch, duration and pan were used in this study. Pan
was mapped to loudness and to waveform (Table 1). Pitch is continuous, and based on
reviewed literature, easily distinguishable. The octave was chosen as the spectral unit,
ranging from 0 to 1. This is because it represents as simplest harmonic oscillation, the
ﬁrst and loudest overtone of a harmonic series (Amon 2005). The octave is used across
various music cultures as a basis for building a scale. To minimize potentially perceived
isophonic loudness ﬂuctuations (Genuit 2010), the fundamental frequency was set to
440 Hz. As loudness is perceived very subjectively due to varying hearing sensitivity, it
was purposefully not used to map elevation data. Instead, the duration of a sound, and
the pace, was chosen to increase with data magnitude (i.e. elevation). To avoid further
lengthening of the long test session, the fastest pace that the used technology allowed
was chosen. The diﬀerence in loudness was mapped to pan, that is to move within two
orthogonal directions in the soniﬁed space. East–west movement in the terrain was
implemented by taking advantage of the binaural perception between the left ear (i.e.
western edge of terrain) and the right ear (eastern edge of terrain). The north–south
direction changes are represented by loudness panning between two types of sound
waves: sine waves (towards the northern edge of the terrain) and square waves (by
going south). As a square wave has a larger integral and thus is louder than a sine wave
of equal amplitude, the square wave’s amplitude was reduced by 38% to get wave types
of equal loudness (Equation 1).

π
r ¼ (cid:1)
0

1 (cid:2) sin x dx

(1)

Soniﬁcation methods

Based on these considerations, two fundamentally diﬀerent sound parameterizations (A
and B) were implemented. To reduce overall cognitive load during the already quite
long and demanding experiment, only two continuous variables were selected simulta-
neously. For method A, the origin of the coordinate system was set to the lower left
corner of the auditory display; that is, the x-axis was rendered to the left ear and the
y-axis to the right ear. With increasing distance from the origin, the pitch rises on each
ear individually up to an octave when reaching the maximum. Elevation was soniﬁed by
using the duration of a sound: The higher the elevation at cursor position, the shorter

Downloaded by [University of New England] at 16:59 03 January 2018 INTERNATIONAL JOURNAL OF GEOGRAPHICAL INFORMATION SCIENCE

9

the sound. The minimal duration t between sound and silence was set to 200 ms and
mapped to elevation as shown in Equation 2, where t is the duration, hmax is the
maximum DEM height and hcursor is the elevation at the cursor position.

t ¼

(cid:3) (cid:4) hmax

200 ms
½
hcursor

(2)

The mapping of the sound variables in method B was designed to be more immersive.
First, the elevation was mapped to pitch as it is commonly done. Second, the location
was mapped on the xy-plane to pan. As mentioned above, participants could take
advantage of binaural hearing to locate a sound through pan on the x-axis. As a novelty,
pan was employed a second time on the y-axis, using two distinct waveforms. In doing
so, participants could orient themselves by comparing nuances of sound changes when
moving away from the origin in the xy-plane in any direction.

Furthermore, the sound variables mapping of method C was almost identical to those
of method B, with one slight diﬀerence in that participants could additionally choose to
use an acoustically rendered elevation proﬁle between two independently chosen
points whenever they desired to do so. After selecting a start and an end point by
pressing a given key, an audio generator successively played the respective sound for
the elevation of every pixel between chosen start/end points with a pace of 15 t/s. Table
3 summarizes the mapped sound variables to the rendered dimensions. The developed
sound parameterizations for each method are visualized in Figure 2.

Method A has its sound mapping origin in the lower left corner of the terrain. In
method B and method C, the origin of the sound source is located in the center of the
terrain, to give the user a stronger feeling of immersion (Figure 2).

Stimuli implementation and data

The auditory stimuli and testing environment were developed using the Java-based
Processing software including the Java Sound library Ess. Three DEMs were constructed
for soniﬁcation with decreasing levels of abstraction, thus increasing realism and com-
plexity (see Table 2 and Figure 3), but at an identical spatial resolution of 1280 × 800
pixels, and equal color depth of 8 bits (i.e. 256 shades of gray). The most abstract terrain,
presented to participants ﬁrst in the experiment, and thus the least complex soniﬁed

Figure 2. Soniﬁcation methods employed in the study.

Downloaded by [University of New England] at 16:59 03 January 2018 10

J. SCHITO AND S. I. FABRIKANT

Figure 3. Three tested digital elevation models (DEMs) with decreasing levels of abstraction,
increasing degrees of realism and increasing complexity levels from left to right.

terrain, consisted of a discrete elevation model including three simple geometric shapes.
An artiﬁcial, continuous elevation model containing two protrusions and two depres-
sions served as the second complexity level. Finally, a DEM from Swisstopo served as the
third and highest complexity level in our empirical study. The employed Swisstopo DEM
extends 256 km in east–west direction and 160 km in north–south direction. The DEM
covers about 35,000 km2 of Swiss territory (84%) and about 5100 km2 of the adjacent
countries including France,
Italy, Germany, Austria and Liechtenstein. The mapped
elevation ranges approx. between 400 and 4200 m above sea level. It also contains
about 1100 km2 of pixels without any elevation information.

Materials and equipment

The technical equipment employed to run the study should optimally support immer-
sive and focused auditory data exploration, while reducing as many potentially distract-
ing noise sources and sources for visual interference as possible. For various reasons, a
Wacom Bamboo CTH-460 tablet with a 14.7 × 9.1 cm active area was chosen. This was
operated with a stylus as an input device for data exploration and to record participant
responses. This is because most importantly, currently available tablets with gesture
interfaces (i.e. touch pads etc.) did not provide the level of spatial resolution we needed
to accurately record user interactions and respective responses on a given area of
1280 × 800 pixels. As the previous test experiences with this kind of tablet and input
modality (Heuten et al. 2006, Zhao 2006, Delogu et al. 2010, Laakso and Sarjakoski 2010)
were deemed good, this choice seemed reasonable. The surface area on the tablet
beyond the DEM display was covered with adhesive tape so that participants without
vision deﬁciencies could feel the borders of the display while solving the test blind-
folded. Participants wore closed, over ear, studio quality earphones (i.e. Beyerdynamic
DT770) during the test session with the auditory displays. To increase immersion and
focus on the sound source, participants also wore sun glasses (i.e. an S. Tacchini model)
with a lens size that covered a large ﬁeld of view and respective vision angle. The lenses
were additionally covered with light-proof, black foil to increase opaqueness. In contrast
to prior reviewed studies, an interactive sound legend providing acoustic information at
cursor location could be activated during the experiment on request and heard through
the earphones, pressing the cursor control keys on the notebook of the experimenter.
This sound legend played the highest/lowest tone for comparison between the current
pitch and the absolute minimum/maximum. Furthermore, the sound on the current

Downloaded by [University of New England] at 16:59 03 January 2018 INTERNATIONAL JOURNAL OF GEOGRAPHICAL INFORMATION SCIENCE

11

Figure 4. Test setup with experimenter (left) and test participant (right).

position could be split into the left/right ear and into its proportions of sine/square wave
to further help determine the cursor’s absolute position on the tablet display.

The experimenter monitored the progress of the experiment using a visual console
and by auditory means. Figure 4 depicts the test environment. The experimenter (ﬁrst
author) seated at the notebook monitors progress on the left-hand side of the photo-
graph. A test participant using the stylus on the tablet to solve a task is shown on the
right-hand side of the image.

Procedure

A test session lasted approximately 90 min. While the experimental procedure was
identical for all tested participants, testing locations, times and lighting conditions varied
randomly. The locations typically included a quiet oﬃce without acoustic disturbances.
First, participants were welcomed to the experiment and seated. After signing the
consent form, information about the testing procedure was provided and participants’
personal information was gathered, including a question about their hearing. Following
that, relevant domain knowledge and abilities were recorded, using a mixture of self-
assessments, questionnaires and ability tests. The response categories related to tech-
nical ability (i.e. the handling of digital devices and respective input mechanisms),
musicality (i.e. their understanding of music) and the ability to read/interpret surface/
terrain features (i.e. topographic map reading). For each assessment type, a sample
question is given in a summary table below (Table 4). The entire questionnaire and
respective scoring of the questions are available in the Appendix A.
Next, participants performed the auditory display portion of

the experiment.
Participants wore darkened glasses and headphones. They were randomly assigned to
one of the three experiment conditions, including one of the three tested soniﬁcation
methods (A, B and C), as discussed earlier (see Table 3 for details). Participants were

Downloaded by [University of New England] at 16:59 03 January 2018 12

J. SCHITO AND S. I. FABRIKANT

Table 4. Sample questions, grouped by assessment category.
Assessment categories [Appendix A]

Question

Background questionnaire/ability tests

Technical ability [1b]

Musicality [2d]

Terrain interpretation ability [3b]

Auditory display question types

Abstract-discrete terrain model [4b]

Abstract-continuous terrain model [5c]

Realistic-continuous terrain model [6e]

‘Without looking at the keyboard.
Show me, where the “D” key is’
‘If the basic tone were 0% and its
octave 100%, estimate the
percentage of the pitch relative
to the basic tone you hear’.
[Experimenter plays an interval]
[Experimenter shows a map] ‘Look

at this map. What is the
orientation of the main valley?
Draw it into the map’

‘Describe the shape of the
geometric object you are
hovering over right now’
‘Find the highest point in the

space and set a marker. Then,
estimate its x-coordinate
relative to the left display edge’

‘Find a lake and set a marker.

Estimate its relative elevation
using the minimum and
maximum elevation values
provided in the sound legend’

Assessed ability

Ability to ﬁnd a key while being

Ability to assess a pitch within a

blindfolded

given range

Ability to interpret large-scale
topographic structures

Ability to identify simple shapes
in an abstract soniﬁed terrain

Ability to identify a location by

attribute in an abstract
soniﬁed terrain

Ability to identify geographic

features in a realistic soniﬁed
terrain and estimate their
elevation on a relative scale

asked to interpret location and elevation information (including surface patterns), as
accurately as possible, based on the sound heard across the entire soniﬁed display and
for various tasks with varying complexity. All participants were tested in three DEM
conditions, with increasing complexity levels; always presented in the same sequence.
While one could argue that an identical sequence would potentially yield learning
eﬀects and introduce response biases, the decision was still taken to apply such a
sequence purposefully. Therefore, we counteracted potential learning eﬀects by increas-
ing the questions’ diﬃculty depending on the complexity level. The ﬁrst complexity level
location, shape, orientation and height of
aimed at correctly interpreting number,
diﬀerent geometric shapes on a discrete DEM. The second complexity level aimed at
additionally ﬁnding the correct location and accurately interpreting surface character-
istics such as slope of local and absolute peaks and sinks on a continuous DEM. Finally,
the third complexity level additionally aimed at correctly interpreting absolute elevation
information and further morphological patterns.

We began each test sequence by orally explaining the general structure of the DEM
and by introducing the main goals of the tasks. Each DEM condition included the
invitation to ﬁrst freely explore the soniﬁed terrain. This was followed by a series of
trials with tasks designed to ﬁnd particular locations in the DEM that matched given
conditions, or to interpret a surface feature at a given location. Participants were closely
monitored while solving the tasks. If participants found diﬃculty with a particular task,
this task was repeated. Participants could request their scores after completion of the
experimental session.

Time, cursor position and key button presses were recorded digitally and saved into a
log ﬁle, including participant responses to test questions. This allowed us to later compare

Downloaded by [University of New England] at 16:59 03 January 2018 INTERNATIONAL JOURNAL OF GEOGRAPHICAL INFORMATION SCIENCE

13

self-reports on performance with actual performance to further provide insights into the
three response accuracy relations as shown in Figure 2. Depending on the task, responses
were recorded at diﬀerent levels of detail. Whenever possible, quantitative responses were
measured on a continuous measurement level which was automatically recorded during
the test session by the testing software. This allowed us to compute a ratio score for the
three components of the response accuracy (as shown in Figure 2). Other responses were
manually coded when necessary, using an ordinal scoring system, as follows: zero for non-
satisfactory responses, 0.5 for fair responses and 1 for a satisfactory response. The experi-
menter graded participant’s answers based on the valuation key listed in Appendix A while
the requirements for obtaining the speciﬁed points depended on the task. Tasks within the
same experimental condition were weighted equally. Participant responses were averaged
and normalized to scores ranging from 0 to 1, and the participant performance was
statistically assessed for each portion of the experiment. A complete overview over all
tasks is provided in Appendix A, including the scoring scheme, the weighting factors and
the formula for calculating the response scores.

Results

Below, the collected data across soniﬁcation methods and terrain complexity levels are
detailed. This is followed by correlation analyses to further evaluate users’ background
knowledge on response accuracy.

How are soniﬁed terrains interpreted by users?

As the soniﬁcation of discrete data has been typically investigated in prior work, we
speciﬁcally wondered, whether and how soniﬁcation might also work in comparison
with continuous data, such as a continuous DEM. The box plots in Figure 5 show
participants’ response accuracies organized by increasing complexity of the soniﬁed
terrain and grouped by soniﬁcation method. The black (trend) lines connect mean
accuracy across soniﬁcation methods and terrain complexity. There are only small
diﬀerences across soniﬁcation methods, but rather high standard deviations (12.8–
13.4%) for all groups in this condition.

Surprising to us, and in direct contrast to what prior research suggests (e.g. Krygier
1994), higher response accuracies (and lower response variability) were observed with
the continuous terrain models, compared to the seemingly easier discrete terrain. For
the discrete DEM, participants’ average response accuracy overall ranges from 69.1% to
72.7%. Figure 5 also shows that participants’ average accuracy increases steeply from the
discrete to the simple continuous terrain model (to approx. 86.7%), when using soniﬁca-
tion methods B and C, but then levels oﬀ at about ±1% accuracy for the most complex
terrain model. In contrast, with soniﬁcation method A, participants’ average accuracy
decreases from 76.7% to 73.3% across complexity levels. It is interesting to see that
lower response averages (i.e. with the discrete terrain) tend to be accompanied with
noticeably larger response variability. A repeated measures ANOVA suggests a signiﬁ-
cant main eﬀect of terrain complexities (F [1, 59] = 14.82, p < 0.001, η2 = 27.3%,
f = 61.3%). A post-hoc analysis including a Bonferroni correction yields signiﬁcant
diﬀerences
in response accuracy between terrain complexity levels 1 and 2

Downloaded by [University of New England] at 16:59 03 January 2018 14

J. SCHITO AND S. I. FABRIKANT

Main Test Accuracy per Sonification Method and Complexity Level

92.9%
92.9%

62.3%
62.3%

y
c
a
r
u
c
c
A

100%

90%

80%

70%

60%

50%

40%

64.0%
64.0%

52.1%
52.1%

65.2%
65.2%

62.4%
62.4%

64.0%
64.0%

52.1%
52.1%

Mean
Trendline of  Means
Complexity Level 1
Complexity Level 2
Complexity Level 3

A (n=20)

B (n=20)

C (n=21)

Combined B/C (n=41)

Sonification Method (Number of Participants)

Figure 5. Response accuracies across soniﬁcation methods and terrain complexity levels.

(p < 0.001), and between terrain complexity levels 1 and 3 (p < 0.01), suggesting that
participants’ response accuracy was signiﬁcantly worse with the discrete terrain model.
In other words, quite in contrast to prior research suggesting that soniﬁcation might be
best used for ordinal data (e.g. Krygier 1994), our participants perform even better with
the more complex continuous terrains, compared to the discrete terrain model.

An interaction eﬀect was also found across soniﬁcation methods A and B/C with
complexity levels (F [1, 59] = 3.541, p < 0.05, η2 = 6.5%, f = 26.4%). This suggests that not
only terrain complexity is relevant for participants’ performance with an auditory display
but also the employed soniﬁcation method, as hypothesized. Thereby, attention is now
drawn to the assessment of the developed sound mapping methods.

What role does the sound mapping method play for terrain interpretation?

One aim of this research was to propose various continuous sound mapping methods to
assess their strength and weaknesses for the soniﬁcation of DEMs. As shown for the
results on terrain complexity above, there seems to be an interaction eﬀect across
soniﬁcation methods.
Indeed, the right panel of Figure 6 reports lowest response
accuracy with method A (M = 74%, SD = 7%), followed by method B (M = 77%,
SD = 6%) and lastly highest elevation interpretation accuracy with method C
(M = 82%, SD = 5%). A repeated measures ANOVA suggests these diﬀerences to be
signiﬁcant (F [1, 59] = 8.990, p < 0.001, η2 = 23.7%, f = 7%, d = 79.7%). A post-hoc analysis
using a Bonferroni correction indicates that participants’ response accuracy diﬀers
signiﬁcantly across soniﬁcation methods A and C (p < 0.001). The magnitude of the
eﬀect expressed by Cohen’s d (1.331) is also large. When using Tukey’s HSD, participants’

Downloaded by [University of New England] at 16:59 03 January 2018 INTERNATIONAL JOURNAL OF GEOGRAPHICAL INFORMATION SCIENCE

15

Questionnaire Accuracy 

Main Test Overall Accuracy

y
c
a
r
u
c
c
A

100%

90%

80%

70%

60%

50%

40%

30%

20%

10%

0%

45.2%

37.6%

100%

90%

80%

70%

60%

50%

40%

30%

20%

10%

0%

y
c
a
r
u
c
c
A

Technical Ability

Musicality Terrain Interpretation

A

B

C

Questionnaire Categories

Sonification Method

Figure 6. Participant response accuracy based on expertise (left) and on soniﬁcation method.

Figure 7. Five rhythms with increasing complexity.

response accuracy not only diﬀers signiﬁcantly between soniﬁcation methods A and C
(p < 0.001) but also between methods B and C (p < 0.05). The size of the eﬀect is again
large (d = 0.797). In other words, a signiﬁcant eﬀect on terrain interpretation could be
found when participants additionally had the opportunity to peruse a soniﬁed elevation
proﬁle (the only diﬀerence between soniﬁcation methods B and C).

As the diﬀerence between soniﬁcation methods B and C is only related to the
addition of the elevation proﬁle, the respective results were merged. Below, we thus
report aggregated results for a new, combined B/C method. Again, combined meth-
ods B/C yield the highest response accuracy (M = 80%, SD = 6%) compared to
method A (M = 74%, SD = 7%). This diﬀerence is signiﬁcant (F [1, 59] = 11.226,
p < 0.01, η2 = 16.0%, f = 43.6%, t [59] = −3.353) showing a large eﬀect (d = 0.915),

Downloaded by [University of New England] at 16:59 03 January 2018 16

J. SCHITO AND S. I. FABRIKANT

with observed power of 0.959. As previously demonstrated, participants perform
signiﬁcantly better with either method B or C compared to A.
In other words,
participants are more accurate in their terrain interpretation when the auditory
variable pan is employed for localization and pitch is used for elevation (methods
B/C), compared to method A, with the auditory variable pitch assigned to localization,
and the sound variable duration mapped to represent elevation. This diﬀerence
seems particularly evident when participants interpret continuous DEMs, again, some-
what surprising based on prior research.

Furthermore, the participants’ accuracy was analyzed when having to estimate abso-
lute elevation, comparing soniﬁcation methods A (M = 79%, SD = 11%) with collapsed
methods B/C (M = 87%, SD = 7%). A t-test yields signiﬁcant diﬀerences across both
soniﬁcation methods (t [59] = −3.187, p < 0.01, |ρ| = 39.2%). Again, our participants are
signiﬁcantly better at estimating elevation when the sound variable pitch is employed to
represent terrain, compared to the sound variable duration. Aside from investigating the
inﬂuence of the data characteristics, and the soniﬁcation methods applied to discrete
and continuous elevation data in soniﬁcation, assessment of the human factor was also
of interest.

Domain knowledge and training

Below, the results relating to users’ background and training are discussed, as hypothe-
sized that participants’ performance with auditory displays might also depend on
domain expertise diﬀerences as prior works contends (i.e. Bearman and Lovett 2010),
but which has not been systematically investigated until now in soniﬁcation research in
GIScience. The detailed catalog of assessed background and training questions is listed
in Appendix A (supplementary).

Boxplots presented in the left panel in Figure 6 show participants’ average accuracy
overall, assessed with the background knowledge and ability questionnaire (normalized
across question types). The box plots are organized by questions related to technical
ability, musicality and the ability to identify terrain features. The boxplots in the right
in Figure 6 depict average response accuracy across soniﬁcation methods for
panel
direct comparisons.

As shown in Figure 6, users’ background knowledge in reading terrain features tends
to be better and less variable than their technical ability or musicality. Participants’
accuracy is especially high for the tasks related to the identiﬁcation of terrain features
(>75%). Musicality and technical ability are somewhat low with approximately 50% of
answers being correct. Overall, participants’ auditory response accuracy is higher and
shows less variability in the main experiment compared to the background and ability
questionnaire.

As one can see in Figure 6, overall, participants’ variability in performance due to
domain expertise seems to be larger, than the accuracy of response variability when
working with the soniﬁed displays.

One of the compelling factors mentioned for the potential success of data soniﬁca-
tion is typically users’ background and level of training in music. Thus, further investiga-
tion was carried out into whether and how domain expertise and relevant abilities (i.e.
technical ability, musicality and topographic map reading) might explain response

Downloaded by [University of New England] at 16:59 03 January 2018 INTERNATIONAL JOURNAL OF GEOGRAPHICAL INFORMATION SCIENCE

17

Table 5. Relationships of participant performance using the soniﬁed display with domain expertise
and ability.

Domain knowledge and abilities
Pearson’s correlation

Domain knowledge and abilities
Spearman’s rho

Terrain complexity level

Discrete

Continuous (abstract) Correlation

Continuous (realistic)

Correlation
Power

Power
Correlation
Power

Technical Musical
0.114
0.221
0.342**
0.863
0.233*
0.571

−0.055
0.110
0.432***
0.972
0.351**
0.880

Topographical
0.183
0.412
0.359**
0.893
0.296*
0.759

Technical Musical
0.098
0.186
0.377**
0.920
0.287**
0.734

−0.061
0.119
0.411***
0.956
0.387**
0.932

Topographical
0.181
0.405
0.334**
0.848
0.431**
0.971

Pearson’s correlations (r) and Spearman’s rho (ρ) including statistical power.
*p < 0.05; **p < 0.01; ***p < 0.001.

accuracy with soniﬁed terrains. The results of this analysis are presented in the next
section below.

How does background knowledge and training inﬂuence terrain data interpretation
with soniﬁed displays?

Correlation analyses were conducted including Pearson’s r and Spearman’s rho for each
participant (applying a Fisher’s r-to-z transformation and converting back to r) to assess
potential associations of individual domain knowledge and abilities scores (assessed
with the background questionnaire) with soniﬁed data interpretation accuracy across
DEM complexity levels (recorded in the main experiment). Results are shown in Table 5.
No signiﬁcant correlations could be found for the lowest (discrete) DEM complexity level.
However, technical ability, musicality and topographic feature interpretation ability
seem to be associated with soniﬁed terrain interpretation for terrain complexity levels
2 and 3. As shown in Table 4, average correlations are signiﬁcant and moderately strong
for the abstract continuous model (level 2) with r coeﬃcients between 0.34 and 0.43. For
the most complex, that is realistic soniﬁed terrain (level 3), r coeﬃcients range only from
0.23 to 0.35 and thus are somewhat weaker and more variable than for terrain complex-
ity level 2. These results suggest that indeed relevant domain knowledge (especially
topographic map reading) and domain-related abilities (i.e. musicality and technical
ability) can inﬂuence the readability of a soniﬁed display.

Discussion

We set out to investigate the utility of PMS for spatial data exploration and wished to
empirically assess how people might interpret auditory displays showing DEMs of
varying complexity. We were also interested in assessing how an auditory elevation
proﬁle might support users in interpreting auditory terrain displays. Furthermore, we
wished to identify to what extent domain expertise and respective relevant abilities (i.e.
technical ability, musicality and topographic map reading) might explain response
accuracy with auditory displays.

Downloaded by [University of New England] at 16:59 03 January 2018 18

J. SCHITO AND S. I. FABRIKANT

How people might interpret auditory displays showing DEMs

First, the results show that continuous spatial elevation models can indeed be success-
fully interpreted by (blindfolded) users, overall yielding a mean accuracy up to 86.7%
(method C) using PMS. What is more, participants’ mean accuracy was even higher when
interpreting continuous DEMs compared to the seemingly easier discrete DEMs. This
ﬁnding contradicts prior research, suggesting that sound variables would not be useful
for the interpretation of continuous data (Krygier 1994). These results show that parti-
cipants were indeed able to more accurately interpret continuous terrain soniﬁcations
compared to discrete versions. Speciﬁcally, participants’ accuracy signiﬁcantly improved
by 6% with method A and 13% with soniﬁcation method B/C. In contrast to method A,
participants’ accuracy did not signiﬁcantly decline with increasing complexity, thus
moving from abstract-continuous to realistic-continuous models using method B/C. In
other words, the soniﬁcation method itself is also an important factor to consider, as it
may add perceptual and cognitive load to the interpretation of terrain models, irrespec-
tive of the complexity of the terrain.

learning eﬀect.

Given that the stimuli sequence was identical for all tested participants, the improve-
ment in response accuracy from the ﬁrst to the second terrain complexity level might
have been inﬂuenced by a potential
If learning eﬀects might have
confounded results (albeit diﬀerently across soniﬁcation methods), then this begs the
question why response accuracy did not further increase for the third, most complex
level. One could explain this by a potential ceiling eﬀect. One the one hand, method A is
perhaps not appropriate enough to explore a natural terrain model that features com-
plex elevation variations, over varying terrain distances. On the other hand, method B/C
might develop its full potential only at the most complex level. As explained earlier,
additional (more complex) questions were also included for the more complex terrain
trials. These questions were speciﬁcally adapted to the changing aﬀordances at each
level. We did this to further distinguish performances across complexity levels. Perhaps,
the rather long duration of the entire experiment might have negatively aﬀected
participants’ performance for the most complex level always presented last due to
fatigue. This might have especially aﬀected participants with method A, as this method
also might have been more diﬃcult. A potential learning eﬀect in our study might mean
for future experiments to randomize the stimuli presentation order, as to better disen-
It might also mean that real-world uses of soniﬁed
tangle this potential confound.
displays might indeed require a training phase to unleash its fully potential. At this
point in time, however, it is not clear whether a learning eﬀect might have aﬀected
results, and how, and if so, how much it matters.

Nevertheless, these results still shed new light on soniﬁcation of spatial data, con-
sidering that most soniﬁcation projects in GIScience have been conducted with catego-
rical spatial data (Fisher 1994, Lodha et al. 1999, Brauen 2006, Zhao 2006, MacVeigh and
Jacobson 2007, Bearman and Lovett 2010, Delogu et al. 2010, Laakso and Sarjakoski
2010), and there are very few studies using continuous data, but mostly outside of
GIScience (Heuten et al. 2006, Yeo and Berger 2008, Geronazzo et al. 2016).

The results also indicate that the speciﬁc combination of sound variables and respec-
tive mapping to data dimensions needs to be carefully considered in soniﬁed displays,
as it can improve or hinder data interpretation. Speciﬁcally, for our study, mapping the

Downloaded by [University of New England] at 16:59 03 January 2018 INTERNATIONAL JOURNAL OF GEOGRAPHICAL INFORMATION SCIENCE

19

sound variables pitch to elevation and pan for orientation along cardinal directions (N–
S–E–W), successfully supported participants in more accurately interpreting soniﬁed
displays (methods B/C), compared to using sound variables pitch for orientation and
duration for elevation (method A). Variable pitch was employed for all tested soniﬁcation
methods, as prior work suggests, and this found that it indeed serves as a key sound
variable. Even though many of the participants could successfully orient themselves in
the soniﬁed terrain by distinguishing two pitches, it was not suggested that pitch is
intuitive for spatial orientation, as its interpretation is based on a musical convention
that might not be universally understood (i.e. full screen height/width was equal to an
octave in our study). Moreover, the comparison of two pitches on diﬀerent ears requires
training.

The proposed method B/C is based on the same sound variables as proposed by
Fisher (1994), but it was decided to map them diﬀerently. As loudness is perceived very
subjectively (Campenhausen 1993) and depends on pitch (Genuit 2010), loudness was
not used to represent a magnitude change of a mapped attribute. Instead, it was chosen
to be mapped to the location change (i.e. pan) to take advantage of binaural localization
in a soundscape and thus to increase the immersive experience of sound spatialization.
Although spatialized sounds were applied in B and C (Nasir and Roberts 2007), immer-
sion might have been increased by implementing HRTF, as suggested by various authors
(Heuten et al. 2006, Zhao 2006, Geronazzo et al. 2016). Still, these results conﬁrm that the
appropriate spatialization of sound variables can indeed support listeners to orient
themselves in a soundscape. For terrain interpretation, the continuous data variable
elevation is more accurately interpreted using pitch compared to duration.

The empirical results for the assessment of two diﬀerent sound parameterization
techniques also suggest that the appropriate use of spatial metaphors, in this case
is also key for the success of soniﬁed displays (Kuhn 1996,
auditory mappings,
Fabrikant 2017). Especially, the consideration of the sound variable pitch for mapping
elevation or the use of pan for orientation in the soundscape transferred very well
into sound spatialization. It was thus suggested to use increasing pitch as a funda-
mental metaphorical mapping of the MORE-IS-UP image schema in soniﬁcation, as it
serves as the basis for understanding any type of linguistic, auditory or graphical
metaphor
interest
to communicate the magnitude or quantity of an item of
(Fabrikant 2017).

How auditory elevation proﬁles might support users in interpreting a DEM

Evidence was found that an auditory elevation proﬁle (method C) could facilitate
soniﬁed DEM interpretation. However, a signiﬁcant diﬀerence could only be detected
when compared directly to a similar sound parametrization. Here again, it is suggested
to more carefully consider the employed spatialization technique. The SOURCE-PATH-
DESTINATION image schema and the MORE-IS-UP image schema might be more eﬀec-
tively combined in future studies. Similarly to Yeo and Berger (2008), one could consider
to map the sound variable pitch to elevation with a soniﬁcation method that plays a
series of sounds within a short amount of time. This is a solution to create a holistic
hearing sensation that might also reduce cognitive load. Such an approach could be
interesting for further studies, for example, to be able to distinguish between important

Downloaded by [University of New England] at 16:59 03 January 2018 20

J. SCHITO AND S. I. FABRIKANT

and unimportant features and to get a general impression of the soniﬁed space. The
visual analog here would be the information visualization mantra ‘overview-ﬁrst, then
details-on-demand’ proposed by Shneiderman (1996), as many participants had diﬃ-
culty to remember the already retrieved information, especially while interpreting com-
plex, realistic elevation models.

Does domain expertise explain response accuracy?

As many prior empirical visualization studies show, we also ﬁnd that background and
training can inﬂuence user’s eﬀectiveness and eﬃciency in decision-making with a
soniﬁed display design. Speciﬁcally, the general ability of handling technical devices
(i.e. blind typing), musicality and terrain feature interpretation ability facilitated the
interpretation of the auditory display in our study, even if this eﬀect turned out not to
be a strong predictor for each of the tested complexity levels. Even if the pattern is not
straightforward, user background and training could have interacted in predictable ways
with potential learning eﬀects, as participants with better abilities generally achieved
signiﬁcantly higher accuracy, at least at complexity levels 2 and 3. A base capacity due to
background and training might be necessary to achieve high interpretation accuracy in
soniﬁcation. Bearman and Lovett (2010, p. 310) suggest that knowledge of geographical
data ‘resulted in an increased likelihood of a correct answer’ with soniﬁed displays,
however, without being statistically signiﬁcant. In contrast, Deutsch (2013) asserts that
musical understanding indeed plays an important role in understanding of complex,
multivariate auditory displays. These results thus also replicate many visualization stu-
dies in suggesting that background knowledge or expertise, for example, in handling
digital devices blindfolded, map reading and spatial data interpretation will
indeed
improve performance with other kinds of visuospatial displays.

Summary and outlook

This study empirically investigated the potential of auditory displays for exploring DEMs by
means of three PMS methods. The empirical results suggest, contrary to own expectations
and the results of prior research, that the sound variable pitch can be successfully inter-
preted by users of soniﬁed displays when it is applied to represent continuous spatial
elevation data. These encouraging results further suggest that soniﬁcation, yet another
spatialization method, indeed has untapped potential for spatiotemporal data exploration
in GIScience and beyond – even on complex terrain models. Perhaps given some time and
practice, it could become as popular as (geographic) information visualization, to support
(or even replace?) visual data encodings, when and where needed. Particularly, PMS might
be an additional attractive multivariate data dimensionality technique for sound spatializa-
tion, as PMS can ﬂexibly map various parameters representing diﬀerent data dimensions to
a set of sound variables. Sound variables can be further ﬁne-tuned by adaptive parameter-
izations, dependent on the data exploration task at hand.

Contrary to prior work suggesting only nominal or ordinal data for soniﬁcation, unique
empirical evidence was found indicating that continuous spatial data models are not inter-
preted signiﬁcantly worse when soniﬁed, compared to soniﬁed displays showing discrete
spatial data. As a potential learning eﬀect might have occurred due to the purposefully chosen

Downloaded by [University of New England] at 16:59 03 January 2018 INTERNATIONAL JOURNAL OF GEOGRAPHICAL INFORMATION SCIENCE

21

test setup, at this point, it is not clear how much training might be needed, to achieve
signiﬁcant accuracy improvements. Novel empirical evidence was also found to suggest that
the sound parameter mapping methods themselves oﬀer new and exciting opportunities for
sound spatialization research in GIScience. Future work could, for example, empirically explore
successful sound variable combinations that could eﬀectively and eﬃciently support users in
exploring geographic data in soniﬁed landscapes. It remains to be further empirically investi-
gated how sound variables might best capture essential spatial data characteristics, for
example, when mapping various data source domains (e.g. continuous terrains of diﬀerent
kinds) into soniﬁed target domains, represented as spatialized soundscapes. Our own success-
ful soniﬁcation method uses binaural pan for west–east orientation and pan between square
and sine waves for north–south orientation to localize elevation data represented with the
pitch sound variable in a soniﬁed terrain. As sound information must be decoded and
processed sequentially, the amount of perceived sound details also depends on the temporal
level of detail or on the auditory scale. Auditory generalization across scales in soniﬁed terrains
could be another line of exciting future research.

Furthermore, as we already know this for visuospatial displays, domain knowledge
and expertise are also important factors to consider to predict data exploration success
with soniﬁed displays. For soundscapes that contain terrain data, musicality and the
ability to read topographic maps are certainly relevant. Furthermore, a general ability to
handle technical devices (without sight) seems to play an important role as well.

Future studies will concentrate on extending this PMS approach to three or more sound
variables, which has not been investigated yet. These results further encourage empirical
soniﬁcation research concerning the communication of continuous geographical datasets.
For example, census data or other types of data collected on the ratio scale from surveys
could be soniﬁed and evaluated, without having to transform the data to the ordinal level.
As the purposefully chosen experimental setup gradually increased terrain complexity,
future studies might further investigate if and how training might inﬂuence performance
with soniﬁed displays. Finally, the sighted participants were blindfolded for the experiment.
It remains to be seen whether performance diﬀerences using auditory displays might also
be explained by the level of visual impairment.

Note

Appendix A (supplementary) contains the administered questions, including level of measurement
of collected data, and weighting schemes for scoring. Appendix B (published online) shows the
questionnaire used for the study. The developed software including use instructions, the spatial
dataset for the highest complexity level and the applied methods B/C as well as some audio
streams and short ﬁlms are available as a JAVA applet on the Web at http://www.geo.uzh.ch/~
jschito. The source code and the calculation of the variable operationalization are available on
request from the ﬁrst author.

Acknowledgments

This project was inspired by and is in honor of Prof. Pete Fisher’s trailblazing research on
soniﬁcation in GIScience. We wish to thank all participants who provided valuable data by serving
as research participants in the experiment. We also thank the anonymous reviewers who provided
feedback to improve the communication of our research results.

Downloaded by [University of New England] at 16:59 03 January 2018 22

J. SCHITO AND S. I. FABRIKANT

Disclosure statement

No potential conﬂict of interest was reported by the authors.

Joram Schito
Sara Irina Fabrikant

http://orcid.org/0000-0003-1138-8496

http://orcid.org/0000-0003-1263-8792

ORCID

References

Adhitya, S. and Kuuskankare, M., 2012. Sum: from image-based soniﬁcation to computeraided
composition. In: Proceedings of the 9th international symposium on computer music modelling
and retrieval. Presented at the international symposium on computer music modelling and retrieval
(CMMR). London: Queen Mary University of London.

Afonso, A., et al., 2005. A study of spatial cognition in an immersive virtual audio environment:
comparing blind and blindfolded individuals. In: Proceedings of the 17th meeting of the interna-
tional conference on auditory display(icad2011), 20-23 June, 2011. 6–9.

Amon, R., 2005. Lexikon der Harmonielehre. Wien: Doblinger.
Andrienko, G., et al., 2013. Visual analytics of movement. Berlin, Germany: Springer Science &

Business Media.

Bearman, N. and Lovett, A., 2010. Using sound to represent positional accuracy of address
locations. The Cartographic Journal, 47 (4), 308–314. doi:10.1179/000870410X12911302296833
Bly, S., 1982. Sound and computer information presentation. CA, USA: Lawrence Livermore National

Lab. California Univ., Davis (USA).

Brauen, G., 2006. Designing interactive sound maps using scalable vector graphics. Cartographica:
International Journal for Geographic Information and Geovisualization, 41 (1), 59–72. doi:10.3138/
5512-628G-2H57-H675

Brazil, E. and Fernström, M., 2011. Navigation of data. In: T. Hermann, J. Neuhoﬀ, and A. Hunt, eds.

The soniﬁcation handbook. Berlin: Logos, 509–523.

Bujacz, M., Skulimowski, P., and Strumillo, P., 2011. Soniﬁcation of 3d scenes using personalized
spatial audio to aid visually impaired persons. In: Proceedings of the 17th international conference
on auditory display (icad2011), 20-23 June, 2011. Budapest, Hungary. International Community
for Auditory Display

Campenhausen, C., 1993. Die Sinne des Menschen. Einführung in die Psychophysik der

Wahrnehmung. 2nd ed. Stuttgart: Georg Thieme Verlag.

Cartwright, W., Peterson, M.P., and Gartner, G., eds. 1999. Multimedia cartography. Berlin

Heidelberg: Springer.

Delogu, F., et al., 2010. Non-visual exploration of geographic maps: does soniﬁcation help?
Disability & Rehabilitation: Assistive Technology, 5 (3), 164–174. doi:10.3109/17483100903100277

Deutsch, D., 2013. The psychology of music. 3rd ed. London: Elsevier.
Fabrikant, S.I. (2017). Spatialization. In: Castree, N., Goodchild, M.F., Kobayashi, A., Liu, W, Marston,
R., Richardson, D. (eds). AAG International Encyclopedia of Geography: People, the Earth,
Environment, and Technology. Wiley. doi:10.1002/9781118786352.wbieg0812

Fisher, P.F., 1994. Hearing the reliability in classiﬁed remotely sensed images. Cartography and

Geographic Information Systems, 21 (1), 31–36. doi:10.1559/152304094782563975

Genuit, K., ed., 2010. Sound-Engineering im Automobilbereich: Methoden zur Messung und

Auswertung von Geräuschen und Schwingungen. 1st ed. Berlin: Springer.

Geronazzo, M., et al., 2016. Interactive spatial soniﬁcation for non-visual exploration of virtual
maps. International Journal of Human-Computer Studies, 85, 4–15. doi:10.1016/j.ijhcs.2015.08.004
Goldstein, E.B., 2002. Wahrnehmungspsychologie. 2nd ed. Berlin: Spektrum Akademischer Verlag.
Grond, F. and Berger, J., 2011. Parameter mapping soniﬁcation. In: T. Hermann, J. Neuhoﬀ, and A.

Hunt, eds. The soniﬁcation handbook. Berlin: Logos, 363–397.

Downloaded by [University of New England] at 16:59 03 January 2018 INTERNATIONAL JOURNAL OF GEOGRAPHICAL INFORMATION SCIENCE

23

Heuten, W., Wichmann, D., and Boll, S., 2006. Interactive 3d soniﬁcation for the exploration of city
maps. In: A. Morch, et al., eds. Proceedings of the 4th Nordic Conference on Human-computer
interaction: changing roles, 14–18 October 2006, Oslo, Norway. New York, NY: ACM.

Jacobson, R.D., 2010. Non-visual geographies. In: B. Warf, ed. Encyclopedia of geography. London:

Sage Publications.

Josselin, D., 2011. Spatial analysis using sonorous cartography. Some propositions. In: A. Ruas, ed.

Proceedings of the 25th international cartographic conference. 3-8 July 2011, Paris, France.

Josselin, D., et al., 2016. Sonorous cartography for sighted and blind people. In: AGILE’2016-19th
AGILE International conference on Geographic Information Science, June 2016, Helsinki, Finland.
Kalinak, K., 1992. Settling the score: music and the classical Hollywood ﬁlm. Madison: University of

Kramer, G., 1992. Auditory display: soniﬁcation, audiﬁcation, and auditory interfaces. Santa Fe:

Wisconsin Press.

Addison-Wesley.

Krygier, J.B., 1994. Sound and geographic visualization. In: Visualization in modern cartography.

New York: Pergamon, 149–166.

Kuhn, W., 1996. Handling data spatially: spatializating user interfaces.

In: M.-J kraak and M.
molenaar, eds. Proceeding of 7th International Symposium on spatial Data handling, SDH'96,
Advances in GIS research II, in Delft, 12–16 August 1996, The Netherlands: IGU

Laakso, M. and Sarjakoski, T., 2010. Sonic maps for hiking – use of sound in enhancing the map use
experience. The Cartographic Journal, 47 (4), 300–307. doi:10.1179/000870410X12911298276237
Lakoﬀ, G. and Johnson, M., 1980. The metaphorical structure of the human conceptual system.

Cognitive Science, 4 (2), 195–208. doi:10.1207/s15516709cog0402_4

Lodha, S.K., Joseph, A.J., and Renteria, J.C., 1999. Audio-visual data mapping for GIS-based data: an
experimental evaluation. In: Proceedings of the 1999 workshop on new paradigms in information
visualization and manipulation in conjunction with the eighth ACM international conference on
Information and knowledge management. 41–48. Kansas City, Missouri: ACM.

MacVeigh, R., and Jacobson, R.D., 2007. Implementing Auditory Icons in Raster GIS. In: Proceedings
of the 13th International Conference on Auditory Display, 26–29 June 2007, Montréal, Québec,
Canada. 530–535

McCormick, B.H., DeFanti, T.A., and Brown, M.D., 1987. Visualization in scientiﬁc computing. ACM

SIGGRAPH Computer Graphics, 21, 6.

Nasir, T., and Roberts, J.C., 2007. Soniﬁcation of spatial data. In: Proceedings of the 13th International

Conference on Auditory Display, 26–29, 2007 June, Montréal, Québec, Canada. 32–37

Paterson, N. and Conway, F., 2014. Engagement, Immersion and Presence. In: K. Collins, B. Kapralos,
and H. Tessler, eds. The Oxford Handbook of Interactive Audio. Oxford, UK: Oxford University
Press.

Pollack, I. and Ficks, L., 1954. Information of elementary multidimensional auditory displays. The

Journal of the Acoustical Society of America, 26, 155–158. doi:10.1121/1.1907300

Ruotolo, F., et al., 2013. Immersive virtual reality and environmental noise assessment: an innova-
tive audio–visual approach. Environmental Impact Assessment Review, 41, 10–20. doi:10.1016/j.
eiar.2013.01.007

Schiewe, J. and Weninger, B., 2012. Acoustic encoding of quantitative information in maps –
results of a study comparing with classical forms of representation. Kartographische Nachrichten,
3, 126–135.

Schwartz, H., 2003. The indefensible ear: a history. In: M. Bull and L. Back, eds. The auditory culture

reader. New York: Berg, 487–501.

Shneiderman, B., 1996. The eyes have it: a task by data type taxonomy for information visualiza-
In: VL '96 Proceedings of the 1996 IEEE Symposium on Visual Languages, 03–06, 1996

tions.
September. Washington, DC: IEEE Computer Society

Simonnet, M., et al., 2009. SeaTouch: a haptic and auditory maritime environment for non visual
cognitive mapping of blind sailors. In: G. Ligozat, et al., eds. Spatial information theory. Presented
at the COSIT 2009. Aber Wrac’h, France: Springer, 212–226.

Théberge, P., 2005. Sound maps: music and sound in cybercartography. Modern Cartography Series,

4, 389–410.

Downloaded by [University of New England] at 16:59 03 January 2018 24

J. SCHITO AND S. I. FABRIKANT

Ware, C., 2008. Visual thinking for design. San Francisco, CA: Morgan Kaufmann.
Williams, M.G., Smith, S., and Pecelli, G., 1990. Computer-human interface issues in the design of an
intelligent workstation for scientiﬁc visualization. ACM SIGCHI Bulletin, 21 (4), 44–49. doi:10.1145/
379106

Yeo, W.S., and Berger, J., 2008. Raster scanning: a new approach to image soniﬁcation, sound
visualization, sound analysis, and synthesis. In: International Computer Music Conference. http://
hdl.handle.net/2027/spo.bbp2372.2006.008

Yeung, E.S., 1980. Pattern recognition by audio representation of multivariate analytical data.

Analytical Chemistry, 52 (7), 1120–1123. doi:10.1021/ac50057a028

Zhao, H., 2006. Interactive Soniﬁcation of Abstract Data - Framework, Design Space, Evaluation, and

User Tool. Dissertation. University of Maryland, College Park.

Downloaded by [University of New England] at 16:59 03 January 2018 INTERNATIONAL JOURNAL OF GEOGRAPHICAL INFORMATION SCIENCE

25

14

14

14

)
d
e
u
n
i
t
n
o
C
(

e
d
u
t
i
t
p
a

e
c
n
e
i
r
e
p
x
E

g
1
;
0
f

s
u
o
m
o
t
o
h
c
i
D

×
3

e
d
u
t
i
t
p
A

g
1
(cid:5)
x
(cid:5)
0
j

R
2
x
f

s
u
o
u
n
i
t
n
o
C

l
a
c
i
r
i
p
m
e
n
a
:
s
t
e
k
r
a
m
d
e
d
i
s
-
o
w

t
n

i

i

g
n
m
o
h
i
t
l
u
M

.
)
1
1
0
2
(

.
S
,

h
c
s
r
e
m
e
r
t
S
&

,
.

V

,

n
a
m
s
d
n
a
L

:
e
e
s

,

n
o
i
t
a
m
r
o
f
n

i

r
e
h
t
r
u
f

r
o
F

.

5

o
t

d
e
t
i

m

i
l

s
i

s
n
o
i
t
a
r
e
n
e
g

e
l
o
s
n
o
c

e
m
a
g

f
o

r
e
b
m
u
n

e
h
T

4
5
–
9
3

,
)
6
(
5
7

,

g
n
i
t
e
k
r
a
M

f
o

l

a
n
r
u
o
J

.
y
r
t
s
u
d
n

i

e
l
o
s
n
o
c

e
m
a
g

o
e
d
i
v

e
h
t

n

i

y
r
i
u
q
n

i

d
r
a
o
b
y
e
k

e
h
t

o
t
n
o

g
n
i
k
o
o

l

t
u
o
h
t
i

w
s
y
e
k

d
r
a
o
b
y
e
k

e
h
t

o
t
n
o

g
n
i
k
o
o

l

t
u
o
h
t
i

w
s
y
e
k

’

J

‘

d
n
a

’

F

‘

e
h
t

n
o

s
k
r
a
m
e
h
t

d
n
ﬁ

’

J

‘

d
n
a

’

F

‘

e
h
t

n
o

s
k
r
a
m
e
h
t

d
n
ﬁ

t
o
n

l

d
u
o
c

t
n
a
p
i
c
i
t
r
a
P
=
0
0

.

l

d
u
o
c

t
n
a
p
i
c
i
t
r
a
P

=

0
1

.

:
s
w
o

l
l

o
f

s
a

l

d
e
t
a
u
c
l
a
c

e
r
a

d
n
a

0
1

.

o
t

.

0
0
m
o
r
f

s
e
g
n
a
r

e
r
o
c
S

–

1

k
s
a
T

t
r
o
p
e
r
-
f
l
e
s

i

g
n
p
y
t
-
d
n

i
l

B

d
r
a
o
b
y
e
k

e
h
t

o
t
n
o

g
n
i
k
o
o

l

t
u
o
h
t
i

w

s
y
e
k

n
e
s
o
h
c

y
l
i
r
a
r
t
i
b
r
a

3

d
r
a
o
b
y
e
k

e
h
t

o
t
n
o

g
n
i
k
o
o

l

t
u
o
h
t
i

w
s
y
e
k

n
e
s
o
h
c

y
l
i
r
a
r
t
i
b
r
a

3

d
n
ﬁ

d
n
ﬁ

t
o
n

l

d
u
o
c

l

d
u
o
c

t
n
a
p
i
c
i
t
r
a
P

t
n
a
p
i
c
i
t
r
a
P

=

=

0
1

.

0
0

.

:
s
w
o

l
l

o
f

s
a

l

d
e
t
a
u
c
l
a
c

e
r
a

d
n
a

0
1

.

o
t

.

0
0
m
o
r
f

s
e
g
n
a
r

e
r
o
c
S

–

2

k
s
a
T

5

o
t

n

i

:
s
w
o

l
l

o
f

s
a

l

d
e
t
a
u
c
l
a
c

e
r
a

d
n
a

0
1

.

.

0
0
m
o
r
f

s
e
g
n
a
r

e
r
o
c
S

–

3

k
s
a
T

i

g
n
p
y
t

d
n

i
l

b

n

i

d
e
c
n
e
i
r
e
p
x
e

t
o
n

i

g
n
p
y
t

d
n

i
l

b

d
e
c
n
e
i
r
e
p
x
e

s
i

s
i

t
n
a
p
i
c
i
t
r
a
P

t
n
a
p
i
c
i
t
r
a
P

=

=

0
1

.

0
0

.

t
n
e
m

s
s
e
s
s
a

e
d
u
t
i
t
p
a

t
x
e
n

e
h
t

n

i

d
e
s
u

r
e
h
t
r
u
f

s
a
w

r
e
w
s
n
a

s
i
h
T

i

g
n
p
y
t
-
d
n

i
l

b

n

i

d
e
c
n
e
i
r
e
p
x
e

t
o
n

s
a
w

t
n
a
p
i
c
i
t
r
a
p

e
h
t

f
i

5
2
1

.

r
o

i

g
n
p
y
t
-
d
n

i
l

b

n

i

d
e
c
n
e
i
r
e
p
x
e

s
a
w

t
n
a
p
i
c
i
t
r
a
p

e
h
t

f
i

1

=

=

e
c
n
e
i
r
e
p
x
e

e
c
n
e
i
r
e
p
x
e

:
s
w
o

l
l

o
f

s
a

l

d
e
t
a
u
c
l
a
c

e
r
a

d
n
a

0
1

.

o
t

.

0
0
m
o
r
f

s
e
g
n
a
r

e
r
o
c
S

t
s
e
t

y
t
i
l
i

b
a

i

g
n
p
y
t
-
d
n

i
l

B

i
Þ
(cid:3)
½

s
e
k
a
t
s
i

i
Þ
(cid:3)
½

s
e
k
a
t
s
i

m
(cid:4)
5
2
:
0
þ
1
ð
(cid:4)
(cid:3)
s
½

m
(cid:4)
5
2
:
0
þ
1
ð
(cid:4)
(cid:3)
s
½

e
m

i
t
(cid:4)
e
c
n
e
i
r
e
p
x
e
h
n
m
n
o
i
s
i
c
e
r
p

i

e
m

i
t
(cid:4)
e
c
n
e
i
r
e
p
x
e
h
t
c
e
j
b
u
s
n
o
i
s
i
c
e
r
p

¼
a

l
a
u
q
e
(

e
r
o
c
s

t
s
e
b
e
h
t
h
t
i

w

t
n
a
p
i
c
i
t
r
a
p
e
h
t

,
y
r
e
t
t
a
b
t
s
e
t

e
l
o
h
w
e
h
t

f
o

t
n
e
m

s
s
e
s
s
a

e
h
t

r
e
t
f
A

%
0
0
1

d
n
a
%
0

n
e
e
w
t
e
b

e
r
o
c
s

e
v
i
t
a
l
e
r

a

n
i
a
t
b
o

o
t

e
r
o
c
s

t
n
a
p
i
c
i
t
r
a
p

e
h
t

y
b

d
e
d
i
v
i
d

s
i

e
u
l
a
v

s
i
h
T

.
r
o
t
a
r
e
m
u
n

e
h
t

n

i

e
c
n
e
r
e
f
e
r

s
a

n
e
s
o
h
c

n
e
e
b

s
a
h

)
s
r
o
r
r
e

t
s
e
w
e
f

e
h
t

o
t

s

’

t
h
g
e
W

i

e
p
y
t

t
s
e
T

(cid:3)

e
c
n
e
i
r
e
p
x
E

0
N
2
k
^
1
(cid:5)
x
(cid:5)
0
j
k5

¼
x

g
n
i
r
o
c
S

]
1
0
[

,

e
g
n
a
r

(cid:1)

t
n
e
m
e
r
u
s
a
e
M

l

e
v
e

l

e
t
e
r
c
s
i
D

h
t
i

w

y
a
p

l

o
t

d
e
s
u

s
i

r
o

s
a
w

t
n
a
p
i
c
i
t
r
a
p

e
h
t

s
n
o
i
t
a
r
e
n
e
g

e
l
o
s
n
o
c

e
m
a
g

f
o

r
e
b
m
u
n

¼
a

:
s
w
o

l
l

o
f

s
a

l

d
e
t
a
u
c
l
a
c

e
r
a

d
n
a

0
1

.

o
t

.

0
0
m
o
r
f

s
e
g
n
a
r

e
r
o
c
S

e
c
n
e
i
r
e
p
x
e

e
m
a
g

o
e
d
V

i

y
t
i
l
i

b
a

l

a
c
i
n
h
c
e
T

e
p
y
t

n
o
i
t
s
e
u
Q

y
r
o
g
e
t
a
c

t
s
e
T

.

g
n
i
r
o
c
s

r
o
f

s
e
m
e
h
c
s

i

g
n
i
t
h
g
e
w
d
n
a

a
t
a
d

d
e
t
c
e

l
l

o
c

f
o

l
i

a
t
e
d

f
o

l

e
v
e

l

i

g
n
d
u
l
c
n

i

,

h
s
i
l

g
n
E

o
t
n

i

n
a
m
r
e
G
m
o
r
f

d
e
t
a
l
s
n
a
r
t

s
n
o
i
t
s
e
u
q

d
e
r
e
t
s
i
n
m
d
A

i

)
n
o
i
t
i
d
e

d
e
t
n
i
r
p

e
h
t

n

i

n
o
i
t
a
c
i
l

b
u
p

r
o
f
(

A

x
i
d
n
e
p
p
A

Downloaded by [University of New England] at 16:59 03 January 2018 26

J. SCHITO AND S. I. FABRIKANT

14

16

16

16

)
d
e
u
n
i
t
n
o
C
(

(cid:3)

(cid:3)

(cid:3)

t
h
g
e
W

i

e
p
y
t

t
s
e
T

g
n
i
r
o
c
S

]
1
0
[

,

e
g
n
a
r

e
d
u
t
i
t
p
A

g
1
(cid:5)
x
(cid:5)
0
j

R
2
x
f

t
n
e
m
e
r
u
s
a
e
M

l

e
v
e

l

s
u
o
u
n
i
t
n
o
C

e
p
y
t

n
o
i
t
s
e
u
Q

:
s
w
o

l
l

o
f

s
a

l

d
e
t
a
u
c
l
a
c

e
r
a

d
n
a

0
1

.

o
t

.

0
0
m
o
r
f

s
e
g
n
a
r

e
r
o
c
S

t
e
l
b
a
t

n
e
p

a

g
n

i
l

d
n
a
h

f
o

n
o
i
s
i
c
e
r
P

(cid:3)
½

i

s
t
n
o
P
d
e
r
e
t
s
i
g
e
r
(cid:4)
0
0
0
1

(cid:3)
½

5
2
6
0
:
0
D
S
(cid:4)
(cid:3)
s

m

½

e
m

i
t

¼
q

h
t
i

w

i
q
h
t
c
e
j
b
u
s
n
o
i
s
i
c
e
r
p

i
q
h
x
a
m
n
o
i
s
i
c
e
r
p

¼
a

.
)
d
e
u
n
i
t
n
o
C
(

y
r
o
g
e
t
a
c

t
s
e
T

e
d
u
t
i
t
p
A

0
N
2
k
^
1
(cid:5)
x
(cid:5)
0
j
k2

¼
x

e
t
e
r
c
s
i
D

×
5

(cid:1)

(cid:1)

(cid:1)

r
e
t
f
A

.
s
e
n

i
l

n
e
v
i
g

e
h
t

o
t

d
e
c
a
r
t

e
h
t

m
o
r
f

n
o
i
t
a
i
v
e
d

d
r
a
d
n
a
t
s

e
h
t

d
n
a

s
t
n
o
p

i

f
o

r
e
b
m
u
n

y
l
l
a
c
i
r
i
p
m
e

n
e
e
b

s
a
h

5
2
6
0
0

.

t
n
e
n
o
p
x
e

e
h
t

,
y
r
e
t
t
a
b

t
s
e
t

e
l
o
h
w
e
h
t

f
o

t
n
e
m

s
s
e
s
s
a

e
h
t

e
h
t

,

d
e
d
e
e
n

e
m

i
t

e
h
t

d
e
r
e
t
s
i
g
e
r

e
r
a
w

t
f
o
s

a

,
y
l
s
u
o
e
n
a
t
l
u
m
i
S

.
e
l
b
i
s
s
o
p

s
a

y
l
e
s
i
c
e
r
p

s
a

e
c
a
r
t

l

d
u
o
h
s

e
h
s
/
e
h

s
e
n

i
l

t
h
g
i
a
r
t
s

g
n
i
z
i
l
a
u
s
i
v

n
e
e
r
c
s

a

t
a

d
e
z
a
g

t
n
a
p
i
c
i
t
r
a
p

e
h
T

e
c
n
a
m
r
o
f
r
e
p

t
s
e
b

r
o
f

0
1

.

f
o

e
r
o
c
s

a

n
g
i
s
s
a

o
t

i

d
e
n
m
r
e
t
e
d

:

n
o
i
t
a
c
ﬁ
i
s
s
a
l
c

i

g
n
w
o

l
l

o
f

g
n
i
s
u

y
b

e
c
n
e
i
r
e
p
x
e

l
a
c
i
s
u
m

r
i
e
h
t

s
s
e
s
s
a

o
t

d
a
h

s
t
n
a
p
i
c
i
t
r
a
P

n
e
d

n

i

e
f
r
ü
w
t
n
e
s
n
e
b
e
L

e
h
c
s
i
l

a
k
i
s
u
m

:
e
m
u
ä
r
i
e
r
F
-
e
m
u
ä
r
b
u
C

l

.
)
5
0
0
2
(

.
S

,
t
g
o
V

4
4

,
r
e
t
i
e
r
n
e
r
ä
B

.
)
6

.
l

o
V
(

s
n

i
l
r
e
B

n
e
r
u
t
l
u
k
d
n
e
g
u
J

)
s
t
n
e
c

.

0
0
0
1
(

d
n
o
c
e
s

i

r
o
n
m
a

n
a
h
t

s
s
e
l

d
e
t
a
i
v
e
d

e
l
a
c
s

n
r
e
t
s
e
w

a

f
o

e
t
o
n

e
n
o

y
l
l

a
m
i
x
a
M
=

)
s
t
n
e
c

.

6
2
6
(

s
i
s
e
i
d

j

r
o
a
m
e
n
o

n
a
h
t

e
r
o
m
d
e
i
r
a
v

e
l
a
c
s

n
r
e
t
s
e
w

a

f
o

e
t
o
n

o
N

=

e
v
o
b
a

d
e
b
i
r
c
s
e
d

n
a
h
t

e
s
r
o
W
=

0
1

.

5
0

.

0
0

.

:
s
w
o

l
l

o
f

s
a

l

d
e
t
a
u
c
l
a
c

e
r
a

d
n
a

0
1

.

o
t

.

0
0
m
o
r
f

s
e
g
n
a
r

e
r
o
c
S

i

o
g
g
e
f
l
o
s

n
r
e
t
s
e
w
e
v
ﬁ

i

g
n
g
n
i
s

f
o

n
o
i
s
i
c
e
r
P

:
s
w
o

l
l

o
f

s
a

l

d
e
t
a
u
c
l
a
c

e
r
a

d
n
a

0
1

.

o
t

.

0
0
m
o
r
f

s
e
g
n
a
r

n
a
i
c
i
s
u
m

l

a
n
o
i
s
a
c
c
o

d
e
t
a
c
u
d
e
-
f
l
e
S

s
e
c
n
e
r
e
f
e
r
p

l

a
c
i
s
u
m
d
e
p
o
l
e
v
e
D

=

0
0

.

n
a
i
c
i
s
u
m

r
u
e
t
a
m
a

s
u
o
i
r
e
S

n
a
i
c
i
s
u
m

l

a
n
o
i
s
s
e
f
o
r
P

e
r
o
c
S

=

0
1

.

=

=

7
6
0

.

3
3
0

.

e
d
u
t
i
t
p
A

0
N
2
k
^
1
(cid:5)
x
(cid:5)
0
j
k2

¼
x

e
t
e
r
c
s
i
D

×
5

e
v
a
t
c
o

t
c
e
f
r
e
p

a

i

n
h
t
i

w
s
l
a
v
r
e
t
n

i

n
r
e
t
s
e
w
e
v
ﬁ

g
n
i
z
i
n
g
o
c
e
r

f
o

n
o
i
s
i
c
e
r
P

:
s
w
o

l
l

o
f

s
a

l

d
e
t
a
u
c
l
a
c

e
r
a

d
n
a

0
1

.

o
t

.

0
0
m
o
r
f

s
e
g
n
a
r

e
r
o
c
S

d
n
o
c
e
s

i

r
o
n
m
a

n
a
h
t

s
s
e
l

d
e
t
a
i
v
e
d

r
e
w
s
n
A

d
e
z
i
n
g
o
c
e
r

y
l
t
c
e
r
r
o
c

e
r
e
w
s
l
a
v
r
e
t
n

I

e
v
o
b
a

d
e
b
i
r
c
s
e
d

n
a
h
t

e
s
r
o
W
=

=

=

0
1

.

5
0

.

0
0

.

e
c
n
e
i
r
e
p
x
E

0
N
2
k
^
1
(cid:5)
x
(cid:5)
0
j
k3

¼
x

e
t
e
r
c
s
i
D

e
c
n
e
i
r
e
p
x
e

l
a
c
i
s
u
M

y
t
i
l

a
c
i
s
u
M

Downloaded by [University of New England] at 16:59 03 January 2018 INTERNATIONAL JOURNAL OF GEOGRAPHICAL INFORMATION SCIENCE

27

16

16

)
d
e
u
n
i
t
n
o
C
(

(cid:3)

e
d
u
t
i
t
p
A

0
N
2
k
^
1
(cid:5)
x
(cid:5)
0
j
k5

¼
x

(cid:1)

s
u
o
m
o
t
o
h
c
i
D

×
5

,
e
u
l
a
v

e
u
r
t

e
h
t

s
i

Þ
X
ð
E

.
s
t
n
o
p

i

0
1

.

f
o

e
r
o
c
s

a

t
n
a
p
i
c
i
t
r
a
p

t
s
e
b

e
h
t

n
g
i
s
s
a

o
t

i

d
e
n
m
r
e
t
e
d

e
v
a
t
c
o
t
c
e
f
r
e
p
A

.

%
0
0
1
d
n
a
%
0
n
e
e
w
t
e
b
e
g
n
a
r

a
n
h
t
i

i

w

r
e
w
s
n
a
n
e
v
i
g
e
h
t

s
i

i

x

s
a
e
r
e
h
w

a

,
s
u
h
T

.

h
c
a
e

s
t
n
e
c

0
0
1

f
o

s
d
n
o
c
e
s

r
o
n
m

i

l
a
u
q
e

2
1

f
o

s
t
s
i
s
n
o
c

t
n
e
m
a
r
e
p
m
e
t

l
a
u
q
e
h
t
i

w

0
0
1

o
t

l
a
u
q
e

s
i

d
n
o
c
e
s

i

r
o
n
m
a

s
a
e
r
e
h
w

,

%
0
0
1
=
s
t
n
e
c

0
0
2
1

o
t

l
a
u
q
e

s
i

e
v
a
t
c
o

t
c
e
f
r
e
p

y
l
l
a
c
i
r
i
p
m
e
n
e
e
b
s
a
h
8
0
1

.

f
o

t
n
a
t
s
n
o
c

e
h
t

,
y
r
e
t
t
a
b
t
s
e
t

e
l
o
h
w
e
h
t

f
o

t
n
e
m

s
s
e
s
s
a

e
h
t

r
e
t
f
A

%
3
3
8

.

=

s
t
n
e
c

:
s
w
o

l
l

o
f

s
a

l

d
e
1
(cid:2)
t
a
!
u
!
c
l
a
c

e
r
a

d
n
a

ﬃ
ﬃ
ﬃ
ﬃ
ﬃ
ﬃ
ﬃ
ﬃ
ﬃ
ﬃ
ﬃ
ﬃ
ﬃ
ﬃ
ﬃ
ﬃ
ﬃ
ﬃ
ﬃ
ﬃ
ﬃ
ﬃ
ﬃ
ﬃ
ﬃ
ﬃ
ﬃ
ﬃ
ﬃ
ﬃ
ﬃ
ﬃ
ﬃ
ﬃ
ﬃ
ﬃ

0
1

.

o
t

.

0
s
0
 
m
o
r
f

s
e
g
n
a
r
 
e
r
o
c
S

Pn

1
¼

i

1

1
(cid:2)
n

2

Þ
Þ
X
ð
E
(cid:2)

i

x
ð

p
x
e

(cid:4)
8
0
:
1

¼
a

e
l
a
c
s

e
g
a
t
n
e
c
r
e
p

:
s
w
o

l
l

o
f

s
a

l

d
e
t
a
u
c
l
a
c

e
r
a

d
n
a

0
1

.

o
t

.

0
0
m
o
r
f

s
e
g
n
a
r

e
r
o
c
S

y
l
t
c
e
r
r
o
c

d
e
ﬁ
i
t
n
e
d

i

t
o
n

y
l
t
c
e
r
r
o
c

d
e
ﬁ
i
t
n
e
d

i

s
a
w

s
a
w

e
p
a
h
s

e
p
a
h
s

e
v
a
w
e
h
T

e
v
a
w
e
h
T

=

=

0
1

.

0
0

.

s
e
p
a
h
s

e
v
a
w
e
v
ﬁ

g
n
i
z
i
n
g
o
c
e
r

f
o

n
o
i
s
i
c
e
r
P

t
h
g
e
W

i

e
p
y
t

t
s
e
T

g
n
i
r
o
c
S

]
1
0
[

,

e
g
n
a
r

t
n
e
m
e
r
u
s
a
e
M

l

e
v
e

l

e
p
y
t

n
o
i
t
s
e
u
Q

e
d
u
t
i
t
p
A

g
1
(cid:5)
x
(cid:5)
0
j

R
2
x
f

s
u
o
u
n
i
t
n
o
C

a

g
n
i
s
u

y
b

e
v
a
t
c
o

t
c
e
f
r
e
p

e
n
o

i

n
h
t
i

w
s
l
a
v
r
e
t
n

i

n
r
e
t
s
e
w
e
v
ﬁ

g
n
i
y
f
i
t
n
a
u
q

f
o

n
o
i
s
i
c
e
r
P

.
)
d
e
u
n
i
t
n
o
C
(

y
r
o
g
e
t
a
c

t
s
e
T

Downloaded by [University of New England] at 16:59 03 January 2018 28

J. SCHITO AND S. I. FABRIKANT

t
h
g
e
W

i

e
p
y
t

t
s
e
T

(cid:3)

16

e
d
u
t
i
t
p
A

0
N
2
k
^
1
(cid:5)
x
(cid:5)
0
j
k2

¼
x

g
n
i
r
o
c
S

]
1
0
[

,

e
g
n
a
r

(cid:1)

t
n
e
m
e
r
u
s
a
e
M

l

e
v
e

l

e
t
e
r
c
s
i
D

×
5

e
p
y
t

n
o
i
t
s
e
u
Q

y
t
i
x
e
l
p
m
o
c

g
n
i
s
a
e
r
c
n

i

f
o

s

m
h
t
y
h
r

g
n
i
c
u
d
o
r
p
e
r

f
o

n
o
i
s
i
c
e
r
P

e
v
ﬁ

:
s
w
o

l
l

o
f

s
a

l

d
e
t
a
u
c
l
a
c

e
r
a

d
n
a

0
1

.

o
t

.

0
0
m
o
r
f

s
e
g
n
a
r

e
r
o
c
S

r
e
t
c
a
r
a
h
c

s
t
i

d
n
a

o
p
m
e
t

s
t
i

g
n
i
r
e
t
l
a

t
u
o
h
t
i

w
d
e
p
p
a
t

m
h
t
y
h
R

=

m
h
t
y
h
r

e
h
t

g
n
i
y
a
c
e
d

t
u
o
h
t
i

w
e
k
a
t
s
i

m
e
n
o

y
l
l

a
m
i
x
a
M
=

s

0
1

.

o
t

5
0

.

f
o

y
a
l
e
d

l

a
t
o
t

a

h
t
i

w
g
n
p
p
a
t

i

s
u
o
u
n
e
T

=

0
1

.

5
0

.

5
0

.

0
0

.

e
v
o
b
a

d
e
b
i
r
c
s
e
d

n
a
h
t

e
s
r
o
W
=

)
7

e
r
u
g
F

i

r
e
f
e
r
(

Þ
5
x

(cid:4)
5
þ

4
x

(cid:4)
4
þ

3
x

(cid:4)
3
þ

2
x

(cid:4)
2
þ

1
x
ð

(cid:4)

c

¼
a

:
s
w
o

l
l

o
f

d
e
t
a
l
u
c
l
a
c

s
a
w
d
n
a

0
1

.

o
t

.

8
0
m
o
r
f

s
e
g
n
a
r

c

r
o
t
c
a
f

t
n
a
t
s
n
o
C

y
l
s
s
e
l
m
a
e
s

d
e
u
n
i
t
n
o
c

t
o
n

s
a
w
m
h
t
y
h
r

y
l
s
s
e
l
m
a
e
s

d
e
u
n
i
t
n
o
c

s
a
w
m
h
t
y
h
r

n
e
v
i
G

n
e
v
i
G

=

=

0
1

.

8
0

.

.
)
d
e
u
n
i
t
n
o
C
(

y
r
o
g
e
t
a
c

t
s
e
T

)
d
e
u
n
i
t
n
o
C
(

14

14

(cid:3)

(cid:3)

e
d
u
t
i
t
p
A

0
N
2
k
^
1
(cid:5)
x
(cid:5)
0
j
k2

¼
x

e
t
e
r
c
s
i
D

×
4

(cid:1)

(cid:1)

:
s
w
o

l
l

o
f

s
a

l

d
e
t
a
u
c
l
a
c

e
r
a

d
n
a

0
1

.

o
t

.

0
0
m
o
r
f

s
e
g
n
a
r

e
r
o
c
S

s
e
r
u
t
c
u
r
t
s

l
a
i
v
u
ﬂ

f
o

s
t
c
e
ﬀ
e

g
n
i
t
a
m

i
t
s
e

f
o

y
t
i
l
i

b
A

t
c
e
r
r
o
c

d
n
a

r
a
e
l
c

g
n
i
y
f
s
i
t
a
s

s
a
w

s
a
w

r
e
w
s
n
A

r
e
w
s
n
A

=

=

e
v
o
b
a

d
e
b
i
r
c
s
e
d

n
a
h
t

e
s
r
o
W
=

0
1

.

5
0

.

0
0

.

t
c
e
r
r
o
c

d
n
a

r
a
e
l
c

g
n
i
y
f
s
i
t
a
s

s
a
w

s
a
w

r
e
w
s
n
A

r
e
w
s
n
A

=

=

e
v
o
b
a

d
e
b
i
r
c
s
e
d

n
a
h
t

e
s
r
o
W
=

0
1

.

5
0

.

0
0

.

e
d
u
t
i
t
p
A

0
N
2
k
^
1
(cid:5)
x
(cid:5)
0
j
k2

¼
x

e
t
e
r
c
s
i
D

×
4

y
l
s
s
e
l
m
a
e
s

m
h
t
y
h
r

n
e
v
i
g

e
h
t

e
u
n
i
t
n
o
c

t
o
n

i

d
d

d
e
u
n
i
t
n
o
c

s
e
r
u
t
c
u
r
t
s

l

l
a
c
i
g
o
o
h
p
r
o
m
o
e
g

t
n
a
c
ﬁ
n
g
i
s

i

g
n
i
z
i
n
g
o
c
e
r

f
o

y
t
i
l
i

b
A

i

n
a
r
r
e
T

:
s
w
o

l
l

o
f

s
a

l

d
e
t
a
u
c
l
a
c

e
r
a

d
n
a

0
1

.

o
t

.

0
0
m
o
r
f

s
e
g
n
a
r

e
r
o
c
S

n
o
i
t
a
t
e
r
p
r
e
t
n

i

m
h
t
y
h
r

d
e
x
i
m
a

o
t

,
y
l
l
a
n
ﬁ

y
b

d
e
i
l

p
i
t
l
u
m

s
a
w

t
l
u
s
e
r

s
i
h
t

f
o
m
u
s

e
h
T

d
n
a

)
t
h
g
i
e
w
d
o
f
r
u
o
f

l

,
4
x
(

s
k
a
e
r
b

h
t
i

w
m
h
t
y
h
r

d
e
t
a
p
o
c
n
y
s

.
)
t
h
g
i
e
w
d
o
f
e
v
ﬁ

l

,
5
x
(

s
t
e
l
p
i
r
t

d
e
t
a
p
o
c
n
y
s

h
t
i

w

g
n

i
l

b
u
o
d

,

d
e
x
i
m
a
o
t

)
t
h
g
i
e
w
e
l
g
n
i
s

,
1
x
(
n
r
e
t
t
a
p
r
a
l
u
g
e
r

a
m
o
r
f
d
e
s
a
e
r
c
n

i

s
a
w
y
t
i
x
e
l
p
m
o
C

a

o
t

)
t
h
g
i
e
w
e
l
p
i
r
t

,
3
x
(

m
h
t
y
h
r

d
e
t
t
o
d

,

d
e
x
i
m
a

o
t

)
t
h
g
i
e
w
e
l
b
u
o
d

,
2
x
(

m
h
t
y
h
r

t
n
a
p
i
c
i
t
r
a
p

e
h
t

f
i

)
8
0

.

=

c

r
o
t
c
a
f
(

%
0
2

y
b

t
l
u
s
e
r

e
h
t

d
e
h
s
i
n
m
d

i

i

h
c
i
h
w

,

c

r
o
t
c
a
f

e
h
t

Downloaded by [University of New England] at 16:59 03 January 2018 INTERNATIONAL JOURNAL OF GEOGRAPHICAL INFORMATION SCIENCE

29

14

14

0

3

4
1

3

4
1

)
d
e
u
n
i
t
n
o
C
(

t
h
g
e
W

i

e
p
y
t

t
s
e
T

g
n
i
r
o
c
S

]
1
0
[

,

e
g
n
a
r

e
d
u
t
i
t
p
A

0
N
2
k
^
1
(cid:5)
x
(cid:5)
0
j
k2

¼
x

t
n
e
m
e
r
u
s
a
e
M

l

e
v
e

l

e
t
e
r
c
s
i
D

×
4

(cid:3)

(cid:3)

(cid:1)

(cid:1)

e
d
u
t
i
t
p
A

0
N
2
k
^
1
(cid:5)
x
(cid:5)
0
j
k2

¼
x

e
t
e
r
c
s
i
D

×
3

e
s
n
o
p
s
e
r

l

y
a
p
s
i
d

y
r
o
t
i
d
u
A

y
r
o
t
i
d
u
A

e
s
n
o
p
s
e
r

l

y
a
p
s
i
d

e
s
n
o
p
s
e
r

l

y
a
p
s
i
d

y
r
o
t
i
d
u
A

g
1
;
0
f

g
1
;
0
f

s
u
o
m
o
t
o
h
c
i
D

×
3

g
1
(cid:5)
x
(cid:5)
0
j

R
2
x
f

s
u
o
u
n
i
t
n
o
C

×
3

s
u
o
m
o
t
o
h
c
i
D

d
n
u
o
r
g
k
c
a
b

s
u
o
n
e
g
o
m
o
h

a

n
o

s
e
p
a
h
s

f
o

r
e
b
m
u
n

t
c
e
r
r
o
c

e
h
t

g
n
i
z
i
n
g
o
c
e
R

l

e
v
e

l

l

y
t
i
x
e
p
m
o
C

n
o
i
t
s
e
u
q

y
r
o
t
c
u
d
o
r
t
n

i

s
i
h
t

r
o
f

)
0
0
(

.

s
t
n
o
p

i

o
n

i

d
e
n
a
t
b
o

s
t
n
a
p
i
c
i
t
r
a
P

t
c
a
r
t
s
b
a
(

1

)
e
t
e
r
c
s
i
d

i

d
o
r
t
n
e
c

e
h
t

o
t

e
c
n
a
r
e
l
o
t

f
o

s
l
e
x
i
p

0
5

i

n
h
t
i

w
e
r
o
c
s

l
l

u
f

a

t
e
g

l

d
u
o
c

s
t
n
a
p
i
c
i
t
r
a
P

d
n
u
o
r
g
k
c
a
b

s
u
o
n
e
g
o
m
o
h

a

n
o

s
e
p
a
h
s

l
a
c
i
r
t
e
m
o
e
g

e
e
r
h
t

g
n
i
y
f
i
t
n
e
d

I

:
s
w
o

l
l

o
f

s
a

l

d
e
t
a
u
c
l
a
c

e
r
a

d
n
a

0
1

.

o
t

.

0
0
m
o
r
f

s
e
g
n
a
r

e
r
o
c
S

y
l
t
c
e
r
r
o
c

d
e
ﬁ
i
t
n
e
d

i

n
e
e
b

t
o
n

y
l
t
c
e
r
r
o
c

d
e
ﬁ
i
t
n
e
d

i

n
e
e
b

s
a
h

s
a
h

e
p
a
h
s

e
p
a
h
s

e
h
T

e
h
T

=

=

0
1

.

0
0

.

’

i

d
o
r
t
n
e
c

s
e
p
a
h
s

l
a
c
i
r
t
e
m
o
e
g

a

f
o

n
o
i
t
a
c
o

l

t
c
e
r
r
o
c

e
h
t

g
n
i
y
f
i
t
n
e
d

I

:
s
w
o

l
l

o
f

s
a

l

d
e
t
a
u
c
l
a
c

e
r
a

d
n
a

0
1

.

o
t

.

0
0
m
o
r
f

s
e
g
n
a
r

e
r
o
c
S

(cid:3)
x
p
½

i

d
o
r
t
n
e
c

o
t

e
c
n
a
t
s
i
d

s
0
t
c
e
j
b
u
s

(cid:3)
x
p
½
0
5
þ
(cid:3)
x
p
½

e
c
n
a
t
s
i
d

d
l
e
i
y
x
a
m

(cid:2)
1
¼
a

t
n
e
s
e
r
p
e
r

i

t
h
g
m
n
o
i
s
n
e
t
x
e

p
a
m
e
h
t

a
e
r
a

h
c
i
h
w
g
n
i
z
i
n
g
o
c
e
r

f
o

y
t
i
l
i

b
A

:
s
w
o

l
l

o
f

s
a

l

d
e
t
a
u
c
l
a
c

e
r
a

d
n
a

0
1

.

o
t

.

0
0
m
o
r
f

s
e
g
n
a
r

e
r
o
c
S

y
t
i
r
a

l
i

m

i
s

c
i
h
p
a
r
g
o
p
o
t

f
o

e
s
u
a
c
e
b

e
l
b
a
n
o
s
a
e
r

e
v
o
b
a

d
e
b
i
r
c
s
e
d

n
a
h
t

e
s
r
o
W
=

t
c
e
r
r
o
c

s
a
w

s
a
w

r
e
w
s
n
A

r
e
w
s
n
A

=

=

0
1

.

5
0

.

0
0

.

e
p
y
t

n
o
i
t
s
e
u
Q

:
s
w
o

l
l

o
f

s
a

l

d
e
t
a
u
c
l
a
c

e
r
a

d
n
a

0
1

.

o
t

.

0
0
m
o
r
f

s
e
g
n
a
r

e
r
o
c
S

s
e
r
u
t
c
u
r
t
s

l
a
c
i
h
p
a
r
g
o
p
o
t

g
n
i
z
i
n
g
o
c
e
r

f
o

y
t
i
l
i

b
A

t
c
e
r
r
o
c

d
n
a

r
a
e
l
c

g
n
i
y
f
s
i
t
a
s

s
a
w

s
a
w

r
e
w
s
n
A

r
e
w
s
n
A

=

=

e
v
o
b
a

d
e
b
i
r
c
s
e
d

n
a
h
t

e
s
r
o
W
=

0
1

.

5
0

.

0
0

.

.
)
d
e
u
n
i
t
n
o
C
(

y
r
o
g
e
t
a
c

t
s
e
T

Downloaded by [University of New England] at 16:59 03 January 2018 30

J. SCHITO AND S. I. FABRIKANT

e
s
n
o
p
s
e
r

l

y
a
p
s
i
d

t
h
g
e
W

i

e
p
y
t

t
s
e
T

(cid:3)

2

4
1

y
r
o
t
i
d
u
A

0
N
2
k
^
1
(cid:5)
x
(cid:5)
0
j
k5

¼
x

g
n
i
r
o
c
S

]
1
0
[

,

e
g
n
a
r

(cid:1)

t
n
e
m
e
r
u
s
a
e
M

l

e
v
e

l

e
t
e
r
c
s
i
D

×
2

e
l
g
n
a
t
c
e
r

a

f
o

d
n
a

e
l
g
n
a
i
r
t

r
a
l
u
g
e
r

a

f
o

n
o
i
t
a
t
n
e
i
r
o

e
h
t

g
n
i
y
f
i
t
n
e
d

i

f
o

y
c
a
r
u
c
c
A

:
s
w
o

l
l

o
f

s
a

l

d
e
t
a
u
c
l
a
c

e
r
a

d
n
a

0
1

.

o
t

.

0
0
m
o
r
f

s
e
g
n
a
r

e
r
o
c
S

e
p
y
t

n
o
i
t
s
e
u
Q

3

4
1

4
31

y
r
o
t
i
d
u
A

l

y
a
p
s
i
d

e
s
n
o
p
s
e
r

y
r
o
t
i
d
u
A

l

y
a
p
s
i
d

e
s
n
o
p
s
e
r

)
d
e
u
n
i
t
n
o
C
(

g
1
(cid:5)
x
(cid:5)
0
j

R
2
x
f

s
u
o
u
n
i
t
n
o
C

g
1
(cid:5)
x
(cid:5)
0
j

R
2
x
f

s
u
o
u
n
i
t
n
o
C

)

t
i

(cid:6)
0
9

2

s

’

.

l

n
o
i
t
u
o
s
n
e
v
i
g
e
h
t
d
n
a
r
e
w
s
n
a

t
n
a
p
i
c
i
t
r
a
p
a
n
e
e
w
t
e
b
n
o
i
t
a
i
v
e
d
r
a
l
u
g
n
a
e
h
t

e
l
g
n
a

e
h
t

f
l
a
h
n
a
h
t

r
e
l
l
a
m

s

s
i

e
s
u
a
c
e
b
n
e
s
o
h
c
n
e
e
b
s
a
h
°
5
2

i

f
o
n
g
r
a
m

r
o
r
r
e

r
e
p
p
u
n
A

<
(cid:6)
5
2
(

e
l
g
n
a
t
c
e
r

a

f
o

r
o

)

(cid:6)
0
6

2

<
(cid:6)
5
2
(

e
l
g
n
a
i
r
t

r
a
l
u
g
e
r

a

f
o

’

’

(cid:3)
½

t
h
g
i
e
h

e
v
i
t
a
l
e
r

e
h
t

f
o

s
t
n
e
c
r
e
p

g
n
i
t
a
i
v
e
d
(cid:2)
1
¼
a
:
s
w
o

l
l

o
f

s
a

l

d
e
t
a
u
c
l
a
c

e
r
a

d
n
a

0
1

.

o
t

.

0
0
m
o
r
f

s
e
g
n
a
r

e
r
o
c
S

n
o
i
t
a
v
e
l
e

s
e
p
a
h
s

a

g
n
i
y
f
i
t
n
e
d

i

f
o

y
c
a
r
u
c
c
A

(cid:3)
½

e
z
i
s

l

a
e
r

e
h
t

f
o

s
t
n
e
c
r
e
p

g
n
i
t
a
i
v
e
d
(cid:2)
1
¼
a
:
s
w
o

l
l

o
f

s
a

l

d
e
t
a
u
c
l
a
c

e
r
a

d
n
a

0
1

.

o
t

.

0
0
m
o
r
f

s
e
g
n
a
r

e
r
o
c
S

e
z
i
s

s
e
p
a
h
s

a

g
n
i
y
f
i
t
n
e
d

i

f
o

y
c
a
r
u
c
c
A

°
5
1

°
0
2

°
5
2

<

<

<

°
0
1

°
5

<

<

φ

φ

≤

≤

°
0

°
5

φ

φ

φ

≤

≤

≤

°
5
2

°
0
1

°
5
1

°
0
2

≥

φ

=

=

=

=

=

=

0
1

.

8
0

.

6
0

.

4
0

.

2
0

.

0
0

.

s
e
b
i
r
c
s
e
d
φ

.
)
d
e
u
n
i
t
n
o
C
(

y
r
o
g
e
t
a
c

t
s
e
T

Downloaded by [University of New England] at 16:59 03 January 2018 INTERNATIONAL JOURNAL OF GEOGRAPHICAL INFORMATION SCIENCE

31

)
d
e
u
n
i
t
n
o
C
(

’

)
n
o
i
s
n
e
t
x
e

s
e
r
u
t
c
u
r
t
s

a

f
o

d
r
i
h
t

a

.
x
o
r
p
p
a
(

e
r
o
c
s

a

t
e
g

o
t

d
e
d
e
e
c
x
e

e
b

t
o
n

t
s
u
m

t
a
h
t

e
c
n
a
r
e
l
o
T
=

l
e
x
i
p

n

i

i

t
n
o
p

t
e
g
r
a
t

e
h
t

o
t

n
o
i
t
i
s
o
p

r
o
s
r
u
c

e
h
t

m
o
r
f

e
c
n
a
t
s
i
d

r
e
t
s
a
r

d
e
r
u
s
a
e
M
=

t
e
g
r
a
T
r
o
s
r
u
c
d

t
e
l
b
a
t

n
e
p

e
h
t

f
o

e
g
d
e

n
a

o
t

n
o
i
t
i
s
o
p

d
e
d
r
o
c
e
r

e
h
t

m
o
r
f

e
c
n
a
t
s
i
d

t
s
e
g
r
a
L

=

1
(cid:5)
bx
(cid:5)
0

s
a
e
r
e
h
w

s
i
x
a
-
x

e
h
t

n
o

d
e
t
a
m

i
t
s
e

t
n
a
p
i
c
i
t
r
a
p

e
h
t

n
o
i
t
i
s
o
P

=

k
s
a
t

s
i
h
t

r
o
f

y
c
a
r
u
c
c
a

d
e
l
e
d
o
M
=

a

1
(cid:5)

by
(cid:5)
0

s
a
e
r
e
h
w
s
i
x
a
-
y

e
h
t

n
o

d
e
t
a
m

i
t
s
e

t
n
a
p
i
c
i
t
r
a
p

e
h
t

n
o
i
t
i
s
o
P

=

s
i
x
a
-
x

e
h
t

n
o

n
o
i
t
i
s
o
p

e
v
i
t
c
e
ﬀ
e

s

’

t
e
g
r
a
T

=

s
i
x
a
-
x

e
h
t

n
o

n
o
i
t
u
o
s
e
r

l

l

s
y
a
p
s
i
d

y
r
o
t
i
d
u
A

=

s
i
x
a
-
y

e
h
t

n
o

n
o
i
t
i
s
o
p

e
v
i
t
c
e
ﬀ
e

s

’

t
e
g
r
a
T

=

s
i
x
a
-
y

e
h
t

n
o

n
o
i
t
u
o
s
e
r

l

l

s
y
a
p
s
i
d

y
r
o
t
i
d
u
A

=

’

’

x
a
m
d

l

o
t
d

bx

t
e
g
r
a
t
x

x
s
e
r

t
e
g
r
a
t

y
s
e
r

by

y

y
l
e
t
a
r
u
c
c
a
w
o
h

,

n
o
i
t
s
e
u
q
e
h
t

o
t

r
e
f
e
r

s
t
e
k
c
a
r
b
o
w

t

t
s
a
l

s
a
l
u
m
r
o
f

e
h
T

.
)
y
e
l
l
a
v

,
k
n
i
s

,

p
d

i

l

d
u
o
c

t
n
a
p
i
c
i
t
r
a
p
a

y
l
e
t
a
r
u
c
c
a
w
o
h

,

n
o
i
t
s
e
u
q
e
h
t

o
t

r
e
f
e
r

s
t
e
k
c
a
r
b
o
w

t

,

n
i
a
t
n
u
o
m

(

e
r
u
t
c
u
r
t
s

l

l
a
c
i
g
o
o
h
p
r
o
m
o
e
g

a

f
o

n
o
i
t
a
c
o

l

t
e
g
r
a
t

y
x

e
h
t

e
t
a
c
o

l

’

t
s
r
ﬁ
s
a
l
u
m
r
o
f

e
h
T

y
l
e
v
i
t
c
e
ﬀ
e

’

s

’

n
o
i
t
i
s
o
p

l

e
t
u
o
s
b
a

t
e
g
r
a
t

e
h
t

y
f
i
t
n
e
d

i

l

d
u
o
c

t
n
a
p
i
c
i
t
r
a
p

a

s
a
h

t
e
k
c
a
r
b

d
n
o
c
e
s

e
h
t

s
a
e
r
e
h
w
5
1

.

h
t
i

w
d
e
t
h
g
i
e
w
n
e
e
b

s
a
h

t
e
k
c
a
r
b

e
h
t

,

n
o
i
t
s
e
u
q

h
c
a
e

r
o
f

n
w
o
n
k

s
a
w

i

t
n
o
p

t
e
g
r
a
t

e
h
t

e
c
n
i
S

.
e
r
u
t
c
u
r
t
s

l

l
a
c
i
g
o
o
h
p
r
o
m
o
e
g

e
h
t

e
h
t

l
e
d
o
m
o
t

n
e
s
o
h
c

n
e
e
b

s
a
h

l
o
t
d

,
t
e
k
c
a
r
b

e
h
t

n
I

.
x
a
m
d

d
n
a

l
o
t
d

:
s
t
e
k
c
a
r
b

o
w

t

n

i

i

s
r
o
t
a
n
m
o
n
e
d

o
w

t

e
h
t

h
t
i

w
d
e
t
a
i
c
o
s
s
a

n
e
e
b

s
a
h

t
I

.

d
e
t
a
l
u
c
l
a
c

s
a
w

s
a
r
d

e
c
n
a
t
s
i
d

t
s
r
ﬁ

t
s
r
ﬁ

s
a
w
e
c
n
a
t
s
i
d

e
c
n
a
r
e
l
o
t

s
i
h
t

f
i

;
e
c
n
a
t
s
i
d

e
c
n
a
r
e
l
o
t

n
e
v
i
g

a

o
t

i

g
n
d
r
a
g
e
r

y
c
a
r
u
c
c
a

x
a
m
d

,
t
e
k
c
a
r
b

d
n
o
c
e
s

e
h
t

n
I

.
s
t
n
o
p

i

o
r
e
z

n

i

d
e
t
l
u
s
e
r

t
e
k
c
a
r
b

t
s
r
ﬁ

e
l
o
h
w
e
h
t

,

d
e
d
e
e
c
x
e

,
y
a
w
s
i
h
t

n
I

.
t
n
e
t
x
e

t
e
l
b
a
t

n
e
p

e
l
o
h
w
e
h
t

r
e
v
o

e
c
n
a
t
s
i
d

e
h
t

l
e
d
o
m
o
t

n
e
s
o
h
c

n
e
e
b

o
t
n

i

n
e
k
a
t

e
b

l

d
u
o
c

t
e
l
b
a
t

n
e
p

e
l
o
h
w
e
h
t

r
e
v
o

s
e
c
n
a
t
s
i
d

g
n
i
v
i
e
c
r
e
p

f
o

e
s
n
e
s

s
a
h

e
h
t

5
0

.

h
t
i

w
d
e
t
h
g
i
e
w
n
e
e
b

t
s
r
ﬁ
e
h
T

.
t
n
u
o
c
c
a

e
h
t

n
e
h
w

t
n
e
m
o
m
e
h
t

t
a

d
e
d
r
o
c
e
r

n
e
e
b

s
a
h

n
o
i
t
i
s
o
p

r
o
s
r
u
c

e
h
t

t
a
h
t

e
t
o
n

e
s
a
e
l
P

s

’

a

f
o

n
o
i
t
i
s
o
p

l

e
t
u
o
s
b
a

e
h
t

i

g
n
d
r
a
g
e
r

n
o
i
t
s
e
u
q

a

d
e
r
e
w
s
n
a

t
n
a
p
i
c
i
t
r
a
p

h
c
a
e

r
o
f

n
w
o
n
k

s
a
w

i

t
n
o
p

t
e
g
r
a
t

e
h
t

e
c
n
i
S

.
e
r
u
t
c
u
r
t
s

l

l
a
c
i
g
o
o
h
p
r
o
m
o
e
g

a

f
o

n
o
i
t
a
c
o

l

e
h
t

e
t
a
m

i
t
s
e

o
t

r
e
d
r
o

n

i

e
v
a
g

t
n
a
p
i
c
i
t
r
a
p

e
h
t

r
e
w
s
n
a

e
h
t

o
t

r
e
f
e
r

s
t
e
k
c
a
r
b

o
w

t

t
s
a
l

e
h
T

i

e
t
a
n
d
r
o
o
c

d
e
x
ﬁ

a

o
t

e
c
n
a
d
r
o
c
c
a

n

i

n
o
i
t
i
s
o
p

e
v
i
t
a
l
e
r

s
t
i

,
)
t
e
g
r
a
t
y

d
n
a

t
e
g
r
a
t
x
(

n
o
i
t
s
e
u
q

d
n
a

x
s
e
r
(

n
o
i
t
u
o
s
e
r

l

y

r
o

x

e
h
t

y
b

e
u
l
a
v

s
i
h
t

i

g
n
d
i
v
i
d

y
b

i

d
e
n
m
r
e
t
e
d

e
b

l

d
u
o
c
m
e
t
s
y
s

s
a
h

e
u
l
a
v

e
v
i
t
a
l
e
r

s
i
h
T

.
s
i
x
a

h
c
a
e

r
o
f

%
0
0
1

d
n
a
%
0

n
e
e
w
t
e
b

e
u
l
a
v

a

o
t

i

g
n
d
a
e
l

,
)
y
s
e
r

n

i

by
d
n
a

bx
n
o
i
t
a
c
o

l

i

t
n
o
p

t
e
g
r
a
t

e
v
i
t
c
e
ﬀ
e

s

’

i

d
e
n
g
a
m

i

e
h
t

f
o
n
o
i
t
a
m

i
t
s
e

t
n
a
p
i
c
i
t
r
a
p
e
h
t

m
o
r
f
d
e
t
c
a
r
t
b
u
s
n
e
e
b

e
h
t

o
t

l
e
d
o
m

l
a
t
n
e
m
e
h
t

n
e
e
w
t
e
b

n
o
i
t
a
i
v
e
d

e
h
t

l
e
d
o
m
o
t

r
e
d
r
o

t
h
g
e
W

i

e
p
y
t

t
s
e
T

4

3
1

e
s
n
o
p
s
e
r

l

y
a
p
s
i
d

y
r
o
t
i
d
u
A

g
n
i
r
o
c
S

]
1
0
[

,

e
g
n
a
r

g
1
(cid:5)
x
(cid:5)
0
j

R
2
x
f

t
n
e
m
e
r
u
s
a
e
M

l

e
v
e

l

s
u
o
u
n
i
t
n
o
C

(cid:7)(cid:7)(cid:7)

s

’

e
p
y
t

n
o
i
t
s
e
u
Q

(cid:6)
(cid:7)(cid:7)(cid:7)

(cid:7)(cid:7)(cid:7)

(cid:5)

(cid:6)
(cid:7)(cid:7)(cid:7)

(cid:3)
x
p
½
t
e
g
r
a
t
y

(cid:3)
x
p
½
y
s
e
r

(cid:2)

(cid:3)
½

by

(cid:2)
1

þ

(cid:3)
x
p
½
t
e
g
r
a
t
x

(cid:3)
x
p
½
x
s
e
r

(cid:2)

(cid:3)
½

bx

(cid:2)
1

þ

(cid:3)
x
p
½
t
e
g
r
a
T
r
o
s
r
u
c
d

(cid:3)
x
p
½
x
a
m
d

(cid:2)
1

(cid:4)

12

þ

(cid:3)
x
p
½
t
e
g
r
a
T
r
o
s
r
u
c
d

(cid:3)
x
p
½
l

o
t
d

(cid:2)
1

(cid:4)

32

¼
a

(cid:5)

(cid:6)

:
s
w
o

l
l

o
f

s
a

l

d
e
t
a
u
c
l
a
c

e
r
a

d
n
a

0
1

.

.

0
0
m
o
r
f

s
e
g
n
a
r

e
r
o
c
S

(cid:5)

o
t

(cid:6)

(cid:5)

)
s
u
o
u
n
i
t
n
o
c

t
c
a
r
t
s
b
a
(

2

s

’

n
o
i
t
a
c
o

l

t
e
g
r
a
t

a

g
n
i
y
f
i
t
n
e
d

i

f
o

y
c
a
r
u
c
c
A

l

e
v
e

l

l

y
t
i
x
e
p
m
o
C

.
)
d
e
u
n
i
t
n
o
C
(

y
r
o
g
e
t
a
c

t
s
e
T

Downloaded by [University of New England] at 16:59 03 January 2018 )
d
e
u
n
i
t
n
o
C
(

32

J. SCHITO AND S. I. FABRIKANT

t
h
g
e
W

i

e
p
y
t

t
s
e
T

4

3
1

e
s
n
o
p
s
e
r

l

y
a
p
s
i
d

y
r
o
t
i
d
u
A

g
n
i
r
o
c
S

]
1
0
[

,

e
g
n
a
r

g
1
(cid:5)
x
(cid:5)
0
j

R
2
x
f

t
n
e
m
e
r
u
s
a
e
M

l

e
v
e

l

s
u
o
u
n
i
t
n
o
C

3
21

y
r
o
t
i
d
u
A

l

y
a
p
s
i
d

e
s
n
o
p
s
e
r

g
1
(cid:5)
x
(cid:5)
0
j

R
2
x
f

s
u
o
u
n
i
t
n
o
C

s

’

’

)
n
o
i
s
n
e
t
x
e

s
e
r
u
t
c
u
r
t
s

a

f
o

d
r
i
h
t

a

.
x
o
r
p
p
a
(

e
r
o
c
s

a

t
e
g

o
t

d
e
d
e
e
c
x
e

e
b

t
o
n

t
s
u
m

t
a
h
t

e
c
n
a
r
e
l
o
T
=

l
e
x
i
p

n

i

n
o
i
t
i
s
o
p

r
o
s
r
u
c

e
h
t

o
t

n
o
i
t
i
s
o
p

i

d
e
n
g
a
m

i

e
h
t

m
o
r
f

e
c
n
a
t
s
i
d

d
e
l
e
d
o
M
=

r
o
s
r
u
C
d
n
m
d

i

1
(cid:5)
bx
(cid:5)
0

s
a
e
r
e
h
w

s
i
x
a
-
x

e
h
t

n
o

d
e
t
a
m

i
t
s
e

t
n
a
p
i
c
i
t
r
a
p

e
h
t

n
o
i
t
i
s
o
P

=

k
s
a
t

s
i
h
t

r
o
f

y
c
a
r
u
c
c
a

d
e
l
e
d
o
M
=

a

1
(cid:5)

by
(cid:5)
0

s
a
e
r
e
h
w
s
i
x
a
-
y

e
h
t

n
o

d
e
t
a
m

i
t
s
e

t
n
a
p
i
c
i
t
r
a
p

e
h
t

n
o
i
t
i
s
o
P

=

s
i
x
a
-
y

e
h
t

n
o

d
e
r
e
t
s
i
g
e
r

n
o
i
t
i
s
o
p

s

’

r
o
s
r
u
C

=

s
i
x
a
-
y

e
h
t

n
o

n
o
i
t
u
o
s
e
r

l

l

s
y
a
p
s
i
d

y
r
o
t
i
d
u
A

=

s
i
x
a
-
x

e
h
t

n
o

n
o
i
t
u
o
s
e
r

l

l

s
y
a
p
s
i
d

y
r
o
t
i
d
u
A

=

s
i
x
a
-
x

e
h
t

n
o

d
e
r
e
t
s
i
g
e
r

n
o
i
t
i
s
o
p

s

’

r
o
s
r
u
C

=

r
o
s
r
u
c
x

’

’

l

o
t
d

bx

x
s
e
r

by

r
o
s
r
u
c
y

y
s
e
r

e
v
i
t
c
e
ﬀ
e

h
t
i

w

)
by
d
n
a
bx
(

e
h
t

t
a

n
o
i
t
a
c
o

l

n
o
i
t
a
c
o

l

i

d
e
n
g
a
m

i

e
h
t

e
t
a
i
c
o
s
s
a

s
t
e
k
c
a
r
b
o
w

t

t
s
r
ﬁ

e
h
T

.

n
o
i
t
i
s
o
p
s

r
o
s
r
u
c

’

e
h
t

h
t
i

w
n
o
i
t
a
c
o

l

d
e
v
i
e
c
r
e
p

t
n
a
p
i
c
i
t
r
a
p

a

d
e
r
a
p
m
o
c

k
s
a
t

s
i
h
T

s

’

o
t

n
e
s
o
h
c

n
e
e
b

s
a
h

x
a
m
d

,
t
e
k
c
a
r
b

d
r
i
h
t

e
h
t

n
I

.
)
r
o
s
r
u
c
y

d
n
a

r
o
s
r
u
c
x
(

n
o
i
t
i
s
o
p

s

’

r
o
s
r
u
c

e
h
t

t
s
r
ﬁ

e
h
T

.
t
n
u
o
c
c
a

o
t
n

i

n
e
k
a
t

e
b

l

d
u
o
c

t
e
l
b
a
t

n
e
p

e
l
o
h
w
e
h
t

r
e
v
o

s
e
c
n
a
t
s
i
d

g
n
i
v
i
e
c
r
e
p

f
o

e
s
n
e
s

e
h
t

,
y
a
w
s
i
h
t

n
I

.
t
n
e
t
x
e

t
e
l
b
a
t

n
e
p

e
l
o
h
w
e
h
t

r
e
v
o

e
c
n
a
t
s
i
d

e
h
t

l
e
d
o
m

n
e
e
b

s
a
h

t
e
k
c
a
r
b

d
r
i
h
t

e
h
t

s
a
e
r
e
h
w
h
c
a
e

h
t
i

w
d
e
t
h
g
i
e
w
n
e
e
b

e
v
a
h

s
t
e
k
c
a
r
b

o
w

t

1
1

2
3

0
1

2
3

h
t
i

w
d
e
t
h
g
i
e
w

e
p
y
t

n
o
i
t
s
e
u
Q

(cid:6)

(cid:5)

(cid:6)
(cid:7)(cid:7)(cid:7)

(cid:7)(cid:7)(cid:7)

(cid:5)

(cid:6)
(cid:7)(cid:7)(cid:7)

(cid:3)
x
p
½

r
o
s
r
u
C
d
n
m
d

i

(cid:3)
x
p
½

l

o
t
d

(cid:2)
1

(cid:4)

0
1

2
3

þ

r
o
s
r
u
c
y

y
s
e
r

(cid:2)

(cid:3)
½

by

(cid:2)
1

(cid:4)

1
1

2
3

þ

r
o
s
r
u
c
x

x
s
e
r

(cid:2)

(cid:3)
½

bx

(cid:2)
1

(cid:4)

1
1

2
3

¼
a

(cid:7)(cid:7)(cid:7)

(cid:5)

:
s
w
o

l
l

o
f

s
a

l

d
e
t
a
u
c
l
a
c

e
r
a

d
n
a

0
1

.

o
t

.

0
0
m
o
r
f

s
e
g
n
a
r

e
r
o
c
S

n
o
i
t
i
s
o
p

r
o
s
r
u
c

e
v
i
t
c
e
ﬀ
e

e
h
t

g
n
i
y
f
i
t
n
e
d

i

f
o

y
c
a
r
u
c
c
A

1
(cid:5)
bz
(cid:5)
0

s
a
e
r
e
h
w

s
i
x
a
-
z

e
h
t

n
o

d
e
t
a
m

i
t
s
e

t
n
a
p
i
c
i
t
r
a
p

e
h
t

t
h
g
i
e
H

=

k
s
a
t

s
i
h
t

r
o
f

y
c
a
r
u
c
c
a

d
e
l
e
d
o
M
=

s
i
x
a
-
z

e
h
t

n
o

t
h
g
i
e
h

e
v
i
t
c
e
ﬀ
e

s

’

t
e
g
r
a
T

=

s
i
x
a
-
z

e
h
t

n
o

n
o
i
t
u
o
s
e
r

l

l

s
y
a
p
s
i
d

y
r
o
t
i
d
u
A

=

’

t
e
g
r
a
t
z

z
s
e
r

a

bz

s

’

:
s
w
o

l
l

o
f

s
a

l

d
e
t
a
u
c
l
a
c

e
r
a

d
n
a

0
1

.

o
t

.

0
0
m
o
r
f

s
e
g
n
a
r

e
r
o
c
S

n
o
i
t
a
v
e
l
e

l

e
t
u
o
s
b
a

t
e
g
r
a
t

a

g
n
i
t
a
m

i
t
s
e

f
o

y
c
a
r
u
c
c
A

t
e
g
r
a
t
z

z
s
e
r

(cid:2)

(cid:3)
½

bz
¼
a

i

e
t
a
n
d
r
o
o
c

d
e
x
ﬁ

a

o
t

e
c
n
a
d
r
o
c
c
a

n

i

t
h
g
i
e
h

e
v
i
t
a
l
e
r

s
t
i

,
)
t
e
g
r
a
t
z
(

n
o
i
t
s
e
u
q

h
c
a
e

r
o
f

n
w
o
n
k

s
a
w

i

t
n
o
p

t
e
g
r
a
t

e
h
t

e
c
n
i
S

.
)
k
n
i
s

,
y
e
l
l
a
v

,
l
l
i

h

,

n
i
a
t
n
u
o
m

(

e
r
u
t
c
u
r
t
s

l

l
a
c
i
g
o
o
h
p
r
o
m
o
e
g

A

e
h
t

l
e
d
o
m
o
t
bz
t
h
g
i
e
h

i

t
n
o
p

t
e
g
r
a
t

e
v
i
t
c
e
ﬀ
e

e
h
t

o
t

l
e
d
o
m

l
a
t
n
e
m
e
h
t

n
e
e
w
t
e
b

n
o
i
t
a
i
v
e
d

i

d
e
n
g
a
m

i

e
h
t

f
o

n
o
i
t
a
m

i
t
s
e

t
n
a
p
i
c
i
t
r
a
p

e
h
t

m
o
r
f

d
e
t
c
a
r
t
b
u
s

s

’

,
)
z
s
e
r
(

n
o
i
t
u
o
s
e
r

l

z

e
h
t

y
b

e
u
l
a
v

s
i
h
t

g
n
i
z
i
d
r
a
d
n
a
t
s

y
b

i

d
e
n
m
r
e
t
e
d

e
b

l

d
u
o
c
m
e
t
s
y
s

n
e
e
b

s
a
h

e
u
l
a
v

d
e
z
i
d
r
a
d
n
a
t
s

s
i
h
T

.

%
0
0
1

d
n
a
%
0

n
e
e
w
t
e
b

e
u
l
a
v

a

o
t

i

g
n
d
a
e
l

.
)
d
e
u
n
i
t
n
o
C
(

y
r
o
g
e
t
a
c

t
s
e
T

Downloaded by [University of New England] at 16:59 03 January 2018 INTERNATIONAL JOURNAL OF GEOGRAPHICAL INFORMATION SCIENCE

33

)
d
e
u
n
i
t
n
o
C
(

1

3
1

5
61

e
s
n
o
p
s
e
r

l

y
a
p
s
i
d

y
r
o
t
i
d
u
A

y
r
o
t
i
d
u
A

l

y
a
p
s
i
d

e
s
n
o
p
s
e
r

g
1
(cid:5)
x
(cid:5)
0
j

R
2
x
f

s
u
o
u
n
i
t
n
o
C

g
1
;
0
f

×
2

t
a
ﬂ

r
o

p
e
e
t
s

s
i

)
y
e
l
l
a
v

,

k
n
i
s

,

p
d

i

,

n
i
a
t
n
u
o
m

(

e
r
u
t
c
u
r
t
s

l

l
a
c
i
g
o
o
h
p
r
o
m
o
e
g

a

f
i

g
n
i
y
f
i
t
n
e
d

I

s
u
o
m
o
t
o
h
c
i
D

:
s
w
o

l
l

o
f

s
a

l

d
e
t
a
u
c
l
a
c

e
r
a

d
n
a

0
1

.

o
t

.

0
0
m
o
r
f

s
e
g
n
a
r

e
r
o
c
S

)
bz
(

t
a

t
h
g
i
e
h

e
h
t

h
t
i

w

t
h
g
i
e
h

i

d
e
n
g
a
m

i

e
h
t

s
e
t
a
i
c
o
s
s
a

a
l
u
m
r
o
f

e
h
T

.

n
o
i
t
i
s
o
p

e
h
t

t
a

t
h
g
i
e
h

e
v
i
t
c
e
ﬀ
e

s

’

e
h
t

h
t
i

w

t
h
g
i
e
h

d
e
v
i
e
c
r
e
p

t
n
a
p
i
c
i
t
r
a
p

a

d
e
r
a
p
m
o
c

k
s
a
t

s
i
h
T

a

s
n
r
u
t
e
r

)
z
s
e
r
(

n
o
i
t
u
o
s
e
r

l

t
h
g
i
e
h

e
h
t

y
b

n
o
i
s
i
v
i
d

e
h
T

.
)
r
o
s
r
u
c
z
(

n
o
i
t
i
s
o
p

)

bz
(

t
h
g
i
e
h

d
e
t
a
m

i
t
s
e

e
v
i
t
a
l
e
r

e
h
t

o
t

i

g
n
d
r
o
c
c
a

,
r
o
r
r
e

e
v
i
t
a
l
e
r

s

’

r
o
s
r
u
c

e
h
t

s

’

r
o
s
r
u
c

t
h
g
e
W

i

e
p
y
t

t
s
e
T

2

3
1

e
s
n
o
p
s
e
r

l

y
a
p
s
i
d

y
r
o
t
i
d
u
A

g
n
i
r
o
c
S

]
1
0
[

,

e
g
n
a
r

g
1
(cid:5)
x
(cid:5)
0
j

R
2
x
f

t
n
e
m
e
r
u
s
a
e
M

l

e
v
e

l

s
u
o
u
n
i
t
n
o
C

e
p
y
t

n
o
i
t
s
e
u
Q

t
a

s
a

n
o
i
t
i
s
o
p

s

’

r
o
s
r
u
c

a

t
h
g
i
e
h

e
v
i
t
c
e
ﬀ
e

e
h
t

g
n
i
t
a
m

i
t
s
e

f
o

y
c
a
r
u
c
c
A

:
s
w
o

l
l

o
f

l

d
e
t
a
u
c
l
a
c

e
r
a

d
n
a

0
1

.

o
t

.

0
0
m
o
r
f

s
e
g
n
a
r

1
(cid:5)
bz
(cid:5)
0

s
a
e
r
e
h
w

s
i
x
a
-
z

e
h
t

n
o

d
e
t
a
m

i
t
s
e

t
n
a
p
i
c
i
t
r
a
p

e
h
t

t
h
g
i
e
H

=

s

’

n
o
i
t
i
s
o
p

r
o
s
r
u
c

e
h
t

t
a

d
e
r
e
t
s
i
g
e
r

t
h
g
i
e
H

=

s
i
x
a
-
z

e
h
t

n
o

n
o
i
t
u
o
s
e
r

l

l

s
y
a
p
s
i
d

y
r
o
t
i
d
u
A

=

’

k
s
a
t

s
i
h
t

r
o
f

y
c
a
r
u
c
c
a

d
e
l
e
d
o
M
=

r
o
s
r
u
c
z

z
s
e
r

(cid:2)

(cid:3)
½

bz
¼
a

e
r
o
c
S

r
o
s
r
u
c
z

z
s
e
r

a

bz

.
)
d
e
u
n
i
t
n
o
C
(

y
r
o
g
e
t
a
c

t
s
e
T

’

s

’

l
e
x
i
p

n

i

n
o
i
t
i
s
o
p

r
o
s
r
u
c

e
h
t

o
t

n
o
i
t
i
s
o
p

i

d
e
n
g
a
m

i

e
h
t

m
o
r
f

e
c
n
a
t
s
i
d

d
e
l
e
d
o
M
=

r
o
s
r
u
C
d
n
m
d

i

i

h
t
d
w
s
y
a
p
s
i
d

l

y
r
o
t
i
d
u
a

e
h
t

o
t

s
d
n
o
p
s
e
r
r
o
c

h
c
i
h
w

,
t
n
e
t
x
e

l
e
d
o
m
m
u
m
i
x
a
M
=

1
(cid:5)
bx
(cid:5)
0

s
a
e
r
e
h
w

s
i
x
a
-
x

e
h
t

n
o

d
e
t
a
m

i
t
s
e

t
n
a
p
i
c
i
t
r
a
p

e
h
t

n
o
i
t
i
s
o
P

=

k
s
a
t

s
i
h
t

r
o
f

y
c
a
r
u
c
c
a

d
e
l
e
d
o
M
=

a

1
(cid:5)

by
(cid:5)
0

s
a
e
r
e
h
w
s
i
x
a
-
y

e
h
t

n
o

d
e
t
a
m

i
t
s
e

t
n
a
p
i
c
i
t
r
a
p

e
h
t

n
o
i
t
i
s
o
P

=

s
i
x
a
-
x

e
h
t

n
o

d
e
r
e
t
s
i
g
e
r

n
o
i
t
i
s
o
p

s

’

r
o
s
r
u
C

=

s
i
x
a
-
x

e
h
t

n
o

n
o
i
t
u
o
s
e
r

l

l

s
y
a
p
s
i
d

y
r
o
t
i
d
u
A

=

s
i
x
a
-
y

e
h
t

n
o

d
e
r
e
t
s
i
g
e
r

n
o
i
t
i
s
o
p

s

’

r
o
s
r
u
C

=

s
i
x
a
-
y

e
h
t

n
o

n
o
i
t
u
o
s
e
r

l

l

s
y
a
p
s
i
d

y
r
o
t
i
d
u
A

=

’

’

x
a
m
d

bx

r
o
s
r
u
c
x

x
s
e
r

by

r
o
s
r
u
c
y

y
s
e
r

r
e
v
o
e
c
n
a
t
s
i
d
e
h
t

s
l
e
d
o
m

x
a
m
d

,
t
e
k
c
a
r
b
d
r
i
h
t

e
h
t
n
I

.
)
r
o
s
r
u
c
y
d
n
a

r
o
s
r
u
c
x
(
n
o
i
t
i
s
o
p
s

r
o
s
r
u
c

e
h
t

e
b
d
u
o
c

l

r
o
r
r
e
n
o
i
t
a
m

i
t
s
e

l
a
m
i
x
a
m
e
h
t

,
y
a
w
s
i
h
t
n
I

.
t
n
e
t
x
e

y
a
l
p
s
i
d
y
r
o
t
i
d
u
a

e
l
o
h
w
e
h
t

s

’

t
n
e
t
x
e

s

’

y
a
l
p
s
i
d

y
r
o
t
i
d
u
a

e
h
t

n
o

e
c
n
a
t
s
i
d

t
s
e
g
r
a
l

e
h
t

s
i

h
c
i
h
w

,

i

h
t
d
w
e
h
t

o
t

d
e
c
u
d
e
r

’

e
v
i
t
c
e
ﬀ
e

h
t
i

w

)
by
d
n
a
bx
(

e
h
t

t
a

n
o
i
t
a
c
o

l

n
o
i
t
a
c
o

l

i

d
e
n
g
a
m

i

e
h
t

e
t
a
i
c
o
s
s
a

s
t
e
k
c
a
r
b
o
w

t

t
s
r
ﬁ

e
h
T

.

n
o
i
t
i
s
o
p
s

r
o
s
r
u
c

’

e
h
t

h
t
i

w
n
o
i
t
a
c
o

l

d
e
v
i
e
c
r
e
p

t
n
a
p
i
c
i
t
r
a
p

a

d
e
r
a
p
m
o
c

k
s
a
t

s
i
h
T

s

’

(cid:6)
(cid:7)(cid:7)(cid:7)

(cid:6)

(cid:5)

(cid:3)
x
p
½

r
o
s
r
u
C
d
n
m
d

i

(cid:3)
x
p
½

x
a
m
d

(cid:2)
1

(cid:4)

12

þ

r
o
s
r
u
c
y

y
s
e
r

(cid:2)

(cid:3)

½

by

(cid:2)
1

(cid:4)

14

þ

r
o
s
r
u
c
x

x
s
e
r

(cid:2)

(cid:3)

½

bx

(cid:2)
1

(cid:4)

14

¼
a

(cid:7)(cid:7)(cid:7)

(cid:5)

(cid:6)
(cid:7)(cid:7)(cid:7)

(cid:7)(cid:7)(cid:7)

(cid:5)

:
s
w
o

l
l

o
f

s
a

l

d
e
t
a
u
c
l
a
c

e
r
a

d
n
a

0
1

.

o
t

.

0
0
m
o
r
f

s
e
g
n
a
r

e
r
o
c
S

n
o
i
t
i
s
o
p

r
o
s
r
u
c

e
v
i
t
c
e
ﬀ
e

e
h
t

g
n
i
y
f
i
t
n
e
d

i

f
o

y
c
a
r
u
c
c
A

l

y
t
i
x
e
p
m
o
C

l

a
e
r
(

3

l

e
v
e

l

)
s
u
o
u
n
i
t
n
o
c

t
c
e
r
r
o
c

s
a
w

r
e
w
s
n
a

e
h
T

t
c
e
r
r
o
c

t
o
n

s
a
w
e
h
T

=

=

0
1

.

0
0

.

Downloaded by [University of New England] at 16:59 03 January 2018 34

J. SCHITO AND S. I. FABRIKANT

t
h
g
e
W

i

e
p
y
t

t
s
e
T

4

5
1

e
s
n
o
p
s
e
r

l

y
a
p
s
i
d

y
r
o
t
i
d
u
A

g
n
i
r
o
c
S

]
1
0
[

,

e
g
n
a
r

g
1
(cid:5)
x
(cid:5)
0
j

R
2
x
f

t
n
e
m
e
r
u
s
a
e
M

l

e
v
e

l

s
u
o
u
n
i
t
n
o
C

5
41

5
11

y
r
o
t
i
d
u
A

l

y
a
p
s
i
d

e
s
n
o
p
s
e
r

y
r
o
t
i
d
u
A

l

y
a
p
s
i
d

e
s
n
o
p
s
e
r

(cid:3)

1
(cid:5)

x5

(cid:5)
0
j
0
N
2
x

(cid:1)

e
t
e
r
c
s
i
D

)
bz
(

a

s
n
r
u
t
e
r

i

)
z
n
m
(cid:2)

z
x
a
m

(

e
g
n
a
r

t
h
g
i
e
h

e
h
t

y
b

n
o
i
s
i
v
i
d

e
h
T

.
)
r
o
s
r
u
c
z
(

n
o
i
t
i
s
o
p

t
a

t
h
g
i
e
h

e
h
t

h
t
i

w

t
h
g
i
e
h

i

d
e
n
g
a
m

i

e
h
t

s
e
t
a
i
c
o
s
s
a

a
l
u
m
r
o
f

e
h
T

.

n
o
i
t
i
s
o
p

s

’

r
o
s
r
u
c

e
h
t

s

’

r
o
s
r
u
c

e
h
t

t
a

t
h
g
i
e
h

e
v
i
t
c
e
ﬀ
e

s

’

e
h
t

h
t
i

w

t
h
g
i
e
h

d
e
v
i
e
c
r
e
p

t
n
a
p
i
c
i
t
r
a
p

a

d
e
r
a
p
m
o
c

k
s
a
t

s
i
h
T

)

bz
(

t
h
g
i
e
h

d
e
t
a
m

i
t
s
e

e
v
i
t
a
l
e
r

e
h
t

o
t

i

g
n
d
r
o
c
c
a

,
r
o
r
r
e

e
v
i
t
a
l
e
r

e
p
y
t

n
o
i
t
s
e
u
Q

s

’

n
o
i
t
i
s
o
p

r
o
s
r
u
c

a

t
a

n
o
i
t
a
v
e
l
e

e
v
i
t
c
e
ﬀ
e

e
h
t

g
n
i
y
f
i
t
n
e
d

i

f
o

y
c
a
r
u
c
c
A

:
s
w
o

l
l

o
f

s
a

l

d
e
t
a
u
c
l
a
c

e
r
a

d
n
a

0
1

.

o
t

.

0
0
m
o
r
f

s
e
g
n
a
r

e
r
o
c
S

(cid:7)(cid:7)(cid:7)

(cid:3)
x
p
½

r
o
s
r
u
c
z

i

(cid:3)
x
p
½
z
n
m
(cid:2)
(cid:3)
x
p
½
z
x
a
m
(cid:2)

(cid:3)
½

(cid:2)
1
¼
a

bz
(cid:7)(cid:7)(cid:7)

1
(cid:5)
bz
(cid:5)
0

s
a
e
r
e
h
w

s
i
x
a
-
z

e
h
t

n
o

d
e
t
a
m

i
t
s
e

t
n
a
p
i
c
i
t
r
a
p

e
h
t

t
h
g
i
e
H

=

)
s
i
x
a
-
z

e
h
t

n
o
(

n
o
i
t
a
v
e
l
e
m
u
m
i
x
a
m

l

s
y
a
p
s
i
d

y
r
o
t
i
d
u
A

)
s
i
x
a
-
z

e
h
t

n
o
(

n
o
i
t
a
v
e
l
e
m
u
m
n
m

i

i

l

s
y
a
p
s
i
d

y
r
o
t
i
d
u
A

=

’

’

s

’

n
o
i
t
i
s
o
p

r
o
s
r
u
c

e
h
t

t
a

d
e
r
e
t
s
i
g
e
r

t
h
g
i
e
H

=

=

r
o
s
r
u
c
z

z
x
a
m

z
n
m

i

k
s
a
t

s
i
h
t

r
o
f

y
c
a
r
u
c
c
a

d
e
l
e
d
o
M
=

a

bz

e
l
g
n
a

e
h
t

f
l
a
h
n
a
h
t

r
e
l
l
a
m

s

s
i

t
i

e
s
u
a
c
e
b
n
e
s
o
h
c
n
e
e
b
s
a
h
°
5
2

i

f
o
n
g
r
a
m

r
o
r
r
e

r
e
p
p
u
n
A

s

’

.

l

n
o
i
t
u
o
s
n
e
v
i
g
e
h
t
d
n
a
r
e
w
s
n
a

t
n
a
p
i
c
i
t
r
a
p
a
n
e
e
w
t
e
b
n
o
i
t
a
i
v
e
d
r
a
l
u
g
n
a
e
h
t

s
e
b
i
r
c
s
e
d
φ

)

(cid:6)
0
9

2

<
(cid:6)
5
2
(

(cid:6)
0
6

2

<
(cid:6)
5
2
(

e
l
g
n
a
t
c
e
r

a

f
o

r
o

)

e
l
g
n
a
i
r
t

r
a
l
u
g
e
r

a

f
o

e
c
n
a
r
e
l
o
t

:
s
w
o

l
l

o
f

s
a

f
o

e
c
n
a
r
e
l
o
t

f
o

s
l
e
x
i
p

5

i

n
h
t
i

w

d
n
u
o
f

t
o
n

s
a
w
a
e
r
a

s
l
e
x
i
p

5

i

n
h
t
i

w
d
n
u
o
f

s
a
w
a
e
r
a

t
e
g
r
a
t

t
e
g
r
a
t

e
h
T

e
h
T

=

=

0
1

.

0
0

.

l

d
e
t
a
u
c
l
a
c

e
r
a

d
n
a

0
1

.

o
t

.

0
0
m
o
r
f

s
e
g
n
a
r

e
r
o
c
S

n
i
a
h
c

n
i
a
t
n
u
o
m
a

f
o

h
t
u
m
i
z
a

e
h
t

g
n
i
y
f
i
t
n
e
d

i

f
o

y
c
a
r
u
c
c
A

:
s
w
o

l
l

o
f

s
a

l

d
e
t
a
u
c
l
a
c

e
r
a

d
n
a

0
1

.

o
t

.

0
0
m
o
r
f

s
e
g
n
a
r

e
r
o
c
S

d
e
n
r
a
e
s
t
n
o
p

i

x
a
m
s
t
n
o
p

i

¼
a

°
5
1

°
0
2

°
5
2

<

<

<

°
0
1

°
5

<

<

φ

φ

≤

≤

°
0

°
5

φ

φ

φ

≤

≤

≤

°
5
2

°
0
1

°
5
1

°
0
2

≥

φ

=

=

=

=

=

=

0
1

.

8
0

.

6
0

.

4
0

.

2
0

.

0
0

.

.
)
d
e
u
n
i
t
n
o
C
(

y
r
o
g
e
t
a
c

t
s
e
T

s
u
o
m
o
t
o
h
c
i
D

g
1
(cid:5)
x
(cid:5)
0
j

R
2
x
f

×
4

)
y
e
l
l
a
v

,
e
k
a
l

,

p
d

i

,

n
i
a
t
n
u
o
m

(

s
e
r
u
t
c
u
r
t
s

l

l
a
c
i
g
o
o
h
p
r
o
m
o
e
g

c
ﬁ
i
c
e
p
s

i

g
n
d
n
ﬁ

f
o

y
t
i
l
i

b
A

Downloaded by [University of New England] at 16:59 03 January 2018 