This article was downloaded by: [FU Berlin]
On: 12 May 2015, At: 23:33
Publisher: Taylor & Francis
Informa Ltd Registered in England and Wales Registered Number:
1072954 Registered office: Mortimer House, 37-41 Mortimer Street,
London W1T 3JH, UK

International Journal of
Geographical Information
Science
Publication details, including instructions for
authors and subscription information:
http://www.tandfonline.com/loi/tgis20

Automatic extraction of
building statistics from
digital orthophotos
A. Stassopoulou a b , T. Caelli a c & R. Ramirez a
a Center for Mapping, The Ohio State University,
Columbus, 43212, USA; Intercollege, 1700
Nicosia, Cyprus
b Center for Mapping, The Ohio State University,
Columbus, 43212, USA; Computation,
Perception and Action Laboratory, Department
of Psychology, University of Alberta, Edmonton,
Canada
c Center for Mapping, The Ohio State University,
Columbus, 43212, USA
Published online: 06 Aug 2010.

To cite this article: A. Stassopoulou , T. Caelli & R. Ramirez (2000)
Automatic extraction of building statistics from digital orthophotos,
International Journal of Geographical Information Science, 14:8, 795-814, DOI:
10.1080/136588100750022796

To link to this article:  http://dx.doi.org/10.1080/136588100750022796

PLEASE SCROLL DOWN FOR ARTICLE

Taylor & Francis makes every effort to ensure the accuracy of all
the information (the “Content”) contained in the publications on our
platform. However, Taylor & Francis, our agents, and our licensors
make no representations or warranties whatsoever as to the accuracy,

completeness, or suitability for any purpose of the Content. Any opinions
and views expressed in this publication are the opinions and views of
the authors, and are not the views of or endorsed by Taylor & Francis.
The accuracy of the Content should not be relied upon and should be
independently verified with primary sources of information. Taylor and
Francis shall not be liable for any losses, actions, claims, proceedings,
demands, costs, expenses, damages, and other liabilities whatsoever
or howsoever caused arising directly or indirectly in connection with, in
relation to or arising out of the use of the Content.

This article may be used for research, teaching, and private study
purposes. Any substantial or systematic reproduction, redistribution,
reselling, loan, sub-licensing, systematic supply, or distribution in any
form to anyone is expressly forbidden. Terms & Conditions of access
and use can be found at http://www.tandfonline.com/page/terms-and-
conditions

Downloaded by [FU Berlin] at 23:33 12 May 2015 int. j. geographical information science, 2000, vol. 14, no. 8, 795± 814

Research Article

Automatic extraction of building statistics from digital orthophotos

A. STASSOPOULOU1,2, T. CAELLI1,3 and R. RAMIREZ1
1Center for Mapping, The Ohio State University, Columbus, 43212, USA
2Intercollege, 1700 Nicosia, Cyprus
3Computation, Perception and Action Laboratory, Department of Psychology,
University of Alberta, Edmonton, Canada

(Received 1 July 1998; accepted 2 February 2000)

Abstract.
In this paper we discuss uses of image segmentation, feature extraction
and Bayesian networks for identifying buildings in digital orthophotos and the
utilisation of the results for the automated computation of building statistics. Our
work diŒers from previous attempts in a number of ways. Firstly, image segmenta-
tion is accomplished using an adaptive multi-scale method which brings together
region and edge information to segment the image into regions. Secondly, auto-
mated building feature extraction (e.g. corners) is optimised to (cid:142) t with expert
human annotation performance. The third aspect of this work is the exploration
of Bayesian networks as a method for fusing available information (ranging from
corner information to solar angles as indicators of shadow location) to classify
segmented regions as corresponding to buildings or not. Such processes then
permit the automatic compilation of building statistics.

1.

Introduction
The current increase in the amount of airborne and satellite image data has led
to a renewed interest in the automatic assessment of images and the general area of
Image Understanding. Applications are of signi(cid:142) cant importance to military, environ-
mental monitoring, urban and regional planning. This paper describes one such
system for the detection of buildings and their statistics from orthoimage s as well as
technologies which support queries about buildings, their relations and related
geometric features.

There are already many building detection systems reported in the literature and
a recent paper by Mayer (1999) provides an excellent overview of past work on the
topic. A reasonable example of current approaches is the work of Lin and Nevatia
(1997)
for the detection of buildings from mononuclear images. They (cid:142) rst use
perceptual grouping to generate 2-D roof hypotheses from detected line segments.
The algorithm then assumes rectilinear buildings to hypothesise parallelograms as
possible roofs and uses some criteria to select a number of parallelograms for
veri(cid:142) cation. For the veri(cid:142) cation stage, shadow and wall information is considered to
provide a shadow and wall score respectively. These scores are obtained by a weighted
sum of evidence for a number of possible building heights. Both scores produce a
combined score and the height which achieves this maximum combined score is chosen

Internationa l Journal of Geographica l Information Science
ISSN 1365-881 6 print/ISSN 1362-308 7 online © 2000 Taylor & Francis Ltd
http://www.tandf.co.uk/journals

Downloaded by [FU Berlin] at 23:33 12 May 2015 796

A. Stassopoulou et al.

as the actual height of the building. An overlap and containment analysis then
follows to decide whether any veri(cid:142) ed hypotheses overlap or contain others, respect-
ively. The average percentage of detected buildings (over 4 images) was 74.6% with
5.6% false alarms—buildings detected by the program but not by humans.

Noranha and Nevatia (1997) tackle the same detection problem but this time
using multiple views. A similar approach to the monocular method described above
is used assuming rectilinear structures (perceptual grouping, detection of parallelo-
grams, veri(cid:142) cation) but, in this case, features provided by the multiple images are
considered. For the veri(cid:142) cation stage a linear summation of the wall and shadow
evidence from all views is used. If the result of the summation is greater than the
threshold then the building is veri(cid:142) ed. Results on this work prove to have a better
accuracy with an average detection percentage 80.2%. In this case the false alarm
rate was 11%.

Collins et al. (1997) detect buildings from multiple, overlapping views. Following
the initial line detection stage, rooftops are detected by identifying possible roof
corners by pairs of lines meeting at an angle. Corners and lines are then entered into
a feature relation graph with potential building roof polygons appearing as cycles
in the graph. After detecting the potential rooftop in one image, geometric evidence
(e.g. height) from other images is sought via epipolar feature matching. Multi-image
triangulation then follows to determine the precise size, shape and position of the
building in the local 3-D site coordinate system. The algorithm assumes (cid:143) at-roofed
rectilinear structures. Evaluation results have been reported separately for the
2-D rooftop detection and for the 3-D reconstruction accuracy.

McKeown (1990) developed BABE (Buildup Area Building Extraction) which
constructs corners from lines by using the assumption that buildings are comprised
of straight line segments linked by (nearly) right-angled corners. It then constructs
chains of edges linked by corners to hypothesise boxes, parallelepides which may
indicate constructed structures. Finally, it evaluates these boxes in terms of size and
line intensity constraints.

Roux and McKeown (1994) use the hypotheses produced by BABE to construct
3-D roof surfaces by matching salient building features (e.g. corners) detected from
diŒerent views to provide 3-D building surface cues. The algorithm performs bottom
up matching using a succession of re(cid:142) ned features (3-D corners, 3-D segments, 3-D
surfaces). Geometric and structural knowledge is then applied to prune away unlikely
matching combinations. Results on 16 buildings proved to be quite successful on
peak roof structures while it had less success in terms of 3-D surface orientations.

Hsieh’s semi-automate d site modeling system (SiteCity) (Hsieh 1996) extracts
buildings by integrating photogrammetry , geometric constraints and Image
Understanding algorithms. The user begins by outlining the building roof in one
image. The height of the building is determined by searching for vertical edges at
the roof corners and the roof lines at the bottom of the buildings. The user intervenes
to adjust the height in case of failure of the automated process. The system attempts
to locate the buildings in other images by projecting the preliminary building model
from the (cid:142) rst image using elevation from DEMs. It then searches along the epipolar
lines to (cid:142) nd the precise building locations. The evaluation was performed on all
visible image points in terms of both standard deviation and pixel distance.

The proposed system diŒers from the above examples in so far as it explores a
more general and adaptable framework for combining evidence from both domain
knowledge and sensed image features to maximally evidence,
in this example,
buildings.

Downloaded by [FU Berlin] at 23:33 12 May 2015 Automatic extraction of building statistics from images

797

2. System overview

The image processing stage involves segmenting the image into regions and
extracting regions features relevant to inferring whether the region is likely to
correspond to a building or not. In the current implementation, such features include
geometric (anything relating to its shape), radiometric (using its spectral properties)
and contextual (for example, road-building relationships) . Some information is dir-
ectly available after the segmentation stage (for example area of region, average
intensity) whereas for inferring the region’s shape, further processing of the extracted
region boundary is required. For this latter task we have explored a corner extraction
technique based on training the system to track expert human performance.

We then utilised a Bayesian network to classify regions by combining these
features with additional domain knowledge including georeferenced information
concerned with sun position, etc. Once regions are labelled it is then easy to compute
building statistics including: number of buildings, average area of buildings and the
average distance between them. In principle, the system consists of a relatively small
set of procedures but is dependent on training the system with examples of building
and non-building regions.

2.1. Getting regions: adaptive multi-scale segmenters

Image segmentation is concerned with grouping pixels into regions which share
similar properties. Traditionally, there are two approaches to segmentation. One,
‘region-driven’ involving clustering pixels by their similarities. Two, ‘boundary (edge)-
driven’ involving detecting dissimilarities between pixels. Our proposed segmentation
algorithm, namely, the multi-scale adaptive segmentation combines both region and
edge information to segment the image into regions.

The basis of this segmenter lies in the use of a recursive multi-scaled segmentation
where region statistics at one scale are computed with those of composite regions at
a (cid:142) ner scale to decide if the region should be segmented at the (cid:142) ner scale. Multi-
scale approaches have also been utilised by other authors to improve segmentation
(Marr and Hildreth 1980, Bouman and Liu 1991, Koep(cid:143) er et al. 1994, Morse et al.
1994, Spann and Grace 1994, Haring et al. 1994 ). For example, Marr and Hildreth
(1980) used an edge detection criterion based upon their existence at more than one
scale. However, such approaches make use of multi-scale techniques using only one
form of information (i.e. not both region information and edge information) . The
current algorithm uses both edge and region attributes to multiple scales to adaptively
and automatically choose the best scale at which to segment diŒerent parts of an
image. In this way, images with widely varying characteristics can be successfully
segmented using one procedure.

The segmenter is based upon the use of a standard edge extraction method, in
this case, the Canny operator (Canny 1986). We then use a piece-wise linear boundary
interpolation model to guarantee closed regions at diŒerent scales. The process is
recursive over scales and regions and results in an adaptive tree (spatial variance)
decomposition, which includes more realistic region shapes than do typical quadtree
decompositions. More formally, we assume at each parent node, as a null hypothesis,
that the sibling subregions generated at the (cid:142) ner scale to a given parent region (and
scale) are sampled from the same normal population as the parent region. We then
perform a one-way Analysis of Variance (ANOVA) to test this hypothesis by deter-
mining whether there is su(cid:141) cient variation between the subregion means (in this
case, average intensity values) compared to the variances within the subregions to

Downloaded by [FU Berlin] at 23:33 12 May 2015 798

A. Stassopoulou et al.

reject this null hypothesis. Under the normal sample assumption, this reduces to the
F-test (F-distribution) :

F(nÕ k, kÕ 1 ) 5

(1)

k

nÕ k
kÕ 1

i= 1
k

ni(XÅ

i Õ XÅ

)2

(ni Õ 1) s2i

i= 1

where k is the number of subregions, ni is the number of pixels in region i, n is the
total number of pixels in all the subregions, s2i is the unbiased sample variance for
subregion i, Xij is intensity of pixel j in region i, XÅ
i is the mean of the intensity in
region i, and XÅ
is the mean of all intensities in all regions. If the null hypothesis is
not rejected (F(nÕ k, kÕ 1) with p > 0.01), then the split is not accepted and the
parent region is retained; if the null hypothesis is rejected (F(nÕ k, kÕ 1 ) with p< 0.01)
then the parent region is replaced by all of the subregions. In this case we have used
the rejection rate of p< 0.01 so de(cid:142) ning a strict criterion for region splitting.

In all, then, region boundaries are de(cid:142) ned by the interpolated edge operator.
However, the decision to further segment a region is based upon regional statistical
analysis (for more details see McCane and Caelli 1997).

Figure 1 shows results of the multi-scale algorithm, where the images (256Ö 256,
8-bit grey level orthophotos) on the left were segmented using three scales (isotropic
two-dimensional Gaussian smoothing (cid:142) lters with s’s of 4, 2 and 1 pixels). These
scales were chosen to enable registration of regions with areas within the range of

Figure 1. Left: Original images. Right: Regions (depicted by average grey intensity values)
resulting from the adaptive, multi-scaled segmenter.

Downloaded by [FU Berlin] at 23:33 12 May 2015 (cid:158)
(cid:158)
Automatic extraction of building statistics from images

799

buildings of interest (McCane and Caelli 1997 ). As can be seen from these results,
the multi-scale segmentation produced quite reasonable segmentation of the buildings
from its surroundings. It is very interesting to note that on the bottom set of images,
the building labelled as A was successfully segmented from a similarly grey
background.

1

Segmented region boundary points (indexed by s

..sn ) were then labelled as
corners if they corresponded to a curvature (k(s)) peak (dk(s)/ds 5 0: local minimum
or maximum) and the absolute curvature value at such a peak was greater than a
threshold, t: |k(s):sc)|>
t. These computations were performed at a given scale de(cid:142) ned
by a Gaussian smoothing (cid:142) lter with sc de(cid:142) ned along the contour. Although it could
be argued that t should be a function of the curvature scale sc, in this case we have
framed the threshold question in terms of the trade-oŒbetween curvature magnitude
and the change of curvature over the boundary contour. Consequently we have
included both s and t as parameters to be estimated in our attempt to (cid:142) t predictions
based on the curvature peaks and absolute curvature values with observed corner
depiction.

1

2

< s

Figure 2 shows a plot of the absolute values of two curvatures, obtained using 2
diŒerent ss (s
) and the horizontal line represents the threshold for acceptance.
Any peaks (detected by a simple (cid:142) nite diŒerence local maxima operator with respect
to the boundary parameter) above that line are classi(cid:142) ed as corners. For example,
results in 7 corners. Shifting
in the case shown in (cid:142) gure 2 the curvature at scale s
the threshold line lower or higher would result in more or less corners, respectively.
A human-in-the-loop approach is introduced at this point to learn the required
parameters, t and sc. An expert is presented with building boundaries and is required
to identify the corners. Such corners are then used as the set of training examples.
The task is to minimise, in the least squares sense:

2

mint, sc (cid:158)

t

j

i ×Ni

D2c (Ot

j, Pt

ij(t, sc))

(2)

where Ot

j is the observed (expert-identi(cid:142) ed) corner position vector j of shape t and

Figure 2. Plotted absolute values of curvature ( |k(s):sc)|) using 2 diŒerent s’s (s
2

). Here
the scale, s, represents the resolution for contour detail while the threshold, t,
determines the magnitude of curvature used to determine whether a curvature peak
corresponded to a corner or not.

> s
1

Downloaded by [FU Berlin] at 23:33 12 May 2015 (cid:158)
(cid:158)
800

A. Stassopoulou et al.

Pt
ij is the ith predicted corner (position vector) neighbour for the observed jth corner
corresponding to the selected peaks (at a given scale and threshold), and Dc corre-
sponds to the distance along the boundary contour in pixels. The summation ranges
over all training patterns, t, over all the distances between each observed corner, j,
and those predicted in its neighbourhood , Nj on the curve. The neighbourhood of a
corner lying between two boundary segments is de(cid:142) ned on the parameterised bound-
ary from the mid-point of one segment up to the mid-point of the second segment
(a boundary segment is the segment between one expert corner to the next). Note
that the expert-identi(cid:142) ed corner is one of the contour points which is judged closest
to the ‘true corner’. This function penalizes for more points in Nj and for the distance
between the observed and predicted corners.

The scale, sc, and the threshold, t, which achieve the minimum error are then
used as the chosen parameters for corner detection. In this case these values were
detected by exhaustive search as the number of parameters and states were relatively
small. Other search procedures such as simulated annealing and genetic algorithms
could have been used. However, these procedures cannot return better results than
exhaustive search and, given this optimisation process only occurred once, the
exhaustive search method was chosen.

Figure 3 shows the nine corners derived on the boundary extracted from the
original image shown in the top left orthophoto of (cid:142) gure 1. By joining each corner
with its successor using straight lines, we obtain a polygonal approximation to the
region which best-(cid:142) ts expert annotation. This is used as evidence for buildings, as
described in the following section.

3. Bayesian networks

Belief or Bayesian networks (Pearl 1986, 1988, Neapolitan 1990 ) are directed
acyclic graphs in which the nodes represent multi-valued variables, comprising a
collection of mutually exclusive and exhaustive hypotheses. The arcs signify direct
dependencies between the linked variables and the strengths of these dependencies
are quanti(cid:142) ed by conditional probabilities.

Consider a typical node X having m children Y
, U

, ..., Ym and n parents
, ..., Un as shown in (cid:142) gure 4 (the node variables will be denoted by capital

, Y

1

2

U

1

2

Figure 3. Derived corners using human-in-the-loop. Note that what constitutes a ‘corner’ is
ambiguous without additional constraints—in this case, expert depiction of corners.

Downloaded by [FU Berlin] at 23:33 12 May 2015 Automatic extraction of building statistics from images

801

Figure 4. The parents and children of a typical node X in a polytree.

letters and the parents and children will also have subscripts to distinguish among
them). What follows is a brief summary of the propagation algorithm developed by
Pearl (1988 ).

The purpose of the Bayesian network is to model the complete joint probability
density function in terms of the products of a set of dependencies between the
variables resulting in a belief in each possible labelling for each node after some
evidence arrives via other nodes. In Bayesian network notation, this is denoted as
BEL . That is, it gives the conditional probability of a node being in each of its states
given the evidence observed. In order to estimate the belief of a node we need the
information sent by the parents (causal), the information sent by the children (dia-
gnostic) and the conditional probability matrices. The messages that communicate
this information obtained by the parents are denoted by p and the messages that
carry information from the children are denoted by l. The p and l messages are
depicted in (cid:142) gure 4.

Before any propagation commences we initialise the network. Every node is
assigned a vector l, a vector p and a vector BEL . These vectors have as many
elements as there are possible states for this node. To initialise the network, we set
all elements of all l vectors to 1. We also set all elements of the p vectors of the root
nodes equal to the prior probabilities for the corresponding states. All the values of
the remaining variables of the network can be computed now from the above
initialised quantities and the elements of the conditional probability matrices. An
element of the conditional probability matrix, P(xi |u
), gives the probability
of state i for node x conditioned on the states of its parent nodes. In addition to the
vector that can be communicated
above, each node, except the leaf nodes, has a p
to its children. Also, each node, except the root nodes, carries a l
vector which
(send)
can be communicated to its parents.

, ..., unjn

(send)

1j1

Upon receiving evidence (in the form of a p or l message), the node will update
its belief and send the corresponding p and l message to all its children and parents
respectively, except for the one it had just received information from. Each of the
parents and children will then recursively follow the same procedure until there are
no more messages to be sent (i.e. all evidence is absorbed) and the network then

Downloaded by [FU Berlin] at 23:33 12 May 2015 802

A. Stassopoulou et al.

reaches an equilibrium. The (cid:142) nal belief of each node will be the actual posterior
probability, i.e. the probability of each node assuming each of its states, given all
evidence observed.

Such networks have been successfully applied to medical diagnosis (Olesen et al.
1989, Hamilton et al. 1994), forecasting (Abramson 1994, Gu et al. 1994), environ-
mental risk assessment (Stassopoulou et al. 1998 ) and other decision-making systems.
However, applications in Computer Vision are more rare.

3.1. Constructing the Bayesian network

The Bayesian network constructed for our building detection problem is shown
in (cid:142) gure 5. Before we explain the reasoning behind the structure of the network, we
(cid:142) rst give an interpretation of each of the nodes along with abbreviations we will
often use in the remaining of the paper:

E Rectangular Decomposition (RD): Measures the rectangularity of the given
region, by the ratio ‘number of 90-degree corners/T otal number of corners’. It
takes values from [0, ...,1].

E Polygon Fit (PF): Measures how well the extracted polygon area (A) (cid:142) ts over
the segmented region (B). This is given by the ratio ‘A> B/A< B’. It takes values
from [0, ...,1].

E Area (A): Area of the region in terms of pixels.
E Slope (S): The slope of the region (omitted ).
E Material (M): The material of each region, which is, again, a number coming
from the Bidirectional Re(cid:143) ectance Distribution Function which estimates the
intensity at each region, from knowing the Sun angles and the normal to the
region (omitted).

E Azimuth (AZ): Solar Azimuth measured in degrees from 0 to 360 and is de(cid:142) ned
as the angle in the horizontal plane between the due south (or north in our
case) line and the projection of the site-to-sun line on the horizontal plane
(Kreith and Kreider 1978).

E Elevation (E): Elevation of the Sun measured in degrees from 0 to 90 and

Figure 5. Bayesian networks constructed for building detection.

Downloaded by [FU Berlin] at 23:33 12 May 2015 Automatic extraction of building statistics from images

803

de(cid:142) ned as the angle from the horizontal plane upward to the centre of the Sun
(Kreith and Kreider 1978).
Intensity (I): The average intensity of the segmented region.

E Road Adjacency (RA): Using an adjacency graph for each region. We measure
the length of the shortest path from given region to road region (if assumed
known). It takes values from [0,...,Y ] with Y being the longest acceptable
value. We base this variable on the fact that every building should have a road
nearby. However, in order to have an unconstraine d solution to the problem,
and due to lack of availability of detected roads, we leave this node
uninstantiated.

E Road Orientation (RO): Majority of buildings are oriented parallel to roads.
States are ( parallel,"parallel ) to one of the polygon extracted sides. Similarly,
this node is left uninstantiated .

E Building (B): States (yes,no).
E Shadow Location (SL): Indicates whether a shadow has been detected. States
are the possible locations of the shadow (north, north-east, etc., or none). We
could assume that a shadow has been detected when there is a dark region
adjacent to the current region in question, in the speci(cid:142) c location we are
searching for.

As can be seen from the network of (cid:142) gure 5, we have represented by nodes all the
variables which in(cid:143) uence either directly or indirectly, the building hypothesis. The
connections (arcs and their directions) of the network were designed using domain
knowledge and form a signi(cid:142) cant part in designing Bayesian networks.

The features of the building,

i.e. Intensity, Area, Rectangular Decomposition,
Polygon Fit, Shadow L ocation, Road Adjacency, Road Orientation are leaf nodes, i.e.
nodes with no children. All of these features are the ‘result’ of the Building. In
Bayesian network terminology, these features are conditionally independed given
Building. For example, if we know the state of Building, a change in probability of
Area will have no eŒect on the Polygon Fit hypothesis (or any other child node
hypothesis) . Otherwise, if the state of Building is not known, the two features are
depended. An change in the probability of Area will cause an update in the Building
hypothesis which then will cause Polygon Fit (and all other child nodes of Building)
to update their belief (see previous section on belief updating). These conditional
independence assumptions are encoded in the network by the connections. (For more
details on causality and learning causal structures see Neapolitan et al. 1997 ).

The location of the shadow depends also on the solar azimuth. The elevation of
the Sun in(cid:143) uences the location of the shadow only in the case where it is close to
90 degrees. In this case the Sun is directly above the region and the absence of the
shadow is expected. In such a case, the absence of a shadow should not be considered
as negative evidence for a region being a building, but, instead, have no contribution
towards the region evaluation. We have therefore drawn the arc from Elevation to
Shadow L ocation to represent this case. The other elevation values would only
in(cid:143) uence the width of the shadow instead of the location and are therefore not
signi(cid:142) cant in our case. The elevation of the Sun also in(cid:143) uences the intensity of the
region, and is derived from the solar azimuth using a deterministic formula. The
Bidirectional Re(cid:143) ectance Distribution Function (BRDF) also tells us that intensity
is also determined by the slope and material of the region. These nodes, however,
and their corresponding arcs are indicated with dotted lines in the network of

Downloaded by [FU Berlin] at 23:33 12 May 2015 E
804

A. Stassopoulou et al.

(cid:142) gure 4, to show that although they are present for completeness of the model, they
are omitted after the necessary assumptions. The reasoning behind this will be
presented in the next section which gives a detailed description of each of the nodes.

3.2. Variable quantization

Since, in this implementation, the Bayesian network is developed for discrete
variables, the observed continuous variables need to be quantized-divided into mean-
ingful states (meaningful in terms of our goal, i.e. to detect buildings). One well-
known measure which characterises the purity of the class membership of diŒerent
variable states is information content or entropy (Shannon and Weaver 1949,
Mitchell 1997 ).

The procedure to estimate the entropy divided by a particular quantization of a
variable is as follows: We observe the values of the variable for a set of building and
non-building examples. For a given quantization into c classes, the entropy is
given by:

E 5

Õ PBilogPBi Õ PNBilogPNBi

(3)

c

i= 1

where PBi is the probability of buildings in class i and PNBi is the probability of non-
buildings in class i. The number and range of classes which results in the minimum
entropy are chosen to quantize the variable. Again, an exhaustive search procedure
was used and the entropy was not weighted with the number of bins used. Such
non-parametric bining procedures are appropriate where parametric mixture models
may not apply.

This minimum entropy principle was applied on all the continuous nodes which
cannot be quantized using their physical properties (as is the case of solar azimuth
and elevation and shadow location). These nodes corresponded to: Rectangular
Decomposition, Polygon Fit, Area and Intensity (see (cid:142) gure 4).

3.2.1. Rectangular decomposition

This variable measures the ‘rectangularity’ of a region. It is the ratio of the
number of 90-degree corners over the total number of corners and ranges between
0 and 1. After obtaining the ratio of each of the building and non-building examples,
we apply the entropy formula and (cid:142) nd the minimum entropy to achieve when we
divide the range into two classes: [0–0.35, 0.35–1].

3.2.2. Polygon (cid:142) t

The polygon (cid:142) t variable measures how well a polygon area, A, (cid:142) ts over the
segmented region area, B, and is given by the ratio ‘A> B/A< B’. The values are
ranging between 0 and 1, with 1 indicating an exact (cid:142) t. Using the minimum entropy
in a similar fashion, we obtain two classes for this variable: [0–0.88, 0.88–1].

3.2.3. Area

This represents the area in terms of pixels of the segmented region and ranges
from 100 pixels to 3000. We have chosen 100 m2 to be the minimum area of a
building. With the 1 m resolution of the orthophoto, this corresponds to 100 pixels
on the image.

The entropy gave the following (cid:142) ve Area classes in terms of pixels:

(100–1000 )
(1000 –1350)

Downloaded by [FU Berlin] at 23:33 12 May 2015 (cid:158)
Automatic extraction of building statistics from images

805

(1350 –1400)
(1400 –1800)
(1800 –3000)

3.2.4. Intensity

3.2.5. Azimuth

The average grey level intensity of each region, given by the adaptive segment-
ation step, ranges between 0 and 255. The intensity classes are derived to be:
[0–162,162–255].

Since the time, day and coordinates of each image are known, the solar angles
(azimuth and elevation) are calculated and represented in the network. In this way,
azimuth information can help us predict the location of the shadow of a possible
building region.

The possible states of the Azimuth node were chosen, due to physical properties,

to be the following:

E North: 337.5ß –22.5ß
E North-east: 22.5ß –67.5ß
E East: 67.5ß –112.5ß
E South-east: 112.5ß –157.5ß
E South: 157.5ß –202.5ß
E South-west: 202.5ß –247.5ß
E West: 247.5ß –292.5ß
E North-west: 292.5ß –337.5ß

3.2.6. Shadow location

According to each of the above azimuth angles, a shadow of a region de(cid:142) ned as
a dark adjacent patch can be expected at the corresponding location. Figure 6 shows
the 0ß
to 360ß circle measuring degrees from the north, and the quantization of the
azimuth as above. The shaded area indicates the section where the shadow would
be expected (NW location) given a SE position of the Sun (any azimuth angle
between 112.5ß –157.5ß ).

The shadow location node has nine classes: (N, NE, E, SE, S, SW, W, NW, none)

Figure 6. Possible sun position and corresponding shadow location range.

Downloaded by [FU Berlin] at 23:33 12 May 2015 806

A. Stassopoulou et al.

each one corresponding to a diagonally opposite Sun position, as indicated in
(cid:142) gure 6. The shadow location none corresponds to an Elevation state close to 90
degrees. The shadow is input in the network by observing a dark patch at the
expected location. The threshold of the intensity of the dark patch was derived by
statistical data consisting of buildings with shadow examples.

3.2.7. Slope and material

Figure 7 demonstrate s the underlying principles. For this model we assume a
Lambertian surface, that is, a surface which appears equally bright from all viewing
directions for a (cid:142) xed distribution of illumination and does not absorb any incident
illumination (Jain et al. 1995). Then the intensity of region is a function of the angle,
h, between the normal to the surface and the source of illumination as well as the
material of the surface.

Assuming, however, (cid:143) at-roofed buildings with a constant material, the only
variable which aŒects the intensity of a region is the angle h of (cid:142) gure 7. According
to the de(cid:142) nition of the Sun elevation given above, the angle theta is simply:

Since we have already deduced that intensity, I, is de(cid:142) ned by:

we can also assume the equivalent expression:

h 5 90ß

elevation

I3 cosh

I3 sin(elevation)

(4)

(5)

(6)

Therefore, under these assumptions, intensity is only dependent on the sine of
the elevation angle. Here slope and material nodes are ignored from the network of
(cid:142) gure 7, since they no longer represent variables. For a more general solution
however, these nodes should be included and the required conditional probability
of Intensity given its parent nodes Building, Elevation, Slope and Material should be
derived using the Bidirectional Re(cid:143) ectance Distribution Function (Haralick and
Shapiro 1992, Jain et al. 1995 ).

3.2.8. Elevation of the Sun

This variable, as the Azimuth variable above, is again di(cid:141) cult to design, due to
lack of data. Digital orthophotos with varying Sun positions were not available for
this work. We have therefore chosen to quantize this node into four equal ranges
between 0 to 80 degrees and an additional class from 80 to 90 degrees to represent

Figure 7. Surface shown with surface normal and the illumination source.

Downloaded by [FU Berlin] at 23:33 12 May 2015 Õ
Automatic extraction of building statistics from images

807

the case when no shadow is expected. Note that the number 80 ideally would have
to be derived from data, since it could be argued that 78 or 79 degrees do not cause
a shadow either. Due to lack of data, however, we assume the value of 80.

Elevation was therefore quantized into the following (cid:142) ve classes:

0ß –14.25ß
14.25ß –29.5ß
29.5ß –47.61ß
47.61ß –80ß
80ß –90ß

Note that the range from sin(0) to sin(80) was divided into four equal ranges
to 80ß . Since it is the sine of the elevation needed for the
instead of the range 0ß
determination of intensity, we have quantized with respect to this, in order to create
meaningful intensity classes.

3.2.9. Road Adjacency and orientation

The Road Adjacency node represents the number of regions that are between
the region in question and a road region. This node has been arbitrarily quantized
into two classes since we will assume that the roads are not detected in the image
and therefore leave this node uninstantiated . To neutralise their eŒect we assume
equal probabilities between these nodes and the building node. Road Orientation
takes as values (parallel,"parallel) according to the orientation of the given region
relative to the road. Similar assumptions apply to this node. Results will show that
even when these nodes are omitted, the system will produce good results.

3.3. Conditional probabilities

Having constructed the network and the nodes the conditional probabilities need
computing—a non-trivial task. Current literature on Bayesian networks concentrates
on ways to learn these parameters (Spiegelhalter and Lauritzen 1990, Russell et al.
1995, Stassopoulou et al. 1996). However, in this case, with supervised learning the
network can be constructed (learned ) empirically from the training data and then
tested on unseen images.

For each non-root node we need to de(cid:142) ne conditional probability tables as
follows: P(A | B), P(RD| B), P(PF|B), P(SL |B, AZ, E), P(RO| B), P(RA| B) and
P(I |B, E), where the nodes are given the abbreviations previously de(cid:142) ned. Each of
these tables gives the conditional probability of a child node to being in each of its
states, given all possible parent state combinations. Note that Elevation is determin-
istically de(cid:142) ned by Azimuth using the equations of Solar angles (Kreith and Kreider
1978 ). As already mentioned, these probabilities are derived from the training data.
For example, the conditional probability of Area being in class 1 given Building is
yes, is determined from data by counting the number of building examples with an
area within class 1, and so on.

However, for the conditional probability of Intensity given Building and Elevation,
the situation is more complicated, since the data (both training and testing) come
from a single digital orthophoto with (cid:142) xed Sun coordinates. We can therefore only
estimate the conditional P(I |B, elevation 5 x), where x is the given elevation. However,
for the purpose of this paper, we assume that the other elevations will have the same
probabilities. Since elevation is always (cid:142) xed (only one image is used ), throughout

Downloaded by [FU Berlin] at 23:33 12 May 2015 808

A. Stassopoulou et al.

this paper, the remaining elements of this probability table are insigni(cid:142) cant for the
building hypothesis.

4. Experimental results

A set of building regions (of varying shape) were initially collected for the corner
detection stage. The expert (human) had to annotate corners on the extracted
boundary of building regions. These expert-identi(cid:142) ed corners were then used as our
training set to derive the required parameters for the corner detection stage.

Two more sets of both building and non-building regions were then selected to
perform the minimum entropy quantization of the continuous variables. These sets
were also used to produce the conditional probabilities to be used by the network.
For an unbiased estimation of probabilities, we have chosen buildings and non-
buildings with a lot of variation in their attributes. A diŒerent set of building and
non-building regions was then collected for testing and evaluation of the system.

Out of 41 buildings, not previously seen during training, 40 were correctly
classi(cid:142) ed as buildings. This is interpreted to 97.6% accuracy where the accuracy is
measured by the ratio of buildings classi(cid:142) ed over the total number of segmented
buildings tested. Out of 25 non-building regions, not used for training, 22 were
correctly classi(cid:142) ed as non-buildings (the number of buildings used for evaluation,
namely 41, compares favourably to other building detection methods reported in
the literature). Further, when computing building statistics (see following section)
over 200 more buildings were involved and correctly detected—of varying size and
in varying contexts.

The process of Bayesian network classi(cid:142) cation is described as follows: Having
collected the available evidence we enter them as input to the network by a vector
(0,...,1,...,0) with 1 at the position of the state being instantiated (i.e. 100% probability
and 0 elsewhere). The network was implemented using the Microsoft Belief Networks
(MSBN) package.

Figures 8 and 9 show the three steps taken to detect buildings in each of the
three images. Images in (cid:142) gures 8 and 9(b) show the segmented regions obtained
using the multi-scale adaptive segmentation on the originals shown in (cid:142) gures 8 and
9(a). The segmentation was achieved using three scales (isotropic two-dimensional
Gaussian smoothing (cid:142) lters with ss of 4, 2 and 1 pixels).

All the region information needed for instantiating the Bayesian network is either
readily available after the segmentation step (i.e. Intensity, Area, Shadow Location)

Figure 8.

(a) Original diameter, (b) Regions resulting from the adaptive, multi-scaled segment,
(c) Detected building with its polygonal approximation. Buildings partly visible on
the border of the image were not evaluated.

Downloaded by [FU Berlin] at 23:33 12 May 2015 Automatic extraction of building statistics from images

809

Figure 9.

(a) Original image, (b) Regions resulting from the adaptive, multi-scaled segmenter.

Letters used for reference. (c) Detected buildings with their polygonal approximations.

or can be obtained by further processing the segmented regions (i.e. Rectangular
Decomposition, Polygon Fit). After entering the data into the Bayesian network the
detected buildings are indicated in (cid:142) gures 8 and 9(c), along with a black boundary
representing their polygonal approximations . The following results were obtained:

In (cid:142) gure 8(c) the probability of the indicated region being a building is 0.99. The
shadow was detected on the NW location as expected with a SE Sun position, the
area was 1605 pixels, average intensity was 206, the polygon (cid:142) t was 0.89 and the
ratio of 90-degree corners was 0.44.

In (cid:142) gure 9(c) all the segmented building regions have been successfully identi(cid:142) ed
with one false alarm. This is region E in (cid:142) gure 9(b) whose shadow creates an illusion
of a building. Region A, in the same image was classi(cid:142) ed as a building with 0.99
probability whereas, the darker building, B, C and D had a 0.78 probability of being
a building. Note that the smaller buildings on the left were still identi(cid:142) ed as buildings,
although the polygonal approximation s to these regions was not as successful. The
reason for this is that the system was trained to identify corners on buildings of a
larger scale (similar to the buildings shown by region A). The larger scale boundaries
often require a larger width in the Gaussian smoothing (cid:142) lter which could prove to
be too large for the boundary perimeter of the smaller buildings. We plan to
investigate the issue of this third parameter (the width of the Gaussian) in our future
work with the possibility of learning this value.

However, the system was still able to classify these regions, as buildings, mainly
because of their bright intensity and/or the presence of a shadow. From statis-
tical data,
intensity proves to be a more reliable evidence than the polygonal
approximation.

It is evident that the (cid:142) nal classi(cid:142) cation results of the Bayesian network, are very
much dependent on the segmentation step. For example, even if some buildings are
visible in the original image of (cid:142) gure 12(a) (far left side), the segmentation of those
was not successful due to the low contrast between the buildings and the surrounding.
Therefore, these buildings were never tested on the building hypothesis.

5. Extraction of building statistics

In this section we will present statistics on buildings detected on some of the
256Ö 256 images using our proposed algorithm. These statistics refer to: the number
of buildings in the image, their average areas in square metres and the average
distance between them (in metres) as a measure of the density of buildings. Such

Downloaded by [FU Berlin] at 23:33 12 May 2015 810

A. Stassopoulou et al.

[a]

[b]

[c]

[d]

Figure 10.

(a) and (b) Small, dense buildings (c) and (d) Larger and sparser buildings.

Table 1. Building statistics on ten images extracted from the north-east Washington East
DC–MD. Quadrangle ( labels on furthest right refer to (cid:142) gure numbers in the text).

Image ID

NE0-11
NE0-12
NE5-13
NE7-9
NE16-12
NE16-16
NE17-4
NE18-22
NE20-21
NE21-18

Number of
buildings

Average
area (m2)

Average
distance (m)

8
7
9
48
11
15
24
48
9
38

1151.62
1460.71
947.11
198.96
1312.64
776.33
204.50
193.58
945.22
195.74

52.04
70.50
43.72
20.25
40.93
40.49
22.72
20.71
71.96
20.30

10c

10a
10d
10b

statistics are important for urban planning, monitoring, rapid disaster assessment as
from earthquakes, tornadoes etc.

The images were obtained from 7 kmÖ 6 km north-east Quarter-Quadrangl e of
the Washington DC area. We count the number of buildings detected (a region is
labeled as building if it has a higher probability of being a building than not be-
ing a building) and record their areas in terms of pixels (or metres, since the
orthophoto has 1 m resolution).

Building centroids were computed to de(cid:142) ne the centroid-to-centroi d building

Downloaded by [FU Berlin] at 23:33 12 May 2015 Automatic extraction of building statistics from images

811

distances and where each centroid (xc, yc) was computed from the boundary
coordinates as:

xi

n

i= 1
n

xc 5

yc 5

yi

n

i= 1
n

1

1

, y

where (x
), ..., (xn, yn) are the n points on the region boundary. We then determined
the shortest Euclidean distance from each building’s centroid to its nearest centroid
to derive the average building density. It should be noted that since we have already
determined best-(cid:142) tting polygons to building regions, more representative measures
of proximity could be used (Miller 1996). However, in this case we have used centroid
distances purely for illustrative purposes.

The images were chosen to have varying characteristics; some containing fewer,
larger and more sparse buildings (indicating possible warehouses or factories)
whereas others contained a greater number of smaller buildings close to each other
(indicating mainly urban areas). Some representative images of both classes are
shown in (cid:142) gure 10. The corresponding statistics of each of these images (cid:142) gures 10(a)
to 10(d) are given in table 1 (corresponding images indicated by the corresponding
letter on the right), along with all the statistics of the other images not shown due
to space limitations.

Figures 11, 12 and 13 show the charts for each of the three statistics extracted.
The mesh corresponds to the north-east, 7 kmÖ 6 km Washington east DC–MD
Quadrangle . Each cell on the 3-D mesh corresponds to a 256Ö 256 image. The
numbers that appear on the Image ID column of table 1 correspond to the coordin-
ates on the mesh, with the (cid:142) rst number being measured on the 7 km axis and the
second number on the 6 km axis. It can be veri(cid:142) ed from these charts and from table 1
that the average area of buildings is positively correlated with the average distance
and negatively correlated with the number of buildings.

6. Conclusions and future work

In this paper we have explored a building classi(cid:142) cation system based upon
adaptable visual routines and a Bayesian network model for combining diverse
evidence for a given region being classi(cid:142) ed as a building or not. The bene(cid:142) ts of this

Figure 11. Number of buildings.

Downloaded by [FU Berlin] at 23:33 12 May 2015 (cid:158)
(cid:158)
812

A. Stassopoulou et al.

Figure 12. Average area (m2).

Figure 13. Average distance between buildings (m).

approach are that it oŒers an additional degree of robustness for feature extraction
and the Bayesian network model allows the system to utilise a wide range of
information without precise co-registration. Given this, our preliminary results com-
pare favourably to those reported in the literature and also allow us to compute
useful building statistics without ever producing CAD model and the like.

Perhaps the greatest bene(cid:142) t of the Bayesian network approach is that it can also
operate in semi-automati c mode. It oŒers decision-support capability for experts in
deciding whether certain image regions are likely to be buildings, whether a given
density of shapes is likely to correspond to, for example, suburban areas or the
degree of damage to buildings after earthquakes.

Acknowledgments

The authors would like to thank Professor Richard E. Neapolitan for his invalu-
able comments on the Bayesian network. The (cid:142) rst author would also like to thank
Dr John D. Bossler for his constructive comments and suggestions. This work was
funded by the National Imagery and Mapping Agency (NIMA) Map Revision
project, contract number NMA100-97-K-502 8 and the National Aeronautics and
Space Administration (NASA) as part of the Image Understanding Initiative with
the Commercial Space Center.

Downloaded by [FU Berlin] at 23:33 12 May 2015 Automatic extraction of building statistics from images

813

References
Abramson, B., 1994, The design of belief network-based systems for price forecasting. Computer

and Electric Engineering, 20, 163–180.

Bouman, C., and Liu, B., 1991, Multiple resolution segmentation of textures images. IEEE

T ransactions on Pattern Analysis and Machine Intelligence, 13, 99–113.

Canny, J. F., 1986, A computational approach to edge detection. IEEE T ransactions on

Pattern Analysis and Machine Intelligence, 8, 679–698.

Collins, R. T., Jaynes, C. O., Cheng, Y-Q., Wang, X., Stolle, F. R., Schultz, H., Hanson,
A. R., and Riseman, E. M., 1997, The UMass Ascender system for 3D site model
construction. In RADIUS: Image Understanding for Imagery Intelligence, edited by
O. Firschein and T. M. Strat (California: Morgan Kaufmann), pp. 209–222.
Gu, Y., Peiris, D., Crawford, J., McNicol, J., Marshall, B., and Jefferies, R., 1994, An
application of belief networks to future crop production. In Proceedings of the 10th
Conference on Arti(cid:142) cial Intelligence for Applications (San Antonio, Texas), pp. 305–309.
Hamilton, P. W., Anderson, N., Bartels, P. H., and Thompson, D., 1994, Expert system
support using Bayesian belief networks in the diagnosis of (cid:142) ne needle aspiration biopsy
specimens of the breast. Journal of Clinical Pathology, 47, 329–336.

Haralick, R. M., and Shapiro, L. G., 1992, Computer and Robot V ision (Reading: Addison

Wesley).

Haring, S., Viergever, M. A., and Kok, J. N., 1994, Kohonen networks for multi-scale image

segmentation. Image and V ision Computing, 12, 339–344.

Hsieh, Y., 1996, SiteCity: A semi-automated site modelling system. In IEEE Conference
on Computer V ision and Pattern Recognition (California: Morgan Kaufmann),
pp. 499–506.

Jain, R., Kasturi, R., and Schunck, B. G., 1995, Machine V ision (Singapore: McGraw

Hill Inc.).

Corporation).

Koepfler, G., Lopez, C., and Morel, J. M., 1994, A multi-scale algorithm for image segmenta-
tion by variational method. SIAM Journal of Numerical Analysis, 31, 282–299.
Kreith, F., and Kreider, J. F., 1978, Principles of Solar Engineering (Hemisphere Publishing

Lin, C., and Nevatia, R., 1997, Building Detection from a monocular image. In RADIUS:
Image Understanding for Imagery Intelligence, edited by O. Firschein and T. M. Strat
(California: Morgan Kaufmann Publishers, Inc.), pp. 153–170.

Marr, D., and Hildreth, E., 1980, Theory of edge detection. Proceedings Royal Society of

L ondon, B207, 187–217.

Mayer, H., 1999, Automatic object extraction from aerial imagery—a survey focusing on

buildings. Computer V ision and Image Understanding, 74, 138–149.

McCane, B., and Caelli, T., 1997, Multi-scale adaptive segmentation using edge and region-
based attributes. In Proceedings of the First International Conference of Knowledge-
based Intelligent Electronic Systems (KES’97), edited by L. C. Jain (New York: IEEE
Press), pp. 72–81.

McKeown, D. M., 1990, Toward automatic cartographic feature extraction. In Mapping and
Spatial Modelling for Navigation, NAT O ASI Series, vol. 65, edited by L. F. Pau
(Berlin: Springer-Verlag), pp. 149–180.

Miller, H. J., 1996, GIS and geometric representation in facility location problems.

International Journal of Geographical Information Systems, 10, 791–816.

Mitchell, T. M., 1997, Machine L earning (McGraw Hill Companies Inc.).
Morse, B. S., Pizer, S. M., and Liu, A., 1994, Multi-scale medial analysis of medical images.

Image and V ision Computing, 12, 327–338.

Neapolitan, R. E., 1990, Probabilistic Reasoning in Expert Systems: T heory and Applications

(New York: Wiley).

Neapolitan, R. E., Morris, S. B., and Cork, D., 1997, Cognitive processing of causal
knowledge. In Proceedings of the T hirteenth Annual Conference on Uncertainty in
Arti(cid:142) cial Intelligence (UAI-97) (California: Morgan Kaufmann), pp. 384–391.
Noronha, S., and Nevatia, R., 1997, Detection and description of buildings from a multiple
aerial image. In RADIUS: Image Understanding for Imagery Intelligence, edited by
O. Firschein and T. M. Strat (California: Morgan Kaufmann), pp. 171–183.

Downloaded by [FU Berlin] at 23:33 12 May 2015 814

Automatic extraction of building statistics from images

Olesen, K. G., Kjaerulff, U., Jensen, U., Jensen, F. V., Falck, B., Andreassen, S., and
Andersen, S. K., 1989, A munin network for the median nerve-a case study on loops.
Applied Arti(cid:142) cial Intelligence, 3, 385–403.

Pearl, J., 1986, Fusion, propagation, and structuring in Belief Networks. Arti(cid:142) cial Intelligence,

29, 241–288.

Pearl, J., 1988, Probabilistic reasoning in intelligent systems: Networks of plausible inference

(San Mateo, California: Morgan Kaufmann).

Roux, M., and McKeown, D. M., 1994, Feature matching for building extraction from
multiple views. In IEEE Conference on Computer V ision and Pattern Recognition,
Seattle, WA (California: IEEE Press), pp. 46–53.

Russell, S., Binder, J., Koller, D., and Kanazawa, K., 1995, Local learning in probabilistic
networks with hidden variables. In Workshop on L earning in Bayesian networks and
other graphical models ( http://www.ai.mit.edu/people/jordan/workshop.html ).
Shannon, C. E., and Weaver, W., 1949, T he Mathematical T heory of Communication (Urbana:

Spann, M., and Grace, A. E., 1994, Adaptive segmentation of noisy and textured images.

University of Illinois Press).

Pattern Recognition, 27, 1717–1733.

Spiegelhalter, D. J., and Lauritzen, S. L., 1990, Sequential updating of conditional probabil-

ities on directed graphical structures. Networks, 20, 579–605.

Stassopoulou, A., Petrou, M., and Kittler, J., 1996, Bayesian and neural networks for
geographic information processing. Pattern Recognition L etters, 17, 1325–1330.
Stassopoulou, A., Petrou, M., and Kittler, J., 1998, Application of a Bayesian network in
a GIS based decision making system. International Journal of Geographical Information
Science, 12, 23–45.

Downloaded by [FU Berlin] at 23:33 12 May 2015 