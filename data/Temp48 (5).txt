This article was downloaded by: [George Mason University]
On: 18 October 2013, At: 10:08
Publisher: Taylor & Francis
Informa Ltd Registered in England and Wales Registered Number: 1072954 Registered
office: Mortimer House, 37-41 Mortimer Street, London W1T 3JH, UK

International Journal of Geographical
Information Science
Publication details, including instructions for authors and
subscription information:
http://www.tandfonline.com/loi/tgis20

Geopot: a Cloud-based geolocation data
service for mobile applications
DongWoo Lee a & Steve H.L. Liang a
a Department of Geomatics Engineering , University of Calgary ,
Calgary, Alberta, Canada
Published online: 05 Jul 2011.

To cite this article: DongWoo Lee & Steve H.L. Liang (2011) Geopot: a Cloud-based geolocation
data service for mobile applications, International Journal of Geographical Information Science,
25:8, 1283-1301, DOI: 10.1080/13658816.2011.558017

To link to this article:  http://dx.doi.org/10.1080/13658816.2011.558017

PLEASE SCROLL DOWN FOR ARTICLE

Taylor & Francis makes every effort to ensure the accuracy of all the information (the
“Content”) contained in the publications on our platform. However, Taylor & Francis,
our agents, and our licensors make no representations or warranties whatsoever as to
the accuracy, completeness, or suitability for any purpose of the Content. Any opinions
and views expressed in this publication are the opinions and views of the authors,
and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content
should not be relied upon and should be independently verified with primary sources
of information. Taylor and Francis shall not be liable for any losses, actions, claims,
proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or
howsoever caused arising directly or indirectly in connection with, in relation to or arising
out of the use of the Content.

This article may be used for research, teaching, and private study purposes. Any
substantial or systematic reproduction, redistribution, reselling, loan, sub-licensing,
systematic supply, or distribution in any form to anyone is expressly forbidden. Terms &
Conditions of access and use can be found at http://www.tandfonline.com/page/terms-
and-conditions

International Journal of Geographical Information Science
Vol. 25, No. 8, August 2011, 1283–1301

Geopot: a Cloud-based geolocation data service for mobile
applications
DongWoo Lee∗ and Steve H.L. Liang

Department of Geomatics Engineering, University of Calgary, Calgary Alberta, Canada

(Received 18 October 2010; ﬁnal version received 21 January 2011)

We propose a novel Cloud-based geolocation data service system, termed ‘Geopot’,
for location-based mobile applications. The exponentially growing number of users of
location-based mobile applications demand a data service that can easily be deployed
and is scalable against a large volume of accesses from mobile devices across the world.
The purpose of our work is to construct a scalable spatial data service that leverages the
powerful beneﬁts of a Cloud-based storage. We focus on highly scalable check-in and
nearby search service, which is a common focus of location-based mobile applications.
Our system comprises two parts: a local data service for indexing and storing geolo-
cation data of check-ins to achieve a compact spatial index database. This is based
on an in-memory R-tree and a local hash table, and it utilizes a Cloud storage that
enables global accesses. The local data service maintains a compact spatial index, with
tempo-spatial clustering, in which check-ins are grouped by their distance through a
time window. A centroid of a cluster is used as a spatial index of R-tree, and members
of the cluster are stored in the local networked hash table temporarily. This insures that
R-tree remains in a compact size that can ﬁt into the system memory. The data stored
in the local hash table will be published into the Cloud storage to allow users to access
it remotely with the same quality of service. Publishing the local data to a Cloud not
only insures staying within the speciﬁed storage size limits of the local data service,
but also promotes scalable access to the Cloud from mobile clients. Our contribution
lies in the design of a new scale-out data service architecture and in implementing this
for mobile applications. We encourage the building of a mobile application with our
proposed system as well as a low-cost Cloud data service for linking to a large-scale
spatial database.

Keywords: spatial data service;
computing

spatial clustering; mobile application; cloud

Introduction

1.
Mobile devices such as smartphones that have received much attention in recent years offer
a framework to develop mobile applications and economic beneﬁts. Processing power is
rapidly growing with the development of System-On-Chip technologies and most mobile
devices have now been equipped with a wireless network and a location service. Moreover,
recently software applications of mobile devices have gained similar functionality to that
of PCs. One of the most important and useful features attracting mobile device users is
the location service. Modern mobile devices use different kinds of location services such

*Corresponding author. Email: dwlee@ucalgary.ca

ISSN 1365-8816 print/ISSN 1362-3087 online
© 2011 Taylor & Francis
http://dx.doi.org/10.1080/13658816.2011.558017
http://www.tandfonline.com

Downloaded by [George Mason University] at 10:08 18 October 2013 1284

D. Lee and S.H.L. Liang

as GPS, Assisted-GPS, Cell-tower positioning, WiFi positioning and so forth. As each
location service has its own accuracy and error, typically more than one method is used to
compensate for others’ locational errors.

Largely due to the emergence of location capability in mobile devices, location-based
services have recently begun attracting widespread interest in ﬁelds such as social network-
ing services (Li and Chen 2010) and in local proximity information services. Furthermore,
the inclusion of geolocation information is generally accepted by various web services.
This enables user-centric information delivery according to users’ location information.
Even under the newly emerging W3C web standards, HTML5 has been introduced with
a Geolocation API (Popescu 2010) for web applications. Recent web browsers including
mobile web browsers support this capability. The volume of geolocation data generated by
mobile applications has been enormously growing.

Most location-based mobile applications use two basic actions: check-in and nearby
search. With the check-in action, a user can log his/her current geolocation and metadata
describing events to his/her service provider. Also, a user can ﬁnd information pertain-
ing to current locations using a nearby search. These actions are an inherent part of any
location-based mobile applications. For a system to cope with these kinds of large-scale
real-time geolocation data streams, it needs a scalable spatial data service. A conventional
relational database system is not suitable for this category of large-scale applications.
This is because some of its characteristics, including relational operations and schema-
based relational data model of the relational database, degrade scalability for use as part
of a large-scale service. To construct this type of a large-scale system, it requires scale-
out architecture in which computing resources are distributed across a network. These
days, to overcome the limitation of relational databases in a large-scale system environ-
ment, NoSQL (Leavitt 2010) database systems have been developed and used. These
are non-relational, schema free, distributed, easily replicated and horizontally scalable
database systems, for example, Google’s BigTable (Chang et al. 2008), Amazon’s Dynamo
(DeCandia et al. 2007), Facebook’s Cassandra (NoSQL 2010) and so forth. However,
fundamentally this entails key-value storage and does not have spatial support, including
neither spatial data type nor spatial query.

Accommodating the massive use of mobile applications for the spatial-centric pur-
poses outlined in this article demands a large-scale spatial data service. It is challenging
to build such a large-scale data service because this service needs a ﬂexible and expand-
able computing resource that responds appropriately to the volume of mobile users. Cloud
computing (Armbrust et al. 2010) meets the high demands of this type of service infras-
tructure because it is a virtualized computing resource in terms of both computation and
storage. Also, the proposed Cloud computing-based infrastructure can be easily provi-
sioned on demand with a service level agreement. We have collected over 265 million
geolocation data points drawn from a partial database downloaded from OpenStreetMap
(Haklay and Weber 2010). It is not a straightforward matter to store and manage this kind
of data set. Even a real-time geolocation data stream from a vast number of mobile users is
a challenge. A Cloud storage service provides a content delivery network-like data service
(Hofmann and Beaumont 2005) in which data access is routed to its nearest data access
edge. Cloud storage replicates the database to several remote sites around the world to
promote geographic locality.

We propose a Cloud-based geolocation data service, Geopot, for mobile applications.
The exponentially growing number of accesses to location-based services for mobile appli-
cations require a data service that can easily be deployed and is scalable to accommodate
a large volume of users of mobile devices from around the world. Also, the service must

Downloaded by [George Mason University] at 10:08 18 October 2013 International Journal of Geographical Information Science

1285

meet requirements in terms of scalability and availability. The purpose of our work is to
make a scalable spatial data service with the help of Cloud-based storage. We focus on
two basic actions of mobile applications: check-in a location and a nearby search, because
most location-based mobile applications rely on these two basic operations as the basis
of their services. With these two basic operations, more complex operations can be made.
Also, it has common patterns of a very high frequency of operations with small size of data
transmissions.

Our system has two parts: a local data service for indexing and storing geolocation
data of check-ins to make a compact spatial index database based on an in-memory R-
tree and a local networked in-memory hash table and Cloud-based data service for global
deployment/access. The local data service controls response to all mobile interactions,
which entails the processing of check-ins and responding to nearby searches. The local data
service maintains a compact spatial index with tempo spatial clustering in which check-ins
are grouped by Euclidian distance through a time window. A centroid of a cluster is used as
a spatial index of R-tree, and members of the cluster are temporarily stored in the local net-
worked hash table. This insures that the R-tree is of a compact size that ﬁts into the system’s
main memory as more check-ins are coming in. The data stored in the local networked hash
table are published into external Cloud storage for global accesses. By publishing local data
to the Cloud, it insures staying below a speciﬁed storage size for the local data service, but
it also promotes scalable access to the Cloud from mobile clients. We used Amazon Simple
Storage Service (S3) (Garﬁnkel 2007), which provides key–object pair storage and also can
be used as a content delivery network with CloudFront.

Our contribution lies in designing a new scale-out service architecture and in imple-
menting this for mobile applications. Without a large infrastructure investment in a
large-scale spatial database, our proposed system can manage a spatial database for
location-based mobile applications in which applications can scale storage capacity using
extendable Cloud storage and compact local spatial data services.

In this article, Section 2 describes related work. Section 3 introduces our scale-out
design for our spatial data service and new Local/Cloud hybrid architecture. Section 4
explains implementation of the Geopot server and data service API. Section 5 shows
performance comparisons with PostGIS. Finally, Section 6 presents our conclusions.

2. Related work
Most conventional spatial databases are based on relational database management sys-
tems. Most of these provide spatial data models and operations related to geometrics.
PostGIS by Ramsey (2005) is a representative spatial database back-ended by PostgreSQL
object relational database. To implement a large-scale database, a tightly coupled database
server clustering is required. There are few studies on large-scale spatial data services
for location-based mobile applications. Recently, Windows Azure Platform 2010, a Cloud
database service, has been extended to provide the support. This works like the spatial data
support in SQL Server 2008 in that it has the same spatial data types and spatial methods
and indices. This is just a virtualized database system and still has the limitations of rela-
tional databases for a large-scale user volume. Facebook (Saab 2010) introduces networked
key-value memory cache server for scaling its social network service. They use more than
800 servers supplying over 28 terabytes of memory for 500 million users’ information.

Wang et al. (2009) studied retrieving and indexing spatial data in a Cloud computing
environment, especially using Google AppEngine. They tried to solve the drawbacks of
spatial data storage using redesigned classic spatial indexing algorithms such as Quad-tree

Downloaded by [George Mason University] at 10:08 18 October 2013 1286

D. Lee and S.H.L. Liang

and R-tree. A spatial data object model is developed based on the Simple Feature Coding
Rules from the OGC, such as Well-Known Binary and Well-Known Text. This is differ-
ent from our work in terms of design policy. They tried to convert a non-Cloud spatial
database into the Cloud using Well-Known Binary and Well-Known Text. Our system,
Geopot, maintains spatial indices in the local data service and raw data of those spatial
indices are stored in the Cloud.

Wan et al. (2009) investigated a method for making a grid index to promote an inde-
pendent data storage partition, so range query is converted into or limited to local query.
Consequently a more effective multilayered grid index has been created for use with huge
amounts of data. This work compiles additional metadata on the spatial index to isolate its
spatial index from the large volume of raw data. Fundamentally, this is different from our
method that minimizes the size of the local spatial index to enhance query performance.

Ester et al. (1998) studied efﬁcient spatial clustering methods on large spatial
databases. The methods are mostly ofﬂine clustering algorithms, so they are not suitable to
online applications such as mobile applications.

In terms of key-value storage, there are few open-source systems in which spatial
indexing is supported: MongoDB and GeoCouch (NoSQL 2010). MongoDB provides
spatial support that is from GDAL (Geospatial Data Abstraction Library)/OGR Simple
Feature Library. GeoCouch is a spatial extension for CouchDB, which provides key-
document storage. It uses the external spatial back end of SpatiaLite, which supports
geometrics of Points, LineStrings and Polygons and provides bounding box search, poly-
gon search and radius search. The key-value storages have been integrated with external
modules of spatial database. This is not suitable to use external Cloud storages such as
Amazon S3. This is different from our system in aspect of decoupling spatial index with
raw data that are deployed into Cloud storage for the large volume of mobile users.

3. Cloud-based geospatial data service

3.1. Scale-Out design for mobile applications
A location-based mobile application usually requires two basic operations: a location
check-in and a nearby search. Most popular mobile location-based services, such as
Dodgeball, Brightkite, Loopt, Foursquare, Gowalla, Google Latitude (Li and Chen 2010)
and so on, consist of these basic operations. With a location check-in function in a
mobile application, current location information, including latitude, longitude, altitude and
precision/error range of the mobile device, could be transferred to the service provider by
input from the mobile user. Also, a nearby search is frequently requested by users to ﬁnd
out allocation-speciﬁc information pertaining to the user. These two operations involve a
very high frequency of requests with a small amount of data being transferred with each
interaction. To cope with these location-based mobile application operations, a data service
provided by a service provider could scale its performance and capacity in proportion to
the number of users.

Scalability is a desirable property of a system, a network or a process. Scalability mea-
sures a system’s ability to either handle growing amounts of work in an efﬁcient manner
or readily enlarge/expand in response to that increased demand. To solve the scalability of
a system, Cloud computing (Armbrust et al. 2010) has emerged as an elastic virtualized
computing resource. It comprises three basic segments: application, platform and infras-
tructure. Mostly IaaS (Infrastructure as a Service) is offered by Cloud providers. Without a
large amount of resource investment, hosted applications in a Cloud use shared resources

Downloaded by [George Mason University] at 10:08 18 October 2013 International Journal of Geographical Information Science

1287

Check-in requests
nerby search request

Mobile
devices

Local services

Nearby search
result

Mobile
devices

Return a link towards
raw data in the Cloud

Cloud storage

Figure 1. Scale-out design: local data service and Cloud data service.

on demand. The resources used are adjusted automatically according to the demand of a
service. As an infrastructure service, a Cloud provides computational resources and storage
resources such as Amazon Web Service’s EC2 (Elastic Compute Cloud) and S3 (Garﬁnkel
2007).

As shown in Figure 1, to scale out our system, Geopot, it is divided into two parts: local
data service and Cloud data service. To maintain the function of the overall data service
in the face of massive data accesses from mobile users, we forward the end-user raw data
access into an external Cloud storage, that is, the Amazon web service S3 Cloud storage.
We use this for external geolocation data storage instead of storing it locally. This alleviates
the burden of provisioning resources required for future use and maintains the size of a
local data service provider as small as possible. Local data service maintains a compact
spatial database containing centroids of clusters in which each cluster is a group of raw
data located in a range of search space. For nearby searches of mobile clients, it returns a
link towards raw data in the Cloud. With this link, mobile clients can directly access its raw
data in the Cloud. Detailed procedures are described in the following sections.

3.2. Hybrid architecture of Local/Cloud data service
The local data service plays a role in collecting check-in geolocation information from
mobile devices, making a spatial cluster of the raw data and indexing it into the spatial
index data structure, that is, R-tree and servicing nearby searches on the R-tree. A cluster
of raw data is represented in a spatial index of R-tree and has a SHA1 (Eastlake and Jones
2001) hash tag that is used as a unique identiﬁcation for future client access to the external
Cloud storage. The local data service pushes the raw geolocation data into the external
Cloud storage and labels it with the SHA1 hashed identiﬁer. Cloud storage is a key-value
storage, so a hashed identiﬁer is used as the key. Consequently this involves two steps to
retrieve actual raw data from mobile devices: the ﬁrst step is from the local data service
and the second step is from the Cloud storage. The current high-speed network between
local and the Cloud, with its high bandwidth and low latency, enables this.

The reasons why the data services are separated into two parts are as follows:

(1) To minimize the size of spatial index (R-tree) for a fast nearby search;
(2) To minimize the cost of building and maintaining geolocation data storage;
(3) To minimize the access cost to data through the geographically distributed Cloud

storage (closely located ones are used by mobile users).

Downloaded by [George Mason University] at 10:08 18 October 2013 1288

D. Lee and S.H.L. Liang

As mentioned earlier, collected geolocation data are grouped into clusters in which
each cluster comprises a set of tightly positioned data. This will be explained in detail in
Section 3.3. The centroids of clusters are used to construct the spatial index to provide
the nearby search. The R-tree does not have an access to all geolocation data through this
method, only centroids of clusters do. The actual raw data in a cluster resides in the Cloud,
thus speeding up a nearby search on small-sized spatial indices relatively. With the help
of expandable Cloud storage, we do not have to build large-scale storage beforehand. This
signiﬁcantly reduces the TCO (Total Cost of Ownership) of a mobile application. Also,
a Cloud storage service is based on multiple data centres around the world. As a result
mobile users can access it near the local edge of the Cloud. This is usually driven by DNS
(Domain Name Service) technologies with geographical locations of IP addresses.

3.3. Constructing geospatial index with temporal clustering
To maintain a lightweight spatial index database (R-tree) in the local data service for a
nearby search, we devise an online clustering method to represent a group of geolocation
data within a particular time window. As shown in Figure 2, a checked-in geolocation data
stream is a set of events that have occurred in a particular geolocation. Data on each event
consist of its geolocation and of metadata describing the event itself. Tightly spaced events
are grouped into a cluster. Data for a cluster consist of its centroid coordinates, a hash key
generated by its location and the radius encompassing its cluster. This information is added
into R-tree. It uses a modiﬁed version of the online Leader–Follower method (Duda et al.
2000) that alters only the cluster centre most similar to a new pattern and spontaneously
creates a new cluster if no current cluster is sufﬁciently close to the data. As mobile users
produce a series of geolocation data within a short period of time, online clustering is more
effective than a K-means (MacQueen 1967) clustering method, which needs a ﬁxed number
of clusters in advance.

Algorithm 1 describes the sequence of our clustering method. It gets an incoming
geolocation stream datum (loc). It ﬁnds a cluster encompassing the incoming datum. To
ﬁnd a cluster, it uses a larger search range (locrange) that ﬁnds the largest set of cluster can-
didates. If it ﬁnds a cluster that is located within the distance of θ , the datum is appended
into the found cluster. If not, a new cluster is created and the datum is appended into the
new one. For the threshold of θ , we use 1 km in this article as a minimum cluster diameter.
By different θ , we can control the number of clusters because it determines the diameter
of a cluster. When a new cluster is created, it makes a temporal cluster that includes a TTL

Events:
geolocation
data set

Cluster centroid
cluster hash key
cluster radius

Figure 2. Clustering closely located events and making a representative hash key and radius of a
cluster.

Downloaded by [George Mason University] at 10:08 18 October 2013 International Journal of Geographical Information Science

1289

Range search

Hash
table

Time window

Figure 3. The overall sequence of Geopot temporal clustering.

(Time-to-Live) value, because it is not possible to maintain all data in the local data ser-
vice. The new cluster’s centroid is added to R-tree and the appended raw datum is stored in
a local hash table. To minimize the size of local storage, any hash tables of expired clusters
are deleted. Upon deleting a local hash table of the expired cluster, the contents of a local
hash table are published into the external Cloud storage. The algorithm repeats the fetching
of a new incoming datum. Figure 3 shows the overall sequence of the described temporal
clustering. Through Algorithm 1, it creates a spatial cluster as a local spatial index and a
local hash table object containing raw data. This is ﬁnally transferred to the Cloud stor-
age in which the local hash table is stored as a key-value object, as shown in Figure 4.
A spatial index is reused after publishing its current local hash table to Cloud storage. It
needs to reset the time stamp of a cluster after its TTL expiration. This is performed by
resetTimeStamp in the event of a check-in if the local hash table is no value, null.

Algorithm 2 clears the expired clusters’ hash table, which are maintained in the local
data service temporarily. It is difﬁcult to retain all the raw data from each cluster, so it is
necessary to delete it after some period of time, that is, TTL. Before deleting a cluster’s
local hash table, the local hash table is moved to Cloud storage, after which it is deleted.
To indicate the availability of raw data that have already been moved to Cloud storage, the

R-tree

Cluster centroid

Local HashTable

Cloud hash table  key-value object

Figure 4. Local R-tree for centroids of clusters and Local/Cloud hash table of key-value objects
for raw data.

Downloaded by [George Mason University] at 10:08 18 October 2013 1290

D. Lee and S.H.L. Liang

cluster’s ccount is increased. By this count value, mobile clients recognize the availability of
these data. If the count is 3, for example, there are three hash tables, with two of the three
being in the Cloud and one being in the local data service. While a local hash table is being
published, the count value is appended into the object name of the Cloud. So, with this
count value, clients can access the recently published hash table object in the Cloud. With
this count value, clusters do not need to merge themselves. Clusters generated in time are
managed with the serial number of the count. As clusters repeat their life cycle, the local
data service has all clusters’ spatial indices and their links (hash tag of identiﬁcation) to
the actual raw data residing in Cloud storage.

Algorithm 3 shows a nearby search that is performed by a request from a mobile user.
At ﬁrst, it ﬁnds centroids (spatial indices) from the R-tree with a search range, R that is
usually several kilometers. To ﬁnd the largest set of centroids, it uses a larger search range.
Then, it merges all raw data from the local hash table if available or just returns its hash
tags for the external Cloud storage if there is no data in the local hash table. The search is
performed in the local data service, but most raw data fetching is performed by a mobile
device by addressing returned hash tags towards the Cloud storage.

3.4. Design considerations on mobile applications

A cluster is made in a timely manner, that is, a sliding time window. That is, each cluster has
its own TTL. A few hours TTL value is recommended for mobile applications in a dense
area. The radius of a cluster is not large, for example 1 km in this article. A cluster could
be overloaded if we do not use the TTL and the small size of radius for a cluster. A cluster’s

Algorithm 1: Temporal clustering

Input: An incoming geolocation datum: loc ∈ Queue
locrange ← [loclat ± R/2, loclng ± R/2]

(1) C = {c1, c2, ...cN } is a collection of clusters. Initial C ← {}
(2) H (key) is the hashing function of (SHA1).
(3)
(4)
(5) Cnb ← Rtree.intersect(locrange)
(6) Find ci of mini{(cid:4) loc − ci (cid:4)}
ci ∈ Cnb, 1 ≤ i ≤ N loc
N loc

= |Cnb|, Cnb ⊂ C
nb is the number of clusters nearby loc (Cnb) (in the search range).

nb , N loc
nb

(7) Decide a closest located cluster and Add it to R-tree:

1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:

if (cid:4) loc − ci (cid:4)< θ then

# ci is the activated cluster.
ctarget ← ci
if HashTable(H(ctarget)) = null then

resetTimeStamp(ctarget)

end if

else

# loc is a new cluster’s centroid.
cnew ← (loc, TTL, loctimestamp, hashtag = H(loc), count = 1)
C ← C ∪ cnew
Rtree.add(cnew)
ctarget ← cnew

end if

(8) HashTable(H(ctarget)) ← HashTable(H(ctarget)) ∪ {loc}
(9) Goto step.3

Downloaded by [George Mason University] at 10:08 18 October 2013 International Journal of Geographical Information Science

1291

TTL will be expired and then the local hash table constructed during the short period of
TTL will be transferred to the Cloud with its counter tag. With this counter tag in addition
to the cluster’s hashed tag(name), a client application can fetch a recently collected cluster
data from the Cloud. This leads to reduced information overload by a mobile application.
A mobile application can request past data with giving the lower counter value for the
cluster’s hashed tag as the object name of the Cloud.

A mobile client can get check-ins of a certain long period of time using a series of
requests of sequential counter values with the hashed tag(name) of a cluster. Actually,
the indices of the R-tree are not deleted in the memory after the expiration of the TTL.
The indices of clusters in R-tree will be stayed all the time, but their counter tag will be
increased after every TTL expirations to ﬂush its local hash table to the Cloud.

Algorithm 2: Expiring clusters and publish to cloud

(1) Clear a local hash table of an expired cluster:

1:
2:
3:
4:
5:
6:
7:
8:

curtime = WallClock()
for all c ∈ C do

if (ctimestamp + cTTL) < curtime then

publish HashTable(chash) to Cloud
HashTable(chash) ← null
ccount = ccount + 1

end if
end for

Algorithm 3: Nearby search

Input: R: Search Range, loclat,lng: Origin of search

(1)
(2) Specify Search Range: srange ← [loclat ± R/2, loclng ± R/2]
(3)
(4) Get all coordinates in the clusters cresult:

cresult ← Rtree.intersect(srange)

locall ← {}
for all c ∈ cresult do

if HashTable(chashtag) = null then

locset ← [chashtag, ccount]

locset ← [HashTable(chashtag), ccount]

end if
locall ← locall ∪ locset

else

1:
2:
3:
4:
5:
6:
7:
8:
9:
end for
return locall

(5)

4.

Implementation

4.1. Geopot server
Geopot server performs mainly two functions: a check-in and a nearby search for mobile
clients, as described in an earlier section. Geopot server’s front-end accepts incoming
check-in requests that are distributed evenly towards internal threads as shown in Figure 5.
The distribution for scaling local data service is achieved by the sharding method (Cryans
et al. 2008) of partitioning horizontally incoming requests to internal threads. We use

Downloaded by [George Mason University] at 10:08 18 October 2013 1292

D. Lee and S.H.L. Liang

Queqe

Threads
(clustering)

R-tree

Local networked
Hash table

THe cloud
storage

Check in
nearby search
requests

Sharing

S3

Local
network

The network

Figure 5. Geopot server implementation.

MD5 as a consistent hashing to make a shard number. Each internal thread has its own
shard number. Each thread accepts two pairs of information for check-in: geolocation
information (latitude and longitude) and the metadata of its application.

Each thread has its own R-tree for indices of centroids of clusters. By the temporal
clustering algorithm, each incoming check-in is indexed into each thread’s own R-tree and
the raw data are stored in the local networked hash table that is a key-value in-memory
storage. For this we use Redis (NoSQL 2010) open-source software, which is a high-
performance method for key-value storage. There are three advantages to use internal
sharding for local data service: the ﬁrst one is to remove internal locking for R-tree, the
second one is to maintain the size of a hash table entry at a moderate number and the third
one is to make the system easily extendable by adding more shards (thread and its R-tree
pairs).

The Geopot server is implemented using a multithreaded programming method. To
make the server highly concurrent, this requires a way of avoiding lock contentions occur-
ring in accessing the internal R-tree from multiple threads at the same time. To remove the
hotspot resulting from competing to acquire an exclusive lock for one R-tree, we deploy an
individual R-tree to each check-in thread. If we use only one R-tree, it degrades the perfor-
mance of accessing R-tree due to the high contention of locking. If we use one R-tree, each
cluster’s hash table size grows as it goes. Through a multiple R-tree of shards, this can be
lessened by partitioning R-tree. Each thread of each shard uses one process in a modern
operating system that is mapped to use one CPU core on the system. By increasing the
number of shards by as much as the number of CPU cores, we can exploit the system so as
to be fully loaded.

Likewise, each incoming request is distributed, whereby raw data are distributed into
a local hash table with a hash tag that is used as a shard number. This is also a horizontal
partition for networked local hash tables. We can easily add more hash table servers to
increase the capacity of the local storage. Upon expiring a cluster in the R-tree, the content
of its local hash table is transferred to the external Cloud storage. According to the local
networked hash table, the TTL can be adjusted. If there is not enough space in the local
hash table, a small TTL could be used to free up local storage. However, if we do have
enough spaces, a marginal TTL will be used to achieve a fast response time for clients who
do not need to visit Cloud storage with returned hash tags of a nearby search to get the raw

Downloaded by [George Mason University] at 10:08 18 October 2013 International Journal of Geographical Information Science

1293

Figure 6. Thrift interface deﬁnition.

data. To transfer the local hash table to Amazon S3, we use Amazon Web Service API that
is described in WSDL (Web Service Deﬁnition Language).

Any mobile client can connect to Geopot server with the HTTP-based RESTful API
that is described in Section 4.2. It supports Apache Thrift (Slee et al. 2007) network inter-
face for the system integration purpose that is an open-source network framework for
scalable cross-language service development developed by Facebook for a large-scale sys-
tem interoperability. Any other programming languages can make Thrift network bindings
to access Geopot server using the interface deﬁnition as described in Figure 6. As deﬁned
in the interface, it has three simple methods of check-in, nearby search and control its
server. Short description on the main source codes of Geopot is available in Appendix 2.

4.2. Data service API

As mentioned before, our data service has two essential operations for mobile applications:
a check-in and a nearby search. In addition to these two operations, Geopot’s front-end can
provide HTTP-based-based RESTful APIs to do extra operations for a mobile application
as follows:

(1) /login: user authentication
(2) /register: user registration/unregistration
(3) /checkin: check-in a place (current location and metadata)
(4) /nearby: request a nearby search within a search range
(5) /recent: request recent events of a user

4.3. Publishing the local hash table to Cloud storage

Each centroid in R-tree of the local data service has its TTL. If this has expired, the content
of the local hash table will be published into external Cloud storage. After publishing on a
cluster is ﬁnished, the content of the local hash table is deleted to free up space. Each index
of R-tree of the local data service has its own hash tag and its count value as presented in
Algorithm 1. This is used as an object name for external Cloud storage. Geopot uses SHA1
(160 bits) to generate the hash tag using the coordinates of the centroid. As external Cloud
storage, AWS S3 has two-level addressing: ﬁrst, to access an object in S3, it needs a bucket
name as a data domain; second, it needs an object name in a bucket. Objects are managed
in a directory-like hierarchical manner.

Before publishing a hash table to the Cloud, the local data service just serves an incom-
ing nearby search request with its local hash table. After publishing, it just returns hash

Downloaded by [George Mason University] at 10:08 18 October 2013 1294

D. Lee and S.H.L. Liang

tags to its clients. Clients directly access Cloud storage with the hash tags. Geopot uses the
hash tag as an object name consistently.

Table 1 presents examples of publishing a local hash table to the Cloud. In this case,
each local hash table will be replicated into two regions (Asia and the United States) by
the Cloud itself and it will be stored as a JSON (Javascript Object Notation) (Crockford
2010) format as shown in Figure 7. AWS S3 Cloud service has three different regional data
centres: The United States, Europe and Asia. Because JSON format is widely used as a
common data exchange format over HTTP for web applications, it can be accessible from
mobile devices directly without any restrictions. AWS S3 provides HTTP for accessing its
objects. It is the same protocol for a general web server. So, it is important to use a HTTP-
friendly data format such as JSON. Network bandwidth to two regions from the University
of Calgary, Canada, where our test clients are located are almost the same as shown in the
table. Mobile clients residing in each region can access the local edge of AWS S3 Cloud
service. This minimizes network latency due to local access.

A published cluster such as a AWS S3 object in the Cloud contains check-ins happened
in the cluster and metadata of its mobile application. It has geolocations of check-ins in a
cluster, their distance to the centroid of a cluster and timestamps. Any web clients can
access the object with HTTP/HTTPS. The Javascript-based client can fetch the JSON
object directly with Ajax (Asynchronous JavaScript and XML) and can show the location
data with a map service such as Google Map.

Table 1. Examples of publishing local HashTable to the Cloud, AWS S3 (Asia and the United States
region).

AWS S3 object name (SHA1 hashed+Count value,
JSON format)

6a86b29fb158a429c709e178cad891cfc9c9e2f3.json
86bbbca7a3419b6484f8cae1390fc3c2b6ff7a6d.json
10586e63ce335ad91934b2447eef9abb5e63d8b1_2.json
6cb2976498e7ed1e877e32812bb4f77cd6938803.json
f9f611da3e861636420718fe3e27944f79187c63.json

Time,
sec
(Asia)

0.15
0.18
0.37
0.68
0.63

Time,
sec the
United
States

0.18
0.17
0.61
0.61
0.67

Size,
Bytes

53832
75191
245355
479302
482405

Number
of raw
data

568
210
1800
2465
31

Figure 7. An example of a published JSON object of a local hash table.

Downloaded by [George Mason University] at 10:08 18 October 2013 International Journal of Geographical Information Science

1295

5. Performance evaluations

We evaluate our data service system using GPS traces (265,504,306 location data in total)
that is crawled from OpenStreetMap, an open GPS trace project. In our local service, we
have one check-in/nearby server and two memory-based hash table servers (one is located
with check-in server and the other resides on the same local network). For Cloud storage,
we use Amazon Web Service S3 as a key-value Cloud storage service that is accessi-
ble through HTTP/HTTPS by mobile users. To evaluate the scalability of Geopot, we
compare the response time with PostGIS only for two basic operations: a check-in and a
nearby search. PostGIS is a full-ﬂedged spatial database plug-in for PostgresSQL relational
database. The purpose of this evaluation is not functional comparison, but the performance
of check-in/nearby performance of Geopot. For each test, we sampled one million check-
ins (see short description on the sampled data set in Appendix 1) from 265 million GPS
trace points.

5.1. Space efﬁciency

Local data service maintains a spatial index of clusters and partial temporal hash tables to
reduce the loaded size of spatial information. This enables a faster nearby search than an
all-in-one centralized spatial database. Figure 8 shows (a) the percentage of the number of
clusters to the number of raw location data of check-ins and (b) a histogram of the size
of clusters. In this case, the total number of check-ins is one million. For these raw data,
the local data service just makes 11,484 clusters (1.14%). This means it only needs 11,484
spatial indices in its spatial database for one million raw data. As shown in Figure 8(b),
most of the clusters have less than 400 raw data. The range from 0 to 50 is the highest
frequency of cluster size. The histogram shows the frequency of each cluster size. It has
the size of clusters more than 50 through 1000 for the one million samples of 265 million
data. The radius of clusters for our experiments is 1 km. We think this reﬂects all kinds of
common coarse and dense areas. In a dense area, the size of cluster during a time period
(TTL) could be growing in the number of 1000 similar to this result. Further studies are
needed on the relationship between the size of clusters and the number of check-ins with
the TTL in more detail.

)

%

(
 
a
t
a
d
 
w
a
r
#
 
o
t
 
s
r
e
t
s
u
c
#
 
f
o
 
e
g
a
t
n
e
c
r
e
P

l

2
.
1

0
.
1

8
.
0

6
.
0

4
.
0

2
.
0

0
.
0

0
0
0
8

0
0
0
6

0
0
0
4

0
0
0
2

0

y
c
n
e
u
q
e
r
F

0

200,000

400,000

600,000

800,000 1,000,000

0

200

400

600

800

1000

The number of raw data

(a)

Size of cluster
(b)

Figure 8.
the size of cluster.

(a) Percentage of the number of clusters to the number of raw data and (b) histogram of

Downloaded by [George Mason University] at 10:08 18 October 2013 1296

D. Lee and S.H.L. Liang

5.2. Response time

The following two evaluations were performed in the same local network of the Geopot
service because response time is affected by network conditions on the client side. Thus,
it is more accurate to test its processing performance on the same network. Moreover, to
measure the response time of requests on each system in a fully loaded condition, test
clients performed bulk requests to each system.

5.2.1. PostGIS versus Geopot: insertion (check-in)
Figure 9 shows the elapsed time of inserting location data into each system. The insertion
benchmark program is a single-threaded standalone program to check each system’s spatial
index complexity. As you can see, the response time of insertion, until the number of 1000
entries, is almost the same. However, after that, Geopot outperforms the PostGIS. From the
one million check-ins test, Geopot is 3.82 times faster than PostGIS. PostGIS and Geopot
took 1345 sec and 351 sec, respectively. The Geopot server has a check-in buffered queue
to increase the response time of check-in (inserting location data). Clustering (Indexing) is
performed by several background threads of the Geopot local server. In this case, we use
the number of threads that is the same as the number of CPUs in the testing server. This is
automatically conﬁgured by Geopot.

After the 104 insertions, the response time has been increased. This is due to the
increased index complexity and the increased index space. This leads to a longer delay
time to get the actual data from disk or cache memory of the system. Because geoloca-
tions of series of insertions are not geographically correlated, it is hard to get a chance to
get higher cache hit ratio with a limited working memory. It is even getting worse disk
I/O performance with a larger number of spatial indices than small. Due to the frequent
replacements of pages of virtual memory between main memory and the disk storage of
the system, it affects the response time to access spatial indices. Geopot has less spatial
indices in its working memory; it gets faster response time than PostGIS.

PostGIS database constructs its spatial index for check-ins. It stores the spatial indices
and check-in entries together in the same database. But, Geopot separates the raw data

PostGIS
GeoPot

)
s
d
n
o
c
e
s
(
 
e
m

i
t
 
d
e
s
p
a
E

l

0
0
2
1

0
0
0
1

0
0
8

0
0
6

0
0
4

0
0
2

0

101

102

103

104

105

106

The number of insertions

Figure 9.

Insertion time: PostGIS versus Geopot.

Downloaded by [George Mason University] at 10:08 18 October 2013 International Journal of Geographical Information Science

1297

(check-ins) from the spatial indices. The check-ins are in the Cloud. The spatial indices
(actually the centroids of clusters) are in the local data service. When we push the exper-
imental raw data set to both systems for benchmarking, both systems do the similar
processing of indexing data and storing them in each system, but Geopot reduces the size
of spatial indices with the centroids of clusters and the capacity for the raw check-ins
with the movement to the Cloud. In addition to the reduced size of indices, the difference
between PostGIS and Geopot is the scalability. PostGIS stores all data in the local storage,
but Geopot does not.

5.2.2. PostGIS versus Geopot: selection (nearby search)

We measured the performance of the nearby search of each system with different num-
ber of clients to see the scalability. Figure 10 shows the performance of PostGIS’s
query for ﬁnding points in proximity to a point location that is one of 1000 randomly
selected data points from our test data. The simple query is an SQL expression simi-
lar to SELECT ∗ FROM checkins WHERE ST_DWithin (the_geom, GeometryFromText
(‘POINT(−128.000 45.000)’, 4326), 0.01). This means ‘ﬁnd all points within 1 km of a
location coordinate’, in this case (−128.000, 45.000). Figure 10(a) shows the box-and-
whisker statistics on the elapsed time of PostGIS’s nearby search with different number
of clients. The plot gives ﬁve-number summaries of the elapsed time measurements: min-
imum observation, lower quartile, median, upper quartile and maximum observation. As
shown in Figure 10(a), we can ﬁgure out that the average completion time of each query
is doubled by as much as the number of additional clients except between the one thread
and four threads. Each number of clients’ average elapsed times are 0.48 sec, 0.58 sec,
0.94 sec, 1.84 sec, 3.5 sec, and 6.8 sec, respectively. In Figure 10(b), the measurements
of 64 threads are presented as an example to see the actual behaviour of threads. In this
case, we used 64 threads (clients) to stress the system with 1000 queries at the same
time. The straight line of each graph indicates its linear regression. Its completion time
has decreased around 600 or so. This is due to the different running time of each thread.
Each thread running simultaneously has evenly distributed number of queries initially. As
shown in Figure 10(b), however, they are not ﬁnished at the same time, so they have
different elapsed time of each query. Each thread’s ending time varies according to the
number of those results returned by a query, that is, the number of raw data of a cluster.

)
s
d
n
o
c
e
s
(
 
e
m

i
t
 
d
e
s
p
a
E

l

0
1

8

6

4

2

0

)
s
d
n
o
c
e
s
(
 
e
m

i
t
 
d
e
s
p
a
E

l

2
1

0
1

8

6

4

2

0

1

4

8

16

32

64

0

200

400

600

800

1000

The number of clients (threads)
(a)

The number of queries (64 threads)
(b)

Figure 10.

(a) PostGIS nearby search time; (b) PostGIS nearby search time with 64 clients.

Downloaded by [George Mason University] at 10:08 18 October 2013 1298

D. Lee and S.H.L. Liang

Each thread serves its query on the basis of the ﬁrst-come-ﬁrst-serve for 1000 queries
for the experiment. After some time (around 600 items of the graph), the number of
idling threads is increased. It leads to improve the performance of I/O of the system. As
much as the number of threads has decreased, the contention and overhead of PostGIS
I/O could be decreased, so it shows decreasing completion time after the number of
600 or so.

The clients are software threads to simulate accesses to each systems. To give a more
stress to each system, we locate them in the local network. Mostly, mobile devices have
smaller network bandwidth rather than a wired local network. The number of clients used
in this experiments can be considered as larger than actual mobile users in terms of network
bandwidth and latency.

Figure 11 shows the performance of Geopot’s nearby search for a local service. This is
mainly performed by local spatial service, that is, R-tree. For a nearby search request, the
local server responds with the coordinates of centroids and their hash tags for addressing
the external S3 object. So, it is much faster than PostGIS, which returns all raw data to
clients. In most test cases, Geopot’s nearby search is completed in under 0.15 sec. As shown
in Figure 11(a), we can ﬁgure out that the average elapsed time of each query is very stable
across the different number of clients until 32 clients. After 32 clients, it increases a little.
Each number of clients’ average elapsed time is 0.61 msec, 0.65 msec, 0.79 msec, 1.1
msec, 4.3 msec, and 1.56 msec, respectively. Figure 11(b) shows the actual measurements
of the test with 64 clients. Across the overall nearby search, it has more stable performance
than those of PostGIS.

The purpose of this section is to show the scalability of PostGIS and Geopot against
a large number of users. From Figures 10 and 11, we can ﬁgure out the different comple-
tion time of each system with the different number of accesses from clients. In case of
PostGIS, it is a centralized system, so it has a limitation to serve requests as the num-
ber of accesses is growing. But Geopot can be running with less resources than what
PostGIS needs, because Geopot uses the Cloud storage for serving global data service
for a large number of users with its virtualized distributed system. This is the same as the
comparison between a single system and a distributed system. For PostGIS in this exper-
iment, it needs more time to complete its search as the number of accesses is increased.
But, Geopot shows stable performance for even more accesses. This means Geopot can
make more stable and more available service for an unpredictable number of mobile
users. Because the Cloud storage has geographically distributed storage resources across

)
s
d
n
o
c
e
s
(
 
e
m

i
t
 
d
e
s
p
a
E

l

5
1
.
0

0
1
.
0

5
0
.
0

0
0
.
0

)
s
d
n
o
c
e
s
(
 
e
m

i
t
 
d
e
s
p
a
E

l

0
2
.
0

5
1
.
0

0
1
.
0

5
0
.
0

0
0
.
0

1

4

8

16

32

64

0

200

400

600

800

1000

The number of clients (threads)
(a)

The number of nearby search requests (64 threads)

(b)

Figure 11.

(a) Geopot nearby search time; (b) Geopot nearby search time with 64 clients.

Downloaded by [George Mason University] at 10:08 18 October 2013 International Journal of Geographical Information Science

1299

the world, the workloads from unpredictable number of users can be well distributed
to their resources. This also leads to avoid a single point of failure or overload of the
service.

5.3. Network performance
We evaluated the network performance of Amazon S3 to clients that are connected with
a fast ethernet (100 Mbps) of the campus network in the University of Calgary, Canada.
Seven hundred randomly selected samples residing in Amazon S3 storage were down-
loaded using different numbers of clients. A client is a Geopot client, which simulates
mobile users. We measured the network transfer time of two regions (Asia and the United
States). Both have almost the same statistics. So, we show only US results here as Figure 12.
This ﬁgure presents a box-and-whisker plot of simultaneous download time. From this we
can determine that most downloads have similar statistics irrespective of the number of
simultaneous downloads. The box-and-whisker plot shows ﬁve-number summaries of the
network transfer measurements: minimum observation, lower quartile, median, upper quar-
tile and maximum observation. It also shows outliers with small circles. The outliers above
0.5 of the chart comes from various reasons: the initial connection time, the network condi-
tion of a local/global, the condition of overloaded system and so forth. These outliers take
very small portions of all observations. The transfer time is not affected by the number of
simultaneous clients in the Cloud. The number of clients indicates the number of simulta-
neous accesses to S3. At maximum, we used 256 simultaneous clients. The overall average
of download times across all clients from the S3 Asia region and the United States region
is 0.186 sec and 0.198 sec, respectively. The size of the downloaded JSON object is varied.
We did not take into account the size of each download because the average download time
is very stable and the number of elements in each download is less than 400, as shown in
Figure 8b. The Cloud tries to improve the availability of objects residing in it by replicating

)
s
d
n
o
c
e
s
(
 

e
m

i
t
 
r
e

f
s
n
a
r
T

0
.
2

5
1

.

0

.

1

5

.

0

0
.
0

1

2

4

16
The number of simultaneous accesses

32

64

8

128

256

Figure 12. Statistics on download transfer time from S3 (the United States region).

Downloaded by [George Mason University] at 10:08 18 October 2013 1300

D. Lee and S.H.L. Liang

objects to other regions. So, it maintains the quality of service by more large number of
replicas. From the experiments, it shows the similar network performance.

6. Conclusions
This article proposes and implements a novel Cloud-based geolocation data service,
Geopot, for location-based mobile applications. This is a data service that could easily
be deployed and is scalable against a large volume of accesses from mobile devices. The
purpose of our work is to create a scalable spatial data service that leverages the beneﬁts
of Cloud-based storage. We focus on highly scalable check-in and nearby search services,
which are a common characteristic of mobile applications. Our system has two parts: a
local data service for indexing and storing geolocation data of check-ins to achieve a com-
pact spatial index database based on an in-memory R-tree and a local hash table and a
second part that draws on Cloud storage as a global access deployment.

By constructing a clustering-based spatial index, it enables more space-efﬁcient
database for a large volume of geolocation data. This leads to get a faster spatial search
for mobile applications. Through our experiments, with one million geolocation data, it
needs 11,484 spatial indices (clusters), just 1.14% of all the data. Also, because it needs
a smaller footprint of working memory for spatial indices, Geopot’s insertion time of new
data is enhanced compared with a conventional spatial database, PostGIS. This is more
effective with a large-scale data. We get almost 3.82 times faster in one million insertions
test. Moreover, Geopot’s approach is to provide a scalable service that is not limited by the
storage capacity of a local data service. In case of nearby search, Geopot shows very stable
elapsed time for searches with regardlessness of the number of accesses. PostGIS shows
that the increasing elapsed time as the number of simultaneous accesses is growing. The
Cloud storage service is highly usable because its network transfer time is stable with load
balancing across geographically distributed storage resources regardless of the number of
simultaneous clients in our study.

Results of the experiments show that our novel Cloud-based architecture provides a
low-cost, large-scale spatial data service for mobile applications. Mobile Applications
having check-ins/nearby searches can be implemented using our proposed Local/Cloud
hybrid services, without a large-scale infrastructure investment for a scalable data service
at the outset.

References
Armbrust, M., et al. 2010. A view of cloud computing. Communications of the ACM, 53 (4), 50–58.
Chang, F., et al. 2008. Bigtable: a distributed storage system for structured data. ACM Transactions

Crockford, D., 2010. Javascript Object Notation (JSON) [online], Available from: http://www.

on Computer System, 26(2), 1–16.

json.org [Accessed 13 December 2010].

Cryans, J., et al. 2008. Criteria to compare cloud computing with current database technology.
Software process and product measurement, Lecture notes in computer science, Springer: Berlin,
5338, 114–126.

DeCandia, G., et al. 2007. Dynamo: Amazon’s highly available key-value store, In: Proceedings of

twenty-ﬁrst ACM SIGOPS, 14–17 October. Stevenson, WA, ACM, 41 (6). 205–220.

Duda, R.O., et al., 2000. Pattern classiﬁcation. 2nd ed New York: Wiley Interscience.
Eastlake, D.E. and Jones, P.E., 2001. US Secure Hash Algorithm 1 (SHA1) [online]. Available from:

http://www.faqs.org/rfcs/rfc3174.html [Accessed 2 September 2010].

Ester, M., et al. 1998. Clustering for mining in large spatial databases. Special issue on data mining,

KI-Journal, ScienTec Publishing, 1, 18–24.

Downloaded by [George Mason University] at 10:08 18 October 2013 International Journal of Geographical Information Science

1301

Garﬁnkel, S., 2007. An evaluation of Amazons grid computing services: EC2, S3 and SQS. Harvard

University, Technical Report TR-08-07, Harvard University.

Guttman, A. 1984. R-trees: a dynamic index structure for spatial searching. In: Proceedings of the
1984 ACM SIGMOD international conference on Management of data, 18–21 June Boston, MA,
14 (2). 47–57.

Haklay, M. and Weber P., 2008. OpenStreetMap: user-generated street maps. IEEE Pervasive

Hofmann, M. and Beaumont L. R., 2005. Content networking: architecture, protocols, and practice.

Computing, 7 (4), 12–18.

Morgan Kaufmann Publisher.

Leavitt, N., 2010. Will NoSQL databases live up to their promise? Computer, 43 (2), 12–14.
Li, N. and Chen, G., 2010. Sharing location in online social networks. IEEE Network, 24 (5), 20–25.
MacQueen, J.B., 1967. Some Methods for classiﬁcation and Analysis of Multivariate Observations.
In: Proceedings of 5th Berkeley Symposium on Mathematical Statistics and Probability,
Berkeley, CA: University of California Press, 281–297.

NoSQL, 2010. [online]. Available from: http://nosql-database.org [Accessed 4 January 2011].
Popescu, A., 2010. W3C HTML5 geolocation API [online]. Available from: http://dev.w3.org/geo

[Accessed 4 September 2010].

Ramsey, P., 2005. Introduction to PostGIS. Refractions Research Inc, Technical report.
Saab, P., 2010. Scaling memcached at Facebook [online]. Available from: http://www.facebook.

com/note.php?note_id=39391378919 [Accessed 4 September 2010].

Slee, M., Agarwal, A., and Kwiatkowski, M., (2007). Thrift: Scalable Cross-Language Services

Implementation. Facebook, Technical Report.

Wan, B., Xu, S., and Yang, L., 2009. Combination of partition table and grid index in large-scale
spatial database query. In: Proceedings of the 2009 First IEEE International Conference on
Information Science and Engineering, Nanjing, China, 2007–2011.

Wang, Y., et al. 2009. Retrieving and indexing spatial data in the cloud computing environment.

Cloud computing, Lecture notes in computer science, Springer: Berlin, 5931, 322–331.

Windows Azure Platform 2010. [online]. Available from: http://www.microsoft.com/windowsazure

[Accessed 4 September 2010].

Appendix 1. The sampled test data set

One million geolocation data are used for the experiments in this article. The size of the
data ﬁle is approximately 30 MB. The one million geolocation data were sampled from a
OpenStreeMap database. The data ﬁle consists of two ﬁelds: the ﬁrst ﬁeld is the sequence
number of each row and the second ﬁeld is a geolocation coordinate (latitude, longitude).
The one million sampled data (geopot_data.zip) are available with the online version of
this article as supplementary materials.

Appendix 2. The main source codes of Geopot

The main source codes of our system are coded by Python programming language. The
main source codes (15 ﬁles, geopot_source_codes.zip) are available with the online ver-
sion of this article as supplementary materials. It consists of the client/server modules of
Geopot. ‘ /gen-py’ has wrapper implementations for client/server communication using
Thrift. ‘ /geopot’ has the server’s core implementation that is explained in this article. The
server uses a multithreaded worker model to process incoming check-ins and nearby search
requests. Each server thread has its own R-tree and because the content of incoming check-
ins is delivered into a local Redis hash table, it requires Redis servers as a local hash table.
A Python runtime system (above 2.5 version) and extra geolocation-related packages are
required.

Downloaded by [George Mason University] at 10:08 18 October 2013 