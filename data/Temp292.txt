This article was downloaded by: [Florida International University]
On: 30 December 2014, At: 11:53
Publisher: Taylor & Francis
Informa Ltd Registered in England and Wales Registered Number:
1072954 Registered office: Mortimer House, 37-41 Mortimer Street,
London W1T 3JH, UK

International Journal of
Geographical Information
Science
Publication details, including instructions for
authors and subscription information:
http://www.tandfonline.com/loi/tgis20

Quantifying positional
error induced by line
simplification
Howard Veregin
Published online: 06 Aug 2010.

To cite this article: Howard Veregin (2000) Quantifying positional error induced
by line simplification, International Journal of Geographical Information Science,
14:2, 113-130, DOI: 10.1080/136588100240877

To link to this article:  http://dx.doi.org/10.1080/136588100240877

PLEASE SCROLL DOWN FOR ARTICLE

Taylor & Francis makes every effort to ensure the accuracy of all
the information (the “Content”) contained in the publications on our
platform. However, Taylor & Francis, our agents, and our licensors
make no representations or warranties whatsoever as to the accuracy,
completeness, or suitability for any purpose of the Content. Any
opinions and views expressed in this publication are the opinions and
views of the authors, and are not the views of or endorsed by Taylor
& Francis. The accuracy of the Content should not be relied upon and
should be independently verified with primary sources of information.
Taylor and Francis shall not be liable for any losses, actions, claims,
proceedings, demands, costs, expenses, damages, and other liabilities
whatsoever or howsoever caused arising directly or indirectly in
connection with, in relation to or arising out of the use of the Content.

This article may be used for research, teaching, and private study
purposes. Any substantial or systematic reproduction, redistribution,

reselling, loan, sub-licensing, systematic supply, or distribution in any
form to anyone is expressly forbidden. Terms & Conditions of access
and use can be found at http://www.tandfonline.com/page/terms-and-
conditions

Downloaded by [Florida International University] at 11:53 30 December 2014 int. j. geographical information science, 2000, vol. 14, no. 2, 113± 130

Research Article

Quantifying positional error induced by line simpli(cid:142) cation

HOWARD VEREGIN
Associate Professor, Department of Geography, University of Minnesota,
267-19th Ave S, Rm 414, Minneapolis, MN 55455, USA
email: veregin@atlas.socsci.umn.edu

(Received 7 August 1998; accepted 25 February 1999)

Abstract. This study examines the e(cid:128) ects of line simpli(cid:142) cation on the positional
accuracy of linear features. The goal is to quantify the relation between the level
of simpli(cid:142) cation and the degree of positional error, so that users can choose
appropriate levels of simpli(cid:142) cation that will yield results meeting speci(cid:142) c accuracy
criteria. The study focuses on the Douglas-Peucker line simpli(cid:142) cation algorithm
and examines both natural and anthropogenic features (streams and roads) derived
from United States Geological Survey Digital Line Graphs. Results show that
error can be modelled at an aggregate level using cumulative frequency curves
and their con(cid:142) dence limits. This makes it possible to identify the level of simpli-
(cid:142) cation that eliminates the largest number of vertices while still attaining a speci(cid:142) c
positional accuracy standard. A simple implementation strategy is described in
which an optimal level of simpli(cid:142) cation is identi(cid:142) ed and simpli(cid:142) cation is applied
selectively for di(cid:128) erent lines. The study shows that management of simpli(cid:142) cation-
induced error is possible using simple tools well within the reach of GIS users.

1.

Introduction
Many users of GIS do not realize the e(cid:128) ects that cartographic generalisation can
have on data quality. Generalisation encompasses a variety of procedures in which
source data are transformed to maintain cartographic (cid:142) delity during scale reduction,
enhance computational e(cid:129) ciency, reduce data storage requirements, or merge data-
bases with di(cid:128) erent resolution levels. These procedures modify the spatial and attrib-
ute components of a database to combine, simplify and reorganize features and their
relations. From a cartographic standpoint these modi(cid:142) cations are not in the least
problematic; indeed one might say that they are the sine qua non of generalisation.
From a data model standpoint, however, these modi(cid:142) cations represent types of
distortion that may introduce unwanted error, cause a database to fail a quality
compliance standard, or induce errors in cartometric estimates of length or area.

Researchers have studied spatial data quality and cartographic generalisation in
isolation, but for the most part the interaction between these two domains has
received little attention. The notion that generalisation causes a deterioration in data
quality is not a new idea but there has been only a smattering of work in which the
errors introduced by generalisation are assessed quantitatively . Most of the research
concerning generalisation ‘quality’ has focused on visual or aesthetic quality rather
than data quality. Perhaps this is because generalisation is often viewed as a

Internationa l Journal of Geographica l Information Science
ISSN 1365-881 6 print/ISSN 1362-308 7 online © 2000 Taylor & Francis Ltd
http://www.tandf.co.uk/journals/tf/13658816.html

Downloaded by [Florida International University] at 11:53 30 December 2014 114

H. Veregin

cartographic process for which the objectives are tied to graphical representation
(Little 1989 ). This view is limited because generalisation methods can permanently
alter the contents of a database and may be applied for non-cartographi c purposes
(JoaÄ o 1995, Weibel 1995 ).

This study examines the relation between generalisation and data quality for line
simpli® cation (also known as line weeding or thinning). In this context, a `line’ means
a `cartographic line’, or a one-dimensional object bounded on either end by nodes
and containing a variable number of intermediate shape points or vertices. (This
de® nition is consistent with the terms `arc’ and `chain’.) Line simpli® cation involves
the selective elimination of vertices along a line. No new vertices are created. The
simpli® ed line has fewer vertices, and hence a simpler geometry, than the original
line. The discrepancy between the geometric positions of the original and simpli® ed
lines is referred to here as `positional distortion’ or `positional error’.

Line simpli® cation is only one component of line generalisation, which also
encompasses operations such as smoothing, merging and displacement (McMaster
1987b). However, simpli® cation is arguably the most commonly-applied line gen-
eralisation operation, being widely available in commercial GIS software packages.
For example, it is the only generalisation operation explicitly identi® ed as such in
Arc/InfoÒ
. (Of course, other types of generalisation are possible using this software
package; see Veregin and McMaster 1997 ). Other systems, such as IdrisiÒ
, also o(cid:128) er
various types of line simpli® cation procedures. While providing the capability to
perform line simpli® cation, such software packages do not provide any means of
assessing its impacts on positional accuracy, and do not address the issue in accom-
panying documentation. Therefore it seems likely that many users employ line
simpli® cation without a clear sense of the errors that they might inadvertently be
introducing.

There are many line simpli® cation algorithms but the best-known and probably
most widely-used is the Douglas-Peucker algorithm (Douglas and Peucker 1973 ).
This algorithm is based on the elimination of vertices that fall within a speci® ed
distance, or bandwidth, of a linear trend-line joining two selected vertices on the
cartographic line. The amount of simpli® cation performed is a function of the
bandwidth: The greater the distance within which vertex elimination can occur,
the greater the degree of simpli® cation. In Arc/InfoÒ
, the user is required to enter a
bandwidth value that determines the amount of simpli® cation that will occur. In
other implementations (e.g. SAS/GraphÒ
) the system assigns a priority value to each
vertex based on the output of the algorithm, and the user can then select a subset
of vertices based on the values of this attribute.

One of the reasons for the widespread use of the Douglas-Peucker algorithm is
that it has proven to be superior to other algorithms both cartographicall y and
quantitatively. Research shows that the algorithm retains essentially the same subset
of vertices as would be selected by visual inspection, which suggests that it produces
results similar to manual generalisation (White 1985, Butten® eld 1991 ). Quantitative
analysis shows that the algorithm outperforms other algorithms in terms of the
amount of error introduced, especially at high levels of generalisation (McMaster
1987a, b, Jenks 1989 ).

Still, there is typically no way for users to apply the Douglas-Peucker algorithm
in a way that minimizes the impacts of error or allows a speci® c accuracy standard
to be attained. What is missing is a way for users to select a bandwidth value that
maximizes the number of vertices eliminated subject to the constraint that the set of

Downloaded by [Florida International University] at 11:53 30 December 2014 Quantifying simpli® cation-induced error

115

simpli® ed lines maintains a certain level of positional accuracy. Such a goal can only
be achieved by trial and error. A better implementation would allow users to specify
the acceptable maximum level of error and harness the power of the system to
determine the bandwidth required to achieve this goal.

In the absence of such an implementation, one requires an understanding of the
quantitative relation between bandwidth and positional error. This paper seeks to
provide such understanding, and in so doing to span a critical gap between theory
and practice by providing users with information that may be useful
in an
applications context. The speci® c objectives of this study are:

E To examine the e(cid:128) ects of line simpli® cation (speci® cally the Douglas-Peucker

algorithm) on the positional accuracy of linear features;

E To quantify the relation between the level of simpli® cation and the degree of

positional error.

E To identify factors that a(cid:128) ect this relation, including feature type (natural
error

versus. anthropogenic ) and geometric properties
potential, etc.).

(shape,

length,

The ultimate goals of the research are:

E To develop an objective method that can be used to select an appropriate level
of simpli® cation (bandwidth), in order to facilitate compliance with accuracy
standards.

E To develop techniques to predict the e(cid:128) ects of simpli® cation, including the
identi® cation of sensitivity or potential error, to prevent the introduction of
excessive levels of error and permit generalisation to be applied selectively.

The results presented here are speci® c to the Douglas-Peucker algorithm; the
methods used might be appropriate for other line simpli® cation algorithms, but the
speci® c quantitative results are not necessarily applicable. This degree of speci® city
is inevitable given that di(cid:128) erent line simpli® cation algorithms operate in di(cid:128) erent
ways and may have quite di(cid:128) erent e(cid:128) ects on positional error. The focus on the
Douglas-Peucker algorithm is appropriate given its widespread use. Other studies
need to be conducted if the e(cid:128) ects of other algorithms are to be reliably assessed.

2. Background
2.1. T he Douglas-Peucker Algorithm

The Douglas-Peucker ( hereafter DP) algorithm is designed to eliminate high-
frequency detail along a line while preserving overall line shape. As with other line
simpli® cation algorithms, the simpli® ed line contains a subset of the vertices de® ning
the original line. The nodes that bound the line are never eliminated, thus ensuring
that topological relations with other lines are preserved.

The DP algorithm is based on the computation of the perpendicular distance
between each vertex on the line and a speci® c straight line segment. Initially the
endpoints of this segment are de® ned to be the start and end nodes of the line. These
endpoints are referred to as the `anchor’ and `¯ oat’ points. Next, the perpendicular
distance is computed between each vertex and the trend line. If the largest of these
distances is greater than a speci® ed distance, called the `bandwidth’, then the vertex
lying farthest away becomes the new ¯ oat point. A new trend line is formed and
distances are recomputed for all intervening vertices. The process is repeated until
all intervening vertices are found to lie within the bandwidth tolerance. The anchor

Downloaded by [Florida International University] at 11:53 30 December 2014 116

H. Veregin

is then moved to the ¯ oat and the ¯ oat is moved to the end node of the line again.
The set of points that have been assigned as anchors in this recursive procedure
comprise the simpli® ed line (Douglas and Peucker 1973 ). In the `hierarchical’ version
of the algorithm (the version implemented in Arc/InfoÒ
) the ¯ oat is moved to the
previous ¯ oat location rather than the end of the line.

Figure 1 shows the results of the DP algorithm for a United States Geological
Survey Digital Line Graph hydrographic layer for Richland, Washington. At small
bandwidth values, there is little visual di(cid:128) erence between the original and simpli® ed
lines. As the bandwidth rises, the amount of simpli® cation increases. When the
bandwidth is large, simpli® cation levels can reach absurd proportions and topological
inconsistencies can easily be introduced. The highest possible level of simpli® cation
occurs when all intermediate vertices have been eliminated and only the nodes
de® ning the boundaries of the lines remain.

A limitation of the DP algorithm is that its performance is based solely on the
amplitude of deviations perpendicular to the trend line. There is no mechanism for
incorporating information on the longitudinal frequency of these deviations. Any
vertex within the speci® ed bandwidth value will be eliminated whether it de® nes an
isolated meander or a portion of a larger and more complex feature. This sort of
problem is of course characteristic of line simpli® cation algorithms that are based
purely on geometric criteria. As will later be shown, this characteristic of the DP
algorithm has important implications for positional error.

2.2. L ine simpli® cation and data quality

Only a handful of studies exists in which an attempt is made to assess the
quantitative impacts of line simpli® cation on positional accuracy. McMaster’s work
(1987a, b) represents the ® rst published e(cid:128) ort to develop and utilize quantitative
measures of positional error in this context. In these studies, McMaster compares
the performance of di(cid:128) erent line simpli® cation algorithms for various naturally-
occurring features (streams, coastlines and contour lines) using objective measures
of error. A similar approach is used by Jenks (1989 ). Little (1989 ) has essentially the
same objectives but examines positional error in terms of published map accuracy
standards. All of these studies show that the DP algorithm is consistently superior
to other methods in terms of introduced positional error characteristics. Results
indicate that the algorithm produces relatively low error levels, especially at high
levels of generalisation where other methods yield poor results. The algorithm also
yields a relatively uniform distribution of error along lines.

The measures of positional error used in these studies require some discussion.
Models of positional error for points are extensions of classical uni-dimensional
models, and as such it is fairly easy to measure error for point data, compute
con® dence limits and perform statistical inference tests. The same is not true for
lines. Lines are discrete approximations of analogue features digitized from maps,
photographs or in situ measurements. They are encoded as sets of co-ordinate pairs
for in¯ ection points subjectively selected to capture the shape of the selected feature.
An in® nite number of such sets exists. Hence the error present in a line is a
combination of positional error in encoded point locations and the error introduced
by the encoding of a particular set of points. The two types of error cannot easily
be separated.

Assessment of positional error based on classical statistical methods adapted to
point data is inappropriate, since one cannot assume that points are selected

Downloaded by [Florida International University] at 11:53 30 December 2014 Quantifying simpli® cation-induced error

117

Figure 1. The Douglas-Peucker line simpli® cation algorithm applied to a 1:100 000-scale
DLG hydrography layer for Richland, Washington.

independently from each other (Goodchild 1991a, b). Moreover, except for well-
de® ned lines with abrupt angular changes, the points selected to represent a particular
linear feature generally have no unambiguous referent that can be used as the basis

Downloaded by [Florida International University] at 11:53 30 December 2014 118

H. Veregin

for error assessment. For these reasons, error models for linear data are generally
based on variants of the epsilon band, an elongated statistical probability distribution
de® ning uncertainty in line location. The epsilon band concept has been used to
assess positional di(cid:128) erences in linear features on maps of di(cid:128) erent scales (Honeycutt
1986 ) and to model the e(cid:128) ects of positional uncertainty on operations such as
point-in-polygo n analysis and map overlay (Chrisman 1982, Blakemore 1983 ).
Unfortunately there is little agreement as to the shape of the band or the statistical
distribution of error within it. The idea that the zone is of uniform width with a
uniform distribution of error has persisted despite studies suggesting that both the
shape and distribution may be non-uniform (Caspary and Scheuring 1993 ). Other
models of positional error have also been proposed but have not yet been widely
adopted (e.g. Goodchild and Hunter 1997 ).

A variety of measures of positional error have been used in the context of line

simpli® cation. These include the following (® gure 2 ).

E Distortion polygons. Measures based on the polygons formed between the

original and simpli® ed line (McMaster 1987a, b, Butten® eld 1991 ).

E Displacement vectors. Measures based on perpendicular lines drawn between

the simpli® ed and original lines (McMaster 1987a, b, Jenks 1989 ).

E Critical distance. Measures based on the proportion of vertices on the original
line that are more than a speci® ed distance threshold away from the simpli® ed
line (Little 1989 ).

An obvious limitation of the critical distance method is that it permits assessment
of the proportion of vertices that exceed the speci® ed distance, but not the degree to
which they exceed this distance. More importantly, the method assumes that error
metrics developed for points can be applied to lines as well. It assesses error independ-
ently at each vertex, despite the fact that these vertices are not independently selected.
The other models of error avoid some of these problems by treating the line as a
whole feature rather than as a series of
independent points. An advantage of
the displacement vector method is that it provides information on the statistical
distribution of error along the line. One can derive histograms and compute statistical

Figure 2. Measurement of positional distortion for lines.

Downloaded by [Florida International University] at 11:53 30 December 2014 Quantifying simpli® cation-induced error

119

descriptions of error, such as the mean and standard deviation. The main advantage
of the distortion polygon method is that it provides a more robust estimate of error.
While displacement vector computations are a(cid:128) ected by the sampling interval, or
the spacing between the vectors, no such problem occurs with distortion polygons.
The distortion polygon method is in fact equivalent to the displacement vector
method when the sampling interval is in® nitely small. It can be shown that with an
in® nitely close spacing of displacement vectors, the mean length of these vectors is
equal to the total area of all distortion polygons divided by the length of the
simpli® ed line. The measure is referred to here as the `uniform distance distortion’
because it represents the width of a uniform bu(cid:128) er zone around the simpli® ed line,
where the area of the zone is equivalent to the total area of the distortion polygons.
This width of this zone is equal to the mean displacement, i.e. the mean perpendicular
distance between the original and simpli® ed lines. (One can also divide the area of
the distortion polygons by the length of the original line. The di(cid:128) erence between this
measure and the uniform distance distortion measure is analogous to the distinction
between producer’s accuracy and user’s accuracy as used in remote sensing
classi® cation accuracy assessment.)

In terms of the objectives of this study, the main limitation of previous research
on line simpli® cation and positional error is that it does not explicitly focus on the
relation between bandwidth and error. In most implementations of the DP algorithm,
users must specify what bandwidth is required. As previous studies demonstrate , the
selection of a bandwidth value has signi® cant implications for the amount of error
that is introduced. However, these studies do not provide any guidance for selecting
particular bandwidth values in speci® c situations in order to achieve a target level
of accuracy. No information is provided that would allow the user to translate
desired accuracy standards into a set of rules for applying the line simpli® cation
algorithm.

3. Analysis
3.1. Objectives

This study seeks to examine the e(cid:128) ects of line simpli® cation on the positional
accuracy of linear features. The goal is to quantify the relation between the level of
simpli® cation and the degree of positional error, so that users can choose appropriate
levels of simpli® cation that will yield results meeting speci® c accuracy criteria. This
study focuses on the Douglas-Peucker line simpli® cation algorithm (speci® cally, the
hierarchical version of the algorithm implemented in Arc/InfoÒ
) and examines both
natural and anthropogenic features.

The speci® c issues examined here are as follows.

L evel of simpli® cation. The amount of error is expected to increase as the level
of simpli® cation (i.e. the bandwidth) rises. However, it is anticipated that the
rate of increase will be a(cid:128) ected by the tendency for features to reach a maximum
level of simpli® cation, in which only the bounding nodes remain. At this point
additional increases in bandwidth will have no impact on positional error.
E T ype of feature. The amount of positional error should be predictable at an
aggregate level for speci® c classes of features. Such classes might be de® ned
broadly (e.g. natural versus anthropogenic ) or more narrowly in terms of
speci® c geometric properties (length, sinuosity, etc.). It is anticipated that the
more narrow the class de® nitions, the more precise the predictions will be.

Downloaded by [Florida International University] at 11:53 30 December 2014 E
120

H. Veregin

E Potential error. It should be possible to develop an index of potential error for
individual features that will be closely correlated with actual positional error.
This will allow accurate prediction of simpli® cation-induced error for individual
features to permit simpli® cation to be applied selectively.

3.2. Data and methods

Data for this study are derived from United States Geological Survey (USGS)
1:100 000-scale (`100K’) Digital Line Graphs (DLGs). DLGs are digital vector repres-
entations of spatial data and are derived mainly from USGS 1:100 000-scale, 30- by
60-minute topographic maps. The 100K DLG series (for the continental US and
Hawaii) covers several themes, including hydrography, transportation , hypsography
(contours), and political and administrative boundaries. Sub-themes can be extracted
using encoded attribute values. 100K DLG data are distributed as groups of ®
les
that correspond spatially to portions of 1:100 000-scale topographic maps. Usually
each map is broken into 15 by 15-minute octets, each of which corresponds to one-
eighth of a USGS 1:100 000-scale topographic map and to exactly four USGS 1:24
000-scale topographic maps (USGS 1989 ).

This study uses a sample of lines from 30 DLGs selected randomly from the
continental US. For each DLG, one of the octets was selected randomly for each
streams and primary roadsÐ were extracted for
feature theme. Two sub-themesÐ
analysis to assess potential di(cid:128) erences in natural and anthropogenic features. For
each set of features, all pseudo-nodes (points de® ned as nodes but bounding only
two intersecting lines) were eliminated to ensure that all nodes bound three or more
lines (with the exception of dangling nodes which bound only one line). This step is
necessary to ensure consistency in cartographic line de® nition. In e(cid:128) ect it introduces
a constraint that eliminates any statistical e(cid:128) ects associated with arbitrary node
placement along lines. The stream and road databases were further reduced by
eliminating lines containing no vertices (only two nodes) as these cannot be simpli® ed.
The size of the streams database was reduced by taking a 10% random sample; this
produced road and stream databases of approximatel y equal size (table 1 ).

Line simpli® cation was applied to each database (using the `generalize’ command
in Arc/InfoÒ with the Douglas-Peucker option) with various bandwidth (`weed
tolerance’) values ranging from 2 m to 250 m. In addition, databases were constructed
in which all intermediate vertices were eliminated for each line. This represents the
maximum level of simpli® cation possible and is used to ® nd the upper bound on
simpli® cation-induced error. Following simpli® cation, the databases were processed
with custom C programs to compute error and other indices for each line.

Table 1. DLG statistics.

Length (m)

Number of vertices

Database

Streams
Roads

n

632
542

Min

52.1
45.0

Max

Mean

Min

Max

Mean

16 311.5
11 700.1

2028.5
1010.2

3
3

299
36

27.7
5.2

Downloaded by [Florida International University] at 11:53 30 December 2014 Quantifying simpli® cation-induced error

121

4. Results
4.1. Aggregate results

Figure 3 shows the cumulative frequency curves of positional error for the streams
database for various bandwidth values. Figure 4 does the same for the roads database.
Error is computed as the uniform distance distortion, as shown in ® gure 2. The
cumulative frequency is de® ned for any distortion value as the proportion of lines
having a distortion equal to or less that value. For example, given a bandwidth of
2 m for the streams database, approximatel y 85% of lines have a distortion of 0.05 m
or less, approximatel y 95% have a distortion of 0.1 m or less, and so on.

The ® gures also show the 95% con® dence limits for the cumulative frequency
curves. Three sets of con® dence limits were derived and evaluated: exact binomial
con® dence limits, con® dence limits derived from jack-kni® ng and con® dence limits
derived from bootstrapping (Mosteller and Tukey 1977, Morisette and Khorram
1998 ). These three sets of con® dence limits were found to be nearly identical, with
a maximum absolute di(cid:128) erence on the order of only 1
2 %. The main di(cid:128) erence between
the jack-knifed and bootstrapped estimates are
the con® dence limits is that

Figure 3. Cumulative frequency of distortion, streams, various bandwidths.

Downloaded by [Florida International University] at 11:53 30 December 2014 122

H. Veregin

Figure 4. Cumulative frequency of distortion, roads, various bandwidths.

the cumulative frequency curve, which causes some over-
symmetrical about
estimation of the upper con® dence limit at the high end of the curve and some
under-estimation of the lower con® dence limit at the low end of the curve. The jack-
knifed and bootstrapped con® dence limits are also slightly wider and hence more
conservative than the limits based on the binomial distribution. The con® dence limits
shown in the ® gures are those derived from bootstrapping .

Results indicate that, for any bandwidth value, mean and maximum distortion
values are lower than the bandwidth. This is in part because the uniform distance
distortion calculation yields a value that is less than the maximum linear displacement
(see ® gure 2 ). For the streams and roads databases, mean values are as low as one-
eighth but generally around one-third of the bandwidth, while maximum values are
as low as one-quarter but generally around two-thirds of the bandwidth.

There are obvious changes in the shape of the cumulative frequency curves as
bandwidth rises. Consider ® rst the curves for the streams database (® gure 3 ). The
2-m curve indicates that a large proportion of lines (approximately 70%) have zero
distortion. This is because many lines are una(cid:128) ected by line simpli® cation (i.e. no

Downloaded by [Florida International University] at 11:53 30 December 2014 Quantifying simpli® cation-induced error

123

vertices are eliminated) when the level of simpli® cation is very low. This e(cid:128) ect
disappears quickly as the bandwidth rises; the 10-m curve shows that only about
5% of lines are una(cid:128) ected.

At moderate levels of simpli® cation (e.g. between 10 and 50 m) the curves

approach the shape of a logistic function, i.e.

P = (1 +e( a+ bD ) )Õ

1

(1 )

where P is the cumulative proportion, D is the uniform distance distortion, and a
and b are coe(cid:129) cients. In non-cumulative form, the curves approximate normal
distributions. Using a curve-® tting model (Caceci and Cacheris 1984 ) to ® t the
logistic function, R2 values of 90% and above were obtained (table 2 ).

For high bandwidth values, the distributions become increasingly skewed toward
the lower end. The cumulative curves are now parabolic rather than logistic in shape.
This is indicative of a higher percentage of low distortion values. These are not zero
values, as in the case of the 2-m curve, but values that are low relative to the
bandwidth. This change is indicative of a saturation e(cid:128) ect in which, as the bandwidth
rises past a certain point, distortion no longer increases for many lines because these
lines have already been simpli® ed to their maximum possible extent. In other words,
only the bounding nodes remain, and any further increase in bandwidth has no e(cid:128) ect
on the level of distortion. These results suggest that it might be possible to ® t a
power function of the form

P = aDb

(2 )

for high bandwidth values.

There are important di(cid:128) erences between the curves for streams and roads. At
low bandwidth values (e.g. 2 m) the curve for roads is nearly a straight line, indicating
that almost all lines (approximately 90%) have zero distortion. This re¯ ects the fact
that roads possess few small meanders that can be eliminated using a low bandwidth.
In contrast to streams, which possess such meanders, roads are largely una(cid:128) ected
by low bandwidth values. Moreover, the curves for roads never attain the logistic
shape seen in the streams case, but instead approach the parabolic shape much more
quickly. This suggests that the saturation e(cid:128) ect is achieved at lower bandwidth values
for roads than for streams. Roads have a much narrower `dynamic range’ in the
sense that low bandwidth values have little discernible e(cid:128) ect, but maximum levels
of simpli® cation are achieved using only moderate bandwidth values.

Table 2. Coe(cid:129) cient estimates for logistic model.

Database

Bandwidth (m)

Streams
Streams
Streams
Streams
Streams
Roads
Roads
Roads
Roads
Roads

2
10
50
250
Maximum simpli® cation

2
10
50
250
Maximum simpli® cation

a

0.748
3.759
5.081
2.597
0.669
2.180
1.669
1.911
0.915
0.976

b

21.1500
2.2070
0.4160
0.0555
0.0098
3.0750
0.9500
0.2590
0.0733
0.0682

R2

0.994
0.999
0.999
0.996
0.937
0.991
0.996
0.967
0.926
0.908

Downloaded by [Florida International University] at 11:53 30 December 2014 Õ
Õ
124

H. Veregin

4.2. Potential error

The cumulative frequency curves discussed above represent distortion at an
aggregate level. It would also be useful to be able to predict distortion for individual
features, as this would make it possible to ¯ ag each line with a measure of potential
error indicating how much distortion would result if simpli® cation were carried out.
This metadata could be carried with the database as a line attribute and used to
perform simpli® cation in a selective manner. That is, bandwidth could be made
inversely proportional to potential error in order to prevent the introduction of
excessive levels of error.

The ability to predict distortion at the feature level requires an understanding of
the conditions under which distortion is introduced. At ® rst glance it seems logical
to assume that distortion is associated with line sinuosity, since the more sinuous
the line, the greater the opportunities for the development of distortion polygons.
There are a variety of measures of sinuosity in use. Figure 5 shows one such measure
derived from the hydrologic literature (Brice 1961 ). The measure is de® ned as the
ratio of the length of the original line to the length of a trend line that connects the
midpoints of line segments on the original line that exhibit changes in turn direction
at either end.

While such measures are appealing, empirical results show that they fail to predict
the amount of distortion introduced by line simpli® cation, at least for the DP
algorithm. In part such measures fail because they do not mimic the recursive nature
of the algorithm, in which a line is broken successively into smaller lines through
the identi® cation of new ¯ oat points. In addition, these measures are inherently
incapable of accounting for di(cid:128) erent bandwidth values. The same line has di(cid:128) erent
levels of potential error for di(cid:128) erent bandwidth values; a larger bandwidth generally
introduces higher distortion because additional vertices are eliminated. (The results
presented in the preceding section demonstrate this fact.) Potential error cannot be
predicted accurately by simple descriptive measures of line geometry that do not
account for bandwidth. Accurate prediction of potential error involves separate
computations for di(cid:128) erent bandwidth values.

Moreover, it is not sinuosity per se but rather the tendency for large distortion
polygons to form that is associated with the introduction of positional distortion.
By de® nition, higher distortion occurs when larger distortion polygons are formed
between the original and simpli® ed lines. For the DP algorithm, these tend to be

Figure 5. Computation of the sinuosity index.

Downloaded by [Florida International University] at 11:53 30 December 2014 Quantifying simpli® cation-induced error

125

polygons with signi® cant longitudinal extent (e.g. polygon B in ® gure 6 ). These
polygons form because the DP algorithm is based on a perpendicular measure of
distance from the trend-line and has no mechanism to vary its performance as a
function of longitudinal variation along the trend line.

These observations suggest that any measure of potential error for the DP
algorithm must account for (a) the bandwidth to be used, and (b) the tendency for
large distortion polygons to form. A proposed measure, shown in ® gure 7, is based
on the area of the triangular polygons formed by each successive set of three vertices
along the line. The measure is computed as the sum of the area of those triangles
for which the height of the triangle (i.e. the perpendicular distance from the base to
the apex) is less than the bandwidth. These are triangles that would be eliminated

Figure 6. Potential error as a function of the area of distortion polygons.

Figure 7. Computation of the potential error index.

Downloaded by [Florida International University] at 11:53 30 December 2014 126

H. Veregin

by the DP algorithm. The measure is then normalized by dividing by the length of
the line.

The correlation between potential error and uniform distance distortion is given
in table 3 for roads and streams. The potential error measure captures the distortion
e(cid:128) ects noted above and provides a means of predicting simpli® cation e(cid:128) ects.
However, correlations decline as bandwidth increases because potential error is a
better approximation of the behaviour of the DP algorithm when bandwidth is
small. When bandwidth is large entire sequences of vertices (rather than just insolated
vertices) may be eliminated. A better solution might be achieved by recognizing that
the best predictor of the e(cid:128) ects of the DP algorithm is the algorithm itself. That is,
one could apply the algorithm to a set of lines using a number of di(cid:128) erent bandwidth
values and compute distortion for each line. (A sample of lines could be used if one
desired information at the aggregate level only.) Computed distortion values could
be stored as a line attribute and then used to perform selective simpli® cation. For
example, for each line one could apply the maximum bandwidth value yielding a
speci® c accuracy level.

5.

Implementation
The results reported above indicate that it is possible to apply simpli® cation
using objectively-selected bandwidth values. This is an improvement over current
methods in which bandwidths are selected arbitrarily or, at best, to achieve a
subjectively-de® ned level of cartographic quality. In order to de® ne bandwidths
objectively, users must be able to specify the level of accuracy that is desired. A
variety of accuracy standards can be developed that make use of the uniform distance
distortion measure. Two simple standards will be examined here.

E Maximum proportion standard (MPS). No more than a certain proportion of
lines may have a distortion greater than a speci® ed threshold. This standard
is analogous to the US National Map Accuracy Standards (NMAS) originally
developed in the 1940s and currently applied to USGS maps, including those
from which the DLGs used in this study are derived.

E Maximum distortion standard (MDS). No lines may have a distortion value
greater than a speci® ed threshold. This is analogous to the ranking system on
USGS Digital Elevation Models (DEMs) in which the error at any point must

Table 3. Correlations between potential error and uniform distance distortion. (All correla-
tion coe(cid:129) cients signi® cant at p< 0.001 ).

Bandwidth (m)

Correlation

Database

Streams
Streams
Streams
Streams
Streams
Roads
Roads
Roads
Roads
Roads

2
10
50
250
Maximum simpli® cation

2
10
50
250
Maximum simpli® cation

0.946
0.683
0.143
0.256
0.145
0.970
0.767
0.527
0.573
0.406

Downloaded by [Florida International University] at 11:53 30 December 2014 Quantifying simpli® cation-induced error

127

be below some maximum threshold (often a multiple of the root mean squared
error of the DEM as a whole).

The MPS can be used to estimate the maximum bandwidth value that will yield
a speci® c proportion of lines having a maximum acceptable distortion value. Assume
that at least 90% of all simpli® ed lines must be within 2.5 m of their original line
locations. For streams, this implies a maximum bandwidth of about 10 m (see
® gure 3 ). A larger bandwidth will not yield the required 90% of lines for a 2.5 m
distortion. (For example on the 50 m curve less than 5% of lines have a distortion
of 2.5 m or less.) A smaller bandwidth will yield the required percentage but fewer
vertices will be eliminated. To achieve maximum bene® t, the optimal level of simpli-
® cation is the largest bandwidth value that achieves the target quality level; this is
equivalent to minimizing the number of vertices subject to the constraint that a
speci® c level of quality must be attained.

When dealing with a sample of lines, one must also consider the con® dence limits
on the cumulative frequency curves. The approach can be implemented as follows:

E Extract a sample of lines from the database.
E Generalise these lines at various bandwidth values.
E Compute the distortion for each line at each bandwidth.
E Construct a cumulative frequency curve and associated con® dence limits for
each bandwidth using equations for the binomial distribution, or jackkni® ng
or bootstrapping methods.

E Use the curves to determine the optimal level of simpli® cation for the database

under consideration.

The implementation strategy for MDS is based instead on selective generalisation.
In other words, di(cid:128) erent bandwidth values are applied to di(cid:128) erent lines in the same
database. The objective is to apply the largest bandwidth that achieves the target
quality level. This strategy might be merged with MPS to ensure that (a) no more
than a certain proportion of lines have a distortion greater than a speci® ed value
and (b) no individual line crosses the maximum distortion threshold.

The MPS and MDS strategies were implemented using a 100K DLG streams
layer for Richland, Washington. Implementation of MPS is based on extrapolation
from NMAS. For a 1:100 000-scale map NMAS speci® es that at least 90% of points
must be within 1/50 in of true locations, which translates into approximatel y 50.8 m
on the ground. One ® nds the set of bandwidths that yields at least a 90% cumulative
proportion for a distortion of 50.8 m. As the bandwidth increases the cumulative
proportion drops. The largest bandwidth that still achieves 90% is the optimal result.
For the Richland DLG, results show that a bandwidth of 129 m yields a lower
95% con® dence limit with the required 90% threshold. There is thus 95% certainty
that, by using a bandwidth of 129 m, at least 90% of all lines are within 50.8 m of
their original locations. Di(cid:128) erent results are obtained if all features in the DLG
hydrographic theme (not just streams) are included. By including sub-themes one
mixes features with distinctly di(cid:128) erent geometric properties, which induces changes
in the shape of the cumulative frequency curves. One e(cid:128) ect is a widening of the
con® dence limits around the curves. This means that the computed optimal band-
width is smaller than when the con® dence limits are tight. Hence fewer vertices are
eliminated and less economy through vertex elimination is achieved.

For the MDS example the maximum distortion threshold is set at 1.25 times the

Downloaded by [Florida International University] at 11:53 30 December 2014 128

H. Veregin

value of 50.8, giving 63.5 m. Using the 129-m bandwidth result those lines that have
a distortion greater than 63.5 m are extracted. (There are 12 such lines, or about 3%
of all lines in the layer.) A binary search method is then used to ® nd the maximum
bandwidth value for each of these lines that yields a maximum distortion of 63.5 m.
Bandwidths as low as 89 m are required. Note that the 90% MPS threshold remains
unchanged since all selected lines exceed the 50.8 m limit.

The MPS and MDS strategies are applicable in commercial GIS packages.
Unfortunately the way in which line simpli® cation is implemented in such packages
makes the approach computationally-intensive . This is because frequently the only
parameter that can be varied is the bandwidth; even if one knows the acceptable
maximum level of error, there is no way to attain it except through trial and error.
However, given the development of faster CPUs and the ability to design parallel-
processing algorithms for line simpli® caiton, the computational demands of the
approach do not seem to be major impediments. An alternate solution is to allow
users to specify the acceptable maximum level of error and then harness the power
of the system to determine the bandwidth required to achieve this goal. The imple-
mentation of such optimal bandwidth line simpli® cation algorithms has been investi-
gated by Cromley and others (Cromley and Campbell 1991, 1992, Barber et al. 1995 )
but as yet is not operational in any commercial system.

6. Conclusions

This study shows that positional error induced by line simpli® cation can be
modelled at an aggregate level. By generating cumulative frequency curves and their
con® dence limits, it is possible to identify the bandwidth that eliminates the largest
number of vertices while still attaining a speci® c positional accuracy standard. The
shape of the cumulative frequency curves is a function of the bandwidth and the
type of feature under consideration (e.g. streams versus roads). Con® dence limits are
tightest at the upper end of the curves, a situation that conforms nicely to the
requirements of accuracy standards that stipulate that no more than a certain
percentage of lines may have an error greater than some value.

In terms of speci® c quantitative properties, this study shows that positional error
increases with bandwidth but at a decreasing rate due to a saturation e(cid:128) ect in which
lines reach their maximum error when all vertices are eliminated. The magnitude of
this saturation e(cid:128) ect is greater for roads than for streams. Due to their more angular
shape, roads have a narrower range of bandwidths within which distortion is intro-
duced. The same e(cid:128) ect may apply to other anthropogenic features as well (e.g.
railroads, boundaries, etc.).

Prediction of error at the level of individual features is more problematic. Simple
geometric descriptors of line geometry, such as sinuosity, are not useful because they
do not capture the error-inducing conditions of the Douglas-Peucker algorithm.
Moreover, potential error cannot be computed independently of bandwidth and any
measure is only meaningful with respect to a speci® c bandwidth (or perhaps a narrow
range of bandwidths). Finally, since every line simpli® cation algorithm works di(cid:128) er-
ently, a di(cid:128) erent index of potential error is needed for each algorithm. The index of
potential error developed here for the Douglas-Peucker algorithm shows that predic-
tion is possible at low bandwidth values. However, at larger bandwidth values, the
index does not adequately model the operation of the Douglas-Peucker algorithm.
It is proposed that the best predictor of the e(cid:128) ects of the Douglas-Peucker
algorithm is the algorithm itself. A simple implementation strategy involves applying

Downloaded by [Florida International University] at 11:53 30 December 2014 Quantifying simpli® cation-induced error

129

the algorithm to a set of
lines using various bandwidth values. The computed
cumulative frequency curves can then be used to determine the optimal level of
simpli® cation for the database under consideration. The error values computed for
each line can also be used to apply simpli® cation selectively, e.g. by selecting for
each line the maximum bandwidth value that maintains a required accuracy level.
This suggests that it is possible to implement two distinct accuracy standards: A
maximum proportion standard stating that no more than a certain proportion of
lines may have an error greater than a speci® ed value, and a maximum distortion
standard stating that no lines may have an error value greater than a speci® ed value.
This study shows that management of simpli® cation-induced error is possible
using fairly simple tools well within the reach of GIS users. Utilization of these tools
will assist in the selection of appropriate levels of simpli® cation to facilitate compli-
ance with accuracy standards and permit more selective generalisation than is
currently practised.

Acknowledgments

collection.

The author wishes to acknowledge the assistance of Phil Thibault in data

References
Barber, C., Cromley, R., and Andrle, R., 1995, Evaluating alternative line simpli® cation
for multiple representations of cartographic lines. Cartography and

strategies
Geographic Information Systems, 22, 276± 90.

Blakemore, M., 1983, Generalisation and error in spatial data bases. Cartographica, 21,

131± 139.

Brice, J. C., 1961, Channel Patterns and T erraces of the L oup Rivers in Nebraska. USGS

Professional Paper 422-B, United States Geological Survey, Reston, VA.

Buttenfield, B. P., 1991, A rule for describing line feature geometry. In Map Generalization:
Making Rules for Knowledge Representation, edited by R. B. McMaster and
B. P. Butter® eld (Harlow: Longman), pp. 150± 171.

Caceci, M. S., and Cacheris, P., 1984, Fitting curves to data. Byte, May, 340± 348.
Caspary, W., and Scheuring, R., 1993, Positional accuracy in spatial databases. Computers,

Environment and Urban Systems, 17, 103± 110.

Chrisman, N. R., 1982, A theory of cartographic error and its measurement in digital data
bases. In Proceedings, Auto Carto 5 (Falls Church: American Congress on Surveying
and Mapping), pp. 159± 168.

Cromley, R. G., and Campbell, G. M., 1991, Noninferior bandwidth line simpli® cation:

Algorithm and structural analysis. Geographical Analysis, 23, 25± 38.

Cromley, R. G., and Campbell, G. M., 1992, Integrating quantitative and qualitative aspects

of digital line simpli® cation. T he Cartographic Journal, 29, 25± 30.

Douglas, D. H., and Peucker, T. K., 1973, Algorithms for the reduction of the number of
points required to represent a digitized line or its caricature. T he Canadian
Cartographer, 10, 112± 122.

Honeycutt, D. M., 1986, Epsilon, Generalization and Probability in Spatial Data Bases.

Unpublished manuscript.

Goodchild, M. F., 1991a, Issues of quality and uncertainty. In Advances in Cartography,

edited by J.-C. MuÈ ller (London: Elsevier), pp. 113± 139.

Goodchild, M. F., 1991b, Keynote address. In Proceedings, Symposium on Spatial Database

Accuracy, 1± 16.

Goodchild, M. F., and Hunter, G. J., 1997, A simple positional accuracy measure for linear
features. International Journal of Geographical Information Science, 11, 299± 306.
Joa~ o, E. M., 1995, The importance of quantifying the e(cid:128) ects of generalization. In GIS and
Generalization, edited by J.-C. MuÈ ller, R. Weibel and J.-P. Legrange (London: Taylor
& Francis), pp. 183± 193.

Downloaded by [Florida International University] at 11:53 30 December 2014 130

Quantifying simpli® cation-induced error

Jenks, G. F., 1989, Geographic logic in line generalization. Cartographica, 26, 27± 41.
Little, A. R., 1989, An evaluation of selected computer-assisted line simpli® cation algorithms
in the context of map accuracy standards. T echnical Papers, ASPRS/ACSM Annual
Convention (Falls Church: American Society for Photogrammetry and Remote Sensing
and American Congress on Surveying and Mapping), 5, 122± 132.

McMaster, R. B., 1987a, Automated line generalization. Cartographica , 24, 74± 111.
McMaster, R. B., 1987b, The geometric properties of numerical generalization. Geographical

Analysis, 19, 330± 346.

Morisette, J. T., and Khorram, S., 1998, Exact binomial con® dence interval for proportions.

Photogrammetric Engineering and Remote Sensing, 64, 281± 83.

Mosteller, F., and Tukey, J. W., 1977, Data Analysis and Regression (Reading: Addison-

Wesley).

USGS, 1989, Digital Line Graphs from 1:100 000-Scale MapsÐ Data Users Guide 2.

(Reston, VA: United States Geological Survey).

Veregin, H., and McMaster, R., 1997, Review and Evaluation of Generalization Methods in

Raster GIS (Cincinnati, OH: GIS/LIS ’97 Conference Presentation).

Weibel, R., 1995, Map generalization in the context of digital systems. Cartography and

Geographic Information Systems, 22, 259± 263.

White, E. R., 1985, Assessment of line generalization algorithms using characteristic points.

T he American Cartographer, 12, 17± 28.

Downloaded by [Florida International University] at 11:53 30 December 2014 