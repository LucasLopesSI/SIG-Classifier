bs_bs_banner

Research Article

Transactions in GIS, 2014, 18(S1): 3–24

Geocomputation over the Emerging Heterogeneous
Computing Infrastructure

Xuan Shi*, Chenggang Lai†, Miaoqing Huang† and Haihang You‡

*Department of Geosciences, University of Arkansas
†Department of Computer Science and Computer Engineering, University of Arkansas
‡Institute of Computing Technology, Chinese Academy of Sciences

substantial

computing power

Abstract
Emerging computer architectures and systems that combine multi-core CPUs and accelerator technol-
ogies, like many-core Graphic Processing Units (GPUs) and Intel’s Many Integrated Core (MIC) coproces-
for many time-consuming spatial-temporal
sors, would provide
computation and applications. Although a distributed computing environment is suitable for large-scale
geospatial computation, emerging advanced computing infrastructure remains unexplored in GIScience
applications. This article introduces three categories of geospatial applications by effectively exploiting
clusters of CPUs, GPUs and MICs for comparative analysis. Within these three benchmark tests, the GPU
clusters exemplify advantages in the use case of embarrassingly parallelism. For spatial computation that
has light communication between the computing nodes, GPU clusters present a similar performance to
that of the MIC clusters when large data is applied. For applications that have intensive data communica-
tion between the computing nodes, MIC clusters could display better performance than GPU clusters.
This conclusion will be beneﬁcial to the future endeavors of the GIScience community to deploy the
emerging heterogeneous computing infrastructure efﬁciently to achieve high or better performance spatial
computation over big data.

1 Introduction

High-resolution geospatial data has become increasingly available along with an accelerating
increase in data volume. Consequently spatial data may be stored as separate tiles for efﬁ-
cient data exchange, integration, processing, analysis and visualization. The distribution or
separation of data has raised a variety of challenges and complexities in spatial computation
for knowledge discovery. Traditional geographic information systems (GIS) and remote
sensing software may only work on one tile of data at a time for computation or analysis. If
all tiles are merged or mosaicked into a single piece, its size typically becomes too huge to
be processed on a desktop computer. If all the data are maintained as separate tiles,
however, the analytic result may not be correct or consistent among multiple separated tiles.
In the case of unsupervised image classiﬁcation, the classes’ mean values are determined by
the global information derived from all tiles. When individual tiles of images are processed
by separate procedures, the class mean values are calculated based on the local information

Address for correspondence: Xuan Shi, Department of Geosciences, University of Arkansas, Fayetteville, AR 72701, USA. E-mail:
xuanshi@uark.edu
Acknowledgments: This research was partially supported by the National Science Foundation through the award OCI-1047916. The
research used resources of the Keeneland Computing Facility at the Georgia Institute of Technology, which is supported by the National
Science Foundation under Contract OCI-0910735. This research also used Beacon, which is a Cray CS300-AC Cluster Supercomputer.
The Beacon Project is supported by the National Science Foundation and the State of Tennessee. The authors would also thank Nvidia
Corporation for K20 GPU donations.

© 2014 John Wiley & Sons Ltd

doi: 10.1111/tgis.12108

4

X Shi, C Lai, M Huang and H You

of each tile. For this reason, even if each tile of the image can be classiﬁed into the same
number of classes, the result is not consistent among different tiles when the spectral statis-
tics of the pixels in different tiles are different. In the case of viewshed analysis, when obser-
vational points are near the border of the Digital Elevation Model (DEM) data, the visible
areas should also cover the neighboring tiles of data. If viewshed calculation is implemented
over each individual tile at a time, the result is not correct.

In the problem-solving and decision-making processes, the performance of geospatial
computation is
severely limited when massive datasets are processed. Heterogeneous
geospatial data integration and analytics obviously magnify the complexity and the opera-
tional time-frame. Many large-scale geospatial problems may not be processible at all if the
computer system does not have sufﬁcient memory or computational power. For example,
simulating urban growth has been a challenging theme, since the difﬁculty of modeling
urban growth can be aggravated due to the massive computational intensity caused by com-
plicated algorithms and large data volume that are often required in the simulation. It was
reported that the comprehensive calibration process of the SLEUTH model, one of the most
widely used urban land-use change models in the world, over a medium size dataset might
need 1,200 CPU hours (Clarke 2003) to accomplish – if the system could run the program
successfully. In emergency response practice, both the HAZUS module (FEMA 2013) and the
MAEviz (NCSA 2009) have been suffering the performance and scalability constraints in
handling large datasets. In public health research, Centers for Disease Control and Preven-
tion (CDC) professionals were looking for alternative technologies that outperform ArcGIS
due to the long processing times for geospatial analysis. In the Extreme Science and Engi-
(XSEDE) community, a request was discussed about
neering Discovery Environment
geospatial data mining on XSEDE when very large data was applied. In service computation,
although Web Processing Service (WPS) was released in 2006, it has not been feasible in real
world practice since the server cannot handle dynamic spatial data processing in a service-
oriented computing environment when big data is involved. In network computing, origin-
destination (OD) cost matrix calculation is
time-consuming when large datasets are
applied.

Scalable and high performance solutions for geospatial computation are increasingly
required since GIS has been employed in a variety of domain applications that have broader
impacts for societal beneﬁt. Emerging computer architectures and advanced computing tech-
nologies, such as Intel’s Many Integrated Core (MIC) Architecture and Graphics Processing
Unit (GPU), provide a promising solution to employ massive parallelism to achieve scalability
with high performance for data intensive computing over large spatial data. In geospatial
applications, a few initiatives utilized a single GPU to build a spatial index (Zhang et al. 2010,
2011) or calculate the viewshed to determine the locations over a digital terrain surface visible
to a set of observer features (Zhao et al. 2013). In processing satellite images, most prior
works deployed a single GPU, such as unsupervised image classiﬁcation (Ye and Shi 2013),
segmentation (Huang et al. 2013), and hyperspectral data analytics (Bernabe et al. 2013a, b).
In geospatial simulation, agent based modeling has been implemented on individual GPUs
(Tang and Bennett 2012) or multiple GPUs without communication between the GPUs (Aaby
et al. 2010). Spatial computation by deploying multiple GPUs through combined MPI and
CUDA programs were reported with a focus on interpolation using IDW and Kriging algo-
rithms (Ye et al. 2011; Shi and Ye 2013). Currently there is no prior work on deploying MIC
for geospatial computation.

From a perspective of efﬁciently utilizing the heterogeneous computer architecture for
three categories of geospatial applications can be

parallelizing geospatial computation,

© 2014 John Wiley & Sons Ltd

Transactions in GIS, 2014, 18(S1)

Geocomputation over the Emerging Heterogeneous Computing Infrastructure

5

identiﬁed. Embarrassingly parallelism is the ﬁrst category of applications in which spatial cal-
culation can be implemented on distributed computing nodes and there is no communication
between nodes. For example, in spatial interpolation, each cell in the output grid can be calcu-
lated by an individual thread and there is no dependency between the threads (Shi et al. 2013).
Data conversion from one data type or projection to other types or projections can be another
use case (Li et al. 2010). In spatial analysis, many functions for map algebra calculation over
raster datasets can be accomplished through embarrassingly parallelism.

In parallel and distributed computation, data communication between computing nodes
may be required in different scenarios. Before the computation is implemented, partial data
may have to be shared or exchanged among the distributed nodes. For example, in Focal
Statistics calculations, the value of each output cell is derived based on the input cells in a
speciﬁed neighborhood. In surface analysis, aspect or slope is calculated based on the values
of the cell’s eight neighbors. For this reason, data at the boundary of the tiles has to be
exchanged before the geospatial computation is executed. In the other scenario, when spatial
data is processed as separate tiles by distributed and parallel computing nodes, geospatial
computation may need to retrieve the global information from separated datasets to com-
plete the task. In spatial statistics calculations, for example, the mean and standard deviation
have to be derived from the entire datasets. As a result, the second category of spatial com-
putation refers to those applications that either need data exchange before the computation
is started, or need post-processing of the result after the computation is accomplished on dis-
tributed nodes. In many applications, such as agent-based modeling, data communication
may have to be executed multiple times in order to complete the computational processes. In
the third category of spatial computation, communication among distributed computing
nodes is required both before and after the computation. One other signiﬁcant issue in data
communication is the amount of data exchanged that may have signiﬁcant impact on the
total performance. In general, data can be exchanged between the computing node in regular
forms for the same amount of data, such as a given number of columns or rows, although
irregular data exchange can be performed that will increase the difﬁculty in handling load
balance.

In this pilot study, the CPU and GPU clusters on Keeneland and the MIC cluster on
Beacon were utilized to implement three benchmarks that could be representative of three cat-
egories of geospatial computation, as discussed above. Kriging interpolation is applied as a use
case of embarrassingly parallelism. Our prior works (Ye et al. 2011; Shi and Ye 2013) on inter-
polation were conducted on GPU clusters, but CPU or MIC clusters were not applied. Unsu-
pervised image classiﬁcation is selected as a representative benchmark for the second category
because communication has to be implemented in each iteration of the classiﬁcation processes
when local summation information has to be integrated and synthesized to derive the global
result. In the prior work, we successfully implemented unsupervised image classiﬁcation on a
single GPU (Ye and Shi 2013), but multiple GPUs and MICs were not applied yet for image
classiﬁcation. Finally, Cellular Automata (CA) is selected as a representative benchmark of the
third category that has intensive data communication in geospatial calculation over distributed
computing nodes. Since the status of each cell is dependent upon the status of its neighbors,
for each iteration in neighborhood dependent operations, the value of the neighboring cells
along the boundary of the data has to be exchanged before the computation is implemented.
After the computation, the status of the cells on distributed nodes has to be summed up. For
this reason, communication is required before and after the calculation. In spatial modeling,
CA have been applied extensively. A generic CA applied as the solution is applicable and
extensible to all CA-based simulations.

© 2014 John Wiley & Sons Ltd

Transactions in GIS, 2014, 18(S1)

6

X Shi, C Lai, M Huang and H You

In the following sections, this article ﬁrst reviews the main characteristics of the emerging
heterogeneous computer architectures and systems that are utilized as the computing infra-
structure for this pilot study. As a result, the chosen approach can be applied commonly in dif-
ferent systems for comparison. The algorithms of the three benchmarks are summarized before
the implementation details are introduced. The result and performance comparison are dis-
cussed in detail to demonstrate that the emerging advanced computer architecture and com-
puting technologies have the potential to transform research in geospatial science. The
solutions exempliﬁed in this study can be extended to other geospatial computations with a
broader impact in the future.

2 Emerging Heterogeneous Computer Architectures and Systems

2.1 Graphic Processing Unit (GPU)

The Graphics Processing Unit (GPU) was traditionally built for the purpose of efﬁciently
manipulating computer graphics. For this reason, image processing has been a prime applica-
tion that beneﬁts from the marvelous parallelism inherent in graphic processing. Today’s GPU
has evolved rapidly to support General-Purpose computing (thus called GPGPU) through
many-core processors capable of intensive computation and data throughput. Thus a modern
GPU enables massively parallel computing for scientiﬁc computation and is not limited to pro-
cessing computer graphics
(Preis et al. 2009; Kalyanapu et al. 2011; Steinbach and
Hemmerling 2012; Simion et al. 2012; Emeliyanenko 2013). By simultaneously executing tens
of thousands of threads on hundreds of specialized GPU cores, GPGPU can achieve high per-
formance computing over desktop and laptop computers.

GPU architecture has been evolving for many years. Taking the GPUs developed by
NVIDIA as examples, it has gone through many generations, such as G80 → GT200 → Fermi
→ Kepler. NVIDIA’s latest Kepler GPU architecture contains 15 streaming multiprocessors
(SMXes), each of which consists of 192 single-precision cores and 64 double-precision cores,
as shown in Figure 1. The Kepler architecture provides three advanced features to: (1) efﬁ-
ciently share the GPU resources among multiple host threads/processes; (2) ﬂexibly create new

Figure 1 Nvidia’s Kepler GPU architecture

© 2014 John Wiley & Sons Ltd

Transactions in GIS, 2014, 18(S1)

Geocomputation over the Emerging Heterogeneous Computing Infrastructure

7

kernels on GPU (i.e. Dynamic Parallelism); and (3) reduce communication overhead across
GPUs through GPUDirect. Although such advanced features cannot be tested in this pilot
study since there are no Kepler clusters that can be accessed, the advantages of the latest gen-
eration of GPU are exempliﬁed in comparison with the old generation of GPUs.

2.2 Intel’s Many Integrated Core (MIC)

The ﬁrst commercially available Intel coprocessor based on Many Integrated Core architecture
(Jeffers and Reinders 2013) is Xeon Phi, as shown in Figure 2. Xeon Phi contains 60 scalar
processors with vector processing units. Direct communication between MIC coprocessors
across different nodes is also supported through MPI. Figure 3 shows two approaches to
parallelizing applications on computer clusters equipped with MIC processors. The ﬁrst
approach is to treat the MIC processors as clients to the host CPUs. As shown in Figure 3a,
the MPI processes will be hosted by CPUs, which will off-load the computation to the MIC
processors. Multithreading programming models such as OpenMP can be used to allocate
many cores for data processing. The second approach, as shown in Figure 3b, is to let each
MIC core directly host one MPI process. In this way, the 60 cores on the same chip are treated
as 60 independent processors while sharing the 8 GB on-board memory on the Xeon Phi
5110P. Although a few works on MIC-based computation have been reported (Crimi et al.
2013; Schmidl et al. 2013; Heinecke et al. 2013), few applications in geospatial sciences have
been explored yet.

2.3 Comparison between Tesla K20 GPU and Intel MIC

Table 1 provides a comparison of the architecture of Nvidia’s Tesla K20 GPU with Intel’s
MIC. Both of them deploy GDDR5 on-board memory to provide a high memory bandwidth.
In terms of processing cores, the Nvidia K20 GPU contains thousands of low-performance
cores running at 706 MHz, including both single precision (SP) and double precision (DP)
cores. On the other hand, the Intel Xeon 5110P consists of 60 comparably more powerful
cores running at 1.053 GHz. Therefore, if the application is ﬁne-grain parallel, i.e. the work-
load can be divided into a large amount of small, however, concurrent pieces, it will ﬁt the

Figure 2 The architecture of Intel MIC

© 2014 John Wiley & Sons Ltd

Transactions in GIS, 2014, 18(S1)

8

X Shi, C Lai, M Huang and H You

Figure 3 Two different approaches to implementing parallelism on an MIC cluster

Table 1 Architecture comparison between Tesla K20 GPU and Intel MIC

Devices

On-board
Memory
Capacity

Memory
Bandwidth

No. of Cores

Core
Frequency

Nvidia Tesla K20
Intel Xeon 5110P

5 GB GDDR5
8 GB GDDR5

208 GB/s
320 GB/s

2496 (SP) +832 (DP)
60

706 MHz
1.053 GHz

GPU architecture quite well. Otherwise, if the application is coarse-grain parallel, i.e. the
workload needs to be distributed into dozens of parallel pieces, it will ﬁt the MIC architecture.
Because Xeon 5110P provides a higher memory bandwidth than that of the Nvidia K20 GPU,
MIC has the potential to provide a better performance for memory-intensive applications
while, on the other hand, computation-intensive applications may run on Nvidia K20 GPU
better because of the large amount of processing cores on the GPU.

2.4 Keeneland and Beacon

We conducted our experiments on two platforms, the NSF sponsored Keeneland supercom-
puter (Keeneland 2014) and Beacon supercomputer (Beacon 2014). The Keeneland Initial
Delivery System (KIDS) is a 201 Teraﬂop, 120-node HP SL390 system with 240 Intel Xeon
X5660 CPUs and 360 Nvidia Fermi GPUs, with the nodes connected by a QDR InﬁniBand
network. Each node has two 6-core 2.8 GHz Xeon CPUs and three Tesla M2090 GPUs. The
NVIDIA Tesla M2090 GPU contains 512 CUDA cores and 6 GB GDDR5 on-board
memory. The Beacon system (Cray CS300-AC Cluster Supercomputer) offers access to 48

© 2014 John Wiley & Sons Ltd

Transactions in GIS, 2014, 18(S1)

Geocomputation over the Emerging Heterogeneous Computing Infrastructure

9

compute nodes and six I/O nodes joined by a FDR InﬁniBand interconnect providing 56
Gb/s of bi-directional bandwidth. Each compute node is equipped with two Intel Xeon
E5-2670 8-core 2.6 GHz processors, four Intel Xeon Phi (MIC) coprocessors 5110P, 256 GB
of RAM, and 960 GB of SSD storage. Each I/O node provides access to an additional 4.8
TB of SSD storage. Each Xeon Phi coprocessor contains 60 1.053 GHz MIC cores and 8 GB
GDDR5 on-board memory. As a result, Beacon provides 768 conventional cores and 11,520
accelerator cores that provide over 210 TFLOP/s of combined computational performance,
12 TB of system memory, 1.5 TB of coprocessor memory, and over 73 TB of SSD
storage.

Both platforms are equipped with a Lustre parallel distributed ﬁle system (Cluster File
Systems, Inc. 2002), in which a technique called ﬁle striping can be applied to signiﬁcantly
improve the I/O performance. File striping is a process in which Lustre automatically divides
data into chunks and distributes them across multiple object storage targets (OSTs). It plays an
important role in running large scale computation by reducing the time spent on reading or
writing a big data ﬁle to signiﬁcantly improve the I/O performance. By setting the stripe count
and stripe size, which are the tunable parameters of the Lustre ﬁle system, the I/O performance
of a particular benchmark can be optimized. Stripe count is the number of OSTs into which a
ﬁle is stored. For example, if the stripe count is set to 10, the ﬁle will approximately be parti-
tioned in equal portions of 10 different OSTs. Stripe size is the chunk size into which a ﬁle is
split to be distributed across OSTs.

3 Overview of the Algorithms for the Selected Three Benchmarks

Geocomputation is fundamental in GIS and remote sensing applications. Commercial and
open source GIS and remote sensing software could have hundreds of computational modules.
As the ﬁrst geospatial application implemented on supercomputers Keeneland and Beacon,
although varieties of options could be applied to conduct such a pilot study, three benchmarks
were selected to explore the generic routines for developing geocomputation over the emerging
heterogeneous computing infrastructure. From the perspective of experimental design for this
study, examining the scalability of the computational solution over the hybrid environment is
of particular interest as many big problems could hardly be resolved by a desktop computer or
by a single GPU or MIC. Utilizing multiple GPUs and MICs should have the potential to
handle large scale data and computational problems.

Considering the complexity of the interpolation algorithm, Kriging was selected as a
typical case of embarrassingly parallelism, though many other functional modules in GIS soft-
ware could be more interesting to a different audience. Unsupervised image classiﬁcation was
selected as a representative use case because a single CPU or GPU could hardly process a large
image. Game of Life is selected for this pilot study since it helps to develop a generic routine to
deal with data communication in a heterogeneous computing environment, while the solution
can be easily adapted to other Cellular Automata based spatial simulation. This section brieﬂy
reviews and summarizes the algorithms of the three benchmarks, followed by sections about
the implementation and the comparison of the results.

3.1 Kriging Interpolation: The Use Case of Embarrassingly Parallelism

Kriging is a geostatistical interpolation method that has a complex implementation and a large
computation load (Oliver and Webster 1990; Cheng et al. 2010; Srinivasan et al. 2010). It can

© 2014 John Wiley & Sons Ltd

Transactions in GIS, 2014, 18(S1)

(1)

(2)

(3)

10

X Shi, C Lai, M Huang and H You

be viewed as a point interpolation that reads input point data and returns a raster grid with
calculated estimations for each cell. Each input point is in the form (xi, yi, Zi) where xi and yi
are the coordinates and Zi is the value. The estimated values in the output raster grid are cal-
culated as a weighted sum of input point values:

ˆ
(
,Z x y

) =

k

=∑

i

1

w Zi
i

where wi is the weight of the i-th input point. Theoretically the estimation can be calculated by
the summation through all input points. In general, users can specify a number k and sum over
k nearest neighbors of the estimated point because the farther the sampled point is from the
estimated value, the less impact it has over a certain distance. By default, ArcGIS uses 12
nearest points (k = 12) in the Kriging calculation.

A set of weights is calculated separately by solving the following matrix equation for each

output point:

where V is the semivariogram matrix, W is a vector of the weights, and D is a vector in which
the semivariogram values for distances from k input points to the output point are ﬁlled in.
This can also be expressed as:

V W D

=

⋅

⎡
⎢
γ
⎢
γ
⎢
⎢
⎢
⎢
⎣

γ

(
(

(

21

0
h
h
31
(cid:3)
h
k
1

1

)
)

)

γ

(

γ

(

γ

(

)

)

)

h
12
0
h
32
(cid:3)
h
k
1

2

γ
γ

γ

23

)
)

(
(cid:2)
h
13
(
(cid:2)
h
(cid:2)
0
(cid:3) (cid:4)
)
(cid:2)
h
k
(cid:2)
1

(

3

γ
γ
γ

)
)
)

k

k

k

(
h
1
(
h
2
(
h
33
(cid:3)
0
1

⋅

=

1
1
1
1
1
0

⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦

⎡
⎢
⎢
⎢
⎢
⎢
⎢
⎣

w
1
ww
1
w
1
(cid:3)
w
1
λ

⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦

1

2

3

γ
⎡
⎢
γ
⎢
γ
⎢
⎢
⎢⎢
⎢
⎣

γ

(
h
p
(
h
p
(
h
p
(cid:3)
h
1

(

pk

)
)
)

)

⎤
⎥
⎥
⎥
⎥
⎥
⎥
⎦

This means the size of the semivariogram matrix V is (k+1)*(k+1). When a Kriging algorithm
is implemented on a single GPU, the size of V may lead to the memory issues when a large
amount of data is applied. In this pilot study, the Gaussian elimination method is used to solve
Equation (2). In a parallel computing environment, each thread can be used to calculate one
cell of the output grid. Since there is no dependency between each node, interpolation can be a
typical case of embarrassingly parallelism.

3.2 ISODATA for Unsupervised Image Classiﬁcation: The Use Case of Light

Communication

Unsupervised image classiﬁcation is the foundation for other classiﬁcation approaches, such as
supervised and object oriented classiﬁcations. The Iterative Self-Organizing Data Analysis
Technique Algorithm (ISODATA) (Jenson 1999) is one of the most frequently used algorithms
for unsupervised image classiﬁcation in remote sensing applications. In general, ISODATA can
be implemented in three steps: (1) calculate the initial mean value of each class; (2) classify
each pixel to the nearest class; and (3) calculate the new class means based on all pixels in one
class. The second and third steps are repeated until the change between two iterations is small
enough.

To perform ISODATA classiﬁcation, several parameters need to be speciﬁed (see Table 2).
The initial class means are derived by the statistics of the original data sets, although the initial
means can be assigned arbitrarily. Accordingly, the initial class means are evenly distributed in
a multi-dimensional feature space along a vector between two points (μ1 – σ1, μ2 – σ2, . . . , μk

© 2014 John Wiley & Sons Ltd

Transactions in GIS, 2014, 18(S1)

Geocomputation over the Emerging Heterogeneous Computing Infrastructure

11

Table 2 Prerequisites for ISODATA implementation

Symbol

Deﬁnition

C
T

M

The number of classes to be created
The convergence threshold, which is the maximum percentage of pixels whose

class values are allowed to be unchanged between iterations

The maximum number of iterations to be performed

– σk) and (μ1 + σ1, μ2 + σ2, . . . , μk + σk) in the ﬁrst iteration, where μi denotes the mean of the
i-th band (i = 1, 2, . . . , k) and k is the total number of bands in the dataset, and σi is the
standard deviation of band i. When the image is classiﬁed into ﬁve classes, the initial ﬁve
cluster means are marked as the dots in the chart and are evenly distributed between (μA – σA,
μB – σB) and (μA + σA, μB + σB).

Throughout the iteration procedures, while the maximum number of iterations (M) and
the convergence threshold (T) are not reached, the means of all classes are recalculated,
causing the class means to shift in the feature space. During the iterative calculations, each
pixel is compared with the new class means and will be assigned to the nearest class mean.
During the process of classiﬁcation, each class is labeled as a certain type of object. The change
between two consecutive iterations can be either the percentage of pixels whose class labels
have been changed between two iterations, or the accumulated distances of the class means
that have been changed in the feature space between two iterations. The iterative process
will not be terminated until either the maximum number of iteration or the convergence
threshold (T) is reached or exceeded, which means the change between two iterations is small
enough, that is, the maximum percentage of pixels whose class values that are unchanged
between iterations.

In a parallel computing environment, each computing node can be used to calculate one
section of the entire image. In order to obtain the cluster mean value, the local summation
value derived from each node has to be integrated by a single node to derive the global mean
value and the new cluster centers. For this reason, data exchange happens at the end of each
iteration and thus can be a light communication use case.

3.3 Game of Life: The Use Case of Intensive Communication

Conway’s Game of Life (GOL) is a well-known classical Cellular Automata model (Gardner
1970). According to the transition rules, a cell can live or die based on the condition of its 3 ×
3 neighborhood. The pseudo code of such transition rules can be described as follows:

FUNCTION Transition (cell, t)
n = number of alive neighbors of cell at time t
IF cell is alive at time t

IF n > 3

IF n < 2

THEN cell dies of overcrowding at time t+1

THEN cell dies of under-population at time t+1

IF n = 2 OR n = 3

THEN cell survives at time t+1

© 2014 John Wiley & Sons Ltd

Transactions in GIS, 2014, 18(S1)

12

X Shi, C Lai, M Huang and H You

ELSE (i.e. cell is dead at time t)

IF n = 3

THEN cell becomes a live cell by reproduction at time t+1

In the parallel computing environment, when the entire matrix is cut into several seg-
ments, for example, the rows at the boundary of each segment have to be exchanged before
the simulation can be implemented. After each simulation, the total living cells have to be
counted and the summation has to be done by aggregating the local values into a global value.
For this reason, data exchange and communication have to be implemented before and after
each iteration, which is a typical use case of intensive communication. The solution on the
GOL benchmark can be applied in varieties of geospatial modeling, such as in a CA-based
simulation of urban sprawl study, infectious disease spread in public health research, and envi-
ronmental change upon possible socioeconomic consequences.

4 Implementation of the Benchmarks

The benchmarks were initially developed in serial C programs in order to establish a standard
for quality control ensuring the output results from all different solutions will be the same.
Consequently the GPU programs running on a single GPU were ﬁrst developed before the
benchmarks were implemented on Keeneland using multiple GPUs. Since the desktop GPUs
are not comparable to those on Keeneland, the performance of the same programs imple-
mented on the latest version of GPU on Keeneland is obviously much better than that from the
old version of GPUs installed on the desktop machine. Lastly, the MPI + MIC solutions were
developed for implementing three benchmarks on Beacon. Result and performance compari-
son of the experimental design are discussed in Section 5.

4.1 Serial C Programs for Quality Control

For each use case, we ﬁrst developed serial C programs to establish a standard for quality
control, understand the computation and performance limits for data, and compute intensive
applications implemented on a desktop machine. In this pilot study, we used a desktop com-
puter in which Windows 7 is the operating system. The desktop machine has an Intel Pentium
4 CPU with 3.00 GHz frequency and 4 GB RAM. We built the serial program in C within
Visual Studio .NET 2010 to implement the Kriging interpolation. The output grid is a matrix
that has 1,440 x 720 = 1,036,800 cells. When 9,817 sample points were used as the input, it
took 2,771 seconds (i.e. more than 46 minutes) to complete the Kriging calculation in the
serial C program when 10 nearest neighbors were used. In the case of the unsupervised image
classiﬁcation, one tile of three-band imagery data was used. That image has a resolution of 0.5
m and the ﬁle size is18 GB (80,000 × 80,000 × 3). Commercial remote sensing software would
take 3 hours, 44 minutes, 37 seconds (13,477 seconds in total) to read and classify this image
into ﬁve classes. In the case of the Game of Life computation, a 100-iteration simulation over
a 10,000 × 10,000 cell space was accomplished in about 100 minutes.

4.2 Benchmark in CUDA C Programs on a Single GPU

The serial C programs were further transformed into CUDA programs tested on a single GPU
over small datasets to guarantee that the output results are the same as that derived from the

© 2014 John Wiley & Sons Ltd

Transactions in GIS, 2014, 18(S1)

Geocomputation over the Emerging Heterogeneous Computing Infrastructure

13

Table 3 Computation time comparison of single-GPU implementation on M2090 and K20 (with
time counted in seconds). K20 is able to achieve a better performance than M2090 in the Kriging cal-
culation and Game of Life simulation without any particular tuning in this pilot study

Game of Life

GPU

Kriging

ISODATA

8,192 x 8,192

16,384 x 16,384

32,768 x 32,768

M2090
K20
Speedup

24.64
10.32
2.39

49.19
46.86
1.05

12.86
3.25
3.96

51.29
12.58
4.08

204.99
46.99
4.36

serial C programs. The experiments were initially conducted on a desktop PC with a NVIDIA
GeForce GTS 450. In the case of the Kriging interpolation, when the abovementioned same
size of data was used as the input, it took 52 seconds to complete the task. In the case of unsu-
pervised image classiﬁcation, due to the memory limit on such an old GPU, a small dataset
was used to validate the parallel program and the quality of the result. The ﬁle size of the
small data is 89.6 MB. In this case, the GPU on a desktop computer can complete the classiﬁ-
cation in 5.43 seconds and achieve a speedup of about 42 when the image is classiﬁed into ﬁve
classes.
the simulation at a size of
10,000×10,000 for 100 iterations took about 6 minutes to complete, achieving a speedup of
16.7 on a single GPU.

the Game of Life computation,

In the case of

Since we do not have access to a computer cluster with NVIDIA Kepler GPUs, in order to
project the performance of a hybrid cluster with Kepler (K20) GPU, we compare the perfor-
mance of the same single-GPU implementation of the Kriging interpolation using 10 nearest
neighbors on both the Tesla M2090 and K20 GPUs. Table 3 displays the result. Due to the
on-board memory limit of the single GPU, the image size is reduced to 10,000 × 10,000 × 3 in
the unsupervised image classiﬁcation test. Based on the performance on the single-node imple-
mentation, it seems that the K20 is able to achieve a better performance than M2090 in the
Kriging calculation and the Game of Life simulation without any particular tuning in this pilot
study. Obviously both the Tesla M2090 and K20 GPUs displayed much better performance
than the old generations of GPUs, such as GTS 400 series and GTX 200 series.

4.3 Benchmarks in Hybrid MPI Programs on Keeneland and Beacon

We developed hybrid MPI programs to deploy multiple CPUs and GPUs on Keeneland and
multiple MICs on Beacon to implement the three benchmarks. The Intel Xeon E5 8-core CPUs
on Keeneland are used to implement MPI + CPU programs for the three benchmarks. Each
MPI process runs on a single CPU. The process on the CPU is a single-thread process. When a
number of MPI processes are created in the parallel application, the same number of CPU pro-
cessors are allocated.

MPI + GPU programs are implemented on Keeneland. Each MPI process runs on a host
CPU, which conveys all the data processing to one client GPU processor. On the NVIDIA
M2090 GPU, all 512 CUDA cores are deployed for computation. If a number of MPI pro-
cesses are created in the parallel application, the same number of CPU processors and the same
number of GPU processors are allocated. The host CPU is responsible for the MPI communi-

© 2014 John Wiley & Sons Ltd

Transactions in GIS, 2014, 18(S1)

14

X Shi, C Lai, M Huang and H You

cation, and the client GPU is in charge of data processing. Since we did not have access to K20
GPU clusters, the GPUDirect technique was not tested yet.

Lastly, the Intel Xeon Phi 5110P on the Beacon is used for such benchmarks through the
MPI + MIC programs. Each MIC core will directly host one MIC process, as shown in
Figure 3b. In this case, when a given number (e.g. N) of Xeon Phi coprocessors are used, N ×
60 MPI processes are created in the parallel implementation. This programming model on the
Intel MIC cluster has better portability than the ofﬂoad model shown in Figure 3a. Legacy par-
allel code using MPI can be directly compiled and executed on systems running on the Intel
MIC without much tuning.

5 Results and Performance Comparison of the Experimental Benchmarks

In this pilot study, the problem size is ﬁxed for each benchmark when the number of partici-
pating MPI processes is increased so that the strong scalability of the parallel implementations
can be examined. In the case of the Game of Life simulation, since the random processes on
different nodes may generate different results, we create an initialization ﬁle through the serial
C program as the starting point for the simulation in a distributed computing environment, to
ensure the result is the same as that derived from the serial C program or the CUDA program
on a single GPU.

5.1 Result of the Kriging Interpolation on Keeneland and Beacon

In the Kriging interpolation benchmark, the calculation is evenly partitioned among a given
number of MPI processes. Each MPI process will perform the interpolation calculation on a
number of rows in the 1,440 × 720 output grid. In this experiment, the value of an unsampled
location will be estimated using the values of the 10, 12, and 15 nearest sample points.
Since the computation in each MPI process is purely local, then there is no cross-processor
communication.

From Table 4 and Figure 4, it can be found that the hybrid implementation on the GPU
and MIC clusters can easily outperform the parallel implementation on CPU clusters. For this
embarrassingly parallel benchmark, the stream processing architecture on the NVIDIA GPU
works quite well and is able to outperform the parallel implementation on the Intel MIC,
which employs relatively low-performance cores.

5.2 Result of ISODATA on Keeneland and Beacon
The input of the ISODATA is a high-resolution image of 18 GB with a dimension of 80,000 ×
80,000 for three bands. The objective of this benchmark is to classify the image into n classes.
In this benchmark test, n = 15. In order to parallelize the computation, the whole image is par-
titioned into blocks of the same size. Each block is sent to a different processor.

The whole classiﬁcation process will go through many iterations until: (1) the number of
pixels that do not switch classes between two consecutive iterations is above a threshold
(i.e. 95%); or (2) the preset maximum number of iterations (i.e. 15) is reached. During each
iteration, each MPI process ﬁrst calculates the local means of 15 classes. Then all MPI pro-
cesses send their local means to the head MPI process (i.e. MPI rank = 0). After the head MPI
process collects all the local means, it calculates the global means for the 15 classes and returns
them to all other MPI processes. Then all the MPI processes start the computation of the next
iteration.

© 2014 John Wiley & Sons Ltd

Transactions in GIS, 2014, 18(S1)

Geocomputation over the Emerging Heterogeneous Computing Infrastructure

15

I

C
M
+
I
P
M

n
o
c
a
e
B

l
a
t
o
T

e
t
i
r

W

.

p
m
o
C

d
a
e
R

l
a
t
o
T

e
t
i
r

W

.

p
m
o
C

d
a
e
R

l
a
t
o
T

e
t
i
r

W

.

p
m
o
C

d
a
e
R

s
r
o
b
h
g
i
e
n

s
r
o
s
s
e
c
o
r
p

U
P
G
+
I
P
M

U
P
C
+
I
P
M

f
o

.

o
N

f
o

.

o
N

9
7
.
7
2

6
5
.
7
9
1

5
2
.
9
9
1

5
2
.
8
1

0
1
.
3
0
1

1
8
.
3
0
1

5
1
.
0
2

1
7
.
1
6
2

4
3
.
2
5
2

6
1
.
2

9
2
.
8

7
2
.
8

7
9
.
1

7
3
.
8

9
9
.
7

7
9
.
2
1

7
5
.
9
2
2

8
5
.
9
1
2

4
.
5
2

8
1
.
8
8
1

6
0
.
0
9
1

6
0
.
6
1

0
8
.
3
9

2
9
.
4
9

3
9
.
6

5
2
.
1
3

8
7
.
1
3

4
2
.
0

9
0
.
1

3
9
.
0

2
2
.
0

3
9
.
0

1
9
.
0

6
2
.
0

0
9
.
0

7
9
.
0

9
6
.
7

6
0
.
5
1

4
6
.
1
3

3
9
.
4

5
5
.
5
1

0
7
.
1
3

3
7
.
3

7
1
.
0
1

9
8
.
8
1

4
8
.
7

9
.
0

5
6
.
6
1

9
9
.
0

1
7
.
1
1

7
3
.
6
2

4
9
.
0

0
6
.
7

2
9
.
5
1

7
7
.
6

1
1
.
7

6
9
.
4
1

2
9
.
3

1
8
.
3

0
3
.
5

6
7
.
2

4
5
.
2

4
9
.
2

3
0
.
0

2
1
.
0

3
0
.
0

3
0
.
0

3
0
.
0

3
0
.
0

3
0
.
0

3
0
.
0

3
0
.
0

0
3
.
2
6
9

5
4
.
4
8
9

4
4
.
9
2
5

3
0
.
9
7
4

6
4
.
9
8
4

7
5
.
1
6
2

5
2
.
8
3
2

7
4
.
3
4
2

3
1
.
1

0
1
.
1

9
2
.
1

8
9
.
0

2
1
.
1

1
0
.
1

5
9
.
0

2
9
.
0

0
9
.
0

2
1
.
0
6
9

3
1
.
3
8
9

3
4
.
8
2
5

7
8
.
7
7
4

2
4
.
8
8
4

0
3
.
7
3
2

4
5
.
2
4
2

6
.
0
6
2

3
0
0

.

9
0

.

1

3
0

.

0

3
0
0

.

3
0

.

0

3
0

.

0

3
0
0

.

3
0

.

0

3
0

.

0

5
1
.
6
5
0
,
1

9
9
.
4
5
0
,
1

0
1

2
1

5
1

0
1

2
1

5
1

0
1

2
1

5
1

2

4

8

)
s
d
n
o
c
e
s
n

i

d
e
t
n
u
o
c

e
m

i
t
h
t
i

w

(
n
o
c
a
e
B
d
n
a
d
n
a
l
e
n
e
e
K
n
o
n
o
i
t
a
l
o
p
r
e
t
n

i

g
n
i
g
i
r
K

4

e
l
b
a
T

S
D
I
K
d
n
a
l
e
n
e
e
K

© 2014 John Wiley & Sons Ltd

Transactions in GIS, 2014, 18(S1)

16

X Shi, C Lai, M Huang and H You

1,000.00

)
s
d
n
o
c
e
s
(
 
e
m
T

i

100.00

10.00

1.00

MPI + CPU (10 neighbors)

MPI + CPU (12 neighbors)

MPI + CPU (15 neighbors)

MPI + GPU (10 neighbors)

MPI + GPU (12 neighbors)

MPI + GPU (15 neighbors)

MPI + MIC (10 neighbors)

MPI + MIC (12 neighbors)

MPI + MIC (15 neighbors)

2

4

8

Number of Processors

Figure 4 Performance chart of the computational time for the Kriging interpolation on different
conﬁgurations. GPU cluster seems to have better performance than CPU cluster and MIC cluster in
resolving embarrassingly parallel computing problems due to the massively parallel computing
nodes and threads in the GPU cluster

Table 5 ISODATA (15 classes) on Keeneland and Beacon (with time counted in seconds)

Keeneland KIDS

Beacon

MPI + CPU

MPI + GPU

MPI + MIC

No. of
processors

36
64
80
100
120

Read

Comp.

Total

Read

Comp.

Total

Read

Comp.

Total

6.04
12.6
15.03
1.29
0.98

232.22
140
129.72
81.31
81.34

238.26
152.59
144.74
82.59
82.39

3.91
3.51
21.11
36.35
22.29

100.26
59.18
47.12
33.81
33.95

62.69
68.24
70.16
56.24

104.17 N/A

41.27
27.01
32.32

58.75
50.01
40.08

100.02
77.02
72.40

The performance of the ISODATA benchmark on three different platforms is shown in
Table 5. When the MPI-native programming mode is used to implement this benchmark on
Beacon, it is found that this 18 GB image cannot be handled by 36 or 64 MIC processors. This
is due to the fact that there is a full software stack on each MIC core to support MPI commu-
nication, which consumes a lot of memory and leaves not much space for data. Therefore, we
allocate more MIC processors, i.e. 80, 100, and 120, on Beacon to implement ISODATA.

Figure 5 displays the performance comparison of various implementations of ISODATA
on Keeneland and Beacon. It can be found that the performance gap between the MIC pro-
cessors and GPUs becomes quite small when more GPUs and MICs are used for computa-
tion. Further, the implementation on Beacon keeps the scalability while the number of
processors increases. One reason is that the Fourteen Data Rate (FDR) InﬁniBand network
on Beacon provides much higher bandwidth than the Quad Data Rate (QDR) InﬁniBand
network on Keeneland KIDS, i.e. 56 Gb/s (FDR) vs. 32 Gb/s (QDR). The advantage of the
more efﬁcient communication network on Beacon is further demonstrated when the number
of participating processors is increased from 100 to 120. Such an advantage is further

© 2014 John Wiley & Sons Ltd

Transactions in GIS, 2014, 18(S1)

Geocomputation over the Emerging Heterogeneous Computing Infrastructure

17

MPI + CPU

MPI + GPU

MPI + MIC

)
s
d
n
o
c
e
s
(
 
e
m
T

i

250

200

150

100

50

0

36

64

80

100

120

Number of Processors

Figure 5 Performance chart of the computational time for ISODATA (15 classes) on different con-
ﬁgurations. The performance gap between the MIC processors and GPUs becomes quite small
when more GPUs and MICs are used for computation

Table 6 Classifying the image into ﬁve, 10, 15, and 20 classes on Keeneland and Beacon

Keeneland KIDS

Beacon

MPI + GPU

MPI + MIC

No. of
processors

No. of
Classes

80 GPUs vs. 80 MICs

100 GPUs vs. 100 MICs

Read

Comp.

Total

Read

Comp.

Total

2.56
2.97
2.63
2.62
54.09
74.11
36.35
62.46

11.47
36.39
77.60
102.05
9.49
32.88
33.81
82.19

14.02
39.37
80.23
104.67
63.58
107.00
70.16
144.65

27.62
15.37
41.27
41.79
36.10
41.99
27.01
22.66

11.71
39.07
58.75
90.00
9.14
31.52
50.01
72.04

39.34
54.44
100.02
131.79
45.24
73.51
77.02
94.70

5
10
15
20
5
10
15
20

observed in different scenarios when the image is classiﬁed into ﬁve, 10, 15, and 20 classes
using 100 and 120 GPUs on Keeneland and 80 and 100 MICs on Beacon. The result is illus-
trated in Table 6 and Figure 6. If we compare the classiﬁcation time used on classifying the
image into different classes, the performance is pretty close on Keeneland and Beacon.

5.3 Result of the Game of Life runs on Keeneland and Beacon

Game of Life (GOL) is a generic Cellular Automata program, in which the status of each cell is
dependent upon its eight neighbors. In this benchmark test, the simulation is implemented for
100, 150, and 200 iterations. For each iteration, the status of all cells is updated simulta-
neously. In order to parallelize the updating process, the cells in the n×n matrix grid are parti-
tioned into stripes in the row-wise order. Each stripe is handled by one MPI process. At the

© 2014 John Wiley & Sons Ltd

Transactions in GIS, 2014, 18(S1)

18

X Shi, C Lai, M Huang and H You

Figure 6 Performance chart of the computational time for classifying the image into ﬁve, 10, 15, and
20 classes on Keeneland and Beacon. The performance gap between the MIC processors and GPUs
becomes quite small when the image is classiﬁed into ﬁve or 10 classes. The performance on
Beacon seems not stable and consistent in this study when the image is classiﬁed into 15 classes

beginning of each iteration, the status of the cells along the boundaries of each stripe has to be
exchanged with its neighbor through the MPI send and receive command. In the case of
running the Game of Life in GPU clusters on Keeneland, applying atomicAdd and shared
memory techniques can further improve the performance.

GOL is tested by three different grid sizes of 8,192 × 8,192, 16,384 × 16,384, and 32,768
× 32,768. By observing the performance results in Table 7, it can be found that the strong scal-
ability is demonstrated for all three cases. The exception happens when 16 MIC processors are
allocated in the implementation as the performance of 16 MIC processors is worse than the
performance of 8 MIC processors. This is due to the fact that the grid is partitioned into 960
MPI processes on 16 MIC processors. At this scale, the performance gain from the reduced
workload on each MIC core is offset by the increase of the communication cost. For this
benchmark, the K20 can consistently outperform the M2090 by ∼4 times speedup based on
the results in Table 2.

The performance comparison of various implementations of Game of Life is shown in
Figure 7. The FDR Inﬁniband network on Beacon clearly demonstrates its advantage for this
intensive communication benchmark. Because the FDR provides the higher bandwidth than
the QDR, it takes less time to communicate on the Beacon cluster than on the Keeneland
cluster. As a result, the implementation on MIC clusters outperforms the implementations on
GPU clusters for most cases. This is because for each iteration, the status of the cells along the
boundaries of each stripe has to be exchanged with its neighbor through the MPI send and
receive command. When GPUs are utilized in this case, data has to be copied back and forth
between the CPUs and GPUs over distributed computing nodes. In order to explore the poten-
tial of whether the hybrid implementation with GPU could outperform the implementation on
the MIC processors, the direct GPU communication capability of Kepler GPUs would be
worth investigating and validating.

© 2014 John Wiley & Sons Ltd

Transactions in GIS, 2014, 18(S1)

Geocomputation over the Emerging Heterogeneous Computing Infrastructure

19

+
I
P
M

I

C
M

3
9
.
6
1
1

3
5
.
3
1
2

7
8
.
8
8
2

7
5
.
8
5

9
2
.
7
0
1

2
2
.
1
4
1

8
9
.
0
3

0
0
.
4
5

6
6
.
0
7

6
1
.
7
1

1
9
.
6
2

0
5
.
5
3

+
I
P
M

U
P
G

8
5
.
3
8
4

2
4
.
5
4
7

8
0
.
1
1
0
1

3
4
.
2
4
2

2
9
.
6
4
5

1
0
.
5
2
7

1
3
.
8
1
1

1
8
.
4
7
2

7
2
.
2
6
3

3
8
.
3
6

6
5
.
8
3
1

7
3
.
1
8
1

+
I
P
M

U
P
C

+
I
P
M

I

C
M

9
1
.
2
4
2
1

2
3
.
7
2
5
1

9
8
.
5
9
9
1

2
9
.
5
2
6

6
4
.
1
7
7

7
1
.
2
1
0
1

4
3
.
1
1
3

7
0
.
2
9
3

5
8
.
5
0
5

5
0
.
9
5
1

1
9
.
1
0
2

3
2
.
8
5
2

6
9
.
8
2

0
9
.
2
5

9
8
.
9
6

5
6
.
4
1

4
1
.
6
2

4
8
.
4
3

7
4
.
8

8
2
.
3
1

6
6
.
7
1

6
5
.
6

0
7
.
9

4
8
.
2
1

+
I
P
M

U
P
G

9
1
.
2
2
1

7
3
.
2
7
1

3
6
.
0
5
2

4
1
.
9
5

2
5
.
4
3
1

9
9
.
7
7
1

6
6
.
9
2

0
2
.
8
6

6
7
.
9
8

2
.
7
1

5
6
.
4
3

0
3
.
6
4

+
I
P
M

U
P
C

9
6
.
2
1
3

3
3
.
1
8
3

9
3
.
4
9
4

4
6
.
5
5
1

2
6
.
2
9
1

8
0
.
9
4
2

4
1
.
8
7

4
7
.
7
9

0
4
.
4
2
1

5
3
.
9
3

0
0
.
0
5

9
8
.
2
6

+
I
P
M

I

C
M

3
3
.
7

3
2
.
3
1

0
4
.
7
1

7
2
.
4

4
6
.
6

1
7
.
8

0
3
.
3

2
9
.
4

0
5
.
6

5
7
.
3

3
6
.
5

5
4
.
7

+
I
P
M

U
P
G

2
9
.
4
2

2
6
.
6
3

3
2
.
0
5

9
7
.
2
1

2
8
.
4
2

7
3
.
2
3

3
.
6

7
0
.
3
1

1
7
.
6
1

5
1
.
4

1
2
.
7

2
1
.
9

+
I
P
M

U
P
C

5
1

.

8
7

1
0

.

5
9

2
0

.

4
2
1

3
1

.

8
4

1
3

.

2
6

2
8

.

1
2

5
3

.

4
2

6
6

.

1
3

1
4

.

0
1

4
4

.

2
1

4
7

.

5
1

2

.

9
3

8
6
7
,
2
3

x

8
6
7
,
2
3

4
8
3
,
6
1

x

4
8
3
,
6
1

2
9
1
8

,

x

2
9
1
8

,

s
n
o
i
t
a
r
e
t
i

s
r
o
s
s
e
c
o
r
p

f
o

.

o
N

f
o

.

o
N

0
0
1

0
5
1

0
0
2

0
0
1

0
5
1

0
0
2

0
0
1

0
5
1

0
0
2

0
0
1

0
5
1

0
0
2

2

4

8

6
1

)
s
d
n
o
c
e
s
n

i

d
e
t
n
u
o
c

e
m

i
t
h
t
i

w

(
n
o
c
a
e
B
d
n
a
d
n
a
l
e
n
e
e
K
n
o
e
f
i
L

f
o
e
m
a
G

7

e
l
b
a
T

© 2014 John Wiley & Sons Ltd

Transactions in GIS, 2014, 18(S1)

20

X Shi, C Lai, M Huang and H You

Figure 7 Performance chart of Game of Life on different conﬁgurations. In all cases over different
sizes of data, MIC clusters on Beacon achieved better performance than GPU clusters on Keeneland

© 2014 John Wiley & Sons Ltd

Transactions in GIS, 2014, 18(S1)

Geocomputation over the Emerging Heterogeneous Computing Infrastructure

21

6 Conclusions

While many geospatial applications are data and computation intensive, geocomputation over
the emerging heterogeneous computing infrastructure could be a promising solution to the big
data challenge. Understanding the main features of the emerging advanced computer architec-
tures and systems will help efﬁciently deploy the latest computing infrastructure and technol-
ogy to advance GIScience research. Through this pilot study, three categories of geospatial
computation benchmarks were implemented on CPU clusters, GPU clusters, and MIC clusters.
The conclusions derived from this research could be beneﬁcial to GIScientists who will build
other geospatial applications with similar characteristics.

It seems that GPU clusters would have exempliﬁed the advantage in the category of embarrass-
ingly parallelism. This is reasonable, since GPU clusters could have sufﬁcient and more computing
threads to perform the calculation when the scale of data and computation could be within the
memory limit. The latest generations of GPUs may have more memory on each GPU card, such as
Tesla M2090 and K20/K20X that have up to 6 GB of memory. When the large data and the com-
putation problem can be partitioned into multiple GPUs, the GPU cluster could have more poten-
tial to accelerate geospatial applications to achieve signiﬁcant performance improvement compared
with the traditional MPI + CPU parallel implementation as well as single-CPU implementations.

In the case of geospatial computation that has simple communication between the distrib-
uted computing nodes, especially when larger data are involved in iterative computation pro-
cedures, the simple MPI-native programming model on the Intel MIC cluster can achieve a
performance equivalent to the MPI + GPU model on GPU clusters when the same number of
processors are allocated. It is implied that an efﬁcient cross-node communication network will
be the key to achieve the strong scalability for parallel applications running on multiple nodes.
In the case of geospatial computation that has intensive data exchange and communication
between the distributed computing nodes, it seems that the simple MPI-native programming model
on the Intel MIC cluster can achieve better performance than the MPI + GPU model on GPU clus-
ters when the same number of processors is allocated. This is because data has to be copied back
and forth between the host CPUs and the GPUs. When the spatial computation and simulation
have to be implemented for a lot of iterations, the communication overhead between CPUs and
GPUs could be prominent.

While NVIDIA’s latest Kepler GPU (K20) is able to outperform its Fermi GPU for most appli-
cations without special performance tuning, K20’s capability of direct cross-GPU communication
may have the potential for GPU to outperform Intel’s MIC processor when dealing with
communication-intensive applications. On the Intel MIC architecture, although the direct support
of MPI on each MIC core makes it straightforward to port MPI + CPU code to the MIC cluster
while achieving signiﬁcant performance improvement, a large amount of on-board memory is used
for OS and MPI support. For this reason, a detailed comparison between the ofﬂoad model and
native model
is worthwhile. Both the direct cross-GPU communication mechanism and the
ofﬂoading model of MIC will be future research directions, while other types of geospatial compu-
tation should be explored and examined in such emerging heterogeneous computing infrastructure.

Appendix

1. URLs of system conﬁguration and access of Keeneland and Beacon:

Beacon: http://www.nics.tennessee.edu/beacon
Keeneland: http://keeneland.gatech.edu/kids-quick-start

© 2014 John Wiley & Sons Ltd

Transactions in GIS, 2014, 18(S1)

22

X Shi, C Lai, M Huang and H You

2. Sample Make ﬁle and PBS script for the CA benchmark on Keeneland using 16 GPUs

Make ﬁle:

PBS script:

#!/bin/bash
nvcc -c kernel.cu
mpicc -o main main.c kernel.o -lcudart -limf -lm -L /sw/keeneland/cuda/4.0/linux
_binary/lib64 -I /sw/keeneland/cuda/4.0/linux_binary/include/
#!/bin/sh
#PBS -N xshi
#PBS -j oe
#PBS -A UT-NTNL0102
#PBS -M xuanshi@uark.edu
#PBS -l nodes=8:ppn=2:gpus=2,walltime=00:30:00

date
cd $PBS_O_WORKDIR

echo “nodeﬁle=”
cat $PBS_NODEFILE
echo “=end nodeﬁle”

#which mpirun
mpirun -np 16 ./main

date

# eof

3. Sample Make ﬁle and shell script for the CA benchmark on Beacon using 16 MICs

Make ﬁle:

CC = mpiicc -mmic

MIC_PROG = kernel
MPI_PROG = main.c

ca_test : main.o kernel.o

$(CC) main.o $(MIC_PROG).o -o ca_32768_200

main.o: $(MPI_PROG)

$(CC) -c $(MPI_PROG) -o main.o

$(MIC_PROG).o: $(MIC_PROG).cpp

$(CC) -c $(MIC_PROG).cpp -o $(MIC_PROG).o

clean:

rm *.o

Shell script:

generate-mic-hostlist micnative 60 > machines
allmicput -t ca_32768_200
time micmpiexec -genvlist
LD_LIBRARY_PATH=/global/opt/intel/impi/4.1.0.024/mic/lib:$LD_LIBRARY_PATH
-machineﬁle machines -n 960 $TMPDIR/ca_32768_200 $PBS_O_WORKDIR/32768 >
200-960mic-32768.log

© 2014 John Wiley & Sons Ltd

Transactions in GIS, 2014, 18(S1)

Geocomputation over the Emerging Heterogeneous Computing Infrastructure

23

References

Aaby B G, Perumalla K S, and Seal S K 2010 Efﬁcient simulation of agent-based models on multi-GPU and
multi-core clusters. In Proceedings of the Third International ICST Conference on Simulation Tools and
Techniques, Torremolinos, Malaga, Spain

Beacon 2014 Beacon. WWW document, http://www.nics.tennessee.edu/beacon
Bernabe S, Lopez S, Plaza A, and Sarmiento R 2013a GPU implementation of an automatic target detection and
classiﬁcation algorithm for hyperspectral image analysis. IEEE Geoscience and Remote Sensing Letters 10:
221–5

Bernabe S, Sanchez S, Plaza A, Lopez S, Benediktsson J A, and Sarmiento R 2013b Hyperspectral unmixing on
GPUs and multi-core processors: A comparison. IEEE Journal of Selected Topics in Applied Earth Observa-
tions and Remote Sensing 6: 1386–98

Cheng T, Zhang Y, Li D, and Wang Q 2010 A component-based design and implementation for parallel kriging
library. In Proceedings of the Second International Conference on Information Science and Engineering,
Nanjing, China

Clarke K C 2003 Geocomputation’s future at the extremes: High performance computing and nanoclients.

Parallel Computing 29: 1281–95

Cluster File Systems 2002 Lustre: A Scalable, High Performance File System. WWW document, http://

www.cse.buffalo.edu/faculty/tkosar/cse710/papers/lustre-whitepaper.pdf

Crimi G, Mantovani F, Pivanti M,Schifano S F, and Tripiccione R 2013 Early experience on porting and

running a lattice Boltzmann code on the Xeon-phi co-processor. Procedia Computer Science 18: 551–60

Gardner M 1970 Mathematical games: The fantastic combinations of John Conway’s new solitaire game “life”.

Scientiﬁc American 223: 120–3

Emeliyanenko P 2013 Computing resultants on graphics processing units: Towards GPU accelerated computer

algebra. Journal of Parallel and Distributed Computing 73: 1494–505

FEMA 2013 Limitations of the HAZUS-MH 2.0 Software. WWW document, http://www.fema.gov/media-

library-data/20130726-1759-25045-8653/hazus2_limitations.pdf

Heinecke A, Vaidyanathan K, Smelyanskiy M, Kobotov A, Dubtsov R, Henry G, Shet A G, Chrysos G, and
Dubey P 2013 Design and implementation of the linpack benchmark for single and multi-node systems
based on Intel Xeon phi coprocessor. In Proceedings of the Twenty-seventh IEEE International Symposium
on Parallel and Distributed Processing, Boston, Massachusetts: 126–37

Huang M, Men L, and Gauch J 2013 Accelerating mean shift segmentation algorithm on hybrid CPU/GPU plat-
forms. In Shi X; Kindratenko V, and Yang C (eds) Modern Accelerator Technologies for Geographic Infor-
mation Science. Berlin, Springer: 157–68

Jeffers J and Reinders J 2013 Intel Xeon Phi Coprocessor High Performance Programming. Oxford, UK, Elsevier
Jenson J R 1999 Introductory Digital Image Processing: A Remote Sensing Perspective (Second Edition).

Englewood Cliffs, NJ, Prentice-Hall

Kalyanapu A J, Shankar S, Pardyjak E R, Judi D R, and Burian S J 2011 Assessment of GPU computational

enhancement to a 2D ﬂood model. Environmental Modelling and Software 26: 1009–16

Keeneland 2014 Keeneland: National

Institute for Experimental Computing. WWW document, http://

keeneland.gatech.edu/

Li J, Humphrey M, Agarwal D, Jackson K, and van Ingen C, and Ryu Y 2010 eScience in the cloud: A MODIS
satellite data re-projection and reduction pipeline in the Windows Azure platform. In Proceedings of the
IEEE International Parallel and Distributed Processing Symposium, Atlanta, Georgia

NCSA 2009 MAEviz Building Damage Tutorial. WWW document, https://wiki.ncsa.illinois.edu/display/MAE/

MAEviz+Building+Damage+Tutorial

Oliver M A and Webster R 1990 Kriging: A method of interpolation for geographical information systems.

International Journal of Geographical Information Science 4: 313–32

Preis T, Virnau P, Paul W, and Schneider J J 2009 GPU accelerated Monte Carlo simulation of the 2D and 3D

Ising model. Journal of Computational Physics 228: 4468–77

Schmidl D, Cramer T, Wienke S, Terboven C, and Müller M S 2013 Assessing the performance of OpenMP pro-
grams on the Intel Xeon Phi. In Wolf F, Mohr B, and Mey D (eds) Euro-Par 2013: Parallel Processing.
Berlin, Springer Lecture Notes in Computer Science Vol. 8097: 547–58

Shi X and Ye F 2013 Kriging interpolation over heterogeneous computer architectures and systems. GIScience

Simion B, Ray S, and Brown A D 2012 Speeding up spatial database query execution using GPUs. Procedia

Steinbach M and Hemmerling R 2012 Accelerating batch processing of spatial raster analysis using GPU. Com-

and Remote Sensing 50: 196–211

Computer Science 9: 1870–79

puters and Geosciences 45: 212–20

© 2014 John Wiley & Sons Ltd

Transactions in GIS, 2014, 18(S1)

24

X Shi, C Lai, M Huang and H You

Srinivasan B V, Duraiswami R, and Murtugudde R 2010 Efﬁcient kriging for real-time spatio-temporal interpo-
lation. In Proceedings of the Twentieth Conference on Probability and Statistics in the Atmospheric Sci-
ences, Atlanta, Georgia

Tang W and Bennett D A 2012 Parallel agent-based modeling of spatial opinion diffusion accelerated using

graphics processing units. Ecological Modelling 229: 108–18

Ye F and Shi X 2013 Parallelizing ISODATA algorithm for unsupervised image classiﬁcation on GPU. In Shi X,
Kindratenko Y, and Yang C (eds) Modern Accelerator Technologies for Geographic Information Science.
Berlin, Springer: 145–56

Ye F, Shi X, Wang S, Liu Y, and Han S Y 2011 Spherical interpolation over graphic processing units. In Pro-
ceedings of the Second International Workshop on High Performance and Distributed Geographic Infor-
mation Systems, Chicago, Illinois: 38–41

Zhang J, You S, and Gruenwald L 2010 Indexing large-scale raster geospatial data using massively parallel
GPGPU computing. In Proceedings of the Eighteenth SIGSPATIAL International Conference on Advances
in Geographic Information Systems, San Jose, California: 450–3

Zhang J, You S, and Gruenwald L 2011 Parallel quadtree coding of large-scale raster geospatial data on
GPGPUs. In Proceedings of the Nineteenth ACM SIGSPATIAL International Conference on Advances in
Geographic Information Systems, Chicago, Illinois: 457–60

Zhao Y, Padmanabhan A, and Wang S 2013 A parallel computing approach to viewshed analysis of large
terrain data using graphics processing units. International Journal of Geographical Information Science 27:
363–84

© 2014 John Wiley & Sons Ltd

Transactions in GIS, 2014, 18(S1)

Copyright of Transactions in GIS is the property of Wiley-Blackwell and its content may not
be copied or emailed to multiple sites or posted to a listserv without the copyright holder's
express written permission. However, users may print, download, or email articles for
individual use.

