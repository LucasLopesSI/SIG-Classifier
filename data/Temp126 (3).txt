Transactions in GIS, 2000, 4(1): 43–64

Research Article

Improving the Elevation Accuracy of
Digital Elevation Models: A Comparison
of Some Error Detection Procedures

Carlos Lo´ pez
Centro de Ca´lculo, Facultad de Ingenierı´a (11), Universidad de la Repu´blica
Montevideo, Uruguay

Abstract
The widespread availability of powerful desktop computers, easy-to-use software
tools and geographic datasets have raised the quality problem of input data to be a
crucial one. Even though accuracy has been a concern in every serious application,
there are no general tools for its improvement. Some particular ones exist, however,
and some results are presented here for a particular case of quantitative raster data:
Digital Elevation Models (DEM). Two procedures designed to detect anomalous
values (also named gross errors, outliers or blunders) in DEMs, but valid also for
other quantitative raster datasets, were tested. A DEM with elevations varying from
181 to 1044 m derived from SPOT data has been used as a contaminated sample,
while a manually derived DEM obtained from aerial photogrammetry was regarded
as the ground truth to allow a direct performance comparison for the methods with
real errors. It is assumed that a ‘‘better’’ value can be measured or obtained through
some methodology once an outlier location is suggested. The options are different
depending upon the user (DEM producers might go to the original data and make
another reading, while end users might use interpolation). Both choices were
considered in this experiment. Preliminary results show that for the available dataset,
the accuracy might be improved to some extent with very little effort. Effort is
defined here as the percentage of points suggested by the methodology in relation
with its total number: thus 100 per cent effort implies that all points have been
checked. The method proposed by Lo´ pez (1997) gave poor results, because it has
been designed for errors with low spatial autocorrelation (which is not the case here).
A modified version was then designed and compared with the method proposed by
Felicı´ simo (1994). The three procedures can be applied both for error detection
during DEM generation and by end users, and they might be of use for other

Address for correspondence: Carlos Lo´ pez, Centro de Ca´ lculo, Facultad de Ingenierı´ a (11),
Universidad de la Repu´ blica, Julio Herrera y Reissig 565, Montevideo, Uruguay. Email:
carlos@fing.edu.uy

(cid:223) 2000 Blackwell Publishers, 108 Cowley Road, Oxford OX4 1JF, UK and
350 Main Street, Malden, MA 02148, USA.

44

Carlos Lo´pez

1 Introduction

quantitative raster data. The choice of the best methodology is different depending
on the effort involved. The conclusions have been derived for a photogrammetrically
obtained DEM; other production procedures might lead to different results.

Geographic Information Systems (GIS) is one of the fastest growing markets in
software today (Anon 1994). That implies that more people have access to proper tools,
and then are able to manipulate and produce data. Data availability will be assured in
the future, through the operation of the so-called Clearinghouses, which will distribute
existing datasets to government, industry and the general public (Nebert 1995, 1996).
The combination of widespread data and ready made, easy to use software raises
some critical points. John (1993) stated that ‘‘. . .very wrong answers can be derived
using perfectly logical GIS analysis techniques, if the users are not aware of the
particular peculiarities of data. . .’’. Data quality is likely to emerge as one of the most
important issues in GIS technology in the next decade. Its management requires
methods to describe, visualize and measure it properly (see Hunter and Goodchild
1996). Standards to describe data quality are presently under development.

Thapa and Bossler (1992) remarked that when setting up a GIS, most of the costs
(maybe up to 80 per cent) are related to the acquisition and/or collection of data. Once
the dataset is obtained further efforts to improve accuracy should be as effective as
possible. This paper reports some results on that subject affecting Digital Elevation
Models (DEMs). The errors in the final product as opposed to those produced in the
process of DEM generation are examined. According to Thapa et al (1992) errors can
be classified into three types: (1) gross errors and blunders, (2) systematic errors and (3)
random errors. Gross errors and blunders are caused by carelessness or inattention of
the observer in using equipment, reading scales or writing down readings, etc. They can
also be caused by malfunctioning of the equipment. Observations affected by this kind
of errors are useless, and should be eliminated. From a statistical point of view they
cannot be considered as belonging to the same population as the other observations.
This paper deals with these types of errors. Systematic errors occur in accordance with
some deterministic system which, if known, may be represented by some functional
relationship. In a statistical sense, systematic errors introduce bias in the observations,
which may not be constant in space. They might arise from improper orientation of the
stereomodel or by neglecting lens distortion in photogrammetric procedures. Unlike
gross errors, they cannot be detected or eliminated by repeated observations (the errors
may be precise, but they will not be accurate). After removal of gross and systematic
errors, differences still exist due to random errors. They cannot be removed by
repeated observation, and they cannot be modeled with a deterministic relationship. If
sufficient observations are taken, random errors pose the following characteristics: (1)
positive and negative errors occur with almost the same frequency, (2) small errors
occur more often than large errors, and (3) large errors rarely occur. They are usually
assumed to belong to a gaussian distribution, with parameters valid for the whole
DEM, and with negligible autocorrelation in space.

O¨ stman (1987a) pointed out that there exists no unique criteria or single measure
for the ‘‘quality’’ of a DEM. He suggested that one should at least, consider accuracy in
elevation, slope and also curvature. However, accuracy reports in terms of slope are

(cid:223) Blackwell Publishers Ltd. 2000

Improving the Elevation Accuracy of DEMs

45

very unusual. An exception can be found in the work of Giles and Franklin (1996) who
compared a 20 m resolution DEM derived from SPOT images with field measurements
in terms of slope. They recognized that the elevation error might have two components
at different scales. To filter out the small scale error they simply applied a 3 by 3
median filter and to remove the larger errors they used a 11 by 11 window, with a
different filter. They claimed that such filtering improved the accuracy in slope without
significantly degrading the accuracy in elevation. This result was also reported by
O¨ stman (1987a) who found that the RMS error in elevation decreases with decreasing
grid size (as expected) but the effect on the RMS error for slope is very limited. Fo¨ rstner
(1983) gave theoretical arguments for this fact.

Accuracy of photogrammetrically sampled DEM depends on the data sources and
procedures involved. It has been a considered an important problem which led to
collective efforts like the one summarized by Torlega˚ rd et al (1986). They examined
DEMs derived independently by several organizations working on the same set of
aerial photographs. Six different terrain types have been chosen, ranging from smooth
terrain to steep and rugged mountains. They found that the errors of the elevations in
photogrammetrically measured DEMs consist
systematic
components. Regarding gross error location, they applied a ‘‘rule of thumb’’ based
on recursive filtering using a 5 by 5 window, and declared that everything which was
located in this way is an error. They concluded that the number of those so defined
gross errors typically varies between 0 and 3 per cent. They noticed that error size is
independent of terrain type and that errors are more frequent in difficult terrain.

to a large extent of

A similar (deterministic) approach was used in an early paper by Hannah (1981),
who detected errors by applying constraints to the slopes and to the changes in slope at
each point. Felicı´ simo (1994) analyzed the differences between the elevation and an
interpolated value from the neighbors. Assuming a gaussian distribution of the errors,
he analyzed the differences by means of a standard Student t test.

Day and Muller (1988) tested three methods for the generation of DEMs based on
SPOT data. The three results were compared with a very carefully, manually digitized
30 m grid DEM in terms of elevation differences. Even though the goal of the work was
to compare the operational behavior of the algorithms, they do not propose a solution
for the location of the errors. The distribution function of the absolute size of such
errors is also presented for each method. They also reported how many checkpoints lie
outside the limit |error-(cid:22)|>3(cid:27).

Any method for locating errors should make assumptions about size, location and
spatial auto correlation. Bethel and Mikhail (1984) proposed the method of maximum
chi-squared ratio for on line quality control, and tested the methodology using
uncorrelated, spike-like blunders of no more than 10 feet (about 3 m). Lo´ pez (1997)
used two error models: one uncorrelated in space (spike-like blunders) and another that
was weakly correlated (pyramid-like) in space.

In the field of Image Processing the term salt-and-pepper has been coined for
weakly autocorrelated errors. They are routinely corrected using filters. The most
popular and simple one is the median filter (Mitra and Yu 1994) but it has the
fundamental inconvenience that it smoothes out all of the DEM; current efforts are
directed towards a division of the problem: to separate error detection from error
correction, and to use state variables for error detection (Abreu et al 1996).

This paper will not discuss the methods for obtaining the DEM itself. There are
well established procedures based on photogrammetry, GPS, etc. However, if the

(cid:223) Blackwell Publishers Ltd. 2000

46

Carlos Lo´pez

equipment or the methods are at their limit today, there will be little chance to improve
the final results by merely pointing out some locations likely to be in error. Fortunately,
this is not the case. Ackermann (1995) points out that the trend in DEM production is
from interpolation to approximation, because the new generation equipment is able to
produce many elevation values, but possibly with less accuracy than traditional
equipment. The surface is approximated using many points,
instead of being
interpolated from a few, very carefully obtained values. Summing up, accuracy is a
concern for the data producer as well as for the end user. Accuracy is usually described
using different statistics of the distribution of elevation error at some checkpoints.

This paper presents test results of some recently proposed methodologies for
locating errors which can be applied both by the producer and the end user. The
methods were tested in a DEM with real errors, and the results are presented. Some
guidelines for the error model for this particular case are presented as well. The paper
is organized in seven sections. Section 2 has a brief outline of both the existing
techniques and the modified technique. Section 3 describes the data and summarizes
some of its statistics. In section 4 the performance of the three methods is compared for
the test DEM under a perfect inspector assumption, while section 5 covers the
imperfect case. Finally, section 6 discusses the broader significance of the results and
section 7 is devoted to conclusions and proposed future work.

2 The Error Detection Procedures in Brief

The methods of Felicı´ simo (1994) and Lo´ pez (1997), and a modification of the latter
method are described in separate sections below.

2.1 The Method of Felicı´simo (1994)

This is the simplest method available for this problem. Assuming that outliers are only
locally correlated, the method analyzes the differences (cid:14)i;j between the elevation value
zi;j and an interpolated guess ^zi;j obtained from its immediate neighbors. Assuming that
the difference has a Gaussian distribution with mean (cid:14) and standard deviation s(cid:14) (both
obtained from the sample) a Student’s t test can be applied to validate the hypothesis
that (cid:14)i;j belongs to the population of deviations. Operationally, we analyze the statistics
ti;j (cid:136) (cid:133)(cid:14)i;j (cid:255) (cid:14)(cid:134)=s(cid:14) which can be regarded as a standardized deviation. Since the number
of data points are usually large, we can assume a distribution t(cid:11)(cid:137)1(cid:138) for ti;j. For
(cid:11) (cid:136) 0:001, the statistical t(cid:11)(cid:137)1(cid:138) has a value of 3.219 for a two-tail test, where the null
hypothesis is H0; (cid:14)i;j (cid:136) (cid:14) and the alternative is H1; (cid:14)i;j 6(cid:136) (cid:14).

The estimated height ^zi;j can be obtained by any weighting scheme and any
definition of neighborhood. A best fit approximation with a biquadratic polynomial
based on the eight closest neighbors was used here. Along the borders a mirror
symmetry was assumed, and in the corners a linear interpolation was performed with
the three closest values available. Any (cid:14)i;j that makes jti;jj > 3.219 was treated as a
candidate to be in error . The author states that even though a significantly high value
of ti;j does not necessarily imply an error, it is an excellent alarm sign. This topic is
examined in more detail later.

Once an error is located and corrected, both statistics (cid:14) and s(cid:14) change and new
candidates appear. The method can be iterated and it might stop if no more ‘‘outlying’’

(cid:223) Blackwell Publishers Ltd. 2000

Improving the Elevation Accuracy of DEMs

47

values remain. This is undesirable because there may still be errors in the dataset, so the
limit is lowered from 3.219 to 3.0 at least once. The new candidates once corrected
modify the statistics, and new candidates with the limit 3.219 appear. Notice that this
process is not equivalent to merely evaluating once ti;j, and sorting the values; the
statistics (cid:14) and s(cid:14) are usually severely affected by gross errors, so ti;j should be re-
evaluated after each step.

The method appears to be extremely simple and is parameter free (in the sense that
the user need not to specify any parameter). In section 3 the test DEM is examined to
see whether or not it fulfils the assumptions under which Felicı´ simo’s method can be
applied (Gaussian distribution, low spatial autocorrelation, etc). The relationship of ti,j
and real errors (available in this experiment) is also presented.

2.2 The Method of Lo´ pez (1997)

The author proposed that any given raster dataset can be analyzed by means of
subdividing it into elongated strips (Figure 1). Each strip is assumed to have length n
and width w (w << n). The method considers the strip as a set of points in the Rw
space. Each cross-section is represented by a point, where the original elevation values
establish the w coordinates. The case of w (cid:136) 3 is illustrated in Figure 2, where each
point Mk represents a cross-section.

The error location procedure directly analyzes the cloud of points in Rw,
disregarding any order among points. This is an important assumption, since the
concept of spatial autocorrelation loses completely all significance in the cloud.
Adjacent profiles (of length n) need not be in any special order, since they are coordinate
axes in the space Rw. The use of the cloud is common practice in statistics (Hadi 1992,
1994; Hawkins 1974, 1993a, 1993b), since the notion of ‘‘spatial auto correlation’’ and
‘‘precedence’’ is meaningless in most tabular data.

The procedure is based upon Principal Component Analysis (PCA), which
attempts to find the direction e1 of the vector in Rw space which minimizes S, defined as

Figure 1

Sketch of the strip notation.

(cid:223) Blackwell Publishers Ltd. 2000

48

Carlos Lo´pez

Figure 2

Sketch of the first principal component, for w (cid:136) 3.

the sum of distances Mk-Hk squared taken over all k (Figure 2). The origin O is the
centroid of the set of points. For the sake of clarity, points with negative coordinates
are not shown in Figure 2.

The projection OHk, which is also the scalar product of vector Mk-O with the
unitary vector e1, is called the score (after Richman 1986). Thus Mk-Hk is orthogonal
to e1. There is one score value associated with vector e1 for each point in Rw. It is also
assumed that e1 is unique.

If all the values MkHk are zero, the problem of original dimension w has been
reduced to a one-dimensional one. All the variability in the observations is explained by
a single vector e1. If this is not the case, the procedure may be repeated with the
remaining variability MkHk, which belongs to a (w(cid:255)1) subspace of Rw orthogonal to
e1. The original measurements Mk(cid:255)O can be replaced with the difference OMk(cid:255)OHk,
which is equal to Mk(cid:255) Hk.

For the new cloud there should be a vector e2 (orthogonal to e1) which minimizes
the distance S in the Rw space. The process continues until w vectors ep have been
created; each new vector ep being orthogonal to all the previous ones. The vectors ep
are called principal components (PC). Each event Mk - O can be expressed as a linear
combination of the PC’s

Mk (cid:255) O (cid:136) a1(cid:133)k(cid:134)(cid:3)e1 (cid:135) a2(cid:133)k(cid:134)(cid:3)e2 (cid:135) a3(cid:133)k(cid:134)(cid:3)e3 (cid:135) . . . (cid:135) aw(cid:133)k(cid:134)(cid:3)ew

(cid:133)1(cid:134)

It can be shown that the scores ai associated with vector ei are uncorrelated with
those of vector ej. The vectors ei are the eigenvectors of the covariance matrix of the
data, and its components are named loadings in the literature. The sum of the
corresponding eigenvalues equals the sum of the squares of the distances MkHk (Lebart
et al 1987).

PCA analysis generates a sequence of principal components, which explains most
(or all, for p (cid:136) w) of the variance of the data. This implies that the RMS error in
approximating the data with a linear combination of their first p vectors is a minimum
for a given p < w; (p (cid:136) 1 in Figure 2). It has been shown that in most cases a good
approximation of data is achieved for p << w. Since the w PC’s form a basis in Rw
space, they can replicate exactly any of the n points in the set, using the scores as
weights. Lo´ pez (1997) claimed that some of the scores contain essential information on
the structure of the cloud, while others are more related to noise. Following Hawkins
(1974) he suggested a rule to identify the noisy scores. Once identified, such scores were
used to pinpoint those points in Rw space which are prone to hold an error.

(cid:223) Blackwell Publishers Ltd. 2000

Improving the Elevation Accuracy of DEMs

49

However, this is not the complete answer to the problem because each point
depends on w elevation values. Which one is wrong? Once a point in Rw space is
selected, the elevation (or elevations) which make it unusual should be indicated. This
is accomplished using a weighted sum of the squared scores which are related to noise
(Hawkins 1974). It is a semi-distance, closely related to the Mahalanobis distance. Its
sensitivity in terms of the elevation values is calculated and those elevations which
generate the most important contribution to the distance value are treated as errors.
The calculations are carried out independently for each outlying point in the Rw space.
The procedure presented above can be used to find an error in a single strip. The
method can be applied to all row-wise strips to cover the entire DEM. The candidates
obtained can be grouped and designated as row-wise candidates. However, the same
procedure can be applied to column-wise strips, and a different set of column-wise
candidates can be obtained.

The candidates belonging to both sets (row-wise and column-wise) represent the
final result. The procedure can be applied iteratively, since, once an error is detected
and ‘‘corrected’’, the cloud is modified to some extent, and so are the scores. Each
iteration will be referred to as a ‘‘step’’ in the following discussion. We keep track of
the point already checked in order to avoid selecting them twice; we form the candidate
set as the intersection of all previous row-wise candidates and all previous column-wise
candidates.

The procedure involves five actions, and it can be outlined as follows:

Given a DEM as a matrix of size m(cid:3)n
subdivide the DEM in row-wise and column-wise strips of width w
repeat until criteria are satisfied:

a) increment the previous row-wise candidate set:

a.1.- locate the columns likely to have candidates
a.2.- within each column, find the rows that identify the candidates

b) increment the previous column-wise candidate set:

b.1.-locate the rows likely to have candidates
b.2.- within each row, find the columns that identify the candidates

c) intersect both sets
d) evaluate criteria
e) correct all errors

end

Some remarks follow. In the pseudo code, a single strip of width w was used for
rows and columns. This simplifies the tuning process, as will be shown later. The
process is supposed to stop when some criterion is fulfilled. Lo´ pez (1997) suggested
stopping if the type I error is too big (defined as the probability of missclassifying a
good value as an error). This criterion is useless for real errors as will be shown below.

2.3 The Modified Method of Lo´ pez (1997)

This variant has been specially designed in order to handle the problem of heavily
correlated errors in space. Notice that the procedure of Lo´ pez (1997) was tested with
synthetic, weakly correlated errors. Its performance decays as the autocorrelation
increases. The procedure of Felicı´ simo suffers from the same problem, since the error at
i,j is highly correlated with the one at the immediate neighbors. Lo´ pez’s procedure does

(cid:223) Blackwell Publishers Ltd. 2000

50

Carlos Lo´pez

not require that the along the strip profiles are contiguous. Therefore we can skip some
of them (the ones most correlated) for the analysis. The strip is chosen as before, but in
the calculations we consider subsets created using every k-th row, k being related to the
range, a geostatistical property (Samper and Carrera 1990) of the error field. It is
assumed in this paper that the range can be estimated from independent analysis: it
might depend on DEM characteristics, method of obtaining it, scale of aerial
photography, etc. The modified method resembles the multigrid approach (Strang
1989) used in scientific computing packages for the solution of differential equations.

3 The Experiment

To test the method with real data, two 12.42 km by 6.9 km, 30 m spacing DEMs in the
Aix-en-Provence region in the South of France were selected. A subset of 360 rows and
216 columns was used for all calculations. Both DEMs have been described elsewhere
(Day and Muller 1988), and include as a significant feature Mount Sainte Victoire. The
first DEM was produced by photogrammetric measurement of spot elevations from
aerial photography. Its accuracy has been estimated by multiple set-up and observation
of several blocks within the DEM. An analysis of 830 duplicate points (i.e. set up and
measured twice) is presented in Table 1. The second DEM was derived from a set of
three SPOT images using a stereo matcher. It was interpolated to a 30 m grid by using
values within a window of size 21 pixels. Elevation values were obtained using kriging
with a spheric variogram of 4000 m2 sill and 3000 m range, assuming an accuracy for
the window of 11 m S.D. Table 2 shows the statistics for the difference between the
interpolated DEM (obtained from the stereo matcher’s output) and the manually
generated DEM.

Figure 3 illustrates the main features of the DEM, and Figure 4 shows the
probability density function of the differences in elevation. The probability of exactly

Table 1 Comparison of 830 duplicate points of the manually derived DEM (from Day and
Muller 1988)

Table 2 Comparison of 95865 points of the SPOT derived DEM against the manually
derived DEM

Mean abs. error
S.D. error
RMSE
Max. (abs. size)
|error-(cid:22)|> 3(cid:27)

Mean abs. error
S.D. error
RMSE
Max.
Min.
|error-(cid:22)|> 3(cid:27)

(cid:255)0.026 m
1.837 m
12.70 m
14.66 m
1.7%

0.93 m
12.67 m
12.70 m
193.83 m
(cid:255)86.22 m
1.43%

(cid:223) Blackwell Publishers Ltd. 2000

Improving the Elevation Accuracy of DEMs

51

Figure 3

Test DEM sketch obtained using only every tenth grid value.

Figure 4
both DEMs.

Sampled probability density function of the discrepancies in elevation between

zero error is negligible as only 6 out 95865 points have exactly the same elevation in
both datasets. This leads to a paradox: since any choice for the locations will succeed in
pointing to a true error, the error type I will be identically zero ignoring the procedure,
and the error type II will decrease linearly with the effort. These results cannot be

(cid:223) Blackwell Publishers Ltd. 2000

52

Carlos Lo´pez

Figure 5
GEOEAS software.

Sampled variogram of the difference of both DEMs versus distance derived with

compared with those presented in Lo´ pez (1997) since these statistics were used to
decide whether to continue or stop the procedure. For real datasets, a possible measure
will be the RMSE between the original and the corrected elevations, and the procedure
might go on as long as the RMSE exceeds a preset threshold.

Geostatistical techniques (Samper and Carrera 1990, Cressie 1993) were used to
describe another interesting result regarding some properties of the discrepancy field,
i.e. the difference between both DEMs. Figure 5 shows a plot of the sampled
variogram. Even though the goal of the present paper is not to model the variogram
itself, it is worth noting that the range can be roughly estimated as 300 m, i.e. 10
times the grid spacing. This numerical result is in agreement with results obtained by
visual analysis of the discrepancy field and can be interpreted as a measure of the
spatial autocorrelation of the error field. Clearly, the occurrence of errors cannot be
regarded as a local phenomenon, a hypothesis assumed by Felicı´ simo (1994) and
Lo´ pez (1997).

Most errors are found in a smooth neighborhood regardless if they occur along
breaklines or as isolated values. The errors typically influence the data over a distance
of 10 pixels. At breaklines the decay should be considered across the line.

Before analyzing the accuracy results, the Gaussian distribution of the errors and
the relationship between outlying values of the ti,j population and the true errors by
Felicı´ simo (1994), can be examined further. Figure 6 shows a QQ-plot of the
distribution of the original ti,j population. The QQ-plot produces a linear relationship
when two distributions are of the same type (even though with different parameters).
In this case the target distribution is Gaussian. The x-coordinate corresponds to the

(cid:223) Blackwell Publishers Ltd. 2000

Improving the Elevation Accuracy of DEMs

53

Figure 6 QQ-plot of
cumulative density function of (cid:14)i;j, for the available DEM.

the N(0,1) cumulative density function versus the sampled

gaussian cumulative density function (cdf), with no units, while the y-coordinate is the
sampled cdf of (cid:14)i,j, measured in m. As it can be seen, a Gaussian distribution is hardly
achieved.

Figure 7, on the other hand, shows a QQ-plot comparing the distribution of the (cid:14)i,j
against the real errors. The linear relationship shows that both cdf’s belong to the same
(unknown) class. However, this nearly linear relationship is not automatically related
to the expected result (a tool to locate big errors by finding big values of (cid:14)i,j). A strong
correlation was anticipated, but unfortunately this is not the case. The regression of (cid:14)i,j
versus real errors (not shown) has an R2 coefficient is 0.0858, well below the ideal value
of 1.00.

4 Results under the ‘‘Perfect Inspector’’ Hypothesis

The results can be analyzed using different statistics. The most interesting one will take
into account the evolution of the elevation accuracy in terms of the editing effort. For
our purposes we measure the editing effort as the number of elevation values checked
divided by the total number of points in the DEM. We assumed that the user has a
correction procedure and that the procedure is perfect (i.e.
‘‘perfect inspector’’
assumption).

Normally the accuracy of a DEM is not directly known to the user; it can be
estimated through the sampling of isolated points if more precise measurements are

(cid:223) Blackwell Publishers Ltd. 2000

54

Carlos Lo´pez

Figure 7 QQ-plot of the sampled cumulative density function (cdf) of the true errors
versus the (cid:14)i,j cdf for the available DEM.

Figure 8
Evolution of the accuracy (measured by the RMSE in m) versus the effort for the
methods of Felicı´ simo (1994) (with the -o- symbol) and Lo´ pez (1997). Results shown for
w (cid:136) 8. Different lines correspond to different numbers of uncontrolled scores. Left plot (8a)
shows details of the right one (8b).

(cid:223) Blackwell Publishers Ltd. 2000

Improving the Elevation Accuracy of DEMs

55

available. For practical purposes it might be more meaningful to use statistics from the
distribution of the errors detected while working with the dataset. For example, its
RMSE will measure the size of the errors detected by the method for a given effort. The
variability of the RMSE for each step precludes any simple analysis.

A clear measure of the effort involved should be included. The effort per step (in
turn) depends strongly on the choice of the margin level. It regulates how much of the
tail of the distribution of the noisy scores will be regarded as being in error. Cutting out
the tails might produce an empty set of candidates. In order to avoid this the margin
level was slightly increased to assure that there will be candidates to check in each step.
Figure 8 shows the evolution of the accuracy measured in terms of the RMSE for a
strip of width w (cid:136) 8. The boundaries of the dashed regions at the top and the bottom
show the worst and best possible operation locus. The former is obtained by
considering first the smallest discrepancies, while the latter corresponds to selecting the
largest discrepancies first.

Under these assumptions both lines should meet at 0 and at 100 per cent. Even
though both limits are hardly of practical interest (because it requires knowing the
errors in advance) they give a better understanding of the process. Lines with the -o-
symbol are for the Felicı´ simo (1994) method while the others are for different
controlled scores and the Lo´ pez (1997) method. Figure 8a has more detail in the low
effort region, while Figure 8b has been extended up to 15 per cent effort. It is clear
that the Felicı´ simo’s method outperforms the Lo´ pez’s method in the long run, but at
low effort they are similar. This region is of primary importance for two reasons.
Firstly because most users will not go too much further. End users neither have
additional data or tools, so they will correct at most the worst errors. DEM producers
might go back and make another measurement, but this might become a boring task
if new values do not differ substantially from the old ones. Secondly, according to
Torlega˚ rd et al (1986) blunders typically account for less than 3 per cent of the
dataset, 0.5 per cent being a median value. Thus pursuing the task over such a limit
might be misleading, because the methods have been designed for finding gross errors
only. In addition, none of the methods shows a slope comparable to the best possible
method at 0 per cent effort, which implies that the most important errors are not
found in the early stages of the procedure.

Some other options for the width parameter were also treated which will not be
presented here. Figure 9 compares the accuracy performance of the modified method
versus Felicı´ simo’s method. These diagrams were obtained after subdividing the DEM
into regions of width 72 rows, and building the strips taking every 9th row within the
region. Thus the ‘‘strip’’ width is again 8. Notice that we skip nearly 10 rows, as
suggested by the range of the variogram. The plot of Felicı´ simo’s method is again
included for comparison. The most striking fact is the difference in the slope at 0 per
cent effort which is markedly closer to the best one. This implies that larger errors are
found earlier, leading to a faster decrease in the RMSE. However, once those important
errors are removed, the remaining ones are difficult to locate, and the simpler method
of Felicı´ simo is better if the effort exceeds 1.75 per cent.

The end user can calculate values of the RMSE already found like those presented
in Figure 10. The x-coordinate is the effort defined as before, while the y-coordinate is
the RMSE of the population already corrected. The 0 per cent value is not defined.
Plots correspond to the Felicı´ simo (1994) approach and the modified method of Lo´ pez
(1997). It is clear that the former finds larger errors in the ‘‘long’’ run (over 1.75 per

(cid:223) Blackwell Publishers Ltd. 2000

56

Carlos Lo´pez

Evolution of the accuracy (measured by the RMSE in m) versus the effort for the
Figure 9
(with the -o- symbol) and the modified method of Lo´ pez
methods of Felicı´ simo (1994)
(1997). Results choosing every 9th row. Different lines correspond to different numbers of
uncontrolled scores. Left plot (9a) shows details of the right one (9b).

Figure 10
Evolution of the RMSE of the cumulated errors up to a given effort vesus the
effort for the methods of Felicı´ simo (1994) (with the -o- symbol) and the modified method
of Lo´ pez (1997). Results choosing every 9th row, resulting strips of w (cid:136) 8. Left plot again
shows details of the right one.

(cid:223) Blackwell Publishers Ltd. 2000

Improving the Elevation Accuracy of DEMs

57

Figure 11
Binary map of the errors located up to the 15 per cent effort with the method
of Felicı´ simo (1994). Black areas indicate locations suggested at 3 per cent effort; gray ones
are obtained after 15 per cent effort.

cent effort) but the latter is slightly better for lower effort values. Three lines with
different numbers of uncontrolled scores are shown, and it is clear that the line of 0
value is very similar to the line of 2, once a small investment of effort is made.

We also analyzed the spatial location of the errors found when a substantial
amount of work is performed. Figure 11 shows the places where Felicı´ simo’s method
pointed out the errors up to 3 per cent (in black) and 15 per cent effort (in gray). We
noticed that most of them are concentrated along significant features of the DEM,
namely breaklines where slope changes abruptly. The second order polynomial is not a
good approximation of the surface at these places, so differences larger than expected
arise. Once some values are corrected, such differences are even more evident, but since
its closer neighbors become
we do not allow any point to be corrected twice,
candidates, explaining the ‘‘clear’’ image. Figure 12 shows the errors predicted with the
original method of Lo´ pez (1997), while Figure 13 shows the pattern predicted with the
modified method. Both images look ‘‘noisy’’ with points randomly located, but it is
clear that the modified method does not follow the features of the terrain, while the
original method is more prone to do so.

5 Results Using a ‘‘Non Perfect Inspector’’ Assumption

Figures 14 and 15 show the results obtained after removing the ‘‘perfect inspector’’
assumption. Under the new hypothesis, the inspector uses an interpolation procedure
to correct those values which are believed to be wrong. The procedure uses the

(cid:223) Blackwell Publishers Ltd. 2000

58

Carlos Lo´pez

Figure 12
Binary map of the errors located up to the 15 per cent effort with the original
method of Lo´ pez (1997). Black areas indicate locations suggested at 3 per cent effort; gray
ones are obtained after 15 per cent effort.

Figure 13
Binary map of the errors located up to the 15 per cent effort with the modified
method of Lo´ pez (1997). Black areas indicate locations suggested at 3 per cent effort; gray
ones are obtained after 15 per cent effort.

(cid:223) Blackwell Publishers Ltd. 2000

Improving the Elevation Accuracy of DEMs

59

Evolution of the accuracy (measured by the RMSE in m) versus the effort for
Figure 14
(with the -o- symbol) and the modified Lo´ pez (1997)
the method of Felicı´ simo (1994)
method. The perfect inspector hypothesis has been removed. Results are shown choosing
every 9th row. Left plot shows details of the right one.

elevations in the neighborhood and finds a best fit using second order polynomials (as
described for the Felicı´ simo 1994 method). After an elevation has been corrected, it
will not be modified again. If errors are located in adjacent pixels, they will mutually
interact masking each other and if the procedure detects one of them, the inspector
will produce a modified but not correct interpolated surface. In this case there is no
best and worst operation line, because the procedure might in principle introduce new
and worse errors in the dataset. Both Figures 14 and 15 were calculated as before
using the high accuracy DEM as a reference.

The performance of the Felicı´ simo method is lower because it pinpoints clusters of
elevations. Once an error is imputed using its neighbors it will become a ‘‘corrected’’
point; and if the neighboors are wrong, it will be wrong as well. Few improvements in
any estimate of accuracy can arise in such a case. The performance of the modified
procedure of Lo´ pez (1997) also deteriorates, but to a lesser extent. This can be easily
explained analyzing the pattern of candidate locations (Figure 13) because our
candidates are typically isolated; once imputed (maybe with also wrong values) the
next candidate is usually not close to the previous one, and the error correction
procedure is not trapped in a neighborhood of few candidates.

(cid:223) Blackwell Publishers Ltd. 2000

60

Carlos Lo´pez

Figure 15
Evolution of the RMSE of the cumulated errors up to a given effort versus the
effort, after removing the hypothesis of the perfect inspector. Plots are for the methods of
Felicı´ simo (1994) (with the -o- symbol) and the modified method of Lo´ pez (1997). Results
choosing every 9th row. Left plot shows details of the right one.

6 Discussion

Two published methods for locating errors (also named outliers or blunders) in raster
datasets have been compared. One method was modified, and a comparative test for all
three methods using real data with known errors was conducted. The method suggested
by Felicı´ simo (1994) is very simple, but no results using either synthetic or real errors
were previously reported. One interesting fact is that this method is parameter free, so
end users have nothing to adjust or choose. However it has been derived under some
hypotheses that do not apply to the DEM used in this study. It relies on a low order
polynomial interpolator using only nearest neighbors. This approach will work better in
smooth terrain. The use of low order polynomials tends to pinpoint locations which are
close to each other, a situation which is more likely to occur with systematic errors. For
further work, a local universal kriging interpolator (Samper and Carrera 1990) based on
more neighbors might be used, consistent with the findings of Giles and Franklin (1996)
who also used a window with 11 by 11 elements. The Kriging approach also can be used
to model different spatial autocorrelation scales. The overall results show that if better
elevation values can be derived using the same raw data, this approach leads to higher
accuracy, provided there are no systematic errors.

The method of Lo´ pez (1997) was designed for and tested with synthetic errors with
very low spatial autocorrelation. For our case, where errors show substantial spatial

(cid:223) Blackwell Publishers Ltd. 2000

Improving the Elevation Accuracy of DEMs

61

autocorrelation, it performs only slightly better than Felicı´ simo’s method for low effort,
but it is outperformed in any other case. Overall, the performance of this method is poor.
A modified form of this method was proposed to handle the spatial auto correlation of
errors. Strips were obtained by subsampling the DEM at each k-th row. From a
programming point of view this is a minor change. In real applications, the number k has
to be fixed a priori. O¨ stman (1987b) suggested that k is related to both the DEM and the
acquisition method. The range can be estimated from the sampled variogram. Lo´ pez
(1997) describes a rule to determine how many scores are considered describing the
structure of the cloud. For this case, this rule suggests a value of 2. Slightly better results
were obtained using 0. However, in a first approximation the rule is still valid.

All three methods have been used in an iterative fashion. Once some errors were
removed, all the calculations are performed again, and new candidates appear. If this is
not the case, some parameters are modified automatically (lowering confidence limits,
for example) in order to continue the operation. We continue until 15 per cent of the
DEM elevation values have been corrected or confirmed. According to Torlega˚ rd et al
(1986) gross errors account for less than 3 per cent of the population, so the 15 per cent
limit is well within either the systematic (as defined by Thapa and Bossler 1992) or the
random error set, provided the first 3 per cent were really gross errors.

This approach assumes that, once an error is located, it can be replaced by a
‘‘better’’ value. In real applications the procedure will be different depending on the
user. In a DEM production environment, some action can be taken to check these
identified isolated values. In photogrammetric measurements these checks can be done
before removing the stereopair. The goal here is to improve the overall accuracy, while
the effort is less crucial. On the other hand, the end user is left alone in most cases,
because (s)he may not be able to go to the original data sources. Therefore (s)he will be
interested in ‘‘evident’’ errors, i.e. those of relevant size (which are typically few).

All the results have been obtained using just one DEM corresponding to hilly
terrain, derived using photogrammetric techniques. The relative weight of gross,
systematic and random errors is different for different DEM production procedures,
and to assess the impact of such differences in the methods presented is a topic of
further research. The modified method attempts to filter out the systematic error, so it
is likely to work well in either hilly or smooth terrain.

A comment about the computer time requirements: the method of Felicı´ simo
(1994) is fairly cheap (of the order of m.n operations, (m,n) being the size of the DEM),
while the method of Lo´ pez (1997) and the modified procedure presented here involve,
for each step, the computation of (m/w).(n/w) covariance matrices of size (w,w), which
takes [(n/w).O(n2) + (m/w).O(m2)].O(w2) operations; to calculate the eigenvectors
requires in turn [(n/w) + (m/w)].O(w2) operations, and to project each strip to
calculate the scores requires (m + n).w operations. Some other operations are required
but depend linearly on m and n. In our example, a DEM of size m (cid:136) 360 and n (cid:136) 216,
for w (cid:136) 8, required about 5 minutes per step using MATLAB on a SUN Sparc 20. The
overall procedure is still considered cheap in terms of computer time.

7 Conclusions

(cid:223) Blackwell Publishers Ltd. 2000

Some methods to locate gross errors in quantitative raster data have been presented,
and they were tested using a grid-based DEM with known errors. The DEM was

62

Carlos Lo´pez

produced by automatic stereo matchers from SPOT image pairs, and has elevations
ranging from 181 to 1044 m. A more accurate DEM of the same area is available; it was
considered as the ground truth. The detection of gross errors is highly desirable,
because they significantly affect all statistics commonly used to report quality. Since
they are typically few, once located it might be practical to take some corrective actions
to replace them with better quality estimates.

The usual hypothesis of errors uncorrelated in space (assuming no gross or systematic
errors) seems to be wrong at least for this case, as is the assumption of a gaussian
distribution for the residuals. This has potentially serious implications for the usefulness
of some previously published algorithms (Felicı´ simo 1994, Lo´ pez 1997) and it motivated
this work. The results suggest that the Felicı´ simo (1994) method finds mostly systematic
errors, mainly due to the interpolation algorithm (biquadratic polynomial). Lo´ pez (1997)
show similar results in terms of RMSEs during the early stages of the correction process.
A modified version of the method of Lo´ pez (1997) has been designed in order to
handle the significant spatial autocorrelation observed. It aims to ignore the systematic
errors, and identify just the gross and random errors. This method was tested with the
same dataset and the results exceeded those of Felicı´ simo up to a certain level of effort,
with effort defined as the fraction of the DEM elevations corrected or revised. This
effort level (1.75 per cent) is of the order of the number of gross errors typically found
in photogrammetrically derived DEM as reported by Torlega˚ rd et al (1986). Its location
pattern looks sparse and random (as expected for gross errors), unlike the smooth
pattern produced by Felicı´ simo’s method, which is typical of systematic errors.

Different DEM production procedures might create particular combinations of
gross, systematic and random errors. The limited scope of this experiment does not
allow us to generalize its conclusions to any DEM irrespective of its lineage; however, it
is felt that gross errors will be easily detected if the systematic errors are negligible or
they can be filtered out by some procedure.

The modified method has some free parameters; the most important is an estimate
of the auto correlation lag (or the range of the variogram). It can be estimated from a
limited number of independent control points; some authors claim that this value
depends on the method used to acquire the DEM and on the DEM itself.

We assumed for most of the work that once an error is identified, it can be corrected
without error. This is known as the perfect inspector hypothesis. In the case of using the error
detection procedure in a semi-automatic production environment, the method warns the
operator about possible errors before the stereopair is unmounted, enabling new measure-
ment. In a fully digital production environment, some correlation thresholds are usually
varied to minimize computer time. The method may be used to selectively strengthen the
correlation thresholds in suspicious points. In case there is no possibility to verify the errors,
e.g. for end users, the algorithm will help to locate the most unlikely values; we have
simulated the situation under an imperfect inspector hypothesis. Unlikely values were
imputed using a polynomial fit and nearest data, and the results were qualitatively similar to
those obtained using ground truth data for the identification of potential errors.

Acknowledgments

This work was started at the Department of Geodesy and Photogrammetry, Division of
Geoinformatics, of the Royal Institute of Technology, Stockholm, Sweden, under the

(cid:223) Blackwell Publishers Ltd. 2000

Improving the Elevation Accuracy of DEMs

63

supervision of Hans Hauska. Jan Peter Muller from University College, London, kindly
supplied both DEMs and the necessary background information. The author wishes to
acknowledge BITS (Swedish Agency for International Technical and Economical
Cooperation) and CONICYT (Consejo Nacional de Investigaciones Cientı´ ficas y
Te´ cnicas, Uruguay) for partial funding. Computer resources were supplied by the
Centro de Ca´ lculo of
the Republic,
Montevideo, Uruguay.

the School of Engineering, University of

References

Abreu E, Lighstone M, Mitra S K, and Arakawa K 1996 A new efficient approach for the removal
of impulse noise from highly corrupted images. IEEE Transactions on Image Processing 5:
1012–25

Ackermann F 1995 Digitale Photogrammetrie – Ein Paradigma-Sprung. Zeitschrift

fu¨ r

Photogrammetrie und Fernerkundung 3: 106–15

Anonymous 1994 ESRI tops the charts again. ARC News 16: 35
Bethel J S and Mikhail E M 1984 Terrain surface approximation and on-line quality assessment.
In: Proceedings of the Fifteenth International Congress of the International Society for
Photogrammetry and Remote Sensing, Rio de Janeiro. Commission III, 25(A3a): 23–32

Cressie N 1993 Statistics for Spatial Data. New York, NY, John Wiley and Sons
Day T and Muller J P 1988 Quality assessment of Digital Elevation Models produced by
automatic stereo matchers from SPOT image pairs. In: Proceedings of the Sixteenth
International Congress of the International Society for Photogrammetry and Remote
Sensing, Kyoto. Commission III: 148–59

Felicı´ simo A 1994 Parametric statistical method for error detection in digital elevation models.

ISPRS Journal of Photogrammetry and Remote Sensing 49: 29–33

Fo¨ rstner W 1983 On the morphological quality of digital elevation models. Unpublished paper
presented at the International Colloquium on Mathematical Aspects of Digital Elevation
Models, Stockholm

Giles P T and Franklin S E 1996 Comparison of derivative topographic surfaces of a DEM
generated from stereoscopic SPOT images with field measurements. Photogrammetric
Engineering and Remote Sensing 62: 1165–71

Hadi A S 1992 Identifying multiple outliers in multivariate data. Journal of the Royal Statistical

Society B 54: 761–71

Hadi A S 1994 A modification of a method for the detection of outliers in multivariate samples.

Journal of the Royal Statistical Society B 56: 393–6

Hannah M J 1981 Error detection and correction in Digital Terrain Models. Photogrammetric

Engineering and Remote Sensing 47: 63–9

Hawkins D M 1974 The detection of errors in multivariate data using Principal Components.

Journal of the American Statistical Association 69: 340–4

Hawkins D M 1993a A feasible solution algorithm for the minimum volume ellipsoid estimator

in multivariate data. Computational Statistics 8: 95–107

Hawkins D M 1993b The feasible set algorithm for least median of squares regression.

Computational Statistics and Data Analysis 16: 81–101

Hunter G and Goodchild M 1996 Comunicating uncertainty in spatial databases. Transactions in

GIS 1: 13–24

109–19

John S A 1993 Data integration in a GIS: The question of data quality. ASLIB Proceedings 45:

Lebart L, Morineau A, and Tabard N 1977 Techniques de la Description Statistique: Methodes et

Logiciels pour l’analyse des Grands Tableaux. Paris, Dunod

Lo´ pez C 1997 Locating some types of random errors. International Journal of Geographic

O¨ stman A 1987a Quality control of photogrammetrically sampled Digital Elevation Models.

Information Science 11: 677–98

Photogrammetric Record 12: 333–41

(cid:223) Blackwell Publishers Ltd. 2000

64

Carlos Lo´pez

O¨ stman A. 1987b Accuracy estimation of digital elevation data banks. Photogrammetric

Engineering and Remote Sensing 53: 425–30

Mitra S K and Yu T H 1994 A new nonlinear algorithm for the removal of impulse noise from
highly corrupted images. IEEE International Symposium on Circuits and Systems 3: 17–20
Nebert D 1995 Status of the National Geospatial Data Clearinghouse on the Internet. WWW

document, www.fgdc.gov/clearinghouse/pubs/esri95/p196.html

Nebert D 1996 Supporting search for spatial data on the Internet: What it means to be a
node. WWW document, www.fgdc.gov/clearinghouse/pubs/esri96/

Clearinghouse
revised.html

Richman M B 1986 Rotation of principal components. Journal of Climatology 6: 293–335
Samper F J and Carrera J 1990 Geoestadı´stica: Aplicaciones a la Hidrologı´a Subterra´ nea.

Barcelona, Centro Internacional de Metodos Numericos en Ingenieria.

Strang G 1989 Introduction to Applied Mathematics. Wellesley, MA, Wellesley-Cambridge Press
Thapa K and Bossler J 1992 Accuracy of spatial data used in Geographic Information Systems.

Photogrammetric Engineering and Remote Sensing 58: 835–41

Torlega˚ rd K, O¨ stman A, and Lindgren R 1986 A comparative test of photogrammetrically

sampled digital elevation models. Photogrammetria 41: 1–16

(cid:223) Blackwell Publishers Ltd. 2000

