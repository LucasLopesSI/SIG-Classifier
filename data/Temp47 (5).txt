This article was downloaded by: [North West University]
On: 19 December 2014, At: 18:18
Publisher: Taylor & Francis
Informa Ltd Registered in England and Wales Registered Number:
1072954 Registered office: Mortimer House, 37-41 Mortimer Street,
London W1T 3JH, UK

International Journal of
Geographical Information
Science
Publication details, including instructions for
authors and subscription information:
http://www.tandfonline.com/loi/tgis20

Scale- and orientation-
invariant scene similarity
metrics for image queries
Anthony Stefanidis , Peggy Agouris ,
Charalambos Georgiadis , Michela Bertolotto &
James D. Carswell
Published online: 10 Nov 2010.

To cite this article: Anthony Stefanidis , Peggy Agouris , Charalambos Georgiadis ,
Michela Bertolotto & James D. Carswell (2002) Scale- and orientation-invariant
scene similarity metrics for image queries, International Journal of Geographical
Information Science, 16:8, 749-772, DOI: 10.1080/13658810210148552

To link to this article:  http://dx.doi.org/10.1080/13658810210148552

PLEASE SCROLL DOWN FOR ARTICLE

Taylor & Francis makes every effort to ensure the accuracy of all
the information (the “Content”) contained in the publications on our
platform. However, Taylor & Francis, our agents, and our licensors
make no representations or warranties whatsoever as to the accuracy,
completeness, or suitability for any purpose of the Content. Any opinions
and views expressed in this publication are the opinions and views of
the authors, and are not the views of or endorsed by Taylor & Francis.
The accuracy of the Content should not be relied upon and should be
independently verified with primary sources of information. Taylor and
Francis shall not be liable for any losses, actions, claims, proceedings,
demands, costs, expenses, damages, and other liabilities whatsoever

or howsoever caused arising directly or indirectly in connection with, in
relation to or arising out of the use of the Content.

This article may be used for research, teaching, and private study
purposes. Any substantial or systematic reproduction, redistribution,
reselling, loan, sub-licensing, systematic supply, or distribution in any
form to anyone is expressly forbidden. Terms & Conditions of access
and use can be found at http://www.tandfonline.com/page/terms-and-
conditions

Downloaded by [North West University] at 18:18 19 December 2014 int. j. geographical information science, 2002
vol. 16, no. 8, 749–772

Research Article

Scale- and orientation-invariant scene similarity metrics for image
queries

ANTHONY STEFANIDIS, PEGGY AGOURIS,
CHARALAMBOS GEORGIADIS
Department of Spatial Information Science and Engineering and National
Center for Geographic Information and Analysis (NCGIA), University of
Maine, USA; e-mail: tony,peggy.harris}@spatial.maine.edu

MICHELA BERTOLOTTO
Department of Computer Science, University College Dublin, Ireland;
e-mail: Michela.Bertolotto@ucd.ie

and JAMES D. CARSWELL
Digital Media Centre, Dublin Institute of Technology, Dublin, Ireland;
e-mail: jcarswell@dit.ie

(Received 9 November 2000; accepted 28 January 2002)

Abstract.
In this paper we extend our previous work on shape-based queries to
support queries on con(cid:142) gurations of image objects. Here we consider spatial
reasoning, especially directional and metric object relationships. Existing models
for spatial reasoning tend to rely on pre-identi(cid:142) ed cardinal directions and minimal
scale variations, assumptions that cannot be considered as given in our image
applications, where orientations and scale may vary substantially, and are often
unknown. Accordingly, we have developed the method of varying baselines to
identify similarities in direction and distance relations. Our method allows us to
evaluate directional similarities without a priori knowledge of cardinal directions,
and to compare distance relations even when query scene and database content
diVer in scale by unknown amounts. We use our method to evaluate similarity
between a user-de(cid:142) ned query scene and object con(cid:142) gurations. Here we present
this new method, and discuss its role within a broader image retrieval framework.

1.

Introduction
Advances in sensor technology have resulted in substantial

increases in the
availability of reliable digital imagery for geospatial applications. The upgraded role
of imagery within the geographic information production, management, and distribu-
tion cycle emphasizes the need for eYcient image retrieval methods. Within the
context of geospatial applications image retrieval solutions need to consider
the actual content of images, namely objects depicted in them (synonymous in this

Internationa l Journal of Geographical Information Science
ISSN 1365-8816 print/ISSN 1362-308 7 online © 2002 Taylor & Francis Ltd
http://www.tandf.co.uk/journals
DOI: 10.1080/13658810210148552

Downloaded by [North West University] at 18:18 19 December 2014 750

A. Stefanidis et al.

paper with object outlines or edges), and the spatial relations of these objects (e.g.
arrangements of buildings in a scene).

In contrast, the majority of image query eVorts have focused on analysing and
comparing images based on their more general properties. These include: colour, in
the form of histogram matching; texture, in the form of image coarseness and contrast
matching; and composition, where an image is divided into homogeneous regions of
colour or texture and the relative positions of these regions analysed (Flickner et al.
1995, Forsyth et al. 1996, Frankel et al. 1996, Carson et al. 1997, Pentland et al.
1996, SclaroV et al. 1997 ).

Furthermore, we have to consider the fact that despite substantial advances
(Gruen et al. 1995, 1997, Lukes 1998 for overviews of on-going activities), fully
automated object extraction from digital imagery remains an unsolved issue. Instead,
semi-structured object outlines are more easily available. Accordingly, the ability to
handle information (object outlines) in the raster domain, without requiring precise
vectorization of image objects, remains important for image queries in geospatial
applications. Considering how a raster-based approach can function equally well
with semi-structured raster edges as well as with fully structured vectorized objects
(as vector-to-raste r conversion is trivial), we can see the full advantage of such an
approach.

Our purpose in this paper is to extend our recently completed work on image
query-by-sketch to handle multiple objects (Agouris et al. 1999b) . Our initial
approach was designed to perform image queries using query elements and structured
databases, all in raster format. Our work focused on single object queries, where the
objective is to identify images containing a single query object (e.g. a cross-shaped
building). Here we want to extend this framework to perform queries on spatial
con(cid:142) gurations of multiple objects. An interesting approach on a developing visual
environments to express spatial relations is presented by Kaushik and Rudensteiner,
(1998). The work of Egenhofer and Franzosa, (1991) on topological and directional
relations, and its recent extension to scene similarity metrics (Goyal and Egenhofer
2000, Blaser 2000) present an ideal framework for such a task. However, these
methods are not optimal for image-based applications, as they tend to rely on the
assumption of pre-de(cid:142) ned cardinal directions, and on minimal scale diVerences
between query and database. These assumptions commonly fail when dealing with
image collections, where orientation and scale may vary widely, and may even be
unknown.

The novelty of this paper is related to the methods introduced here to assess the
similarity between two scenes based on the relative positions of objects, and on the
distances between these objects. The metrics introduced here bypass the need for the
a priori knowledge of cardinal directions, and furthermore allow us to assess similarit-
ies even in the presence of large scale variations between two scenes. Both of these
properties were lacking from currently existing scene similarity solutions and are
important for image-related applications.

The paper is structured as follows: In §2 we present an overview of our image
retrieval environment, and the role of a scene similarity metric within this environ-
ment. In §3 we introduce our innovations in scene similarity metrics, more speci(cid:142) cally
our new varying baseline methods for the comparison of relative positions of objects
and of object distances. In §4 we provide a working example to demonstrate the
application of our newly introduced methods. Conclusions and future plans are in §5.

Downloaded by [North West University] at 18:18 19 December 2014 Similarity metrics for image queries

751

2.

Image query framework
Images depict objects and capture their relationships. This information exists
implicitly within images, and has to become explicit in order to support complex
database operations. In doing so, we essentially transform a raw image into its
computational counterpart, a main goal of image understanding research. Among
other issues, this transformation process involves two types of operations:

identifying objects within these images, and

E modeling the spatial relationships of these objects.

Accordingly, our approach to image queries re(cid:143) ects this duality. Our objective
is to proceed by (cid:142) rst identifying images that contain objects resembling the objects
provided as query input, and subsequently, by analyzing the results, to identify these
images in which the con(cid:142) gurations of
the objects best resemble the input
con(cid:142) guration.

In our previous work we addressed the (cid:142) rst part of this query, namely establishing
a framework and relevant processes that would enable the comparison of a raster
template to a collection of images to retrieve images containing objects that resemble
the query template (Agouris et al. 1999a, b, Carswell 2000). Here we present an
extension of this work by introducing metrics to compare the con(cid:142) gurations of
multiple objects.

2.1. Query environment

In our environment we assume that a typical query of an image database
comprises metadata, semantic data, and a sketched con(cid:142) guration of image objects
((cid:142) gure 1). As shown in the (cid:142) gure, our comprehensive image database comprises
actual images, and three related libraries, namely a metadata, a semantic, and a
feature library. The metadata and semantic libraries are common indexing mechan-
isms, relating properties (e.g. metadata values) to image (cid:142) les. The feature library is
an organised arrangement of outlines based on their mutual geometrical similarity
as it is expressed via matching similarity coeYcients. It is also an indexing structure,
linking outline templates to image locations in which similar features appear. The
reader is referred to Agouris et al. (1999a) for a detailed description of the theoretical
issues behind the feature library, and the interrelationships among these libraries.

Consider an example where a user wishes to retrieve all images from the State
of Maine over the last 2 years with jet airplanes, airplane hangers and runways
that match a particular con(cid:142) guration (e.g. airplanes between the hangers and the
runways). The query would:

E Process the metadata library and retrieve all images that match the speci(cid:142) ed
metadata criterion (in this case the time of acquisition should be during the
last 2 years, and the image footnote should be in the State of Maine).

E Follow the links from this subset of images to the semantic library where it

would identify which of these images contained airports.

E Use this further reduced subset of imagery to identify the subset of feature
library that will be searched for objects similar to the ones in the query sketch.
E Perform a matching-based similarity search to identify the library features that

best match the query content.

E Retrieve all imagery linked to the best matched features.

Downloaded by [North West University] at 18:18 19 December 2014 E
752

A. Stefanidis et al.

Figure 1. The query scene similarity matching work(cid:143) ow.

E Determine which of these images contain all of the objects in the query

con(cid:142) guration.

E Analyse the spatial relations of the query scene on this (cid:142) nal subset of imagery

and return a prioritized list of imagery as the query result.

In this paper we introduce metrics to support the performance of the last bullet
of the above process, namely analysing the spatial relations of objects considering
the particularities of our image-driven applications (operation 9 in (cid:142) gure 1). Our
previous work focused on the use of a single query object to retrieve images con-
taining at least one object similar to the query (operations 1–8 and 10 of the ones
depicted in (cid:142) gure 1). In the next section we provide a brief description of our object
matching approach, as it is a key issue in our framework.

2.2. Shape matching for single object queries

Image matching is used to identify similar objects in two or more images. A very
powerful method to perform this task is least squares matching (LSM). In traditional
LSM, an image window is compared to another template window by analysing the
grey value diVerences between them and examining whether these diVerences could
be minimized through a geometric transformation (e.g. aYne transformation) . The
remaining grey value diVerences are indicators of the accuracy of the matching
process. Limitations of
it can be a highly computa-
tion intensive operation, even for small patches of pixels, and that good initial

this approach are that

Downloaded by [North West University] at 18:18 19 December 2014 Similarity metrics for image queries

753

approximations for positioning the template patch within the image are required for
determining a correct match. Advantages of the approach include its very high
accuracy potential (on the order of 0.1 pixel or better), and the ability to accommodate
geometric variations between the two matched windows (Gruen and Agouris 1994,
Gruen et al. 1995a).

In our framework we use a variation of LSM to compare the sketched object
outline to the feature library, in order to identify the feature that best resembles the
query sketch. In our variation of LSM we use edges instead of grey scale pixels as
the metric for query scene to image similarity. By doing so, we avoid having to test
full patches of pixels against each other. Instead, we reduce the patch into its
information content by considering only those pixels that contain object outline
information, i.e. only those that constitute image object edges. Edge information can
be obtained from the images in our image library by performing common edge
detection operations, or even by having operators manually digitizing object outlines.
Since object edges are used instead of grey scale pixels, grey level observations are
replaced by feature existence/absence observations. This resembles the comparison
of grey values in traditional least-squares matching and the use of image gradients
to identify shifts, rotations, and scalings.

The matching process then reduces to checking only if the input query template
edge pixels overlay edge pixels from the binary edge-image; the answer then can
only be either a ‘yes’ or ‘no’. If yes, the corresponding template edge pixel ‘votes’ to
stay where it is. If no, then this template edge pixel votes to move. By summarising
and analysing the voting patterns of all the pixels that make up an object’s edge, a
global decision as to where and how far to shift the query template is made. Following
iterations we determine the best match between the template and a window. The
least squares framework of our solution and the modelling of the relations between
the two matched objects ensures invariance for scale, orientation, and shifts.

The details of this novel matching approach are beyond the scope of this paper.
The reader may refer to Agouris et al. (1999b) for a detailed analysis of our matching
tool. What is important here is that this matching process provides us with accuracy
measures for every matching result. These measures are expressed as a percentage,
with a maximum of 100% indicating a perfect match and a minimum of 0% indicating
no match. In-between values express variations in our matching accuracy, with
higher values corresponding to better matches than lower ones. A perfect match
implies that there exists a set of aYne transformation parameters that could be used
to transform the query template so that it matches perfectly the library feature and,
correspondingly, objects in images in our image database.

Using this matching tool within our image query environment we can identify
for a query sketch the most similar feature library member, and the corresponding
image locations where this feature appears. Thus, responses to our query are images,
locations within them where objects similar to our sketch appear, and percentages
expressing how similar these objects are to our sketch. Object locations within the
image are communicated as the minimum bounding rectangles (MBR) containing
the object of interest.

2.3. Extension for multi-object queries

Our approach to multi-object queries is a two-stage approach:

E First identify the images (and locations within them) where each of the query

objects appears independently of the other objects.

Downloaded by [North West University] at 18:18 19 December 2014 754

A. Stefanidis et al.

E Subsequently examine these results to identify the con(cid:142) guration that best

resembles the spatial properties of the query sketch.

Assuming that the query sketch contained n objects, the (cid:142) rst issue is equivalent
to performing n queries as described in §2.1 and 2.2. above, one for each object in
the sketch. The second part requires the use of a scene similarity metric that takes
into account spatial and topological relationships between objects. The importance
of topology, orientation, and distance in assessing spatial similarity of scenes is well-
documented (Egenhofer and Franzosa 1991, ShariV 1996, Goyal and Egenhofer
2000). A combinatorial expression for all these properties has been introduced by
Blaser (2000) to function within a general query-by-sketch environment. We make
use of this expression to de(cid:142) ne a function Smet that assesses the similarity metric
between a query con(cid:142) guration Q and an image I in the database. The function
combines diVerent similarity metrics for individual object shapes and relations
between them like topology, orientation, and distance. More speci(cid:142) cally Smet
is
de(cid:142) ned in our case as:

Smet (Q, I )=Ssh(Q, I )wsh+Stop (Q, I )wtop+Sor (Q, I )wor +Sdist (Q, I )wdist

(1)

The elements of this formula are as follows:

E Ssh is a function measuring the degree/percentage of shape similarity between
the objects in Q and the corresponding objects in I. For example, assuming
that obj1, ..., objn indicate the n objects in Q, then Ssh(Q, I )=[S match%(obji)]/n,
where match%(obji) is the matching percentage between object obji in Q and
the corresponding object in I. We can further constrain this formula if we
so wish by imposing acceptability constraints. For example we can require
that for each i=1, ..., n math%(obji)>t, with t a given threshold value. This
would make us consider an object obji in Q as ‘found’ in I if and only if the
corresponding object in I matches to it more than a preset threshold (e.g. 50%).
E Stop is a function measuring the degree/percentage of similarity between the set
of topological relations characterizing the set of objects in Q and the topological
relations among the corresponding objects in I.

E Sor is a function measuring the degree/percentage of similarity between the set
of orientation relations characterizing the set of objects in Q and the orientation
relations among the corresponding objects in I.

E Sdist is a function measuring the degree/percentage of similarity between the
set of distance relations characterizing the set of objects in Q and the distance
relations among the corresponding objects in I.

E wsh, wtop , wor, wdist are weight coeYcients establishing the relative importance
of their corresponding similarity metrics for the overall scene similarity assess-
ment. By minimizing for example the (cid:142) rst three coeYcients, we search for
con(cid:142) gurations resembling the query sketch only in terms of distances between
the objects, regardless of their shape, topology, and orientation.

All above similarity metrics are in the range [0, 1] with higher values correspond-
ing to higher similarity. By enforcing SjµJ wj=1, J={sh, top, or, dist} we ensure that
the overall scene similarity metric Smet will also have a value in the range [0, 1].
Equation 1 is based on the approach of Blaser (2000) for general scene similarity
metrics. In our case however, and considering the particularities of image databases,

Downloaded by [North West University] at 18:18 19 December 2014 Similarity metrics for image queries

755

we introduce new metrics for orientation and distance similarities, and make use of
image matching techniques to provide shape similarity measures.

Contrary to orientation and distance,

topological relations are relatively
unaVected by variations in image scale and orientation. Of course, extreme scale
variations may cause two disjoint objects, for example, to appear merged into a
single blob. Extreme oblique views may have similar eVects, distorting the apparent
topological relations of objects. However, such exceptional cases are considered
beyond our interests. Therefore, we assume the use of the well-established hierarchical
topological models introduced by Egenhofer and Franzosa (1991) and Egenhofer
and Al-Taha, (1992) to describe topological relations in our scene similarity metric.
According to this model the binary topological relations between simply connected
regions range progressively from disjoint to meets, overlap, equal, covers/covered,
and contains/contained. The topological similarity index between two scenes
describes how far their contents are in this arrangement. The reader is referred to
the above mentioned references for a detailed description of these metrics.

3. The varying baselines approach for scene similarity

In this section we introduce two similarity metrics to compare the orientation
and distance content of scenes for use in our query environment. These new metrics,
which are at the core of this paper, are based on the use of new matrices, namely
the position relation matrix, and the distance ratio matrix. We refer to both approaches
under the term ‘varying baselines approach ’, as they inherently employ the establish-
ment of baselines connecting objects, and the comparison of properties derived from
these baselines.

3.1. Position relation matrix for orientation similarity metrics

The direction (orientation) between features in a spatial database is required to
further re(cid:142) ne their spatial relationships. When querying, it is not enough to return
all the scenes where feature A is disjoint from feature B (although this would be a
good (cid:142) rst approximation) . It is also important to consider the directional relationship
between them.

To make sense of direction, a reference frame must (cid:142) rst be established. In general

there are three types of reference frames:

Intrinsic, where the reference frame is in respect to the orientation of the feature
itself, e.g. front or back, left or right of a building.

E Deictic, where the reference frame is relative to each individual looking at the

scene, e.g. what is ‘in front’ for me might be ‘to the left of ’ someone else.
E Extrinsic, where the reference frame is established independently of
orientation of the features or the observers, e.g. north, south, east, west.

the

For con(cid:142) gurations of spatial objects in a GIS or digital image, representing real
positions and orientations of the environment, it has been customary to use extrinsic
reference systems.

Traditional methods to determine direction between spatial entities have been
simpli(cid:142) ed to determining the direction between their point approximations . For
example, the position of the centroid of a feature is determined and compared to
the position of the centroid of a neighboring feature. This gives the precise bearing
between the two points, e.g. feature A is 45°15ê 33 W of feature B. But because areal
features may have non-symmetric shapes, their centroids may not even lie within
their respective boundaries. To overcome this case, the minimum bounding rectangle

Downloaded by [North West University] at 18:18 19 December 2014 E
756

A. Stefanidis et al.

(MBR) was introduced to approximate the feature’s shape, and the centers of these
MBRs are compared to determine direction. Again though, only the direction
between two points is being compared. Of course, as MBRs are only approximations
of an object’s shape they might prove inadequate approximations in cases of complex
shapes as we will see in §4.

The latest research into determining the direction between spatial entities uses a
direction relation matrix (Goyal and Egenhofer 2000). In this approach, an extended
MBR is placed around feature A and the percentage of the area of feature B that
falls within the various regions is recorded in the direction relation matrix dir(A, B).
Pairs of direction relation matrices are then compared and their diVerences analysed
to determine their respective similarity.

Although the direction relation matrix can distinguish between most con(cid:142) gura-
tions of query/image objects, it has the drawback that it depends upon an extrinsic
reference frame, i.e. both the query and the image must be orientated to the cardinal
directions of north, south, east and west. Unfortunately, when dealing with raw
raster imagery, as in our case, there are no exterior orientation parameters known
a priori. Therefore, another approach must be implemented for testing direction
similarity between the query and image scenes.

To overcome the lack of exterior orientation information in the query and image
scenes we propose to use a new matrix, the position relation matrix. This matrix
re(cid:143) ects an intrinsic reference frame, and models query/image object orientations in
respect to `left-of ’ or `right-of ’ the features themselves.

Let us assume we have a scene with n objects obji, ..., objn . For each object obji
(such that 14i<n), an extended, imaginary baseline is identi(cid:142) ed, connecting the
MBR-centroid of obji to the centroid of each object objj (such that i<j4n). Object
obji is arbitrarily considered as the `top’ object and objj the `bottom’ object, and for
every other object objk in the scene (such that 14k4n, k
j), it is determined
whether objk lies left-of or right-of this baseline. Fixing the same objects in both the
query and image scenes to be either top or bottom renders any rotations in the
scenes immaterial. The calculation of
left-of or right-of makes use of the pixel
coordinates of each feature’s MBR in the image and query scenes.

i, k

Given a con(cid:142) guration of objects in an image, the input for the automatic computa-
tion of relative positions are the center coordinates (Xc, Y c) of each object and the
top left (Xtl, Y tl ) and bottom right (Xbr, Y br) coordinates of its Minimum Bounding
Rectangle (MBR). We assume a right-handed coordinate system, and consider MBRs
to be de(cid:142) ned in the regular manner, parallel to the coordinate axes. We proceed by
identifying three potential cases when comparing an object to a selected baseline:

Case 1: baseline parallel to X-axis
E Baseline equation: Y =b

If Y tl and Y br>b then the corresponding matrix value is 1
If Y tl and Y br<b then the corresponding matrix value is - 1
In any other case the corresponding matrix value is 0

Case 2: baseline parallel to Y -axis
E Baseline equation X=b

If Xtl and Xbr>b then the corresponding matrix value is 1
If Xtl and Xbr<b then the corresponding matrix value is - 1
In any other case the corresponding matrix value is 0

Downloaded by [North West University] at 18:18 19 December 2014 E
E
E
E
E
E
Similarity metrics for image queries

757

Figure 2. Baseline intersection points used to populate the position relation matrix.

Case 3: oblique baseline

E Baseline equation Y =aX+b
E Calculate the points of intersection of the oblique baseline by the MBR sides
(substituting Xtl, Y tl, Xbr, Y br in the line equation) . As seen in (cid:142) gure 2, point
1 is the intersection of the line with a line parallel to Y -axis which begins from
the bottom right corner, point 3 is the intersection of the line with the line
parallel to X which begins from the bottom right corner, point 2 is the
intersection of the line with the line parallel to Y -axis which begins from the
top left corner and point 4 is the intersection of the line with the line parallel
to X-axis which begins from the top-left corner.
If Xtl>x4 and Xbr>x3 and Y tl<y2 and Y br<y1 then the corresponding
matrix value is - 1
If Xtl<x4 and Xbr<x3 and Y tl>y2 and Y br>y1 then the corresponding
matrix value is - 1
If Xtl>x4 and Xbr>x3 and Y tl>y2 and Y br>y1 then the corresponding
matrix value is 1
If Xtl<x4 and Xbr<x3 and Y tl<y2 and Y br<y1 then the corresponding
matrix value is 1
In any other case the corresponding matrix value is 0.

These relations are tabulated in the position relation matrix. The position relation
matrix P for a query scene of n objects has n(n - 1)/2 rows (one for each line
connecting two objects) and n columns (one for each object). Each row corresponds
to a pair (obji, objj) of objects with 14i4n, i<j4n, and each column corresponds
to an object in the scene. An element phk in P (with h the index of the row of P
corresponding to a pair (obji, objj) such that 14i<n, i<j4n, and 14k4n, k
j)
is set to - 1 (negative 1), if the MBR of object objk in the image scene lies left-of the
line connecting the MBR centroids of objects obji and objj. Conversely, phk is set to
1 if the MBR of object objk lies right-of this line. Furthermore, Phk is set equal to 0
if the MBR of object objk intersects this extended line. Obviously, for each element
phk in the matrix such that k=i or k=j, phk is equal to 0. As an example, (cid:142) gure 3
shows a scene containing four image objects. The 6×4 position relation matrix
corresponding to (cid:142) gure 2 is:

i, k

Downloaded by [North West University] at 18:18 19 December 2014 E
E
E
E
E
758

A. Stefanidis et al.

Figure 3. Example image scene.

By design, the information contained in this matrix is independent of orientation.
The role of cardinality is indeed nulli(cid:142) ed by the selection of diVerent baselines
(corresponding to each row) and the analysis of the scene content accordingly. It
should also be noted that, by partitioning the space in two equal regions (‘left-of ’
and ‘right-of ’) every time we populate the matrix, we also avoid the unfavorable
bias that may be introduced in models where the NE, NW, SE and SW areas are
larger in size than the N, E, S, and W regions. In this sense our approach preserves
the unbiased nature of the traditional four-region models like the projection-base d
models.

One can easily see that the matrix content also conveys general information
about the distribution of objects in a scene. More than two 0 elements in a row
indicate the alignment of three or more objects in a scene. In an extreme example,
where all four objects would be aligned, the above matrix would become null. In
the case where the objects are scattered in our scene, with no three of them aligned,
each matrix row would contain exactly two 0 elements.

In our query environment, the shape similarity process produces candidate image
objects that resemble the query ones. Combinations of these candidate image objects
create candidate image scenes that have to be compared to the query scene.
Accordingly, a position relation matrix is constructed for the query scene and for
each candidate image scene returned from the initial query. Our objective is to
identify the query/image scene combination that is most similar among the available
options. This corresponds to identifying the most similar combination of position
relation matrices among the options. We use the normalized correlation coeYcient
between the two images to describe their orientation similarity. Assuming that I and
Q are the nm position relation matrices for the image and query scenes respectively,
their correlation coeYcent Cor is:

Cor =

m
å
j=1

n
å
i=1
m
å
j=1

n

S å

i=1

(Iij - I

) (Qij - Q

)

(Iij - I

n
)2 å
i=1

m
å
j=1

(Qij - Q

)2

(2)

Downloaded by [North West University] at 18:18 19 December 2014 ±
±
±
±
Similarity metrics for image queries

759

where m is the number of objects, n is the number of object combinations, and I
and Q

are the average values of the respective matrix elements:

n
å
i=1

m
å
j=1
n · m

(Iij)

I

=

n
å
i=1

m
å
j=1
n · m

(Qij)

and Q

=

(3)

This coeYcient is scaled between 0 and 1 to give a total scene position matching
percentage between the query and image object con(cid:142) gurations independent of any
arbitrary scene rotations.

3.2. Distance ratio matrix

Similar to the extrinsic reference frame approach, distance information is also an
important part of a scene similarity comparison. For the purpose of this research,
where no scale information on the query/image scene is provided a priori, it is
necessary to analyse the relative distances between image objects. Thus, we consider
a scale-independent approach, making use of a new matrix, the distance ratio matrix.
Let us consider a scene with n objects. The distance ratio matrix for this scene is a
square matrix D of rank n(n - 1)/2. Its rows and columns correspond to baselines
formed between object pairs (e.g. AB, AC, AD, BC etc.). The matrix elements are
describing distance ratios for these baselines. Speci(cid:142) cally, for every row of D, we
pick the corresponding entry to serve as the unit distance, and populate the row by
the ratio of all other distances over this unit distance. More formally, each entry dij
in D corresponds to the ratio of the ith distance (Si) over the j th distance (Sj):

dij=

Si
Sj

(4)

There are two properties of this D matrix that become immediately apparent.
First, its diagonal elements are equal to 1, as they correspond to the ratio of a
distance to itself. Second, dhk is equal to 1/dkh . To better illustrate the design of the
distance ratio matrix, let us consider the scene of (cid:142) gure 4. In (cid:142) gure 3 we have an
example scene of three objects. The 3×3 position relation matrix corresponding to
(cid:142) gure 3 is:

AB

1

0.57

0.44

AC

AB

BCC

AC

1.74

1

BC

2.30

1.32

0.76

1

D

For example, element (1, 2) of this matrix corresponds to the ratio of AC over

Figure 4. Example image scene for distance ratio matrix.

Downloaded by [North West University] at 18:18 19 December 2014 ±
±
±
±
760

A. Stefanidis et al.

AB, and element (3, 1 ) corresponds to the ratio of BC over AB. There exists a unique
situation when the centers of two objects coincide, which would result in a division
by 0. This situation might occur when one object is located inside another (e.g. a
house inside a land lot) and the centres of the two objects coincide, and as such it
is extremely rare. There exist various ways to avoid such a problem, and the easiest
is to check the centre point locations for all objects before we proceed with scene
analysis. If two centre locations coincide we perturb one of them by one pixel and
proceed regularly. This minor modi(cid:142) cation of a single object coordinate has minimal
eVect on the resulting correlation coeYcients, and does not distort the overall
comparison of two scenes.

Similar to the use of position relation matrices, distance ratio matrices are formed
for a query and candidate scenes and they are compared via their normalized
correlation coeYcients. Assuming I and Q to be the distance ratio matrices for the
image and query scenes respectively, the correlation coeYcient Cdist for these n*n
distance matrices is:

Cor =

n
å
j=1

n
å
i=1
n
å
j=1

n

S å

i=1

(Iij - I

)(Qij - Q
)

(Iij - I

n
)2 å
i=1

n
å
j=1

(Qij - Q

)2

(5)

where n is the number of object combinations, and I
are the average values
of the respective matrix elements calculated similar to equation (3). This coeYcient
is scaled between 0 and 1 to give a total scene distance matching percentage between
the query and image object con(cid:142) gurations independent of any arbitrary scene
rotations.

and Q

The ratio approach presented here to calculating the relative distances between
objects does not require absolute scale information and is calculated using the pixel
coordinates of the image object centroids returned from the feature matching algo-
rithm. Using this information, a distance ratio matrix D is easily built for every
query/image scene and the above analysis allows us to measure their similarity. This
similarity measure is becoming invalid only in cases where severe variations in image
orientation (e.g. highly oblique photography compared to a vertical query scene)
may distort substantially the apparent distances between objects. However, such
extreme cases are beyond the scope of our work, and are rather rare in common
GIS processes.

It should also be noted that in the formulation of both metrics in §3.1 and §3.2
we have the underlying assumption that the two scenes that are being compared
have equal numbers of objects in them (i.e. a scene comprised of n objects is compared
to another scene comprised of n objects). This is in accordance to the general query
work(cid:143) ow of (cid:142) gure 1, whereby we (cid:142) rst identify similar objects (establishing one-to-
one correspondence between query objects and image objects) and then compare the
scenes de(cid:142) ned by these objects. In the case that an image contains multiple instances
of an object (e.g. we have m multiple versions of object A in the image of (cid:142) gure 2)
this results into m candidate scenes to be compared, each one comprising objects B,
C, D and one of the m multiples of A. The scene that maximizes the similarity metric
Smet (equation 1) is the one selected as the best match to the query.

A similar approach would be followed when we want to compare a query with
n objects to a scene with m objects. This might happen when one of the query objects

Downloaded by [North West University] at 18:18 19 December 2014 ±
±
±
±
±
±
Similarity metrics for image queries

761

does not have a match in the image database (hence n>m). In this case the query
scene is reduced to the m objects that have matches in the database and we proceed
with our query regularly.

4. Experiments

A working example is included in the appendix showing in detail how the
similarity coeYcients are calculated and used to compare object combinations,
demonstrating the eVects of various geometric conditions on our queries.
Furthermore, the similarity metrics introduced in this paper have been implemented
in a Windows environment (Pentium III, 1000 MHz, 256 RAM), using Visual Basic.
We used our I.Q. prototype system to perform single object retrieval based on shape
similarity, searching an image database to provide us with MBR coordinates for
individual query objects. Agouris et al.
(1999a) give a detailed analysis of the
performance of that system. The matching-based technique used to compare a query
object to templates in the feature database is of subpixel accuracy. This minimizes
the number of wrong matches, which depends solely on the parameters used to
structure our template database (with wrong matches typically kept below 5%). Also,
our approach to structuring the feature library reduces the search space by well over
50% in small databases (200 templates), and increasingly higher percentages in larger
ones (where duplicates dominate). The typical search time for our applications was
kept below 1 minute for the above computer con(cid:142) guration. What is of interest for
this paper is the performance of the scene similarity metrics introduced here.

The orientation and distance scene similarity metrics were implemented using
the above mentioned computer system. In order to examine the computational cost
as a function of scene complexity we increased gradually the number of objects that
comprise a scene and estimated the associated computational costs. Figure 5 shows
the computational time as a function of the number of objects comprising a scene
when searching a database of 100 candidate matches. As can be seen, the computa-
tional cost increases nearly exponentially with the number of features. For example,
the time requirement for a scene comprising 20 objects (assuming 100 candidate
scenes of 20 objects each) equals approximately 10 times the corresponding time for
a scene comprising 10 objects (140 and 13.5 seconds respectively in our implementa-
tion). This observation suggests a strategy to be followed when pursuing scenes of
many objects, which is to break them into smaller subsets and query for each subset
separately, and then integrate the results. Following this approach we can compare
a very complex scene of 60 objects in a fraction of the required time, by breaking it
into 6 blocks of 10 objects each (thus requiring approximately 1 minute in our
implementation) . The (cid:142) gure also shows that the computational cost of orientation
(position) metrics is consistently more economical than the computation of distance
matrices. This can also be exploited to improve the performance of queries by (cid:142) rst
querying only orientation metrics, and using these results to eliminate candidate
matches that are below a certain threshold. Then, we introduce distance metrics in
the reduced candidate pool. This allows us to avoid the computation of distance
metrics for candidate scenes that are already failing to satisfy the desired distance
con(cid:142) gurations. In typical set-ups we encountered in our applications (comparing a
scene of (cid:142) ve objects to 200 candidate scene con(cid:142) gurations) the computational time
was below 10 sec for the above mentioned computer con(cid:142) guration.

An interesting case occurs when we simply want to compare a query of n objects
to a general scene of m objects to (cid:142) gure out which combination of n scene objects

Downloaded by [North West University] at 18:18 19 December 2014 762

A. Stefanidis et al.

best resembles the query con(cid:142) guration. It should be noted that this is not the
intended use of our metrics within the application environment described in this
paper. However, it might be of interest in general geospatial queries where object
individual object similarities) are examined. In this
con(cid:142) gurations (regardless of
case our query produces ‘n choose m’ potential candidates, with the number of
permutations increasing exponentially as a function of n and m:

A n
mB =

n!
m !(n - m)!

(6)

The number of permutations de(cid:142) nes the number of candidate scenes, and thus
is linearly related to increases of run times as they are outlined in (cid:142) gure 5. For
instance, in our applications we typically experimented with combinations of m=5
objects out of a set of n=10, which produced a total of 252 permutations (and
candidate matches for our query), corresponding to a runtime of approximatel y
10 seconds.

In order to demonstrate the performance of our metrics we made use of frames
selected from video sequences, which are ideal for our experiments as they contain
numerous con(cid:142) gurations of the same objects. In (cid:142) gure 6 you can see a con(cid:142) guration
of objects used as a query. Figure 7 is a composite of outlines from a time series
showing object C moving within a (cid:142) xed scene comprising objects A, B, D and E.
We show seven diVerent positions of object C, marked T 1, ..., T 7 superimposed
within a single frame. This is equivalent to de(cid:142) ning seven con(cid:142) gurations, each
con(cid:142) guration formed by the four stationary objects and one instance of the mobile
object C.

For these seven con(cid:142) gurations we analysed the resulting coeYcients to examine
the behaviour of our distance and orientation metrics to distinguish very similar
con(cid:142) gurations. The results are tabulated in table 1, where we have ranked the seven
con(cid:142) gurations according to their similarity values. We can see that instance T 4 is
correctly selected as the most similar. We can then identify three pairs of symmetric

Figure 5. Computational time (in seconds) as a function of the number of objects comprising
a scene. Dashed line: distance metrics, continuous line: orientation metrics.

Downloaded by [North West University] at 18:18 19 December 2014 Similarity metrics for image queries

763

Figure 6. A query template.

Figure 7. A query scene.

positions that are increasingly dissimilar to the query scene, namely (T 3, T 5), (T 2,
T 6) and (T 7, T 1). Indeed, it can be easily seen in (cid:142) gure 7 that positions T 3 and T 5,
for example, form a con(cid:142) guration that is closer to the query set-up than T 1 or T 7.
In these experiments we assigned equal weight to both orientations and distances.
However, this need not be the case. The operator may decide to assign higher weights
to speci(cid:142) c properties (e.g. distance) if this information is more critical to a query.
This would have obvious eVects on the results, providing a bias for scenes where the
emphasized properties are satis(cid:142) ed. Alternatively, in an application environment the
assignment of weights may take into account statistics on past queries. For example,

Downloaded by [North West University] at 18:18 19 December 2014 A. Stefanidis et al.

Table 1. Ranking the seven con(cid:142) gurations of (cid:142) gure 7.

Query scheme

Distance coeYcient

Orientation coeYcient

Combined metric

1
0.92
0.85
0.67
0.59
0.48
0.51

1
0.91
0.83
0.76
0.79
0.78
0.71

0.915
0.84
0.715
0.69
0.63
0.61

764

T4
T3
T5
T2
T6
T7
T1

by analysing past queries we can identify the most varying of the above mentioned
metrics (e.g. as the ones with the higher variance of values) and assign higher weights
to these metrics, in order to accentuate diVerences in our (cid:142) nal ranking. This intelligent
assignment of weights is an issue for further research.

In order to evaluate the robustness of our metrics in the presence of rotations
and scalings we performed another experiment. In (cid:142) gure 8 we show a con(cid:142) guration
of 4 objects. Object A is a hangar, B a runway intersection, C a refueling station,
and D an aircraft. An analyst may form this query by selecting object templates from
an object library, or by selecting objects from imagery displayed on-screen. After
forming this query template we performed a search for each object separately, and
in (cid:142) gure 9 we see a recovered image where we have one match for each object A, B,
and C, and six potential matches for aircraft D (marked D through I in the (cid:142) gure).
For each of the six options for aircraft we can form a scene in the image and compare
it to the query. The scene similarity metrics allow us to identify the situation that
most resembles the query provided (e.g. an analyst may have declared this set-up as
an alert condition). The six candidate scenes are termed D through I (corresponding

Figure 8. A query con(cid:142) guration.

Downloaded by [North West University] at 18:18 19 December 2014 Similarity metrics for image queries

765

to the selection of each aircraft from (cid:142) gure 9) and are ranked according to distance
and orientation in table 2. We focus on these two indices only, to demonstrate the
performance of the metrics introduced in this paper. Again, we assigned equal weight
to each similarity metric. In table 2 we can see that D is a perfect match. Indeed, we
had selected the query template using a rotated and scaled version of (cid:142) gure 9, and
we can see that our algorithms were able to recover the eVects of these variations.
Con(cid:142) guration E (the one comprising objects A, B, C, and E) is the second choice,
as object E is just slightly oV the position of object D.

Overall, in our experiments using numerous con(cid:142) gurations of objects and types
of imagery the percentage of correctly retrieved scenes varied between 90 and 100%,
with an average of approximately 93%. A correct retrieval is de(cid:142) ned as one where
the correct (visually identi(cid:142) ed as such) match is among the top three ranked matches.
The few erroneous cases were associated with elongated and/or strangely shaped
objects, and scenes where objects might have been too close to each other. Depending
on their location in the image, the MBRs of these objects may be extending well
beyond the actual object, often containing neighboring objects. This distorts the
corresponding similarity values and aVects our queries. Figure 10 shows this prob-
lem, where object B falls within the MBR of object A on the right-hand side, thus
distorting their relationship. This is a common problem when using MBRs instead
of actual edges to describe the position of an object. We can identify two ways to

Figure 9. A recovered image with six possible con(cid:142) gurations.

Table 2. Ranking the six con(cid:142) gurations of (cid:142) gure 8.

Distance coeYcient

Orientation coeYcient

Combined similarity

Con(cid:142) guration D
Con(cid:142) guration E
Con(cid:142) guration F
Con(cid:142) guration G
Con(cid:142) guration H
Con(cid:142) guration I

1.0
0.98
0.95
0.81
0.71
0.84

1.0
1.0
0.5
0.5
0.5
0.75

1.0
0.99
0.725
0.655
0.605
0.795

Downloaded by [North West University] at 18:18 19 December 2014 766

A. Stefanidis et al.

Figure 10. Example of MBR-related distortions of spatial relations.

overcome this problem. The (cid:142) rst of course is to use actual edges, but the obvious
additional computational requirements make it practically impossible to use. The
second is to consider MBRs that are not oriented parallel to the major axes. Within
our framework this can be accomplished by having objects in our feature library
oriented within their template so that the resulting MBR has the minimum possible
area (termed Minimum MBR, or MMBR). This can be achieved by selecting discrete
rotations of the object and comparing the area of the resulting MBR, and selecting
that MBR which minimizes the area. Subsequently, when projecting this MMBR
onto the image we make use of its actual projection, which in general will be an
oblique rectangle. Relations are then based on this oblique rectangle rather than a
regularly-shaped MBR. This makes the computation of the corresponding position
relations more expensive. In preliminary experiments we noticed an increase in time
requirements of approximately 45% compared to the straight-forward solution men-
tioned above. Furthermore, this approach would eliminate completely only some of
the problems, namely those related to elongated objects. Objects with complex shapes
could still produce some problems. Referring to (cid:142) gure 10 again, if object B were
inside the gap of the L -shaped object A, we would still incorrectly model their
relation, even if we were to use an oblique MBR for each object. Such problems
with complex outlines could only be completely eliminated if we made use of actual
object outlines instead of bounding rectangles. Considering the rather high additional
computational cost, and the low number of initial problematic cases, we decided
that the originally implemented technique was adequate for our applications.

5. Conclusions

The approach introduced in this paper allows us to compare the orientation and
distance content of scenes regardless of pre-de(cid:142) ned cardinal directions, and scale
variations. This is a signi(cid:142) cant advantage that becomes especially important for
image query applications, where scale and orientation may vary by large and often
unknown amounts. In this paper we also presented a framework where these metrics
can complement our previous work on shape-based queries to provide a comprehen-

Downloaded by [North West University] at 18:18 19 December 2014 Similarity metrics for image queries

767

sive approach for scene-based image queries. The relationships used to populate the
position relation and distance ratio matrices are easy to calculate, requiring no more
than a few seconds for hundreds of con(cid:142) gurations. Indeed, the ‘left-of ’/‘right-of ’ and
distance information used to populate these two matrices is readily available once
the individual object MBRs are de(cid:142) ned. This makes the approach presented here
computationall y eYcient in addition to being versatile in terms of cardinality and
scale.

The working example in the Appendix presented a practical illustration of the
calculation of the similarity metrics between four query scenes and a given image,
while additional experimental results using actual image databases demonstrated the
eYciency of the technique. The results demonstrated how our approach overcame
substantial diVerences in cardinality and scale to identify the most similar scene.

Our future plans include a framework for the analysis of gradual changes in the
overall scene similarity metric to monitor the movement of objects within a scene in
spatiotemporal applications. Furthermore, we plan to investigate the use of the
position relation matrix to model the distribution of point data in larger scenes.

Acknowledgments

This work was supported by the National Science Foundation through grants
CAREER IIS-9702233 , DGI-9983445, and ITR 0121269, by CNR through grant
number 106701-00/97/10003, and by a grant by the National Reconnaissance OYce
through BAE Systems.

References
Agouris, P., Carswell, J., and Stefanidis, A., 1999a, An environment for content-based
image retrieval from large spatial databases. ISPRS Journal of Photogrammetry &
Remote Sensing, 54, 263–272.

Agouris, P., Carswell, J., and Stefanidis, A., 1999b, Sketch-based-image queries in topo-
graphic databases. Journal of V isual Communication and Image Representation, 10,
113–129.

Blaser, A., 2000, Sketching Spatial Queries. Ph.D. Dissertation Thesis, University of Maine,

Orono, ME.

Carson, C., Belongie, S., Greenspan, H., and Malik, J., 1997, Region-based image querying,
IEEE Workshop on Content-Based Access of Image and V ideo L ibraries, San Juan, PR,
IEEE Press, pp. 42–49.

Carswell, J., 2000, Using Raster Sketches for Digital Image Retrieval, Ph.D. Dissertation

Thesis, University of Maine, Orono, ME.

Egenhofer, M., and Al-Taha, K., 1992, Reasoning about gradual changes of topological
relationships. In T heories and Methods of Spatio-T emporal Reasoning in Geographic
Space, edited by A. Frank, Berlin: Springer-Verlag, pp. 196–219.

Egenhofer, M., and Franzosa, R., 1991, Point-set topological spatial relations. International

Journal of Geographical Information Systems, 5, 161–174.

Flickner, M., Sawhney, H. S., Ashley, J., Huang, Q., Dom, B., Gorkani, M., Hafner, J.,
Lee, D., Petkovic, D., Steele, D., and Yanker, P., Query by image and video
content: The QBIC system. IEEE Computer, 28, 23–32.

Forsyth, D. A., Malik, J., Fleck, M., Greenspan, H., Leung, T., Belongie, S.,
Carson, C., and Bregler, C., 1996, Finding pictures of objects in large collections of
images. In Object Representation in Computer V ision II, Proceedings of ECCV’96,
Lecture Notes in Computer Science, Vol. 1144 (Berlin: Springer Verlag), pp. 335–360.
Frankel, C., Swain, M., and Athitsos, W., 1996, Webseer: an image search engine for the
World Wide Web. TR-96-14, Department of Computer Science, University of Chicago.
Goyal, R., and Egenhofer, M., 2002, Cardinal directions between extended spatial objects.

IEEE T ransactions on Knowledge and Data Engineering (in press).

Gruen, A., and Agouris, P., 1994, Linear feature extraction by least squares tem-

Downloaded by [North West University] at 18:18 19 December 2014 768

A. Stefanidis et al.

plate matching constrained by internal shape forces. International Archives of
Photogrammetry and Remote Sensing, 30 (Part 3/1), 316–323.

Gruen, A., Agouris, P., and Li, H., 1995a, Linear feature extraction with dynamic program-
ming and globally enforced least squares matching. In Automatic Extraction of Man-
Made Objects f rom Aerial and Space Images, edited by A. Gruen, O. Kuebler and
P. Agouris (Basel: Birkhaeuser Verlag), pp. 83–94.

Gruen, A., Kuebler, O., and Agouris, P.,

(editors), 1995b, Automatic Extraction of

Man-Made Objects f rom Aerial and Space Images (Basel: Birkhaeuser Verlag).
Gruen, A., Baltsavias, E., and Henricsson, O., (editors), 1997, Automatic Extraction of

Man-Made Objects f rom Aerial and Space Images II. (Basel: Birkhaeuser Verlag).

Kaushik, S., and Rudensteiner, E., 1998, Direct-manipulation spatial exploration using

SVIQUEL, V isual Database Systems 4 (London: Chapman & Hall ), pp. 179–185.

Lukes, G., (editor), 1998, Proceedings DARPA Image Understanding Workshop (Monterey, CA:

Morgan Kaufmann).

Pentland, A., Picard, R. W., and Scarloff, S., 1996, Photobook: content-based manipula-
tion of image databases. International Journal of Computer V ision, 18, 233–254.
Sclaroff, S., Taycher, L., and La Cascia, M., 1997, ImageRover: a content-based image
browser for the World Wide Web. IEEE Workshop on Content-Based Access of Image
and V ideo L ibraries (San Juan, PR: IEEE Press), pp. 2–9.

Shariff, A. R., 1996, Natural Language Spatial Relations: Metric Re(cid:142) nement of Topological

Properties, University of Maine, Orono, ME, USA.

Appendix

In this appendix we will show an example of how the scene similarity metrics
introduced in this paper can be used to compare four diVerent query scenes ((cid:142) gure 11)
to a given image I ((cid:142) gure 12). The query scenes comprise con(cid:142) gurations of the same
three object shapes, i.e. an outline of an airplane (obj1), a rectangular airplane hanger
(obj2), and runway outlined by two parallel lines (obj3). For each of the four query
scenes, Q1, Q2, Q3, and Q4, we will calculate the value of Smet using the scene
similarity metric provided by equation (1). As the viewer can easily see, the four
query scenes are diVerent con(cid:142) gurations of the above mentioned three objects.
Accordingly, the shape similarity measures Ssh among individual query objects and
corresponding image features will be similar for all four query scenes. Our feature
matching algorithm returned the following matching percentages:

E match%(obj1)=87% (the matching percentage between the airplane sketch and

the edge outline in the upper middle box in the image of (cid:142) gure 12)

E match%(obj2)=70% (hanger matching)
E match%(obj3)=90% (runway matching )

Thus the overall value for shape similarity for all query scenes is:

ssh(Qi, I)=

87+70+90
3

=82.3%, (where i=1, ... 4).

Figure 11. Four sample query scenes.

Downloaded by [North West University] at 18:18 19 December 2014 E
Similarity metrics for image queries

769

Figure 12. Sample edge image with superimposed MBRs.

In §A.1 we will establish the position relation matrix and the distance ratio matrix
for the image scene. In §A.2–A.5 we will establish these matrices for each of the
query scenes, and we will use them together with standard topological relation
measures to estimate the overall scene similarity between each query and the given
image. In §A.6 we will compare these results to identify the best match.

A.1 Spatial relations for the image scene

For image I, spatial relations are derived using minimum bounding rectangles
(MBRs). The centroid pixel coordinates (row, col) for the three objects are obj1=
(44 105 ); obj2=(106 105 ); obj3=(7122). Therefore the relevant metrics are provided
as follows:

T opological relations between objects in I: All objects are disjoint.

Direction relations between objects in image I: The Position Relation Matrix for I is:
obj2
0

obj1
0

obj3
- 1

12

Distance relations between objects in image I: The Distance Ratio Matrix for I is:

13

23C
23C

13

12

0

- 1

1

0

12

1

0.71

0.69

13

1.41

1

23

1.45

1.03

0.97

1

0

0D
D

In the following sections, we will calculate the values of stop , sor, and sdist for all four
query scenes Q1, Q2, Q3, and Q4.

Downloaded by [North West University] at 18:18 19 December 2014 770

A. Stefanidis et al.

A.2 Spatial relations for query scene 1

For this query scene the centroid pixel coordinates (row,col ) of the three objects

are: obj1=(4059 ); obj2=(180 117 ); obj3=(152 305 ).

T opology comparison between objects in Q1 and I: As objects in Q1 are disjoint, their
topological relations are similar to to relations among corresponding objects in I,
and so: stop (Qi, I)=100%.

Direction comparison between objects in Q1 and I: The Position Relation Matrix for
Q1 is:

obj1
0

0

1

13

12

23C

obj2
0

- 1

1

obj3
1

0

0D

Therefore,

Cor =S (I - I)(Q - Q)
ã S (I - I)2 (Q - Q)2 =

- 2.6447
ã 2.4489×2.889

= - 0.99

where I and Q are the position relation matrices for the image and query scenes
respectively, and

sor (Qi, I)=

- 0.99+1
2

=0.5%

The reader can see that in Q1 objects are arranged in nearly mirror-like positions
compared to the image con(cid:142) guration. Accordingly, the absolute value of their correla-
tion coeYcient is high to indicate a strong relationship to the query scene, however
it is negative because it results from a process (mirror reversal) that is not considered
acceptable in our database. The only occasion where this might be considered
acceptable is when our database includes scanned negatives as well. In that case we
would have to avoid the normalisation of the correlation coeYcients, using their
absolute values instead. For implementation purposes this can be handled easily
by allowing the operator to select an on-screen menu option to allow mirror
con(cid:142) gurations.

Distance comparison between objects in Q1 and I: The Distance Ratio Matrix for Q1 is:

12

1

0.56

0.80

13

12

23C

13

1.78

1

23

1.25

0.70

1.42

1

D

Therefore,

Cdist =S (I - I)(Q - Q)
ã S (I - I)2 (Q - Q)2 =

0.6288
ã 0.8871×1.1421

=0.6247

Downloaded by [North West University] at 18:18 19 December 2014 E
Similarity metrics for image queries

771

and

sdist(Qi, I)=

0.6247+1
2

=81.2%.

Finally, assuming equal weighting for all four components in equation (1) (i.e.
shape similarity is as important as topology, orientation, and distances), the value
of Smet for Q1 becomes:

E Smet (Q1,I)=(82.3×0.25 )+(100×0.25 )+(0.5×0.25 )+(81.2×0.25 )=66%.

A.3 Spatial relations for query scene 2

Centroid pixel coordinates (row, col): obj1=(239 82); obj2=(103 113 ); obj3=

(153 305 ).

T opology comparison between objects in Q2 and I: All objects in Q2 are disjoint,
therefore stop(Q2 , I)=100%.

Direction comparison between objects in Q2 and I: By computing the Position Relation
Matrix for Q2 we obtain sor (Q2 , I)=1+1/2=100%. Indeed, since Q2 is a 180° rotation
of I, all position relations are 100% similar to the position relations among the
corresponding objects in I.

Distance comparison between objects in Q2 and I: By computing the Distance Ratio
Matrix for Q2 we obtain sdist (Q2 , I)=0.7337+1/2=86.7%.

Using the above information, the value of Smet for Q2 becomes:
E Smet (Q2, I)=(82.3×0.25 )+(100×0.25 )+(100×0.25 )+(86.7×0.25 )=92.3%.

A.4 Spatial relations for query scene 3

Centroid pixel coordinates (row, col): obj1=(223 309 ); obj2=(127 83); obj3=

(143 211 ).

T opology comparison between objects in Q3 and I: As for Q2, we have for Q3
stop(Q3 , I)=100%.
Direction comparison between objects in Q3 and I: By computing the Position Relation
Matrix for Q3 we obtain Sor for Q3 is sor (Q3 , I)=0.59+1/2=79%.
Distance comparison between objects in Q3 and I: By computing the Distance Ratio
Matrix for Q3 we obtain sdist (Q3 , I)= - 0.326+1/2=33.7%.

Using the above information, the value of Smet for Q3 becomes:
E Smet (Q3, I)=82.3×0.25+100×0.25+79×0.25+33.7×0.25=73.8%.

A.5 Spatial relations for query scene 4

Centroid pixel coordinates (row,col): obj1=(139 251 ); obj2=(129 85); obj3=

(165 251 ).

T opology comparison between objects in Q4 and I: As we can see obj1 and obj2 are
disjoint; obj2 and obj3 are disjoint; obj1 is contained by obj3. Therefore, the topological
relation between the (cid:142) rst two pairs of objects in Q4 is 100% similar to the topological
relations between the corresponding objects in I, while the topological relation
between the third pair of objects is 0% similar. The value of Stop is Stop (Q4 , I)=
100+100+0/3=66.7%.

Downloaded by [North West University] at 18:18 19 December 2014 E
772

Similarity metrics for image queries

Direction comparison between objects in Q4 and I: By computing the Position Relation
Matrix for Q4 we obtain Sor (Q4 , I)=0.62+1/2=81%.

Distance comparison between objects in Q4 and I: By computing the Distance Ratio
Matrix for Q4 we obtain Sdist (Q4 , I)= - 0.3+1/2=35%.

Using the above information, the value of Smet for Q4 becomes:

E Smet (Q4, I)=(82.3×0.25 )+(66.7×0.25 )+(81×0.25 )+(35×0.25 )=66.3%.

A.6 Comparison of results

The results of all similarity metric calculations can be seen in table 3. From these
results it can be seen that Query scene 2 is the best-matched con(cid:142) guration for the
given image. This agrees with what a human observer would choose as the best
matched con(cid:142) guration since Query scene 2 is plainly a 180° rotation of the image,
sketched at a signi(cid:142) cantly reduced scale. This result demonstrates the ability of our
varying baseline approach to handle arbitrary rotations and scaling of varying
query/image scenes in addition to the capacity to distinguish between con(cid:142) gurations
and shapes of individual image objects.

Table 3. Tabulated Smet results for the four query scenes.

Query scene 1

Query scene 2

Query scene 3

Query scene 4

Ssh
Stop
Sor
Sdist
Smet

82.3

100

0.5
81.2
66

82.3

100
100

86.7
92.3

82.3
100
79
33.7
73.8

82.3
66.7
81
35
66.3

Downloaded by [North West University] at 18:18 19 December 2014 