Geoinformatica (2011) 15:29–47
DOI 10.1007/s10707-010-0113-4

A hybrid classification scheme for mining multisource
geospatial data

Ranga Raju Vatsavai · Budhendra Bhaduri

Received: 27 October 2008 / Revised: 2 December 2009 /
Accepted: 14 June 2010 / Published online: 22 July 2010
© US Government 2010

Abstract Supervised learning methods such as Maximum Likelihood (ML) are often
used in land cover (thematic) classification of remote sensing imagery. ML classifier
relies exclusively on spectral characteristics of thematic classes whose statistical
distributions (class conditional probability densities) are often overlapping. The
spectral response distributions of thematic classes are dependent on many factors
including elevation, soil types, and ecological zones. A second problem with statis-
tical classifiers is the requirement of the large number of accurate training samples
(10 to 30 × |dimensions|), which are often costly and time consuming to acquire over
large geographic regions. With the increasing availability of geospatial databases, it
is possible to exploit the knowledge derived from these ancillary datasets to improve
classification accuracies even when the class distributions are highly overlapping.
Likewise newer semi-supervised techniques can be adopted to improve the para-
meter estimates of the statistical model by utilizing a large number of easily available
unlabeled training samples. Unfortunately, there is no convenient multivariate statis-
tical model that can be employed for multisource geospatial databases. In this paper
we present a hybrid semi-supervised learning algorithm that effectively exploits
freely available unlabeled training samples from multispectral remote sensing images
and also incorporates ancillary geospatial databases. We have conducted several

Prepared by Oak Ridge National Laboratory, P.O. Box 2008, Oak Ridge,
Tennessee 37831-6285, managed by UT-Battelle, LLC for the U. S. Department
of Energy under contract no. DEAC05-00OR22725.

R. R. Vatsavai (B) · B. Bhaduri

Computational Sciences and Engineering Division,
Oak Ridge National Laboratory, Oak Ridge, TN 37831, USA
e-mail: vatsavairr@ornl.gov, rvatsavai@gmail.com

B. Bhaduri
e-mail: bhaduribl@ornl.gov

30

Geoinformatica (2011) 15:29–47

experiments on Landsat satellite image datasets, and our new hybrid approach shows
over 24% to 36% improvement in overall classification accuracy over conventional
classification schemes.

Keywords MLC · EM · Semi-supervised learning

1 Introduction

Land management organizations and the public have a need for the latest and
accurate regional land cover information to manage natural resources and monitor
land cover change. Remote sensing, which provides inexpensive and landscape scale
data, has proven to be very useful in land cover mapping, environmental monitoring,
and forest and crop inventory management. A common task in analyzing remote
sensing imagery is supervised classification, where the objective is to construct a
classifier based on few labeled training samples and then to assign a label (e.g., forest,
water, urban) to each pixel (is a vector, whose elements are spectral measurements)
in the entire image. There is a great demand for accurate land use and land cover
classification derived from remotely sensed data in various applications.

The commonly used maximum likelihood classifier (MLC) has two well known
limitations. First, it works well if the land cover classes are spectrally separable.
In reality, the classes under investigation are often spectrally overlapping as the
reflectance recorded by remote sensing satellites for many of these thematic classes
is dependent on several extraneous factors like terrain, soil type, moisture content,
acquisition time, atmospheric conditions, etc. The usefulness of ancillary data for im-
proving classification accuracy is well known, but there is no convenient multivariate
statistical model for this multi-source data (i.e., images and ancillary geo-spatial data
together). Several recent studies have focused on incorporating ancillary information
into the classification process. The most notable approaches are neural networks,
expert (knowledge based, rule based) systems [3, 20, 21].

The traditional maximum likelihood and maximum a posteriori (MAP) classifiers
have also been extended to incorporate ancillary information via a priori term.
However, MLC/MAP uses maximum likelihood estimation (MLE) technique for
estimating parameters of the probability distributions of each class, which requires
large amounts of accurate training data (10 to 30 ×|dimensions|) [13] per class.
Collecting ground truth data for a large number of samples is very difficult. Apart
from time and cost considerations, in many emergency situations like forest fires,
land slides, floods, it is very difficult to collect accurate training samples. As a result,
supervised learning is often carried out with a small number of training samples,
which leads to inaccurate estimation of parameters and thus higher classification
error rates. However, a large number of training samples without labels are always
available for classification of remote sensing images. Recently, semi-supervised
learning techniques that utilize large unlabeled training samples in conjunction with
small labeled training data are becoming popular in machine learning and data
mining. This popularity can be attributed to the fact that several of these studies have
reported improved classification and prediction accuracies, and that the unlabeled
training samples almost come free. However, semi-supervised learning techniques

Geoinformatica (2011) 15:29–47

31

also suffer from same limitations when dealing with multisource geospatial data for
the lack of convenient multivariate normal distribution. In this study we extend a
semi-supervised learning algorithm for classifying multisource geospatial data.

1.1 Related work and our contributions

Supervised methods are extensively used in remote sensing imagery classification
[11, 18]. Several recent studies have aimed at incorporating ancillary information into
MAP classifier via a priori probabilities. Studies by [12, 23] show that accuracy can be
improved by incorporating a priori probabilities computed from ancillary geospatial
data sets. A priori probability can be thought of as a scaling factor which shifts
(scales down or scales up) the decision boundaries in proportion to the expected
class volumes (frequency) in N-dimensional feature space. Though these approaches
may marginally increase accuracy, they have inherent limitations. For example, since
a priori probabilities are a global phenomenon, it cannot accurately model local or
regional spatial relationships.

Similarly, several approaches can be also be found in the literature that specifically
deal with small sample size problems in supervised learning [8, 9, 17, 22, 24]. These
methods are aimed at designing appropriate classifiers, feature selection, and pa-
rameter estimation so that classification error rates can be minimized while working
with small sample sizes. However, only recently attempts have been made to incorpo-
rate unlabeled samples in supervised learning, which gave raise to new breed of tech-
niques, collectively known as semi-supervised learning methods. Well-known studies
in this area include, but not limited to [5, 10, 14, 15]. The semi-supervised learning
techniques [19] have also been developed for remote sensing data analysis. The
common thread between many of these methods is the Expectation Maximization
(EM) [7] algorithm. Many of the semi-supervised learning methods pose class labels
as the missing data and use the EM algorithm to iteratively improve initial (either
guessed or estimated from small labeled samples) parameter estimates. Although
several previous studies showed that adding unlabeled training samples improves
overall classification accuracy, semi-supervised learning techniques also suffer from
the same limitations when dealing with multisource geospatial data for the lack of
convenient multivariate normal distribution.

In this paper, we provide two new hybrid semi-supervised learning methods based
on the expectation maximization (EM) algorithm. We exploit spectral and spatial
knowledge derived from ancillary geospatial datasets to create stratified spatial
units. The main objective for this stratification is to split the geographic region into
different spatial units where each spatial unit contains classes that are easily discrim-
inable. In the first approach, a semi-supervised learning is done separately on each
stratified unit. The training data in each segment included only a subset of classes
that are appropriate to that segment. In the second approach, for each stratified
unit, we converted class distribution into a priori probabilities, which are then
used in the semi-supervised classification. We have conducted several experiments
to evaluate the usefulness of these two methods in thematic classification of multi-
source geospatial datasets.

Though we focused on EM based semi-supervised learning, there are several
other extensions or related techniques that incorporate unlabeled training samples

32

Geoinformatica (2011) 15:29–47

in supervised learning. For example, the concept of self-training is in practice for a
long time. In self-training, an initial classifier is trained using a small labeled dataset.
This initial classifier is then applied on the unlabeled training samples. Unlabeled
training samples with the most confident predictions were then added to the labeled
training dataset. One disadvantage with self-training is that it uses hard labels, so
if the initial model is not good, then the performance of the final model is also
not good. Semi-supervised support vector machine or Transductive support vector
machines (TSVMs) are extensions of the standard support vector machines to exploit
unlabeled training data. Transfer learning also uses unlabeled training samples,
however, it is often used in the context of learning multiple tasks [1], thus it admits
additional labeled data sets. For several other techniques and recent advances, please
refer to the recent survey paper [25].

1.2 Scope

As mentioned earlier, there are a wide variety of techniques available for supervised
learning. In this study, we focused on the extension of the widely used parametric
classification scheme. However, there are several non-parametric learning schemes,
notably, neural networks and decision trees, which are highly suitable for multi-
source data classification [3, 20, 21]. Both neural networks and decision trees have
several advantages as well as disadvantages over parametric classifiers such as MLC.
For example, neural networks require a complex and expensive design phase [4],
and are sensitive to network topology [16]. On the other hand, the decision trees for
continuous attributes (such as remote sensing data) are overly complex. Apart from
these known limitations, at present there is no good way to incorporate unlabeled
training samples into these classifiers. Thus, in this paper we focused on the extension
of well-known parametric classifier, MLC, which is stable, robust, and one of the
most widely used techniques in remote sensing image classification.

The rest of this paper is organized as follows. In Section 2, we provide a basic
statistical framework for Bayesian classification and maximum likelihood based
parameter estimation. In Section 3, we present our hybrid classification scheme.
Experimental results are given in Section 4, followed by conclusions and future
directions in Section 5.

2 Statistical classification framework

In this section we present a general statistical framework for the classification of mul-
tispectral remote sensing image data. Each pixel in a remotely sensed image can
be thought of as a feature vector (x) and as an instance of a continuous random
variable. A continuous random variable is described by a probability density function
( p(·)). In the classification of a remote sensing image, our objective is to assign a
class label (yi) to each pixel (x) based on a certain decision criterion. Maximum
likelihood classification (MLC) and maximum a posteriori (MAP) classification are
two of the most widely used classifiers in remote sensing. Bayesian decision theory
plays a central role in statistical pattern classification. Both MLC and MAP are based
on Bayesian decision theory.

Geoinformatica (2011) 15:29–47

2.1 Maximum likelihood and maximum a posteriori classification

The outcome of a Bayesian decision rule is determined by the class conditional
densities p(x|yi) as well as the a priori probabilities P(yi). For p(x|yi), if we assume
a multivariate normal or Gaussian density, then p(x|yi) is given by

p(x|yi) =

(cid:2)

1
(2π )−N|(cid:3)i|

−1
2

e

(x−μi)t|(cid:3)i|−1(x−μi)

where μi and (cid:3)i are the mean vector and covariance matrix of the data for any given
class yi. The p(x|yi) is also called the likelihood of yi with respect to x. If we have no
knowledge about the prior distributions P(yi), then we can assume that all classes are
equally probable, that is, P(y1) = P(y2) = . . . = P(yM), where M is the number of
classes. As a consequence, we can further drop a priori term P(yi) in the computation
of the discriminant function gi(x); the resulting classifier is known as the maximum
likelihood classifier (MLC) and the discriminant function is given by

gi(x) = − ln |(cid:3)i| − (x − μi)t|(cid:3)i|−1(x − μi)
On the other hand, if we have knowledge about the prior distributions P(yi),
then the resulting classifier is known as the maximum a posterior (MAP), and the
discriminant function is given by:
gi(x) = ln P(yi) − 1
2

(x − μi)t|(cid:3)i|−1(x − μi)

ln |(cid:3)i| −

−1
2

(3)

(2)

The covariance matrix (cid:3) plays a key role in discriminant analysis. Covariance
accounts for the shape (size and orientation) of classes in the feature space. The
effectiveness of ML/MAP classification depends on the quality of the estimated
parameter vector (cid:4) (e.g., mean vector μ and the covariance matrix (cid:3) for each class)
from the training samples.

Here we used well-known parameter estimation technique, maximum likelihood
estimation (MLE), to obtain the model parameters from the training samples. For
Gaussian model, MLE yields the formulas for μ and (cid:3) as:

33

(1)

(4)

(5)

ˆμ = 1
n

n(cid:3)

k=1

xk

ˆ(cid:3) = 1
n

n(cid:3)

(cid:4)

k=1

xk − ˆμ

(cid:5)(cid:4)

(cid:5)
xk − ˆμ

t.

Therefore, from the given training samples of each class yi, we can estimate the
parameters (θi = μi, (cid:3)i) of the corresponding probability density function, p(x|θi).
By plugging these parameters into decision functions given in Eqs. 2 and 3, we can
predict the most probable class label, yi, for each new sample, x.

2.2 Limitations of ML/MAP classifiers

There are two problems with ML/MAP classifiers. First, the maximum likelihood
estimates have desired properties, such as, unbiased, consistent, and efficient. How-
ever, these properties are asymptotically true (n → ∞), meaning we need a large

MLC Performance

Supervised (BC) vs Semi-supervised (BC-EM)

Geoinformatica (2011) 15:29–47

34

y
c
a
r
u
c
c
A

5
7

0
7

5
6

0
6

5
5

0
5

5
4

0
4

80

70

60

50

40

y
c
a
r
u
c
c
A

ML Accuracy
Best EM accuracy

20

40

80

60
Number of (labeled) samples
(a) MLC

100

20

30

0

40
80
60
Number of samples
(b) SSL

100

120

Fig. 1 Classification performance as the number of (labeled) training samples increases a MLC,
b Semi-supervised

number of training samples. The plot in Fig. 1a shows the performance of the
Maximum likelihood classifier (BC) as a function of varying number of training
samples (more details can be found in the Section 4, Experiment 1). As can be
seen from the plot, MLC performance improves as the number of training samples
increases. Second problem arises due to the basic assumption that all training samples
are of the same type, for example, in the case of remote sensing image data, all
the attributes are continuous random variables and were drawn from a multivari-
ate Gaussian distribution. However, assumptions do not hold well in the case of
multisource data. For example, ancillary data is often nominal, and the attributes
(e.g., road and population density, uplands, lowlands) are discrete random variables.
Therefore, we cannot simply mix these continuous (images) and discrete (ancillary
geospatial data) variables together in the ML/MAP classification framework. We
now address these two problems in the following sections.

2.3 Semi-supervised learning

In this section, we address small training sample problem. In many supervised
learning situations, the class labels (yi)s are not readily available. However, assuming
that the initial parameters (cid:4)k can be guessed (as in clustering), or can be estimated
(as in semi-supervised learning), we can iteratively compute the parameter vector (cid:4)
using the expectation maximization algorithm. First let us assume that each sample
xi comes from a super-population D, which is a mixture of a finite number (M)
of populations D1, . . . , DM in some proportions α1, . . . , αM, respectively, where
(cid:6)
α j = 1 and α j ≥ 0( j = 1, . . . , M). Compared to our discussion in Section 2.1, we
i=1 as being generated

can think of α j as P(y j). Now we can model the data D = {xi}n
independently from the following mixture density.

M
j=1

p(xi|(cid:4)) =

α j p j(xi|θ j)

(6)

M(cid:3)

j=1

35

(7)

(9)

Geoinformatica (2011) 15:29–47

Here p j(xi|θ j) is the pdf corresponding to the mixture j and parameterized by
θ j, and (cid:4) = (α1, . . . , θM, θ1, . . . , θM) denotes all unknown parameters associated with
the M-component mixture density. In general, each component corresponds to a
class, y j. For a multivariate normal distribution, θ j consists of elements of the
mean vectors μ j and the distinct components of the covariance matrix (cid:3) j. The log-
likelihood function for this mixture density can be defined as:

⎡

⎣

n(cid:3)

M(cid:3)

L((cid:4)) =

ln

α j p j(xi|θ j)

⎦ .

i=1

j=1

⎤

In general, Eq. 7 is difficult to optimize because it contains the ln of a sum
term. However, this equation greatly simplifies in the presence of unobserved (or
incomplete) samples. Let us now pose X as an incomplete dataset, and assume that
we have unobserved data Y = {yi}n
i=1 such that yi tells us which component density
generated each xi. Assuming that we know the values of Y, the log-likelihood in Eq. 7
can be simplified as:

(cid:4)

(cid:5)

n(cid:3)

(cid:4)

L((cid:4)) = ln

P(X, Y)|(cid:4))

=

ln

P(xi|yi)P(y)

=

(cid:5)

n(cid:3)

(cid:4)
αyi pyi

ln

(xi|θyi

)

(cid:5)

.

(8)

i=1

i=1

However, in many supervised learning situations, the class labels (yi)s are not
available. However, assuming the the initial parameters (cid:4)k can be guessed (as in clus-
tering), or can be estimated (as in semi-supervised learning), we can easily compute
p j(xi|θ k
j

) in Eq. 6. Now, using Bayes’ rule, we can compute

(cid:4)

(cid:5)

p

yi|xi, (cid:4)k

=

(cid:11)

(cid:12)

(cid:11)

(cid:12)

αk
yi

pyi
(cid:4)

xi|θ k
yi
(cid:5) =

p

xi|(cid:4)k

αk
yi

pyi

(cid:6)

M
j=1

αk

j p j

xi|θ k
yi
(cid:11)
xi|θ k
j

(cid:12)

In a nutshell this is the EM algorithm. It consists of two steps. The first step, known
as expectation or E-step, maximizes the expectation of the log-likelihood function,
using the current estimate of the parameters and conditioned upon the observed
samples. In the second step of the EM algorithm, called maximization or M-step, the
new estimates of the parameters are computed. The EM algorithm iterates over these
two steps until the convergence is reached. These two steps are formalized below:

E-step: At the kth step of the iteration, where (cid:4)(k−1) is available, compute the

expected value of

(cid:4)
(cid:4), (cid:4)(k−1)
Q

(cid:5)

(cid:13)

= E

ln p(X, Y|(cid:4))|X, (cid:4)(k−1)

(cid:14)

.

(10)

(cid:5)
(cid:4)
(cid:4), (cid:4)(k−1)
, the
This step is called the expectation step. In the function Q
first argument (cid:4) corresponds to the parameters that need to be opti-
mized by maximizing the log-likelihood, and the second argument (cid:4)(k−1)
corresponds to the current estimate of the parameters that we used to
evaluate the expectation.

36

Geoinformatica (2011) 15:29–47

(cid:4)
(cid:4), (cid:4)(k−1)
M-step: Compute the new estimates of (cid:4) by maximizing the Q

(cid:5)
, that is,

find:

θ (k) = arg max
(cid:4)

(cid:4)
(cid:4), (cid:4)(k−1)
Q

(cid:5)

.

The second step is called the maximization step. These two steps are repeated
until convergence is reached. The log-likelihood function is guaranteed to increase
until a maximum (local or global or saddle point) is reached. For multivariate
normal distribution, the expectation E[.], which is denoted by pij, is nothing but the
probability that Gaussian mixture j generated the data point i, and is given by:

The new estimates (at the kth iteration) of parameters in terms of the old

parameters at the M-step are given by the following equations:

pij =

(cid:6)

(cid:16)

e

(cid:15)
(cid:15)
−1/2
(cid:15)
(cid:15)
(cid:15) ˆ(cid:3) j
(cid:15)
(cid:15)
(cid:15)
(cid:15) ˆ(cid:3)l

M
l=1

(cid:15)
−1/2
(cid:15)
(cid:15)

(cid:16)

e

− 1
2

(xi− ˆμ j)t ˆ(cid:3)−1

j

(xi− ˆμ j)

− 1
2

(xi− ˆμl)t ˆ(cid:3)−1

(xi− ˆμl)

l

(cid:17)

(cid:17)

n(cid:3)

ˆαk
j

= 1
n

pij

and

ˆμk
j

=

i=1
(cid:6)
n
i=1 pij

(cid:11)

(cid:12) (cid:11)

xi − ˆμk
j
(cid:6)
n
i=1 pij

ˆ(cid:3)k
j

=

(cid:6)
n
i=1 xi pij
(cid:6)
n
i=1 pij
(cid:12)

t

xi − ˆμk
j

(11)

(12)

(13)

(14)

More detailed derivation of these equations can be found in [2]. Standard semi-
supervised algorithms obtain initial estimates of the parameters using the labeled
samples Dl, and then uses EM algorithm (Eqs. 12–14) and unlabeled samples Dul to
refine the initial estimates. Basic semi-supervised learning framework is summarized
in Algorithm 1.

3 Hybrid classification scheme

Figure 2 shows the overall architecture of our hybrid classification scheme. This
hybrid classification scheme consists of two major components. The first component
consists of exploiting ancillary geospatial databases to create stratified regions. The
second component consists of building a semi-supervised learning model for each
of the stratified regions. Each of these components are described in detail in the
following sections.

3.1 Knowledge-based image stratification

In creating stratified units we exploit both satellite images and ancillary geospatial
datasets. This unit of hybrid classification scheme exploits both spectral and spatial
knowledge to create a set of stratified spatial units in such a way that the spectrally
overlapping classes fall into different spatial units.

Geoinformatica (2011) 15:29–47

Fig. 2 Hybrid classification
scheme

37

Spectral
Knowledge

Spatial
Knowledge

Image

Ancillary
Geospatial
Datasets

Stratified
Image

Semi-supervised Learning

Spectral Knowledge Object extraction from spectral relationships alone is al-
most impossible, nonetheless it is interesting and useful to find simple spectral
rules, like: ∀Pixel( p), I F(b and1( p) > b and2( p) > b and3( p)) T H EN Output( p) =
‘WATER’. Even though finding such rules is difficult, the main contribution of
spectral knowledge is in finding inherent data structures within the image. Often
transformations, like normalized density vegetation index (NDVI) and Tasseled
Cap (TC), will yield more insights into the structure of the data. The Tasseled Cap
concept involves identifying the existing data structures for a particular sensor and
application, and changing the viewing perspective such that those data structures can
be viewed directly [6]. Tasseled Cap transformation especially optimizes the data
viewing for vegetation studies. In case of Landsat data, the six bands of reflected
data can be effectively projected on to three dimensions. The first two planes
define the soils, vegetation, and a transition zone between them, while the third axis
corresponds to canopy and soil moisture. We have also extracted spectral knowledge
derived from greenness index channel of the TC transformation for stratifying the
remote sensing image. These rules are summarized in Table 1.

Spatial Knowledge The purpose of the spatial knowledge base is to stratify the
remote sensing image into homogeneous regions with the following properties:

Let R be any given image.
The purpose of image stratification is to find:

finite set of regions R1, R2, ...., Rq, such that

R = ∪q

i=1 Ri, Ri ∩ R j = ∅

and k classes Cik ε Ri are spectrally separable
(i.e., inter-class variation is minimum and intra-class variation is maximum).

Our objective is to find regions in such a way that signature continuity holds within
any region Ri and for any class: if yrk = y jk, then r = j. But in practice we may
not find such regions, so there may be some common classes among the regions.

38

Geoinformatica (2011) 15:29–47

Spectral Knowledge Base

Spatial Knowledge Base

Geometrically Corrected
(May & Sept. ETM+)

1990 Wessex
Transportation
(TIGER)

1990 Wessex
Census Block
(TIGER)

Water

True

(b1 > b2 > b3)

Non-Water (TMN)

Tasseled Cap
Transformation

Tasseled Cap Image
(b2 = greenness)

LineDensity &
Thersholding

#Persons / Sq.
Miles

Transportation
(Undev, Lo, Hi)

Population Density
(Undev, Lo, Hi)

Trasportation Density (0.0078  <  (0.0078 - 0.0145) > 0.0145)
Population Density (1000 < (1000 - 5000) > 5000)
Greenness ( 25 > (25 - 15) < 15)

TMN
Developed

Developed/
Undeveloped

Developed Hi (3) & Lo (2)
Density Image

TMN
Undeveloped (TMUD)

Overlay

TMUD
Lo Land (TMLO)

Upland/
LoLand

NWI Up/Lo
Land Mask

Developed +
Loland (TML)

Undeveloped +
Upland (TMU)

Fig. 3 Flow chart showing the use of spectral and spatial knowledge base

In the training phase we have to collect sufficient samples for overlapping classes to
avoid artificial contours in the final classified image. Exploratory analysis of geospa-
tial databases, visual exploration, and transformed divergence between probability
distribution of each class pair, and expert inputs, have led us to the development
of this algorithm for stratifying images into different geographic regions. The flow
chart for extracting spectral and spatial knowledge to derive these regions is shown
in Fig. 3.

3.2 Hybrid semi-supervised classification

We now describe two simple variations of hybrid semi-supervised learning. In the
first approach, semi-supervised learning algorithm (Table 2) is constructed on each

Geoinformatica (2011) 15:29–47

Table 1 Spectral and spatial
knowledge base

Knowledge base
(TassledCap.Greenness ≤ 15) &&

(Pop.Density > 5,000) ||
(Road.Density > 0.0145)

(15 > TassledCap.Greenness ≤ 25)
&& (1000 ≤ Pop.Density ≤ 5,000)
|| (0.0078 ≤ Road.Density ≤ 0.0145

39

Class/region

High density
developed

Low density
developed

stratified unit separately. Though this algorithm is straight forward to implement,
it has certain limitations. First, though the space is stratified into different disjoint
units, the spatial distribution of classes may still not be disjoint. For example, it may
still possible to find forest (e.g., urban forests and parks) inside the urban stratified
unit. Second, the boundaries themselves may not be accurate, therefore, may not
follow real boundaries of the objects. As a result, one has to select training samples
for forest in both urban segment as well as upland/lowlands. This will increase the
training efforts and the number of labeled training samples required. A second
problem is the discontinuity in the spatial distribution of classes across contiguous
segments. Though it is clear from the satellite images that class (spatial) distribution
is continuous, classified image shows discontinuities across adjacent segments.

In the second approach, instead of fitting a semi-supervised learning algorithm
for each stratified segment, we introduced an indicator variable to account for a
stratified image. For each segment, we generated ‘a priori’ distribution of classes.
Figure 4 shows the indicator variable and the corresponding a priori probabilities of
each class. The idea is very simple. We note that the percentage of wetland class in
lowlands segment is about 20%, thus, the algorithm automatically scales the posterior
probability of the wetlands class if the indicator variable is ‘lowlands(2).’ Thus, we
can easily model the class continuity across segments using this second variation,
without resorting to full training of wetlands class in the lowlands segment (as in
the first variation of the hybrid semi-supervised approach). As shown in the exper-
imental section, though both methods have similar accuracies, the second approach
is more preferable, as the final classified image captures the true boundaries of the
classes and there are no artificial discontinuities in spatial distribution of the classes
across the segments. Moreover, the second approach is more flexible, as there is no

Table 2 Semi-supervised learning algorithm

Inputs:

Training data set D = Dl ∪ Dul, where Dl consists of labeled samples

Initial Estimates:

Loop:

E-step:

M-step:

Output:

and Dul contains unlabeled samples.

Build initial classifier (MLC or MAP) from the labeled training samples,
Dl. Estimate initial parameter using MLE, to find ˆθ (see Eqs. 4 and 5).

While the complete data log-likelihood improves:
Use current classifier to estimate the class membership of each unlabeled
sample, that is, the probability that each Gaussian mixture component
generated the given sample point, pij (see Eq. 12).

Re-estimate the parameter, ˆθ, given the estimated Gaussian mixture

component membership of each unlabeled sample (see Eqs. 13, 14)
A MLC (Eq: 2) or MAP (Eq: 3) classifier, that takes the given sample

(feature vector) and predicts a label.

40

Geoinformatica (2011) 15:29–47

1

2

1

Segment
(Indicator Variable)

"a priori" probabilities

Ag(1) Up. Conif(2) Up. Hw(3) Low. Hw(4) Low. Conif(5) Wet(6)

1   (Uplands)

2    (Lowlands)

0.01

0.08

0.20

0.07

...

0.20

0.01

...

0.05

0.15

0.01

0.15

0.08

0.20

...

Fig. 4 Stratified units (segments) and corresponding ‘a priori’ probabilities

need to train all the possible classes in each segment. New, ancillary information can
be readily incorporated into the second approach via modifying the indicator variable
and corresponding ‘a priori’ probabilities without resorting to the full retraining of
the classifier. A wide variety of techniques can be incorporated to come up with
a suitable estimation of ‘a priori’ probabilities. For example, one can use an initial
classifier in conjunction with previous estimates (old maps) to come up with good
estimates of ‘a priori’ probabilities.

4 Experimental results

We used a spring Landsat 7 scene, taken on May 31, 2000, over the town of Cloquet
located in Carlton County, Minnesota. We designed two different experiments to
validate our hypothesis that adding ancillary geospatial datasets and unlabeled train-
ing samples improve the classification performance. For both of these experiments
the test dataset was fixed and consisted of 85 plots. We considered the following
ten classes (Water, Bare Soil, Agriculture, Upland Conifer, Upland Hardwood,
Lowland Conifer, Lowland Hardwood, Wetlands, Low density Urban and High
density Urban) for all experiments. Figure 5 shows example raw satellite image (red,
green, blue bands) and corresponding classified image clips. For discussion purposes
we summarized key results as graphs and contingency tables for easy understanding.
Classification accuracy was summarized using the producers accuracy (PA), users
accuracy (UA), and overall accuracy (OA).

Experiment 1 This experiment is designed to understand the implications of train-
ing a maximum likelihood classifier with a small number of training samples. Overall
we collected 100 labeled plots. From this dataset, we generated smaller subsets of 20,
40, 60, and 80 labeled plots. We used these sub-sampled datasets to train ML classifier
and tested its performance using the test dataset described above. Accuracy results
were summarized as a graph (Fig. 1). We also collected an unlabeled training dataset
consisting of 300 plots for use in semi-supervised learning.

Experiment 2 The second experiment was designed to understand the performance
of a hybrid classification scheme. First we used census data, TIGER road maps,
and the National Wetlands Inventory (NWI) database to generate four different
stratified spatial regions using the procedure described in Section 3.1. The main
objective behind these four stratified units is to better delineate the high- and

Geoinformatica (2011) 15:29–47

41

(a)

(b)

Fig. 5 Example raw and classified image clips a RGB, b classified

low-density urban classes, and the up- and low-land forest classes. We have used the
same training datasets (labeled and unlabeled) as in experiment 1. Accuracy results
were summarized as contingency tables (Tables 3, 4, 5, 6, 7, and 8).

4.1 Discussion

Overall classification accuracy results from Experiment 1 are summarized in Fig. 1.
From this experiment it is clear that maximum likelihood estimates are highly
dependent on both the quantity and the quality of labeled training samples. The

Table 3 MLC accuracy for S1

Ground truth

1

2

3

4

5

6

7

8

9

10

UA

Agriculture (1) 87.00
0.00
Upland

4.00
37.00

22.00
1.00
0.00
30.00 41.00 25.00

2.00
0.00

13
2

0 51.00 48.33
0.00 27.41
0

0.00

1.00 150.00 10.00 19.00

0.00

Lowland

0.00

8.00

58.00 29.00 20.00

0.00

0.00

24.00

1.00

5.00 60.00

0.00

0.00 103.00
0.00
3.00

1.00
0.00

2.00 26.00 130.00
0.00
0.00

8
0.00 143

0

0

0

0.00
0.00

0.00

0.00

0.00

0.00
0.00

0

0

0

0
0

0.00 83.33

2.00 24.79

0.00 66.67

9.00 46.59
7.00 93.46

3.00

0.00

0.00

0.00

0.00

0.00

54 76.00

0 11.00 52.78

0.00
3.00
90.62

35.00
5.00
17.05

0.00
0.00
0.00
0.00
0.00
0.00
57.25 33.33 39.74

0.00
9.00
92.20

0.00
9.00

0
0
65 89.41 100 25.93 (OA) 55.63

46
0.00 56.79
0 28.00 51.85

conifer (2)

Upland

hardwood (3)

hardwood (4)

Lowland

conifer (5)
Wetlands (6)
Low density
urban (7)
High density
urban (8)

Water (9)
Bare soil (10)
PA

42

Geoinformatica (2011) 15:29–47

Table 4 SSL accuracy for S1

Ground truth

1

2

3

4

5

6

7

8

9

10

Agriculture (1)
Upland

157.00
0.00

0.00
117.00

0.00
3.00

0.00
10.00

0.00
0.00

17.00
5.00

0.00
0.00

UA

87.22
86.67

6.00
0.00

0.00

10.00

105.00

64.00

0.00

1.00

0.00

0.00

58.33

Lowland

4.00

19.00

6.00

74.00

0.00

7.00

0.00

1.00

63.25

0
0

0

6

0.00

68.00

0.00

5.00

17

0.00

0.00

0.00

0.00

18.89

23.00
0.00

53.00
2.00

0.00
0.00

8.00
0.00

11
0

74.00
0.00

109.00
105.00

0.00
43.00

1.00
3.00

26.52
68.63

0.00

0.00

0.00

0.00

0

0.00

7.00

120.00

17.00

83.33

0.00
6.00
82.63

8.00
0.00
42.24

0.00
0.00
92.11

0.00
0.00
45.96

0
0
50

0.00
1.00
98.67

4.00
14.00
39.03

0.00
0.00
73.62

68
0
100

1.00
33.00
53.23

83.95
61.11
61.57

conifer (2)

Upland

hardwood (3)

hardwood (4)

Lowland

conifer (5)
Wetlands (6)
Low density
urban (7)
High density
urban (8)

Water (9)
Bare soil (10)
PA

0
0

0

0

0

0
0

0

plot in Fig. 1a shows that as the number of training (labeled) samples increases, the
conventional maximum likelihood estimates gets better and hence, the classification
performance of the Maximum likelihood classifier (BC) also improves. It is also
interesting to note that the variance between best and worst accuracies gets reduced
as the number of samples increase. This is because the noise averages out as the
number of samples increases.

The second part of the experiment shows the performance of a semi-supervised
learning algorithm against different sizes of labeled training samples. This experi-
ment shows that the semi-supervised algorithm works well when the labeled training

Table 5 MS-SSL accuracy for S1

Ground truth

1

2

3

4

5

6

7

8

9

10

UA

Agriculture (1) 168.00
Upland

0.00
0.00 89.00

0.00
0.00
6.00 40.00

2.00
0.00

5.00
0.00

5.00
0.00

0.00
0.00

0.00
0.00

0.00 93.33
0.00 65.93

0.00

0.00 144.00 36.00

0.00

0.00

0.00

0.00

0.00

0.00 80.00

Lowland

1.00

4.00

6.00 90.00 16.00

0.00

0.00

0.00

0.00

0.00 76.92

0.00 18.00

0.00 16.00 55.00

1.00

0.00

0.00

0.00

0.00 61.11

4.00
0.00

1.00
0.00

0.00
0.00

9.00 31.00 234.00
0.00
0.00

0.00
0.00 124.00

0.00
29.00

0.00
0.00

0.00 83.87
0.00 81.05

0.00

0.00

0.00

0.00

0.00

0.00

17.00 118.00

0.00

9.00 81.94

5.00
0.00
13.00
0.00
90.32 76.07

0.00
0.00
0.00
0.00
0.00
0.00
92.31 47.12 52.88

2.00
10.00
92.86

4.00
2.00
81.58

70.00
0.00 86.42
0.00
0.00 29.00 53.70
0.00
80.27 100.00 76.32 79.33

conifer (2)

Upland

hardwood (3)

hardwood (4)

Lowland

conifer (5)
Wetlands (6)
Low density
urban (7)
High density
urban (8)

Water (9)
Bare soil (10)
PA

Geoinformatica (2011) 15:29–47

Table 6 MLC accuracy for S2

Ground truth

2

3

4

5

6

7

8

9

10

UA

1

20
0

0

0

0

Agriculture (1)
Upland

conifer (2)

Upland

hardwood (3)

Lowland

hardwood (4)

Lowland

conifer (5)
Wetlands (6)
Low density
urban (7)
High density
urban (8)

Water (9)
Bare soil (10)
PA

0.00
76.00

0.00
2.00

143.00
52.00

0.00
1.00

12.00
2.00

0.00
0.00

5.00
2.00

1.00

60.00

119.00

0.00

0.00

0.00

0.00

2.00

5.00

103.00

7.00

0.00

0.00

0.00

9.00

0.00

40.00

41.00

0.00

0.00

0.00

59
0

2.00
6.00

0.00
0.00

113.00
4.00

68.00
0.00

18.00
133.00

0.00
10.00

0.00
0.00

0

4.00

0.00

10.00

0.00

8.00

122.00

0.00

0
1
25

6.00
0.00
71.70

0.00
0.00
89.55

6.00
34.00
16.51

0.00
0.00
35.04

0
0
100

1.00
4.00
74.72

0.00
9.00
86.52

68.00
0.00
90.67

0
6
100

0
0

0

0

0

19
0

0

0
0

0

0

0

0
0

0

43

11.11
56.30

33.33

88.03

45.56

6.81
86.93

84.72

83.95
11.11
45.86

samples are few. The usefulness of unlabeled samples diminishes (see Fig. 1b)
as the number of labeled samples increase. The classification accuracy results of
semi-supervised learning (SSL) are summarized in the form of contingency tables
(Tables 4 and 7).

The second experiment (Experiment 2) shows the relative performance of hybrid
classification scheme (using ancillary spatial datasets) against maximum likelihood
classification (without ancillary spatial datasets). Each table consists of accuracy
results from maximum likelihood classification (MLC), regular semi-supervised
learning without ancillary data (SSL), and the hybrid semi-supervised learning

Table 7 SSL accuracy for S2

Ground truth

1

2

3

4

5

6

7

8

9

10

UA

Agriculture (1) 158.00
Upland

0.00
0.00 117.00

0.00
3.00
0.00 17.00

0.00
0.00

3.00
0.00

12.00
0.00

2.00
0.00

0.00
1.00

2.00 87.78
0.00 86.67

0.00

0.00 132.00 48.00

0.00

0.00

0.00

0.00

0.00

0.00 73.33

Lowland

1.00

11.00

12.00 81.00 12.00

0.00

0.00

0.00

0.00

0.00 69.23

0.00

21.00

0.00 16.00 53.00

0.00

0.00

0.00

0.00

0.00 58.89

9.00
0.00

1.00
21.00

0.00
0.00

8.00 30.00 226.00
0.00
0.00

5.00
0.00 108.00

0.00
24.00

0.00
0.00

0.00 81.00
0.00 70.59

0.00

0.00

0.00

0.00

0.00

0.00

13.00 131.00

0.00

0.00 90.97

0.00
5.00
91.33

1.00
0.00
68.02

0.00
0.00
0.00
0.00
0.00
0.00
91.67 46.82 55.79

2.00
4.00
96.17

5.00
2.00
74.48

0.00 73.00

0.00 90.12
17.00
0.00 26.00 48.15
75.29 98.65 92.86 78.20

conifer (2)

Upland

hardwood (3)

hardwood (4)

Lowland

conifer (5)
Wetlands (6)
Low density
urban (7)
High density
urban (8)

Water (9)
Bare soil (10)
PA

44

Geoinformatica (2011) 15:29–47

Table 8 MS-SSL accuracy for S2

Ground truth

1

2

3

4

5

6

7

8

9

10

UA

Agriculture (1) 160.00
Upland

0.00
0.00 122.00

0.00
3.00
2.00 10.00

0.00
0.00

4.00
0.00

9.00
0.00

1.00
0.00

0.00
1.00

3.00 88.89
0.00 90.37

0.00

3.00 141.00 36.00

0.00

0.00

0.00

0.00

0.00

0.00 78.33

Lowland

0.00

11.00

2.00 91.00 13.00

0.00

0.00

0.00

0.00

0.00 77.78

0.00

18.00

0.00 17.00 55.00

0.00

0.00

0.00

0.00

0.00 61.11

8.00
0.00

1.00
14.00

0.00
0.00

9.00 30.00 231.00
0.00
0.00

0.00
0.00 114.00

0.00
25.00

0.00
0.00

0.00 82.80
0.00 74.51

0.00

0.00

0.00

0.00

0.00

0.00

13.00 131.00

0.00

0.00 90.97

0.00
4.00
93.02

1.00
0.00
71.76

0.00
0.00
0.00
0.00
0.00
0.00
97.24 54.82 56.12

3.00
4.00
95.45

4.00
0.00
81.43

0.00 73.00

0.00 90.12
16.00
0.00 30.00 55.56
75.72 98.65 90.91 81.25

conifer (2)

Upland

hardwood (3)

hardwood (4)

Lowland

conifer (5)
Wetlands (6)
Low density
urban (7)
High density
urban (8)

Water (9)
Bare soil (10)
PA

(second variation) with ‘a priori’ probabilities generated from the ancillary data
(MS-SSL). We have not reported results from the hybrid semi-supervised learning
(first variation), as the results were very close to the second variation and the
differences in accuracies were not significant. However, the quality of class bound-
aries across segments in the final classified image, are much better in the second
variation of the MS-SSL classifier. This is because, in the first case, the partitions
were hard, therefore, continuity of objects across the partition was not guaranteed,
unless the object boundaries in ancillary data coincides with object boundaries in
the image. In the second variation of MS-SSL, object boundaries across the image
are more continuous. We reported results on only two subsets of labeled training
samples (S1 & S2, each consisting of two labeled plots per class) for brevity, though
experiments were conducted on all the ten subsets of the training dataset. These two
subsets represent the least and best MLC accuracies over the ten subsets. This experi-
ment shows that basic semi-supervised classification achieved about 6 to 32% overall
accuracy improvement over the MLC, and the hybrid semi-supervised achieved an
improvement of about 24 to 36% overall accuracy over MLC. Hybrid approach
generally performed better than semi-supervised learning in all experiments. This
improvement can be attributed to the addition of unlabeled training samples as
well as to the better discrimination among overlapping classes with the addition of
ancillary geospatial data.

5 Conclusions and open research problems

In this study, we presented a hybrid semi-supervised classification scheme for
extracting thematic information from multisource geospatial databases. This new
scheme addresses two major limitations in the most widely used maximum likelihood
classifier, namely, small training samples, and multisource data. The experimental

Geoinformatica (2011) 15:29–47

45

results shows improved accuracy 24% to 36%, and flexibility in incorporating
ancillary geospatial databases into the classification process.

Though the hybrid semi-supervised classification framework itself is very flexible,
stratified image segment construction from ancillary data is not. The stratification
of geospatial databases into regions is an expert driven task. It would be interesting
to automatically discover these geographic regions such that spectrally overlapping
regions can be easily discriminated. One possible solution is to construct a decision
tree on ancillary geospatial data and applying semi-supervised learning on leaf
nodes. However, this requires a new objective function for selecting the attribute
at each node in the decision tree. Second, it is assumed that the training samples are
independent and identically distributed which is not true for geospatial databases, as
the samples are often spatially autocorrelated. Our future work will be focused on
extending the hybrid classification to automatically discover the geographic regions
(segments) and also model spatial autocorrelation in the hybrid semi-supervised
learning process.

Acknowledgements We would like to thank our collaborators Prof. Shekhar and Prof. Thomas
E. Burk at the University of Minnesota for their contributions and support. We would like to
thank ORNL reviewers Eddie Bright, Phil Coleman, Veeraraghavan Vijayaraj, and the unanimous
SSTDM-07 workshop reviewers whose comments have greatly helped us in improving the technical
quality of this paper. This research was partially supported by the LDRD initiative on “Emerging
Science and Technology for Sustainable Bioenergy."

References

1. Ando RK, Zhang T (2005) A framework for learning predictive structures from multiple tasks

and unlabeled data. J Mach Learn Res 6:1817–1853

2. Bilmes J (1997) A gentle tutorial on the EM algorithm and its application to parameter estima-
tion for Gaussian mixture and hidden Markov models. Technical Report, University of Berkeley.
ICSI-TR-97-021, 1997

3. Bolstad P, Lillesand T (1992) Rule-based classification models: ﬂexible integration of satellite

imagery and thematic spatial data. Photogramm Eng Remote Sensing 58(7):965–971

4. Bruzzone L, Conese C, Maselli F, Roli F (1997) Multisource classification of complex rural areas
by statistical and neural-network approaches. Photogramm Eng Remote Sensing 63(5):523–533
5. Cozman F, Cohen I, Cirelo M (2003) Semi-supervised learning of mixture models. In: Twentieth

international conference on machine learning (ICML)

6. Crist E, Kauth RJ (1986) The tasseled cap de-mystified. Photogramm Eng Remote Sensing

52(1):81–86

7. Dempster A, Laird NM, Rubin DB (1977) Maximum likelihood from incomplete data via the

EM algorithm. J R Stat Soc Ser B 39(1):1–38

8. Duin R (2000) Classifiers in almost empty spaces. In: Proc. 15th int. conference on pattern
recognition (Barcelona, Spain, Sep.3–7), vol 2. IEEE Computer Society, Los Alamitos, pp 1–7
9. Fukunaga K, Hayes RR (1989) Effects of sample size in classifier design. IEEE Trans Pattern

Anal Mach Intell 13(3):252–264

10. Goldman S, Zhou Y (2000) Enhancing supervised learning with unlabeled data. In: Proc. 17th

international conf. on machine learning. Morgan Kaufmann, San Francisco, pp 327–334

11. Jensen JR (1996) Introductory digital image processing, a remote sensing perspective. Prentice

Hall, Upper Saddle River

12. Maselli F, Conese C, Petkov L, Resti R (1992) Inclusion of prior probabilities derived from a non-
parametric process into the maximum likelihood classifier. Photogramm Eng Remote Sensing
58(2):201–207

13. Mather PM (2004) Computer processing of remotely-sensed images: an introduction. Wiley,

New York

46

Geoinformatica (2011) 15:29–47

14. Mitchell T (1999) The role of unlabeled data in supervised learning. In: Proceedings of the sixth

international colloquium on cognitive science, San Sebastian, Spain

15. Nigam K, McCallum AK, Thrun S, Mitchell TM (2000) Text classification from labeled and

unlabeled documents using EM. Mach learn 39(2/3):103–134

16. Paola JD, Schowengerdt RA (1997) The effect of neural-network structure on a multispectral

land-use/land-cover classification. Photogramm Eng Remote Sensing 63(5):535–544

17. Raudys SJ, Jain AK (1991) Small sample size effects in statistical pattern recognition: recom-

mendations for practitioners. IEEE Trans Pattern Anal Mach Intell 13(3):252–264
18. Richards JA, Jia X (1999) Remote sensing digital image analysis. Springer, New York
19. Shahshahani B, Landgrebe D (1994) The effect of unlabeled samples in reducing the small
sample size problem and mitigating the Hughes phenomenon. IEEE Trans Geosci Remote Sens
32(5):1087–1095

20. Skidmore A (1989) An expert system classifies eucalypt forest types using thematic mapper data

and a digital terrain model. Photogramm Eng Remote Sensing 55(10):1449–1464

21. Skidmore A, Turner B, Brinkhof W, Knowles E (1997) Performance of a neural network:
mapping forest using GIS and remotely sensed data. Photogramm Eng Remote Sensing 63(5):
501–514

22. Skurichina M, Duin R (1996) Stabilizing classifiers for very small sample sizes. In: Proc. 10th int.

conference on pattern recognition. IEEE Computer Society, Los Alamitos, pp 891–896

23. Strahler A (1980) The use of prior probabilities in maximum likelihood classificaiton of remote

24. Tadjudin S, Landgrebe DA (1999) Covariance estimation with limited training samples. IEEE

sensing data. Remote Sens Environ 10:135–163

Trans Geosci Remote Sens 37(4):2113–2118

25. Zhu X (2008) Semi-supervised learning literature survey. Technical report (TR 1530, University

of Wisconsin, Madison)

Ranga Raju Vatsavai has been a research scientist in the Computational Sciences and Engineering
Division at the Oak Ridge National Laboratory (ORNL) since December 2006. He has been con-
ducting research in the area of spatiotemporal databases and data mining for the past 18 years. Before
joining ORNL, he worked at IBM-Research (2004–06; IITDelhi campus), U of Minnesota (1999–
2004; Twin-cities, MN), AT&T Labs (1998; Middletown, NJ), Center for Development of Advanced
Computing (1995–98; C-DAC, Pune, India), and National Forest Data Management Center (1990–
95; FRI Campus, Dehradun, India). He has published over thirty peer-reviewed articles and served
on program committees of several international conferences. He also contributed to several highly
successful software systems in various capacities (UMN-MapServer—a world leading open source
WebGIS, *Miner—a spatiotemporal data mining workbench, EASI/PACE Parallel Fly!, Parallel
SAR, and the first parallel softcopy photogrammetry system for IRS-1C/1D satellites). His research
interests include databases, data mining, machine learning, parallel computing, and spatiotemporal
informatics.

Geoinformatica (2011) 15:29–47

47

is the leader of the Geographic Information Science & Technology (GIST)
Budhendra Bhaduri
group in the Computational Sciences and Engineering Division at the Oak Ridge National Lab-
oratory. Dr. Bhaduri’s responsibilities include conceiving, designing, and implementing innovative
computational methods and algorithms to solve a wide variety of geospatial problems involving
population dynamics modeling, evacuation modeling, critical infrastructure protection, natural
resource studies, and emergency management. In addition to DOE, he is actively involved with
research efforts with several federal agencies including DoD, DHS, USEPA, USGS, and NIH.
He has published extensively in a number of leading scientific journals. Dr. Bhaduri is a principal
founding member of DOE’s Geospatial Sciences Steering Committee. In 2004, he was invited to be
a lead theme writer for the first National R&D plan for Critical Infrastructure Protection developed
by the Department of Homeland Security’s Science & Technology Directorate. He has served
on national and international panels dealing with geospatial sciences research including a recent
National Academy of Sciences panel recommending future geospatial research for the National
Geospatial-Intelligence Agency. Dr. Bhaduri received his Ph.D. in Earth & Atmospheric Sciences
from Purdue University. He has a M.S. from Kent State University, and a M.Sc. and a B.Sc. in
Geology from Presidency College, University of Calcutta.

