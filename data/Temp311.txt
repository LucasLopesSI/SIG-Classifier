This article was downloaded by: [Moskow State Univ Bibliote]
On: 31 October 2013, At: 11:06
Publisher: Taylor & Francis
Informa Ltd Registered in England and Wales Registered Number: 1072954 Registered
office: Mortimer House, 37-41 Mortimer Street, London W1T 3JH, UK

International Journal of Geographical
Information Science
Publication details, including instructions for authors and
subscription information:
http://www.tandfonline.com/loi/tgis20

A rough set approach to the discovery
of classification rules in spatial data
Yee Leung a , Tung Fung a , Ju‐Sheng Mi b & Wei‐Zhi Wu c
a Department of Geography and Resource Management , Center
for Environmental Policy and Resource Management , and Institute
of Space and Earth Information Science , The Chinese University
of Hong Kong , Hong Kong
b College of Mathematics and Information Science , Hebei Normal
University , Shijiazhuang, Hebei, 050016, P. R. China
c Information College , Zhejiang Ocean University , Zhoushan,
Zhejiang, 316004, P. R. China
Published online: 20 Nov 2007.

To cite this article: Yee Leung , Tung Fung , Ju‐Sheng Mi & Wei‐Zhi Wu (2007) A rough set approach
to the discovery of classification rules in spatial data, International Journal of Geographical
Information Science, 21:9, 1033-1058, DOI: 10.1080/13658810601169915

To link to this article:  http://dx.doi.org/10.1080/13658810601169915

PLEASE SCROLL DOWN FOR ARTICLE

Taylor & Francis makes every effort to ensure the accuracy of all the information (the
“Content”) contained in the publications on our platform. However, Taylor & Francis,
our agents, and our licensors make no representations or warranties whatsoever as to
the accuracy, completeness, or suitability for any purpose of the Content. Any opinions
and views expressed in this publication are the opinions and views of the authors,
and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content
should not be relied upon and should be independently verified with primary sources
of information. Taylor and Francis shall not be liable for any losses, actions, claims,
proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or
howsoever caused arising directly or indirectly in connection with, in relation to or arising
out of the use of the Content.

This article may be used for research, teaching, and private study purposes. Any
substantial or systematic reproduction, redistribution, reselling, loan, sub-licensing,
systematic supply, or distribution in any form to anyone is expressly forbidden. Terms &

Conditions of access and use can be found at http://www.tandfonline.com/page/terms-
and-conditions

Downloaded by [Moskow State Univ Bibliote] at 11:06 31 October 2013 International Journal of Geographical Information Science
Vol. 21, No. 9, October 2007, 1033–1058

Review Article

A rough set approach to the discovery of classification rules in spatial
data

YEE LEUNG*{, TUNG FUNG{, JU-SHENG MI{ and WEI-ZHI WU§
{Department of Geography and Resource Management, Center for Environmental
Policy and Resource Management, and Institute of Space and Earth Information
Science, The Chinese University of Hong Kong, Hong Kong
{College of Mathematics and Information Science, Hebei Normal University,
Shijiazhuang, Hebei, 050016, P. R. China
§Information College, Zhejiang Ocean University, Zhoushan, Zhejiang, 316004, P. R.
China

(Received 12 September 2006; in final form 14 December 2006 )

This paper proposes a novel rough set approach to discover classification rules in
real-valued spatial data in general and remotely sensed data in particular. A
knowledge induction process is formulated to select optimal decision rules with a
minimal set of features necessary and sufficient for a remote sensing classification
task. The approach first converts a real-valued or integer-valued decision system
into an interval-valued information system. A knowledge induction procedure is
then formulated to discover all classification rules hidden in the information
system. Two real-life applications are made to verify and substantiate the
conceptual arguments. It demonstrates that the proposed approach can effectively
discover in remotely sensed data the optimal spectral bands and optimal rule set for
a classification task. It is also capable of unraveling critical spectral band(s)
discerning certain classes. The framework paves the road for data mining in mixed
spatial databases consisting of qualitative and quantitative data.

Keywords: Classification; GIS; Knowledge discovery; Land cover classes;
Remote sensing; Rough sets

2000 Mathematics Subject Classifications: 68T30; 68U35

1.

Introduction

Improvement in classification accuracy is always a concern in the classification of
spatial data in general and remote sensing data in particular. How to extract
accurate and timely knowledge from images relies upon their quality and resolution,
and the classification techniques used. The improvement in spatial and spectral
resolutions of images, such as IKONOS, Quickbird and hyperspectral images, has
induced new challenges for image classification which leads to the development of
new classification techniques. Per-pixel based maximum likelihood classification
(Foody 1999), for example, is plagued by its restrictive parametric assumption and
inability to take into consideration of higher order image characteristics. Artificial

*Corresponding author. Tel: (852)26096473; Fax: (852)26037242. Email: yeeleung@

cuhk.edu.hk

International Journal of Geographical Information Science
ISSN 1365-8816 print/ISSN 1362-3087 online # 2007 Taylor & Francis
http://www.tandf.co.uk/journals
DOI: 10.1080/13658810601169915

Downloaded by [Moskow State Univ Bibliote] at 11:06 31 October 2013 1034

Y. Leung et al.

neural networks (see for example Gopal and Fischer 2001, Paola and Schowengerdt
1995), fuzzy classifications (see for example Bardossy and Samaniego 2002, Wang
1990), object-oriented multiresolution segmentation and classification, and rule-
based classification capitalizing on the advancement
intelligence
technologies are thus devised.

in artificial

The fuzzy sets approach can perform classification with partial and multiple
memberships (see for example Bardossy and Samaniego 2002, Foody 1999, Leung
and Leung 1993a,b, Wang 1990). It is particularly useful in classifying remotely
sensed images with mixed pixels. However, it depends generally on the notion of a
membership function.

Artificial neural networks, on the other hand, are devised to solve complex non-
linear classification problems. They have been used for supervised classification
(Kavzoglu and Mather 2003), unsupervised classification,
feature selection
(Kavzoglu and Mather 2002), change detection (Chan et al. 2001) and image
segmentation (Visa and Peura 1997). Multilayered feedforward, Hopfield and
Kohonen networks (Foody 1999) have been the major types of networks applied to
remote sensing image classification. However,
the specification of network
architecture and learning parameters such as learning rate and momentum term
in multilayered perception requires careful design. Various studies have provided
different suggested values for this purpose (see for example Benediktsson et al. 1990,
Kanellopoulos et al. 1997, Kavzoglu and Mather 2003, Paola and Schowengerdt
1995). In addition to its dependence on a parametric network topology, results of a
neural network model are often difficult if not impossible to explain. The black-box
approach cannot render clear interpretation and explanation to users.

With the advancement of artificial

intelligence, a rule-based approach to
classification has been developed as an alternative or complement to the algorithmic
tradition of the above methodologies (Leung 1997). Rule-based expert systems for
spatial classification have thus been constructed in Leung and Leung (1993a,b). The
advantage of rule-based systems is that their reasoning is tractable and their
explanation is logically clear. They are essentially knowledge-based systems with
embedded domain-specific knowledge.

To build the knowledge base in rule-based systems, the top-down approach relies
on the acquisition of rules by knowledge engineers from domain-specific experts, a
rather person-dependent method which may lead to personalized, incomplete and
sometimes inconsistent rule sets with very little learning capability. The bottom-up
approach, on the other hand, is to let data (or examples) speak for themselves. Rules
are discovered from available data. Data mining and knowledge discovery (see for
example Fayyad et al. 1996, Han and Cercone 1993, Han and Kamber 2000,
Komorowski and Zytkow 1997) have thus been developed and applied to the mining
of knowledge (often rules) in spatial databases (Miller and Han 2001).

The basic issue of rule-based system is the determination of a minimal set of
features (and feature values) and the optimal (usually the minimal) set of consistent
rules for classification. All of this has to be achieved with the data available. The
discovery of non-trivial, previously unknown, and potentially useful knowledge
from databases is thus important in information processing. Rough set theory,
proposed by Pawlak (1982, 1991), is an extension of set theory for the study of
information systems characterized by insufficient and incomplete information and
has been demonstrated to be useful in fields such as pattern recognition, machine
learning, and automated knowledge acquisition (see e.g. Leung and Li 2003, Leung

Downloaded by [Moskow State Univ Bibliote] at 11:06 31 October 2013 Knowledge discovery in spatial data

1035

et al. 2006, Polkowski and Skowron 1998, Polkowski et al. 2000, Yasdi 1996). Its
basic idea is to unravel an optimal set of decision rules from an information system
(basically a feature-value table) via an objective knowledge induction process which
determines the features constituting the optimal rule set for classification.

Although rough set theory has not been commonly applied to the analysis of
spatial databases such as vector-based GIS and remotely sensed data, recent works
by some researchers have argued for the advantages of using a rough set approach
to geo-referenced data, specifically qualitative data (see Bittner and Stell 2002, Stell
and Worboys 1998, Wang et al. 2002, Worboys 1998a,b). For data mining in spatial
databases, Aldridge (1998) has developed a rough-set methodology for obtaining
knowledge from multi-theme geographic data, and applied the classical rough set
method to estimate landslide hazards in New Zealand. Wang et al. (2001) have
employed the rough set method to discover land control knowledge, with a case
study indicating its feasibility. Ahlqvist et al. (2000, 2003) and Ahlqvist (2005) have
also applied the rough set method for spatial classification and uncertainty analysis.
These studies, however, have not explicitly studied the mining of rules for the
classification of spatial data, particularly remotely sensed data.

Although using the rough set approach for knowledge discovery in spatial
databases is still in its early stage, it is not difficult to see its advantages. It is an
objective way to unravel decision rules from information systems with incomplete and
qualitative data. It renders an effective methodology to optimally select features, e.g.
selection of the most relevant spectral bands, constituting an optimal rule set for a
classification task. However, the standard Pawlak’s rough set model that has been
applied to discover knowledge in databases so far is actually not appropriate in
handling spatial information, particularly remotely sensed data, which is real-valued
or integer-valued in nature. It should be noted that equivalence classes is a key notion
in Pawlak’s rough set model. It is the basic building block for the knowledge induction
procedure. With real-valued or integer-valued information, we most likely will have
way too many equivalence classes which will eventually lead to too large a number of
classification rules. Though such classification rules may fit the training data, their
generalization capability will be rather low since perfect match of the real-valued or
integer-valued condition parts of the rules will be difficult.

To make it effective and efficient in knowledge discovery in spatial databases, it is
thus essential to develop novel rough set models for real-valued information. Since
integer-valued information is a particular class of real-valued information and our
proposed method applied to both, we henceforth only use the term ‘real-valued’ for
simplicity of presentation. The idea that we propose in here is to first transform a
real-valued information system into an interval-valued information system, and then
construct a new rough-set knowledge induction method to select optimal decision
rules with a minimal set of features for the classification of real-valued spatial
information in general, and remotely sensed data in particular.

To facilitate our discussion, we first give some relevant notions about information
systems in section 2. The transformation of a real-valued information system into an
interval-valued information system and the associated knowledge discovery
approach is then discussed in section 3. A pedagogical real-life application is
subsequently employed to substantiate the proposed theoretical arguments in
section 4. To demonstrate the effectiveness of
the proposed approach, a
classification of tree species with hyperspectral data is discussed in section 5. We
then conclude the paper with a summary and an outlook for further research.

Downloaded by [Moskow State Univ Bibliote] at 11:06 31 October 2013 1036

Y. Leung et al.

2. Some notions related to information systems and rough sets

2.1 Pawlak information systems

The notion of an information system provides a convenient representation of objects
in terms of their attribute. An (complete) information system can be defined by a
pair S5(U, A) where

N U is a nonempty finite set of objects called the universe of discourse.
N A is a nonempty finite set of attributes, i.e. a: URVa is an information function

for a g A, where Va is called domain of a.

Elements of U are called objects which can be cases, states, processes, pixels,
observations, etc. Attributes can be features, variables, spectral bands, etc.

A special case of an information system is a decision table. A decision table is an
information system of the form S5(U, A<{d}), where d6 [A is a distinguished
attribute called decision. The elements of A are called conditional attributes. We can
interpret the decision attribute as a kind of classifier on the universe of objects given,
for example, by an expert or a decision-maker. In machine learning, decision tables
are called sets of training examples.

Without loss of generality, we assume that Vd5{1, 2, …, I}. It can be observed
the decision d determines a partition of
that
the universe of discourse,
(cid:1)
(cid:2), where X d
U=d~ x½
(cid:1)d : x[U
g, i51, 2, …,
f
i
I. The set X d
is called the ith decision class of the decision table S5(U, A<{d}).
i
Thus i may be treated as the label of the class X d
i .

~ x[U : d xð Þ~i

2 , . . . , X d
I

(cid:2)~ X d
(cid:1)

1 , X d

For an information system S5(U, A), one can describe relationships between
objects through their attribute values. With respect to an attribute subset B(A, a
binary equivalence relation RB can be defined as:

x, y[U, x, y

ð

Þ[RBua xð Þ~a yð Þ, Va[B:

RB is the relation with respect to B derived from the information system S, and we
call (U, RB) the Pawlak approximation space with respect to B induced from S. With
the relation B, two objects are considered to be indiscernible if and only if they have
the same value on each a g B. Based on the approximation space (U, RB), one can
derive the lower and upper approximations of an arbitrary subset X of U defined,
respectively, as

B Xð

Þ~ x[U : x½

(cid:1)

(cid:1)B(X

(cid:2), B Xð

(cid:1)

Þ~ x[U : x½

(cid:1)B\X =1

(cid:2),

Þ

Þ, B Xð

where [x]B5{y g U:(x, y) g RB} is the B-equivalence class containing x. The pair
(cid:4) is the representation of X in the Pawlak approximation space (U, RB),
(cid:3)
B Xð
or is referred to as the Pawlak rough set of X with respect to (U, RB). Based on the
lower and upper approximations of the decision classes X d
i , i51, 2, …, I, with
respect to (U, RA) in the decision table (U, A<{d}), all the certain and possible
decision rules can be unravelled (Pawlak 1991).

2.2 Data transformation

Given a number of facts, generalization can be performed in many different
directions (Han et al. 1993). In order to extract interesting rules from databases,
learning should be directed by some background knowledge. For example, to
discover patterns in remotely sensed data we need to know initially the classes of

Downloaded by [Moskow State Univ Bibliote] at 11:06 31 October 2013 Knowledge discovery in spatial data

1037

interest and the plausible spectral bands which might be relevant to the classification
task.

Differing from most rough-set applications, integer-valued attributes (spectral
bands) need to be employed to discover knowledge in remotely sensed data. That is,
we are to classify objects by integer-valued spectral reflectance. A direct application
of conventional rough-set models to such a database will most likely lead to a huge
induction is based.
number of
Consequently, a large number of decision rules will be discovered with low
generalization capability.

classes on which knowledge

equivalence

To make the rough-set approach effective, efficient and practical, and to achieve
high level of generalization, a novel rough-set framework is thus necessary. Our
proposed approach is to first convert the real-valued information system into an
interval-valued information system through a simple manipulation of the data.
Then, we formulate a new rough-set model for knowledge induction in interval-
valued data bases.

Let
N d1, d2, …, dI be I classes;
N Oi5{oij : j51, 2, …, Ji} be the random sample set of the ith class, i51, 2, …, I;
N A5{a1, a2, …, am}5{ak : k51, 2, …, m} be a finite set of attributes which
(cid:3)
ij[Rz is the gray scale value of oij

(cid:4)~vk

represent m spectral bands, and ak oij
measured by spectral band ak.

Such a (training) data set can be represented by a decision table (O, A<{d}),
where O5{oij : i51, 2, …, I, j51, 2, …, Ji} is a finite set of objects; A5{a1, a2, …,
am} is an attribute (spectral band) set, such that ak oij
ij[Rz for all j51, 2, …, Ji;
i51, 2, …, I; k51, 2, …, m; d is the decision attribute; Vd5{1, 2, …, I} is the value
set of decision such that

(cid:4)~vk

(cid:3)

(cid:3)
d oij

(cid:4)~i; Vj~1, 2, . . . , Ji; i~1, 2, . . . , I;

Oi5{oij : j51, 2, …, Ji} is the random sample set of the ith class of objects.

3. Knowledge discovery in an interval-valued information system

3.1

Interval-valued information systems

An interval-valued information system can be defined by a pair K5(U, A), where the
universe of discourse U5{ui : i51, 2, …, I} is represented as a set of distinct I classes,
(cid:6), for all
A5{a1, a2, …, am} is the attribute (spectral band) set such that ak uið Þ~ lk
i51, 2, …, I and k51, 2, …, m, where lk
i and uk
i are the lower and upper limits of the
interval for class i under attribute ak. Especially, each class i is signified by a value
range, an interval, under spectral band k. Signifying a class by an interval of spectral
values under a spectral band is evident in theory and applications (Jenson 1996,
Jenson and Langari 1999, Ji 2003). Due to the variation of values of the sample points
belonging to a class under a specific spectral band, taking interval-values will not
result in information loss but will actually make it more true-to-life and representative.
This is particularly relevant in region-based classification. This justifies the conversion
of integer-valued remote sensing database into the one with interval values.

i , uk
i

(cid:5)

We can transform a real-valued information system S5(O, A<{d}) into an
interval-valued information system K5(U, A) by methods such as statistics,
discretization, or expert opinions. Discretization may be based on experience or

Downloaded by [Moskow State Univ Bibliote] at 11:06 31 October 2013 1038

Y. Leung et al.

specification of arbitrary cut-off points—also, whether a determination could be
made to select a given discretization method based solely on the data characteristics
of an attribute or a data set (Chmielewski and Grzymala-Busse 1996). Sometimes
expert opinions are very useful and reliable in the identification of cut-off points
demarcating the intervals (Leung and Leung 1993a,b). Statistical methods, on the
other hand, may be used to capture most of the data variation under some
probability density function fitting the data. A simple statistical method is that for
each attribute, we only include values that fall within an interval, say m¡2s, under a
particular probability density function, say normal distribution with parameters m
and s2. Taking randomness into account, such an interval would be a good
representation of the data since it accounts for 95.6% of the variation in the normal
distribution case. A similar method can be used in other probability distributions
fitting the data. It should be noted that in the discretization method, interval-valued
set {ak(ui):ui g U} forms a partition of a set for the same attribute ak, but in the
distribution-based statistical method, the value intervals may have non-empty
intersections for distinct objects in the universe of discourse. This is rather natural,
since the gray values of different objects might have rather close spectral signature in
the same spectral band.

REMARK 1. It should be pointed out that we only use the statistical method to
preprocess the data, i.e. by transforming real-valued attributes into interval-valued
attributes. Other than that, the rough set knowledge induction method to be
discussed has nothing to do with any statistical arguments. That is, the knowledge
induction process is independent of the way the intervals are formed by either the
statistical method, the discretization method, or expert opinion.

EXAMPLE 1. For illustration, table 1 depicts an interval-valued information system.
U5{u1, u2, …, u5} is the universe of discourse containing five classes of objects, and
A5{a1, a2, a3, a4} is a set of four attributes (e.g. spectral bands), with each of its
attribute value ak(uj) being an interval obtained by a distribution-based statistical
method, in this case, only those values that fall within m¡2s under the density
function are included.

3.2 Attribute reduction

Let K5(U, A) be an interval-valued information system and B(A, we define a
binary relation, denoted by RB, on U as:

(cid:3)
(cid:1)
RB~ ui, uj

(cid:4)[U|U : ak uið Þ\ak uj

(cid:3) (cid:4)=1, Vak[B

(cid:2):

U

u1
u2
u3
u4
u5

Table 1. An interval-valued information system.

a1

[60, 77]
[77, 81]
[74, 97]
[51, 242]
[54, 57]

a2

[44, 68]
[74, 80]
[65, 116]
[73, 276]
[38, 42]

a3

[19, 30]
[53, 60]
[63, 107]
[108, 205]
[94, 139]

a4

[5, 16]
[5, 19]
[56, 107]
[104, 204]
[29, 41]

Note: These are the experimental results obtained in the real-life application discussed in
section 3. For simplicity, attributes are coded as ak, k51, 2, …, 4, and classes are coded as uj,
j51, 2, …, 5.

Downloaded by [Moskow State Univ Bibliote] at 11:06 31 October 2013 Knowledge discovery in spatial data

1039

Two classes ui and uj have relation RB if and only if they cannot be separated by the
attribute set B. Obviously, RB is reflexive and symmetric, but may not be transitive.
So RB is a tolerance relation which satisfies:
RB~ \

R bf g:

b[B

Denote

SB uið Þ~ uj[U : ui, uj

(cid:1)

(cid:3)

(cid:4)[RB

(cid:2), ui[U

SB(ui) is the tolerance classes of ui with respect to RB, uj g SB(ui) if and only if ui and
uj cannot be separated by the attribute set B.

One fundamental aspect of rough set theory involves the search for particular
subsets of attributes which provide the same information for classification purposes
as the full set of attributes. Such subsets are called attribute reducts. To acquire
concise decision rules from the information systems, knowledge reduction is needed.
Many types of attribute reducts and decision rules have been proposed in rough set
research. For example, Kryszkiewicz (2001) has established static relationships
among conventional types of knowledge reduction in inconsistent complete decision
tables. Zhang et al. (2003, 2004) have introduced a new kind of knowledge reduction
called a maximum distribution reduct which preserves all maximum decision rules.
Mi et al. (2004) have proposed approaches to knowledge reduction based on
variable precision rough set model. Wu et al. (2005) have investigated knowledge
reduction via Dempster–Shafer theory of evidence in information systems. In this
section, we study knowledge reduction in interval-valued information systems. They
can be used in the construction of optimal classification rules in interval-valued
information systems.

Let K5(U, A) be an interval valued-information system and B(A, if RB5RA, then
B is referred to as a classification consistent set in K. If B is a classification consistent
set in K, B2{b} is not a classification consistent set in K for all b g B, i.e. RB2{b}?RA,
then B is called a (global) classification reduct in K. The set of all classification reducts
in K is denoted by re(K). The intersection of all classification reducts is called the
classification core in K, the elements of which are those attributes that cannot be
eliminated without introducing contradictions to the data set.

On the other hand, for u g U, B(A, if SB(u)5SA(u), then B is referred to as a
classification consistent set of u in K. If B is a classification consistent set of u in K,
B2{b} is not a classification consistent set of u in K for all b g B,
i.e.
SB2{b}(u)?SA(u), then B is called a (local) classification reduct of u in K. The set
of all classification reducts of u in K is denoted by re(u). The intersection of all
classification reducts of u is called the classification core of u in K.

It should be noted that, in general, a local reduct may not necessarily be included
in any global reducts. However, if B is a global reduct, for any class u, there must
exist a local reduct of u such that the local reduct is a subset of B, and such a local
reduct can simplify a classification rule generated from the global reduct and may be
of higher generalization capability.

A classification consistent set in K is a subset of the attribute set that preserves the
tolerance classes of objects. A classification reduct is a minimal consistent set that
preserves the tolerance relation and, consequently, leads to the same classification.
The remaining attributes are then redundant, and their removal does not affect (e.g.
worsen) the classification.

Downloaded by [Moskow State Univ Bibliote] at 11:06 31 October 2013 1040

Y. Leung et al.

In what follows, we propose a Boolean reasoning method to calculate the

attribute reducts by introducing a discernibility matrix.

Let K5(U, A) be an interval-valued information system. Denote

Dij~ ak[A : ak uið Þ\ak uj

(cid:3) (cid:4)~1

(cid:2), i=j, and Dii~1 for all i~1, 2, . . . , I:

(cid:1)

Dij is called the discernibility set of classes ui and uj in K, containing attributes
separating classes ui and uj. Denote

(cid:3)

M~ Dij : i, j~1, 2, . . . , I

(cid:4):

M is referred to as the discernibility matrix of K. Let
(cid:2):

M0~ Dij : Dij=1

(cid:1)

j(I,
(cid:4)=[RA. Since RB5RA, we have ui, uj

THEOREM 1 (Judgment Theorem). Let K5(U, A) be an interval-valued information
system. Then B(A is a classification consistent set in K, i.e. RB5RA, iff B>D?Ø,
;D g M0.
PROOF. ‘)’ Suppose RB5RA. If D g M0, then by the definition of M0 there exist
1(i,
i?j, such that D5Dij?Ø. By the definition of RA we then have
(cid:4)=[RB, which implies that there exists an
(cid:3)
ui, uj
attribute ak g B such that ak(ui)>ak(uj)5Ø, that is, ak g Dij. Hence ak g B>D?Ø.
‘Z’ Assume that B>D?Ø, ;D g M0. If by contradiction RB?RA, then we know
(cid:4)6 [RA. By
(cid:3)
exists ak g A such that
from which we have Dij g M0. Since by
al g Dij. Then
(cid:4)6 [RB which is in contradiction to (ui, uj) g RB.

from RA(RB that RA,RB. Thus there exists (ui, uj) g RB such that ui, uj
(cid:3)
ui, uj
ak(ui)>ak(aj)5Ø. Hence ak g Dij,
assumption B>Dij?Ø,
(cid:3)
al(ui)>al(uj)?Ø. Thus
Therefore RB5RA.

(cid:4)=[RA we know that Dij?Ø. Then there

al g B such

exists

ui, uj

there

that

(cid:3)

REMARK 2. According to Theorem 1, B(A is a classification reduct in K iff B is the
minimal set satisfying B>D?Ø, ;D g M0.
THEOREM 2. Let K5(U, A) be an interval-valued information system. Then ak g A
is an element of classification core in K iff there exists D g M0 such that D5{ak}.
PROOF. ‘)’ Assume that ak g A is an element of classification core in K. Let:

Mk~ D[M0 : ak [D

f

g:

If card (D)>2 for all D g Mk, denote:
B~ [
D[M0

D{ akf g
ð

Þ:

It is easy to see that:

B\D=1, VD[M0:

By Theorem 1 we can see that B is a classification consistent set in K. Then there
exists C(B such that C is a classification reduct in K. Clearly, ak 6[C which
contradicts ak being an element of classification core in K.

Downloaded by [Moskow State Univ Bibliote] at 11:06 31 October 2013 Knowledge discovery in spatial data

1041

‘Z’ Suppose that there exists D g M0 such that D5{ak}. Then there exist 1(i,
i?j, such that Dij5{ak}. By definition, we have ak(ui)>ak(uj)5Ø and
(cid:4)[RA{ akf g and

l?k, 1(l(m. Consequently,

ui, uj

j(I,
al(ui)>al(uj)5Ø for all
(cid:3)
ui, uj

(cid:4)6 [RA. It follows that:

(cid:3)

RA{ akf g=RA
It should be noted that ak is a classification core element in K iff RA{ akf g=RA.
Therefore ak is an element of classification core in K.

EXAMPLE 2. In Example 1, the discernibility sets can be obtained as table 2. Since
Dij5Dji, for simplicity, we only list Dijs with 1(j,i,I. According to Theorem 1, it
can be verified that the system has two classification reducts: B15{a2, a3}, B25{a3,
a4}, and the classification core is {a3}. The remaining attribute a1 is then redundant,
and its removal does not worsen the classification.

Reduct computation can be translated into the computation of prime implicants
of a Boolean function. It was shown in Skowron and Rauszer (1992) that the
problem of finding reducts of a given Pawlak (complete) information system may be
solved as a case in Boolean reasoning. The idea of Boolean reasoning is to represent
a problem with a Boolean function and to interpret its prime implicants (an
implicant of a Boolean function f is any conjunction of literals (variables or their
negations) such that for each valuation v of variables, if the values of these literals
are true under v then the value of the function f under v is also true; a prime
implicant is a minimal implicant) as solutions to the problem. This is a useful
approach to the calculation of the reducts of classical information systems. We will
generalize this approach to our proposed interval-valued information systems here.
It should be pointed out that we are interested in implicants of monotone Boolean
functions only, i.e. functions constructed without negation.

Let K5(U, A) be an interval-valued information system. A discernibility func-
tion fK for the system K is a Boolean function of m Boolean variables a¯ 1, a¯ 2, …, a¯ m
corresponding to the attributes a1, a2, …, am, respectively, and is defined as
follows:

fK ¯a1, ¯a2, . . . , ¯am

ð

Þ~ ^ _Dij : Dij[M0

(cid:1)

(cid:2)

where _Dij is the disjunction of all variables a¯ such that a g Dij.

THEOREM 3. Let K5(U, A) be an interval-valued information system. Then an
ak is a prime implicant of
attribute subset B(A is a classification reduct in K iff ^
ak[B
the discernibility function fK.

Table 2. Discernibility set.

u3

u4

u5

u1
u2
u3
u4
u5

u1

a2a3
a3a4
a2a3a4
A

u2

a3a4
a3a4
A

a3
a1a2a4

a2a4

Downloaded by [Moskow State Univ Bibliote] at 11:06 31 October 2013 1042

Y. Leung et al.

PROOF. ‘)’ Assume that B(A is a classification reduct in K. By Theorem 1 we
have:

B\Dij=1, for all Dij[M0:

We claim that ;b g B, there must exist Dij g M0 such that B>Dij5{b}. (In fact, if
card (B>Dij)>2, for all Dij g M0 with b g Dij, let B95B2{b}, then by Theorem 1
we can see that B9 is a classification consistent set in K, which contradicts B being a
classification reduct.) It follows that ^B is a prime implicant of the discernibility
function fK.

‘Z’ If ^B is a prime implicant of the discernibility function fK, then B>Dij?Ø, for
all Dij g M0, and moreover, ;b g B, there exist Dij g M0 such that B>Dij5{b}.
Consequently, B2{b} is not a classification consistent set in K. Thus we conclude
that B is a classification reduct.

From the above theorem we know if:

fK a1, a2, . . . , am

ð

Þ~ ^ _Dij : Dij[M0

(cid:1)

(cid:7)

t

(cid:2)~ _
l~1

sl
^
q~1

apq

(cid:8)

apq , l(t, are all the prime implicants of the discernibility function fK, then

(cid:2), l(t, are all the classification reducts in K.

Without causing any confusion, we shall write ak instead of ak in the discussion to

sl
where ^
q~1
Bl~ apq : qƒsl

(cid:1)

follow.

EXAMPLE 3. In Example 1, we obtain the Boolean function:

fK a1, a2, a3, a4

ð

Þ~ a2 _ a3

ð

Þ ^ a3 _ a4

ð

Þ ^ a2 _ a3 _ a4

ð

Þ

^ a1 _ a2 _ a3 _ a4

ð

Þ ^ a3 ^ a2 _ a4
ð

Þ ^ a1 _ a2 _ a4

ð

Þ:

After simplification (using the absorption laws), we obtain the prime implicants
representation of the Boolean function:

fK a1, a2, a3, a4

ð

Þ~~ a2 _ a4
ð

Þ ^ a3~ a2 ^ a3
ð

Þ _ a3 ^ a4

ð

Þ

Hence there are two classification reducts in the system: {a2, a3} and {a3, a4}.

If we instead construct a Boolean function by restricting the conjunction to run
over only column i (instead of over all columns) in the discernibility matrix, we
obtain the so-called i discernibility function, denoted by fi, that is,

fi a1, a2, . . . , am

ð

Þ~ ^
f

j:Dij [M0

g

(cid:3)

_Dij

(cid:4):

The set of all prime implicants of function fi determines the set of all (local)
classification reducts of ui in K. These classification reducts reveal the minimum
amount of information needed to discern the class ui from all other classes which are
not included in the tolerance classes of ui. We summarize this in the following
theorem:

THEOREM 4. Let K5(U, A) be an interval-valued information system, ui g U. Then
an attribute subset B(A is a classification reduct of ui in K iff ^
ak is a prime
ak[B
implicant of the discernibility function fi.

Downloaded by [Moskow State Univ Bibliote] at 11:06 31 October 2013 Knowledge discovery in spatial data

1043

EXAMPLE 4. In Example 1, we can obtain the Boolean function with respect to ui for
i51, 2, …, 5, and then obtain the classification reduct of each class as follows:

Since

f1 a1, a2, a3, a4

ð

ð

Þ~ a2 _ a3
~ a2 _ a3

ð

Þ ^ a3 _ a4

ð

Þ ^ a3 _ a4

ð

Þ ^ a2 _ a3 _ a4
ð
Þ~ a2 ^ a4

Þ _ a3,

ð

Þ ^ a1 _ a2 _ a3 _ a4

ð

Þ

{a3} and {a2, a4} are the classification reducts of u1, i.e., re(u1)5{{a3}, {a2, a4}}.

Similarly,

f2 a1, a2, a3, a4

ð

f3 a1, a2, a3, a4

ð

f4 a1, a2, a3, a4

ð

f5 a1, a2, a3, a4

ð

ð

ð

Þ~ a2 ^ a4
Þ~ a1 ^ a3
Þ~ a2 ^ a3
ð
Þ~a2 _ a4, re u5ð

ð

Þ _ a3, re u2ð

Þ~ a3f g, a2, a4

f

f

g:
g
Þ~ a1, a3
f

f

Þ, re u3ð
Þ~ a2, a3
f

f

Þ _ a3 ^ a4

Þ, re u4ð

g, a3, a4

f

g

g:

Þ _ a2 ^ a3

ð

Þ _ a3 ^ a4

ð

g, a2, a3

f

g, a3, a4

f

g

g:

Þ~ a2f g, a4f g

f

g:

We can see that the local attribute reducts {a1, a3} and {a2, a4} are not included in
any global reduct.

3.3 Classification rules

After a classification reduct B has been calculated, classification knowledge hidden
in an interval-valued information system may be discovered and expressed in the
form of classification rules as follows:

If ak xð Þ[ lk

i , uk
i

(cid:5)

(cid:6), for all ak[B, then the sample x should be classified into the class ui:

EXAMPLE 5. In Example 1, based on the classification reduct of each class in
Example 4, all classification rules hidden in the interval-valued information system
can be discovered and expressed as follows:

r1: If a3(x) g [19, 30], then x g u1.
r19: If a2(x) g [44, 68] and a4(x) g [5, 16], then x g u1.
r2: If a3(x) g [53, 60], then x g u2.
r29: If a2(x) g [74, 80] and a4(x) g [5, 19], then x g u2.
r3: If a1(x) g [74, 97] and a3(x) g [63, 107], then x g u3.
r39: If a2(x) g [65, 116] and a3(x) g [63, 107], then x g u3.
r30: If a3(x) g [63, 107] and a4(x) g [56, 107], then x g u3.
r4: If a2(x) g [73, 276] and a3(x) g [108, 205], then x g u4.
r49: If a3(x) g [108, 205] and a4(x) g [104, 204], then x g u4.
r5: If a2(x) g [38, 42], then x g u5.
r5: If a4(x) g [29, 41], then x g u5.

Moreover, if we use the classification reduct B15{a2, a3} in the system, then we
can obtain the classification rule set: r1, r2, r39, r49, r59. Similarly, if we use the
classification reduct B25{a3, a4} in the system, then we can obtain the classification
rule set: r1, r2, r39, r4, r5.

Downloaded by [Moskow State Univ Bibliote] at 11:06 31 October 2013 1044

Y. Leung et al.

4. A pedagogical real-life application

4.1 Experimental design

A SPOT-4 multispectral data acquired on 22 November 2000 in the northwestern
part of Hong Kong were used in this application. The data were acquired in four
multispectral bands (green, red, near-infrared and short wave infrared) at 20 m
spatial resolution. A 2566256 image was extracted covering the Maipo Ramsar
Wetland site. The Ramsar Site covers an area of 1500 ha and is rich in both flora and
fauna (Fung 2003). It hosts over 50,000 migrant birds annually with 340 different
bird species among which 23 are rare species. Fringing the Deep Bay coastline,
in the Ramsar site.
mangrove and mudflats form the major conservation foci
Further inland are fish ponds and shrimp ponds (named gei wai locally) which are
noted as artificial wetland. Other than the natural landscape, low-density residential
estates and the Yuen Long industrial estate form the major urban land covers. In
this study, we use five general land covers to test the effectiveness of rough set
concepts in classification. The land covers are water, vegetation, mudflat, residential
land and industrial land.

We intend to demonstrate in this pedagogical experiment the capability of the
proposed method in the discovery from remotely sensed data optimal spectral bands
and optimal rule set for a classification task. We will also show that the method is
capable of discovering ‘the’ spectral band(s) discerning certain classes.

Based on field experience aided by high-resolution aerial photographs, two sets of
independent samples were extracted in the experiment. The first set was used for
training purpose with each class comprising 30–60 independent pixels randomly
selected. The second set was used for test and the number of independent samples
ranges from 30 to 36. Tables 3 and 4 illustrate the data ranges of the four spectral
bands in the training and test samples, respectively.

4.2 Data transformation

Let (O, A<{d}) be the information system obtained from the randomly selected
training samples. We assumed that for each sample set Oi5{oij : j51, 2, …, Ji}, and
each attribute (spectral band) ak, the gray values
satisfy a
(cid:3)
normal distribution N mk
i , sk
. Such an observation is also made in studies such as
i
Jenson (1996), Jenson and Langari (1999), and Ji (2003). Again, the principle of the
transformation method can likewise be applied to other probability distributions.

ij : j~1, 2, . . . , Ji
vk

(cid:4)2

n

o

(cid:10)

(cid:9)

We then transformed S5(O, A<{d}) into an interval-valued information system
K5(U, A), where U5{ui:i51, 2, …, 5} is represented as classes (the distinct five
classes) called the universe of discourse, A5{a1, a2, a3, a4} is the attribute (spectral
band) set such that ak uið Þ~ lk

(cid:6), for all i51, 2, …, 5 and k51, 2, 3, 4, where
(cid:2)

i , uk
i

(cid:1)

(cid:2)

(cid:1)

(cid:5)

lk
i

(cid:3)
~int max mk
i

{2sk

i , 0

(cid:4)z1; uk
i

(cid:3)
~int min mk
i

z2sk

i , 255

(cid:4):

Since, in this example, the gray value ak(ui) is a positive integer between 0 and
(cid:2), i51,
{2sk
255 for each i51, 2, …, 5 and k51, 2, 3, 4, and none of the max mk
i
i
2, …, 5, k51,2,3,4,
the integer function int(x),
for example
int(6.56)56,
that
employed. We
needs
(cid:6)
(cid:5)
(cid:6).
{2sk
ak uið Þ[ max mk
i , 255
i
Hence, we transformed the raw data set into an interval-valued information system
as shown in table 1.

if and only if ak uið Þ[ lk

be
(cid:2), min mk

is an integer,

observe

z2sk

i , uk
i

i , 0

can

to

(cid:2)

(cid:1)

(cid:1)

(cid:1)

(cid:5)

i

Downloaded by [Moskow State Univ Bibliote] at 11:06 31 October 2013 Table 3. A description of the training samples.

Spectral band

No. of
samples

Green (a1)

Red (a2)

NIR (a3)

SWIR (a4)

mean

variance min max mean

variance min max mean

variance min max mean

variance min max

68.45
79.22
85.02

20.05
1.43
36.08

77
81
97

56.13
77.13
90.40

37.13
3.64
171.36

68
80
116

24.25
56.47
84.92

8.70
4.80
129.54

19
53
63

30
60
107

10.73
12.17
81.57

8.37
13.60
172.32

5
5
56

16
19
107

146.20

2300.79

242

174.73

2590.20

276

156.30

597.94

108

205

154.23

631.22

104

204

55.60

1.26

57

40.05

1.71

42

116.38

133.66

94

139

34.95

10.90

29

41

Table 4. A description of the test samples.

Spectral band

No. of
samples

Green (a1)

Red (a2)

NIR (a3)

SWIR (a4)

mean

variance min max mean

variance min max mean

variance min max mean

variance min max

65.94
78.74
82.88

12.35
4.38
46.61

72
82
96

52.20
76.35
82.91

13.11
9.69
72.27

59
82
99

24.37
56.82
78.73

24.83
12.82
42.83

34
63
91

11.20
12.79
75.36

7.05
18.23
29.49

135.47

2034.60

225

156.70

2648.01

259

139.07

1441.38

215

135.77

984.19

56.08

1.91

58

40.42

3.34

44

115.97

159.28

141

35.28

10.66

15
50
66

64

91

6
5
65

74

29

16
21
86

198

41

44
74
65

73

38

45
71
66

54

37

60
77
74

51

54

59
75
70

46

54

Land cover

Water
Mudflat
Residential
land
Industrial
land
Vegetation

Land cover

Water
Mudflat
Residential
land
Industrial
land
Vegetation

60
60
60

30

60

35
34
33

30

36

K
n
o
w
l
e
d
g
e

d
i
s
c
o
v
e
r
y

i
n

s
p
a
t
i
a
l

d
a
t
a

1
0
4
5

Downloaded by [Moskow State Univ Bibliote] at 11:06 31 October 2013 1046

Y. Leung et al.

4.3 Analysis results and interpretations

From applying the methods discussed in section 2, we obtained two classification
reducts from table 1 in our experiment, each of which contains two spectral bands.
To obtain the classification rules which discriminate one class from the others, at
most two bands, {a2, a3} or {a3, a4}, are necessary. This means that the proposed
method reduces the number of spectral bands (attributes) by 50%. The green band,
or Band a1, does not appear in any reduct. Since the green band (Band a1) and the
red band (Band a2) have a very high correlation coefficient of 0.96, they are more or
less identical
in information content. Thus, only one of them is needed in
classification and the removal of green band will not worsen the classification. The
two reducts share a common spectral band a3, or the near-infrared band. It
demonstrates the importance of the near-infrared band for delineating land from
water, and vegetation from non-vegetation land covers. Its elimination will affect
the classification results significantly. Therefore, the proposed method manages to
identify which spectral band is necessary and which spectral band is redundant for a
classification task. It produces a sound result for feature selection highlighting the
discriminatory power in different combinations of spectral bands (or attributes). It
also sheds light on the use of appropriate spectral band(s) in each level of a
hierarchical classification should such a procedure be preferable. That is, we may,
for example, want to use a particular band to separate major land covers first, and
then use relevant band(s) to separate sub-covers.

While classification rules are derived from a sample data set, an independent set
of samples is used as reference data for accuracy verification to test the effectiveness
of the proposed rough set method. The composition of error matrix helps generate
standard accuracy indices, including producer’s accuracy and user’s accuracy for
individual classes, as well as overall accuracy and Kappa coefficient of agreement
for the entire data set (Congalton and Green 1999).

Since the two classification reducts all have two spectral bands, to search for the
optimal reduct, we have to compare the overall accuracies of classification. From
the classification reduct {a2, a3}, and from using the local reducts, we obtained five
classification rules: r1, r2, r39, r4, r5 (in Example 5). Similarly, another five
classification rules: r1, r2, r30, r49, r59 were obtained from using the classification
reduct {a3, a4} and the corresponding local reducts. The corresponding error
matrices, user’s accuracies, producer’s accuracies, overall accuracies, and ˆK value of
the two classifications for the training samples are depicted in tables 5 and 6,
respectively. Since the overall accuracy of the first classification (0.944) is greater
than that of the second one (0.896), so is the ˆK value, we assert that the spectral
band set {a2, a3} is the optimal reduct, and the optimal classification rules are: r1, r2,
r39, r4, r5. The combination of red and NIR bands tend to provide a good result with
all classes having both the producer’s and user’s accuracies greater than 0.90. Only
mudflat and vegetation have user’s accuracy less than 0.95 with 4 samples being
unrecognizable with the classification rules. The overall accuracy of classification
corresponding to the optimal reduct for the training samples is 0.944.
The corresponding results for the test samples with respect

to the two
classifications are summarized in tables 7 and 8, respectively. Again, most classes
have their user’s and producer’s accuracies greater than 0.80. Only industrial land
has a relatively poor user’s accuracy (0.667 for B25{a2, a3} and 0.433 for B25{a3,
a4}) showing confusion with residential land. Clearly, these results show that the
classification derived from the reduct {a2, a3} (overall accuracy of 0.857) is more

Downloaded by [Moskow State Univ Bibliote] at 11:06 31 October 2013 Table 5. Classification accuracy from applying classification reduct B15{a2, a3} and five rules: r1, r2, r39, r4, r5 to the training samples.

Training samples

Water

Mudflat

Resident

Industry

Vegetation

Unrecognizable

User accuracy

Water (60 samples)
Mudflat (60 samples)
Resident (60 samples)
Industry (30 samples)
Vegetation (60 samples)

Water (60 samples)
Mudflat (60 samples)
Resident (60 samples)
Industry (30 samples)
Vegetation (60 samples)

57
0
0
0
0

57
0
0
0
0

0
55
0
0
0

0
55
0
0
0

0
1
58
0
0

0
0
56
0
0

0
0
2
29
0

0
0
4
16
0

0
0
0
0
56

0
0
0
0
58

Producer accuracy

57/5751.0

55/5551.0

58/5950.983

29/3150.935

56/5651.0

overall accuracy5255/27050.944
ˆK~0:931351

Table 6. Classification accuracy from applying classification reduct B25{a3, a4} and five rules: r1, r2, r30, r49, r59 to training samples.

Training samples

Water

Mudflat

Resident

Industry

Vegetation

Unrecognizeable

User accuracy

57/6050.95
55/6050.917
58/6050.967
29/3050.967
56/6050.933

57/6050.95
55/6050.917
56/6050.933
16/3050.533
58/6050.967

3
4
0
1
4

3
5
0
14
2

Producer accuracy

57/5751.0

55/5551.0

56/5651.0

16/2050.80

58/5851.0

overall accuracy5242/27050.896
ˆK~0:873116

K
n
o
w
l
e
d
g
e

d
i
s
c
o
v
e
r
y

i
n

s
p
a
t
i
a
l

d
a
t
a

1
0
4
7

Downloaded by [Moskow State Univ Bibliote] at 11:06 31 October 2013 Table 7. Classification accuracy from applying classification reduct B15{a2, a3} and five rules: r1, r2, r39, r4, r5 to the test samples.

Test samples

Water

Mudflat

Resident

Industry

Vegetation

unrecognizable

Water (35 samples)
Mudflat (34 samples)
Resident (33 samples)
Industry (30 samples)
Vegetation (36 samples)

Water (35 samples)
Mudflat (34 samples)
Resident (33 samples)
Industry (30 samples)
Vegetation (36 samples)

30
0
0
0
0

30
0
0
0
0

0
28
0
0
0

0
28
0
0
0

0
1
33
7
0

0
0
33
6
0

0
0
0
20
0

0
0
0
13
0

0
0
0
0
33

0
0
0
0
33

Producer accuracy

30/3051.0

28/2851.0

33/4150.805

20/2051.0

33/3351.0

overall accuracy5144/16850.857
ˆK~0:828644

Table 8. Classification accuracy from applying classification reduct B25{a3, a4} and five rules: r1, r2, r30, r49, r59 to the test samples.

Test samples

Water

Mudflat

Resident

Industry

Vegetation

Unrecognizable

User accuracy

user accuracy

30/3550.857
28/3450.824
33/3351.0
20/3050.667
33/3650.917

30/3550.857
28/3450.824
33/3351.0
13/3050.433
33/3650.917

5
5
0
3
3

5
6
0
11
3

Producer Accuracy

30/3051.0

28/2851.0

33/3950.846

13/1351.0

33/3351.0

overall accuracy5137/16850.815
ˆK~0:782247

1
0
4
8

Y

.

L
e
u
n
g

e
t

a
l
.

Downloaded by [Moskow State Univ Bibliote] at 11:06 31 October 2013 Knowledge discovery in spatial data

1049

efficient and effective than the one derived from reduct {a3, a4} (overall accuracy
being 0.811). Also,
the generalization of our method
is reasonably good, or alternatively, the situation of over-generalization will not
occur.

the result shows that

One might suspect that a higher level of accuracy could be achieved if more
spectral bands are employed to classify images. For the present application, if we use
three spectral bands, namely red, NIR and SWIR {a2, a3, a4}, for classification, we
can generate ten classification rules (i.e., r1, r19, r2, r29, r39, r30, r4, r49, r5, r59) on the
basis of local reducts. The results obtained from applying them to the training and
test samples are summarized in tables 9 and 10, respectively. It should be pointed out
that the green band (band a1) is again redundant, so rule r3 is omitted (we have
checked that the overall accuracies of the classifications corresponding respectively
to the ten rules (i.e. r1, r19, r2, r29, r39, r30, r4, r49, r5, r59) and the eleven rules (i.e. r1,
r19, r2, r29, r3, r39, r30, r4, r49, r5, r59) are the same). Compared to the results obtained
from applying the classification rules derived from the optimal classification reduct
(tables 7 and 8), the improvement in overall accuracy are only 1.9% and 5.4%, with
reference to training and test samples, respectively. Confusion between industrial
and residential land still remains. In this regard, less confusion is found in the two-
band {a2, a3} classification. It means that, compared to the use of the whole set of
attributes (original spectral bands), if we use the optimal reduct for classification,
the decrease in classification accuracy is rather small. In other words, the loss of
information is almost negligible by using only spectral bands that really matter. It
should further be pointed out that, if we only want to discern the special class of
‘water’ (respectively, mudflat, vegetation) from other classes, one and only one band
is sufficient. That is, a further parsimony in the use of spectral bands is achieved.
Such discriminatory power of the proposed approach will prove to be important in
knowledge discovery in hyperspectral data. Under that situation, our ability to
minimize the number of spectral bands used becomes pertinent.

5. Classification of tree species with hyperspectral data

5.1 Experimental design

To further demonstrate the power of the proposed rough set approach,
it is
employed to classify 15 tree species with hyperspectral data in this application. A
high spectral resolution spectrometer PSD2000 was used for collecting hyperspectral
data. It could sample spectra from 210 to 1050 nm with a spectral resolution of
2.6 nm. In this study, we only examined data from 400 to 900 nm to avoid bands
with too much noise. There were in total 689 bands within the 400–900 nm region.
Fifteen tree species commonly found in Hong Kong were selected for the study.
They are listed as follows: Acacia confusa (u1), Araucaria heterophylla (u2), Acacia
mangium (u3), Bauhinia variegata (u4), Cinnamomum camphora (u5), Casuarina
equisetifolia (u6), Aleurites moluccana (u7), Ficus microcarpa (u8), Firmiana simplex
(u9), Ficus variegata (u10), Hibiscus tiliaceus (u11), Melaleuca quanqueenervia (u12),
Pinus elliottii (u13), Schima superba (u14), Sapium sebiferum (u15).

For each type of tree, 36 sample spectra were taken in the laboratory. They were
separated randomly into two independent sets. The first set was used for training
purposes with 18 independent samples for each class. The second set was used for
testing and the number of samples is also 18 for each class.

Downloaded by [Moskow State Univ Bibliote] at 11:06 31 October 2013 Table 9. Classification accuracy from applying ten rules and three bands (a2, a3, a4) to the training samples.

Training samples

Water

Mudflat

Resident

Industry

Vegetation

Unrecognizable

User accuracy

Producer accuracy

59/5951.0

56/5651.0

56/5750.982

29/3350.879

60/6051

overall accuracy5260/27050.963
ˆK~0:959081

Water (60 samples)
Mudflat (60 samples)
Resident (60 samples)
Industry (30 samples)
Vegetation (60 samples)

Water (35 samples)
Mudflat (34 samples)
Resident (33 samples)
Industry (30 samples)
Vegetation (36 samples)

59
0
0
0
0

35
0
0
0
0

0
56
0
0
0

0
29
0
0
0

0
1
56
0
0

0
1
33
8
0

0
0
4
29
0

0
0
0
20
0

0
0
0
0
60

0
0
0
0
36

1
3
0
1
0

0
4
0
2
0

Table 10. Classification accuracy from applying ten rules and three bands (a2, a3, a4) to the test samples.

Test samples

Water

Mudflat

Resident

Industry

Vegetation

Unrecognizable

User accuracy

59/6050.983
56/6050.933
56/6050.933
29/3050.967
60/6051.0

35/3551.0
29/3450.853
33/3351.0
20/3050.667
36/3651.0

Producer accuracy

35/3551.0

29/2951.0

33/4250.786

20/2051.0

36/3651.0

overall accuracy5153/16850.911
ˆK~0:889894

1
0
5
0

Y

.

L
e
u
n
g

e
t

a
l
.

Downloaded by [Moskow State Univ Bibliote] at 11:06 31 October 2013 Knowledge discovery in spatial data

1051

5.2 Data transformation

Let (O, A<{d}) be the information system obtained from the randomly selected
training tree samples. We assumed that for each sample set Oi5{oij : j51, 2, …, Ji},
and each attribute (spectral band) ak, the spectra
satisfy a
normal distribution N mk

ij : j~1, 2, . . . , Ji
vk

(cid:10)
.

(cid:4)2

o

n

(cid:9)

(cid:3)
i , sk
i

We then transformed S5(O, A<{d}) into an interval-valued information system
K5(U, A), where U5{ui:i51, 2, …, 15} is the set of classes (the 15 tree species), and
(cid:6),
A5{a1, a2, a3, …, a689} is the attribute (spectral band) set such that ak uið Þ~ lk
for all i51, 2, …, 15 and k51, 2, 3, …, 689, where:

i , uk
i

(cid:5)

(cid:1)

lk
i
uk
i

~max mk
i
~min mk
i

(cid:1)

{2sk

i , min ai xð Þ : x[uk

f

g

z2sk

i , max ai xð Þ : x[uk

f

g

(cid:2);
(cid:2):

5.3 Analysis results and interpretations

By applying the methods discussed in section 2, we obtained a reduct B5{a39, a56,
a89, a107, a164, a203, a295, a336, a368, a377, a412, a420, a434, a452, a540} from the interval-
valued information system K5(U, A) containing only 15 spectral bands as shown in
table 11, a very substantial reduction from 689 bands.

The result of this feature selection process selects 4 blue bands, 2 green bands
(including the green peak at 550 nm), 4 red bands, 4 bands along the red edge and 1
NIR band. The 4 bands selected along the red edge echo earlier work that these
bands possess strong discriminatory power for tree species identification (Fung et al.
2003).

The reduct can be used to obtain the classification rules which discriminate the
tree species from each other. That means the proposed method significantly reduces
the number of spectral bands (attributes) by 97.8%. It gives a sound result for
feature selection by highlighting the discriminatory power in different combinations

Table 11. Spectral bands selected for classification.

Spectral Band

a395428.71 nm
a565441.36 nm
a895465.87 nm
a1075479.2 nm

a1645521.31 nm
a2035550 nm

a2955617.32 nm
a3365647.16 nm
a3685669.65 nm
a3775676.89 nm

a4125701.47 nm
a4205707.96 nm
a4345717.34 nm
a4525731.01 nm

a5405793.37 nm

Description

Blue band

Green band

Red band

Red Edge

Near Infrared Band

Downloaded by [Moskow State Univ Bibliote] at 11:06 31 October 2013 1052

Y. Leung et al.

of the spectral bands (or attributes). Using the reduct B, the reduced interval-valued
information system of the training set is listed in table 12.

While classification rules are derived from a sample data set, an independent set
of samples is used as reference data for accuracy verification to test the effectiveness
of the proposed rough set method. For simplicity’s sake, we only list the basic
statistics of the reduced 15 spectral bands in the test samples in table 13.

The corresponding error matrices, user’s accuracies, overall accuracies of the
classifications for the training samples and the test samples are depicted in table 14.
Comparing to the use of the whole set of attributes (original 689 spectral bands),
it is noticed that the decrease in classification accuracy is very small when the reduct
(15 spectral bands) is employed. In other words, the loss of information is almost
negligible if we only use the spectral bands that really matter. This experiment
demonstrates that the proposed approach significantly minimizes the number of
spectral bands necessary for a classification task.

6. Conclusion

We have proposed in this paper a general framework for the discovery of
classification rules in real-valued or integer-valued information system. Particular
emphasis has been placed on the analysis of remotely sensed data which are integer-
valued in nature. The approach involves the transformation of real-valued or
integer-valued decision table into interval-valued information system in the data
preprocessing step and the construction of a rough-set based knowledge induction
procedure to discover optimal rules for a classification task. We have introduced
several useful concepts such as local and global classification reducts as well as
classification core pertinent to data analysis in interval-valued information systems.
A method by Boolean functions to compute the classification reducts in the interval-
valued information system has also been proposed. Theoretical analysis and the
real-life experiment all show that the proposed approach is effective in discovering
classification rules hidden in remotely sensed data. It is also instrumental
in
dimension reduction by unraveling the minimal number of features (spectral bands)
and the optimal number of rules for a classification task. Furthermore, critical
features for differentiating specific classes can also be discovered. Such an ability can
facilitate the orderly use of relevant features to classify remotely sensed data in a
hierarchical manner. For example, we can use fewer key spectral bands to classify
broad types and then use relevant spectral bands to classify subtypes. All these
spectral bands can be discovered automatically from the data by the proposed
method.

Although our emphasis has been placed on knowledge discovery in remotely
sensed data, the proposed approach is general enough to mine knowledge from any
information system. As aforementioned,
real-valued or integer-valued spatial
Pawlak’s rough set model is essentially catered for qualitative data. It is ineffective
and inefficient in analyzing quantitative, e.g. real-valued and or integer-valued, data
commonly encountered in real-life problems. Our extension of the conventional
rough set model in this paper has greatly extended its applicability. Furthermore, it
has built a basis for knowledge discovery in mixed, e.g. qualitative and quantitative,
databases. As is well-known, the use of spectral signatures alone is not sufficient to
classify complex remotely sensed images, rich high order image characteristics such
as shape, shadow, size, texture, pattern, site and association should be used to
perform classification with higher level of accuracy. The rough set approach to

Downloaded by [Moskow State Univ Bibliote] at 11:06 31 October 2013 Table 12. The reduced interval-valued information system of the training tree samples.

U

u1

u2

u3

u4

u5

u6

u7

u8

u9

u10

u11

u12

u13

u14

u15

a39

a56

a89

a107

a164

a203

a295

a336

a368

a377

a412

a420

a434

a452

a540

[0.541,2.332]

[0.609,2.441]

[1.278,2.141]

[1.604,25.33]

[2.933,3.657]

[4.455,5.668]

[2.519,3.569]

[2.144,3.022]

[1.658,2.793]

[1.881,2.731]

[4.283,6.901]

[5.689,8.329]

[8.412,15.544]

[11.830,23.778]

[14.435,32.546]

[1.067,2.309]

[1751,2.283]

[1.377,2.369]

[1.484,2.201]

[3.657,5.694]

[4.419,7.689]

[2.547,4.411]

[1.971,3.217]

[1.852,2.663]

[1.622,2.712]

[4.917,8.949]

[6.846,11.749]

[10.855,18.429]

[14.306,26.613]

[16.662,35.237]

[1.824,3.111]

[1.906,2.665]

[1.841,3.111]

[2.279,3.012]

[3.345,5.556]

[4.709,8.112]

[3.084,4.380]

[3.023,3.995]

[2.180,3.070]

[2.172,3.249]

[6.610,8.319]

[6.916,11.750]

[10.730,17.260]

[15.473,23.998]

[19.884,34.953]

[2.127,4.916]

[3.101,4.881]

[2.929,4.937]

[3.102,5.014]

[4.882,8.274]

[6.641,11.491]

[4.185,7.399]

[3.462,6.067]

[2.943,5.073]

[2.885,4.995]

[7.363,12.556]

[9.673,16.979]

[13.762,24.649]

[18.271,32.153]

[22.231,35.401]

[0.936,1.980]

[1.071,2.698]

[1.404,2.886]

[1.423,3.029]

[4.217,7.023]

[6.487,10.857]

[3.061,5.662]

[2.171,4.219]

[1.635,2.810]

[1.677,2.935]

[7.735,11.802]

[11.751,17.094]

[15.484,26.604]

[20.879,39.745]

[24.091,54.207]

[1.098,2.303]

[1.422,2.025]

[1.237,2.187]

[1.267,2.171]

[1.877,3.024]

[2.430,4.029]

[1.819,2.904]

[1.637,2.591]

[1.558,2.363

[1.553,2.441]

[2.889,4.962]

[3.868,6.505]

[5.844,8.750]

[8.390,14.693]

[10.961,20.009]

[2.090,2.667]

[3.165,5.810]

[2.845,5.425]

[2.733,5.621]

[3.949,8.421]

[5.415,11.855]

[3.224,7.170]

[2.954, 6.324]

[2.854,5.987]

[2.909,6.063]

[5.856,13.001]

[9.505,17.849]

[15.085,28.877]

[22.421,43.027]

[35.759,58.376]

[0.745,2.178]

[0.712,1.440]

[0.923,1.672]

[0.878,1.603]

[1.937,4.385]

[2.970,6.731]

[1.767,3.603]

[1.498, 2.617]

[1.284,2.010]

[1.271,1.939]

[3.482,7.982]

[4.760,11.072]

[7.264,17.448]

[10.206,26.670]

[12.849,34.317]

[1.980,3.242]

[1.861,2.641]

[2.041,2.605]

[2.204,2.690]

[3.518,5.615]

[5.384,9.244]

[2.771,4.345]

[2.169, 3.009]

[2.042,2.969]

[2.022,3.097]

[7.013,10.411]

[8.974,15.462]

[12.755,24.857]

[16.862,35.316]

[19.145,42.579]

[1.023,3.896]

[1.821,3.216]

[2.273,2.840]

[2.435,3.014]

[4.473,5.436]

[6.550,8.516]

[3.627,4.375]

[3.033, 3.739]

[3.071,3.489]

[2.893,3.593]

[7.437,9.659]

[9.817,14.325]

[14.613,24.474]

[19.912,39.851]

[23.379,57.995]

[1.417,3.737]

[1.491,2.917]

[2.141,3.295]

[2.204,3.540]

[2.733,5.232]

[3.764,7.980]

[2.392,4.380]

[2.233, 3.967]

[2.044,3.589]

[2.079,3.660]

[5.912,6.964]

[8.457,12.288]

[14.922,21.551]

[24.017,34.990]

[29.907,48.812]

[1.482,3.495]

[1.707,3.101]

[1.980,3.591]

[2.110,3.675]

[4.287,7.681]

[5.837,11.567]

[4.380,7.827]

[3.340, 6.079]

[2.812,4.277]

[2.480,4.093]

[6.769,13.665]

[8.503,17.691]

[11.101,24.243]

[13.832,30.927]

[15.902,35.365]

[0.516,2.824]

[0.661,2.415]

[1.137,2.107]

[1.296,1.923]

[2.251,3.138]

[2.984,4.451]

[2.035,2.848]

[1.898,2.635]

[1.719,2.384]

[1.961,2.376]

[3.694,5.707]

[5.005,8.233]

[8.831,13.271]

[11.014,20.079]

[14.179,27.532]

[1.322,2.664]

[1.794,2.690]

[2.141,2.639]

[2.204,2.603]

[3.083,4.453]

[3.989,6.623]

[2.731,3.865]

[2.489,3.019

[2.333,2.896]

[2.360,2.905]

[4.427,6.610]

[5.788,9.931]

[8.620,16.103]

[12.595,24.003]

[17.241,41.265]

[0.812,2.388]

[0.796,2.135]

[1.207,2.015]

[0.953,1.877]

[2.021,4.040]

[3.120,6.165]

[1.835,3.593]

[1.390,2.716]

[0.991,1.849]

[0.985,1.881]

[4.222,7.799]

[6.357,11.498]

[10.581,19.628]

[16.229,31.368]

[34.351,44.381]

K
n
o
w
l
e
d
g
e

d
i
s
c
o
v
e
r
y

i
n

s
p
a
t
i
a
l

d
a
t
a

1
0
5
3

Downloaded by [Moskow State Univ Bibliote] at 11:06 31 October 2013 Table 13. A description of the test tree samples.

U

u1 min
max
mean
variance

u2 min
max
mean
variance

u3 min
max
mean
variance

u4 min
max
mean
variance

u5 min
max
mean
variance

u6 min
max
mean
variance

u7 min
max
mean
variance

u8 min
max
mean
variance

a39

0.550
2.415
1.039
0.293
1.028
2.169
1.763
0.094
1.835
3.067
2.453
0.125
2.133
4.903
3.444
0.765
0.951
1.974
1.511
0.133
1.269
2.202
1.610
0.075
2.995
5.565
4.185
0.689
0.746
2.141
1.218
0.142

a56

0.699
2.306
1.158
0.168
1.595
2.200
1.903
0.032
1.939
2.651
2.352
0.044
2.659
4.871
3.662
0.418
1.281
2.321
1.940
0.094
1.423
2.137
1.632
0.052
2.491
5.713
4.198
0.885
0.625
2.143
1.164
0.146

a89

1.279
2.135
1.584
0.100
1.501
2.243
1.928
0.059
1.837
2.908
2.474
0.076
2.969
4.930
4.091
0.261
1.570
2.723
2.147
0.093
1.282
2.169
1.731
0.052
2.567
5.409
4.194
0.660
0.766
2.075
1.333
0.137

a107

1.557
2.480
1.812
0.072
1.489
2.130
1.888
0.025
2.101
2.891
2.520
0.043
3.205
5.015
4.200
0.214
1.741
2.869
2.187
0.114
1.351
2.160
1.730
0.057
2.629
5.519
4.195
0.733
0.743
1.982
1.362
0.121

a164

2.937
3.767
3.294
0.094
3.259
5.666
4.663
0.333
3.039
5.087
4.231
0.297
4.884
8.254
6.640
0.838
4.493
7.263
5.506
0.552
1.977
3.023
2.473
0.141
3.779
8.465
6.080
1.720
1.783
4.379
3.125
0.814

a203

4.455
5.555
4.806
0.160
4.246
7.429
5.945
0.510
4.233
7.568
5.997
0.819
6.701
11.331
9.053
1.874
6.797
10.804
8.678
1.560
2.543
4.157
3.267
0.309
5.270
11.795
8.367
3.361
2.720
6.915
4.840
1.994

a295

2.367
3.350
3.068
0.059
2.371
4.437
3.416
0.205
2.624
4.374
3.765
0.225
4.192
7.365
5.818
0.758
3.428
5.455
4.324
0.442
1.950
2.953
2.372
0.121
3.125
7.165
5.177
1.276
1.575
3.779
2.722
0.588

a336

2.091
2.913
2.618
0.051
1.720
3.251
2.534
0.105
2.679
3.905
3.307
0.105
3.505
6.064
4.854
0.497
2.491
3.955
3.125
0.222
1.713
2.599
2.152
0.084
2.757
6.323
4.633
0.980
1.375
2.963
2.122
0.235

a368

1.819
2.500
2.187
0.039
1.334
2.655
2.001
0.080
2.141
3.065
2.668
0.061
2.991
5.029
4.145
0.287
1.887
2.769
2.263
0.069
1.675
2.419
2.008
0.055
2.657
5.922
4.612
0.892
1.162
2.291
1.683
0.098

a377

1.756
2.484
2.183
0.045
1.363
2.653
2.011
0.088
2.290
3.206
2.741
0.061
3.023
4.982
4.111
0.278
1.878
2.745
2.233
0.065
1.683
2.440
2.016
0.064
2.668
6.970
4.773
1.211
1.101
2.242
1.735
0.110

a412

a420

a434

a452

a540

4.635
6.471
5.371
0.338
4.832
9.310
6.841
0.977
4.930
8.670
6.978
0.532
7.374
12.089
9.683
2.340
7.704
11.477
9.585
1.576
2.807
4.955
3.791
0.464
5.738
12.799
9.866
2.758
3.494
7.880
5.632
2.450

6.185
9.088
7.375
0.891
6.902
12.512
9.299
1.692
6.698
11.662
9.491
1.125
9.563
15.921
12.851
4.119
10.759
16.517
13.717
3.418
3.772
6.464
5.038
0.912
8.907
17.775
13.816
4.175
4.775
11.065
7.946
5.281

9.107
14.777
11.697
3.961
10.908
18.260
13.875
4.103
9.009
17.548
14.701
3.750
14.071
22.105
18.334
6.762
15.628
25.950
20.930
13.052
5.690
9.933
7.483
1.559
16.994
28.742
22.416
8.895
6.894
17.841
12.709
15.652

15.571
12.620
31.232
22.515
23.484
17.476
33.874
13.083
16.452
13.990
33.226
25.456
25.381
19.654
32.563
11.933
20.226
15.156
34.275
23.793
28.647
21.447
13.366
4.944
22.819
18.935
39.841
31.045
31.183
25.414
29.038
16.203
24.555
20.907
52.509
38.609
29.587
37.377
41.070 106.560
10.972
8.528
19.872
14.680
15.240
11.355
11.634
5.467
35.761
27.721
58.363
43.010
44.694
33.796
84.963
25.304
12.741
10.032
35.343
25.511
24.721
18.662
70.182
34.600

1
0
5
4

Y

.

L
e
u
n
g

e
t

a
l
.

Downloaded by [Moskow State Univ Bibliote] at 11:06 31 October 2013 U

u9 min
max
mean
variance

u10 min
max
mean
variance

u11 min
max
mean
variance

u12 min
max
mean
variance

u13 min
max
mean
variance

u14 min
max
mean
variance

u15 min
max
mean
variance

a39

1.983
3.053
2.417
0.126
1.267
4.561
2.343
0.578
1.435
3.410
2.084
0.240
1.296
3.429
2.695
0.267
0.527
2.818
1.556
0.639
1.430
2.647
1.903
0.119
0.824
2.160
1.502
0.142

a56

1.869
2.487
2.181
0.045
1.825
3.739
2.418
0.216
1.505
2.905
2.259
0.159
1.708
3.100
2.656
0.217
0.724
2.401
1.509
0.317
1.795
2.663
2.130
0.079
0.595
1.893
1.374
0.097

a89

2.113
2.776
2.356
0.026
2.275
3.367
2.650
0.089
2.145
3.241
2.642
0.111
1.983
3.718
3.009
0.273
1.140
2.040
1.607
0.091
1.778
2.685
2.251
0.038
1.069
2.037
1.515
0.061

Table 13. (Continued.)

a107

2.056
2.687
2.355
0.025
2.440
3.285
2.717
0.059
2.133
3.293
2.675
0.129
2.144
3.842
3.090
0.253
1.311
2.074
1.689
0.046
1.901
2.587
2.314
0.027
0.870
2.053
1.418
0.069

a164

3.867
5.443
4.626
0.209
4.474
5.780
4.998
0.126
2.953
4.923
3.983
0.495
5.040
7.336
6.504
0.320
2.272
3.309
2.792
0.104
3.086
4.446
3.690
0.140
1.998
4.227
2.964
0.403

a203

5.725
9.013
7.358
0.915
6.555
9.034
7.627
0.593
4.051
7.574
5.821
1.587
5.997
10.798
8.809
2.428
3.143
4.844
3.924
0.307
4.106
6.622
5.304
0.647
3.125
6.397
4.551
0.936

a295

3.069
4.221
3.580
0.104
3.801
4.930
4.118
0.084
2.625
4.133
3.446
0.318
4.384
7.826
6.309
1.274
2.219
2.913
2.555
0.045
2.803
3.895
3.241
0.083
1.840
3.577
2.656
0.259

a336

2.396
3.233
2.763
0.046
3.239
4.244
3.508
0.065
2.457
3.596
3.066
0.187
3.361
6.075
5.027
0.739
2.118
2.580
2.324
0.022
2.422
3.243
2.801
0.042
1.357
2.613
1.981
0.130

a368

2.281
2.967
2.591
0.044
3.075
3.957
3.270
0.050
2.280
3.355
2.835
0.147
2.815
4.205
3.583
0.256
1.713
2.303
2.080
0.020
2.100
2.901
2.498
0.032
0.961
1.952
1.455
0.067

a377

2.368
2.998
2.642
0.043
3.056
4.061
3.312
0.058
2.303
3.398
2.859
0.140
2.681
4.059
3.445
0.238
1.725
2.354
2.092
0.024
2.090
2.911
2.508
0.032
0.951
1.850
1.429
0.061

a412

a420

a434

a452

a540

7.028
10.065
8.357
0.775
7.483
9.933
8.422
0.582
5.927
8.710
7.150
0.674
6.838
12.606
10.577
3.892
3.857
5.883
4.902
0.277
4.510
7.520
5.857
0.779
5.932
8.773
6.825
0.645

9.199
15.108
12.105
3.070
9.824
14.318
11.923
2.252
8.692
11.301
10.327
0.571
8.814
16.765
13.462
6.844
5.138
8.413
6.754
0.936
6.140
9.926
8.269
2.185
8.311
12.006
9.914
1.549

13.513
24.232
18.892
11.322
14.621
25.082
19.080
13.021
14.968
20.411
17.315
3.472
11.847
22.729
18.011
13.799
7.720
13.583
10.522
3.586
9.267
16.101
13.165
7.519
12.547
19.582
16.206
5.559

20.547
18.124
42.001
34.688
31.270
26.338
49.703
28.827
23.475
19.416
60.374
40.917
28.386
37.450
61.171 201.214
29.741
21.482
48.779
33.629
37.720
27.181
54.964
17.735
16.846
14.458
35.350
29.605
26.538
22.746
37.188
25.383
14.327
11.123
28.099
20.477
20.668
15.498
24.876
10.778
18.549
13.661
41.232
24.631
30.543
19.941
83.923
19.298
34.401
17.961
45.092
31.265
37.188
25.021
10.495
16.771

K
n
o
w
l
e
d
g
e

d
i
s
c
o
v
e
r
y

i
n

s
p
a
t
i
a
l

d
a
t
a

1
0
5
5

Downloaded by [Moskow State Univ Bibliote] at 11:06 31 October 2013 1056

Y. Leung et al.

Table 14. Comparison of classification accuracies from applying classification reduct B to the
training and test tree samples.

No. of
training
samples

No. of training
samples correctly
identified

Training
accuracy

No. of
test
samples

No. of test
samples correctly
identified

Test
accuracy

U

u1
u2
u3
u4
u5
u6
u7
u8
u9
u10
u11
u12
u13
u14
u15

18
18
18
18
18
18
18
18
18
18
18
18
18
18
18

12
16
15
16
16
17
15
17
16
14
16
17
14
16
16

0.667
0.889
0.833
0.889
0.889
0.944
0.833
0.944
0.889
0.778
0.889
0.944
0.778
0.889
0.889

0.863

18
18
18
18
18
18
18
18
18
18
18
18
18
18
18

13
14
15
16
16
15
15
14
14
14
13
16
14
15
15

0.722
0.778
0.833
0.889
0.889
0.833
0.833
0.778
0.778
0.778
0.722
0.889
0.778
0.833
0.833

0.811

Overall

270

233

270

219

knowledge discovery in such a mixture of qualitative and quantitative information
deserves further investigation. Moreover, the dimension reduction capability of the
proposed method will be very useful in the analysis of high dimensional data. All
these problems can be solved by further extending the current framework in future
studies.

Acknowledgements
This work is supported by the earmarked grant CUHK 4126/04H of the Hong Kong
Research Grants Council.

References
AHLQVIST, O., 2005, Using uncertain conceptual spaces to translate between land cover
categories. International Journal Geographical Information Science, 19, pp. 831–857.
AHLQVIST, O., KEUKELAAR, J. and OUKBIR, K., 2000, Rough classification and accuracy
assessment. International Journal Geographical Information Science, 14, pp. 475–496.
AHLQVIST, O., KEUKELAAR, J. and OUKBIR, K., 2003, Rough and fuzzy geographical data
intgration. International Journal Geographical Information Science, 17, pp. 223–234.
ALDRIDGE, C.H., 1998, A theory of empirical spatial knowledge supporting rough set based
knowledge discovery in geographic databases. PhD thesis, University of Otago,
Dunedin, New Zealand.

BARDOSSY, A. and SAMANIEGO, L., 2002, Fuzzy rule based classification of remotely sensed
images. IEEE Transcation on Geoscience and Remote Sensing, 40, pp. 362–374.
BENEDIKTSSON, J.A., SWAIN, P.H. and ERSON, O.K., 1990, Neural network approaches versus
statistical methods in classification of multi-source remote sensing data. IEEE
Transactions on GeoScience and Remote Sensing, 28, pp. 540–552.

BITTNER, T. and STELL, J.G., 2002, Vagueness and rough location. Geoinformatica, 6, pp.

99–121.

CHAN, J.C., CHAN, K.P. and YEH, A.G.O., 2001, Detecting the nature of change in an urban
environment: a comparison of machine learning algorithms. Photogrammetric
Engineering and Remote Sensing, 67, pp. 213–225.

Downloaded by [Moskow State Univ Bibliote] at 11:06 31 October 2013 Knowledge discovery in spatial data

1057

CHMIELEWSKI, M.R. and GRZYMALA-BUSSE, J.W., 1996, Global discretization of continuous
attributes as preprocessing for machine learning. International Journal of Approximate
Reasoning, 15, pp. 319–331.

CONGALTON, R.G. and GREEN, K., 1999, Assessing the Accuracy of Remotely Sensed Data:

Principles and Practices (Boca Raton: Lewis Publications).

FAYYAD, U.M., PIATETSKY-SHAPIRO, G. and SMYTH, P., 1996, From data mining to
knowledge discovery: An overview. In Advances in Knowledge Discovery and Data
Mining, U.M. Fayyad, G. Piatetsky-Shapiro, P. Smyth and R. Uthurusamy (Eds), pp.
1–34 (Boston: AAAI MIT Press).

FOODY, G.M., 1999, Image classification with a neural network: from completely-crisp to
fully-fuzzy situations. In Advances in Remote Sensing and GIS Analysis, P.M.
Atkinson and N.J. Tate (Eds), pp. 17–37 (Chichester: Wiley).

FUNG, T., 2003, Landscape dynamics in the Maipo Ramsar wetland site. In Geoinformatics
for Tropical Ecosystems, P.S. Roy (Ed.), pp. 539–553 (Dehradun: Asian Association
of Remote Sensing).

GOPAL, S. and FISCHER, M., 2001, Fuzzy ARTMAP—a neural classifier for multispectral
image classification. In GeoComputational Modeling Techniques and Applications, M.
Fischer and Y. Leung (Eds), pp. 165–194 (Berlin: Springer).

HAN, J. and KAMBER, M., 2000, Data Mining: Concepts and Techniques (San Francisco:

Morgan Kaufmann Publishers).

HAN, J., CAI, Y. and CERCONE, N., 1993, Data-driven discovery of quantitative rules in
relational databases. IEEE Transactions on Knowledge and Data Engineering, 5, pp.
29–40.

JENSON, J.R., 1996, Introduction to Digital Image Processing: A Remote Sensing Perspective

JENSON, J.R. and LANGARI, R., 1999, Fuzzy Logic: Intelligence, Control and Information

(Englewood Cliffs, NJ: Prentice Hall).

(Englewood Cliffs, NJ: Prentice Hall).

JI, M., 2003, Using fuzzy sets to improve cluster labelling in unsupervised classification.

International Journal of Remote Sensing, 24, pp. 657–671.

KANELLOPOULOS, I., WILKINSON, G. and MEGIER, J., 1997, Neurocomputation in Remote

Sensing Data Analysis (Berlin: Springer).

KAVZOGLU, T. and MATHER, P.M., 2002, The role of feature selection in artificial neural

network applications. International Journal of Remote Sensing, 23, pp. 2919–2937.

KAVZOGLU, T. and MATHER, P.M., 2003, The use of backpropagating artificial neural
networks in land cover classification. International Journal of Remote Sensing, 24, pp.
4907–4938.

KOMOROWSKI, J. and ZYTKOW, J., 1997, Principles of Data Mining and Knowledge

Discovery. Lecture Notes in Artificial Intelligence 1263 (Berlin: Springer-Verlag).

KRYSZKIEWICZ, M., 2001, Comparative study of alternative types of knowledge reduction in

inconsistent systems. International Journal of Intelligent Systems, 16, pp. 105–120.

LEUNG, Y., 1997, Intelligent Spatial Decision Support Systems (Berlin: Springer).
LEUNG, Y. and LEUNG, K.S., 1993a, An intelligent expert system shell for knowledge-based
geographic information systems: 1. The tools. International Journal of Geographical
Information Systems, 7, pp. 189–199.

LEUNG, Y. and LEUNG, K.S., 1993b, An intelligent expert system shell for knowledge-based
geographic information systems: 2. Some applications. International Journal of
Geographical Information Systems, 7, pp. 201–213.

LEUNG, Y. and LI, D.Y., 2003, Maximal consistent block technique for rule acquisition in

incomplete information systems. Information Sciences, 153, pp. 85–106.

LEUNG, Y., WU, W.Z. and ZHANG, W.X., 2006, Knowledge acquisition in incomplete
information systems: a rough set approach. European Journal of Operational
Research, 168, pp. 164–180.

MI, J.S., WU, W.Z. and ZHANG, W.X., 2004, Approaches to knowledge reduction based on
variable precision rough sets model. Information Sciences, 159, pp. 255–272.

Downloaded by [Moskow State Univ Bibliote] at 11:06 31 October 2013 1058

Knowledge discovery in spatial data

MILLER, H.J. and HAN, J., 2001, Geographic Data Mining and Knowledge Discovery (London:

Taylor and Francis).

PAOLA, J.D. and SCHOWENGERDT, R.A., 1995, A detailed comparison of backpropagation
neural network and maximum likelihood classifiers for urban land use classification.
IEEE Transactions on Geoscience and Remote Sensing, 33, pp. 981–996.

PAWLAK, Z., 1982, Rough sets. International Journal of Information and Computer Sciences,

11, pp. 341–356.

Academic Publishers).

PAWLAK, Z., 1991, Rough Sets: Theoretical Aspects of Reasoning about Data (Boston: Kluwer

POLKOWSKI, L., and SKOWRON, A. (Eds),

1998, Rough Sets in Knowledge Discovery 1:

Methodology and Applications, 2: Applications (Heidelberg: Physica-Verlag).
POLKOWSKI, L., TSUMOTO, S. and LIN, T.Y., 2000, Rough Set Methods and Applications

(Heidelberg: Physica-Verlag).

SKOWRON, A. and RAUSZER, C., 1992, The discernibility matrices and functions in
information systems. In Intelligent Decision Support—Handbook of Applications and
Advances of the Rough Sets Theory, R. Slowinski (Ed.), pp. 331–362 (Boston: Kluwer
Academic Publishers).

STELL, J.G. and WORBOYS, M.F., 1998, Stratified map spaces: a formal basis for
In SDH’98 Proceedings 8th International
multiresolution spatial databases.
Symposium on Spatial Data Handling, T.K. Poiker and N. Chisman (Eds), pp.
180–189 (Vancouver: International Geographical Union).

VISA, A. and PEURA, M., 1997, Generalization of neural network based segmentation results
for classification purposes. In Neuro-computation in Remote Sensing Data Analysis,
I. Kanellopoulos, G.G. Wilkinson, R. Roli and J. Austin (Eds), pp. 255–261 (Berlin:
Springer-Verlag).

WANG, F., 1990, Improving remote sensing image analysis using fuzzy information
representation. Photogrammetric Engineering and Remote Sensing, 56, pp. 1163–1169.
WANG, S.L., WANG, X.Z. and SHI, W.Z., 2001, Development of a data mining method for

land control. Geo-Spatial Information Science, 4, pp. 68–76.

WANG, S.L., LI, D., SHI, W.Z. and WANG, X.Z., 2002, Geo-rough space. Geo-Spatial

Information Science, 6, pp. 54–61.

WORBOYS, M.F., 1998a, Computation with imprecise geographical data. Computers

Environment and Urban Systems, 22, pp. 85–106.

WORBOYS, M.F., 1998b, Imprecision in finite resolution spatial data. GeoInformatica, 2, pp.

257–279.

WU, W.Z., ZHANG, M., LI, H.Z. and MI, J.S., 2005, Knowledge reduction in random
information systems via Dempster–Shafer theory of evidence. Information Sciences,
174, pp. 143–164.

YASDI, R., 1996, Combining rough sets learning and neural learning: method to deal with

uncertain and imprecise information. Neuralcomputing, 7, pp. 61–84.

ZHANG, W.X. and MI, J.S., 2004, Incomplete information system and its optimal selections.

Computers and Mathematics with Applications, 48, pp. 691–698.

ZHANG, W.X., MI, J.S. and WU, W.Z., 2003, Approaches to knowledge reductions in

inconsistent systems. International Journal of Intelligent Systems, 18, pp. 989–1000.

Downloaded by [Moskow State Univ Bibliote] at 11:06 31 October 2013 