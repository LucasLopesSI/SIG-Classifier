This article was downloaded by: [University of Miami]
On: 24 November 2014, At: 17:22
Publisher: Taylor & Francis
Informa Ltd Registered in England and Wales Registered Number: 1072954 Registered
office: Mortimer House, 37-41 Mortimer Street, London W1T 3JH, UK

International Journal of Geographical
Information Science
Publication details, including instructions for authors and
subscription information:
http://www.tandfonline.com/loi/tgis20

GIS-augmented video surveillance
A. Milosavljević a , A. Dimitrijević a & D. Rančić a
a CG & GIS Lab, Faculty of Electronic Engineering , University of
Niš , Niš, Serbia
Published online: 04 Sep 2010.

To cite this article: A. Milosavljević , A. Dimitrijević & D. Rančić (2010) GIS-augmented video
surveillance, International Journal of Geographical Information Science, 24:9, 1415-1433, DOI:
10.1080/13658811003792213

To link to this article:  http://dx.doi.org/10.1080/13658811003792213

PLEASE SCROLL DOWN FOR ARTICLE

Taylor & Francis makes every effort to ensure the accuracy of all the information (the
“Content”) contained in the publications on our platform. However, Taylor & Francis,
our agents, and our licensors make no representations or warranties whatsoever as to
the accuracy, completeness, or suitability for any purpose of the Content. Any opinions
and views expressed in this publication are the opinions and views of the authors,
and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content
should not be relied upon and should be independently verified with primary sources
of information. Taylor and Francis shall not be liable for any losses, actions, claims,
proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or
howsoever caused arising directly or indirectly in connection with, in relation to or arising
out of the use of the Content.

This article may be used for research, teaching, and private study purposes. Any
substantial or systematic reproduction, redistribution, reselling, loan, sub-licensing,
systematic supply, or distribution in any form to anyone is expressly forbidden. Terms &
Conditions of access and use can be found at http://www.tandfonline.com/page/terms-
and-conditions

International Journal of Geographical Information Science
Vol. 24, No. 9, September 2010, 1415–1433

GIS-augmented video surveillance

A. Milosavljevic´*, A. Dimitrijevic´ and D. Rancˇic´

CG & GIS Lab, Faculty of Electronic Engineering, University of Nisˇ, Nisˇ, Serbia

(Received 4 October 2009; final version received 14 March 2010)

Registration in augmented reality is a process that merges virtual objects generated by a
computer with real-world images captured by a camera. In this article, we present a
method for registration of geospatial data applicable to outdoor video surveillance
systems consisting of several PTZ cameras. PTZ is an abbreviation for pan-tilt-zoom,
and in the terminology of video surveillance it indicates cameras that can rotate in the
horizontal (pan) and vertical planes (tilt) and change their level of magnification (zoom).
Registration is based on transforming these relative camera view parameters into the
absolute position, orientation and field of view required by the three-dimensional geo-
graphic information systems (3D GISs). Once the 3D GIS and camera views are aligned,
it is possible to identify geospatial objects from the camera image, as well as to overlap
the virtual scene with the real one. In addition, inverse transformation of the view
parameters allows for selecting and pointing the appropriate camera to some geo-
referenced feature or event. Based on the proposal, we developed GeoScopeAVS, a
GIS-based system for augmented video surveillance and suggested application of such
a system in emergency situation management and urban planning.

Keywords: augmented reality; geographic information system; video surveillance;
virtual reality

1.

Introduction

The ability of a geographic information system (GIS) to handle and process both location
and attribute data distinguishes it from other information systems. It also establishes GIS as a
technology that is important for a wide variety of applications (Chang 2005). Traditionally,
the majority of GISs were limited to the visualisation of geospatial data in two dimensions
(2D GIS). While 2D GIS can be used to perform numerous spatial analyses and applications,
visualisations are generally limited to viewing either individual GIS layers or the results of
data queries. The fact that we relate to our world in three or more dimensions suggests that
some types of data may be more readily visualised and analysed in 3D (Brooks and Whalley
2008). With the development of graphics hardware, virtual reality techniques originally
developed for interactive computer games are exerting more and more influence in the field
of 3D GIS.

The advantages of 3D GIS over 2D GIS arise from the fact that the former enables
visualisation and understanding of terrestrial phenomena and features that are only discern-
ible in three dimensions. It also makes better presentations to those with little or no
experience within the mapping. Li et al. (2004) expressed the need for 3D GIS for urban

*Corresponding author. Email: alexm@elfak.ni.ac.rs

ISSN 1365-8816 print/ISSN 1362-3087 online
# 2010 Taylor & Francis
DOI: 10.1080/13658811003792213
http://www.informaworld.com

Downloaded by [University of Miami] at 17:22 24 November 2014 1416

A. Milosavljevic´ et al.

environments in order to understand a 3D landscape with many high buildings. They also
emphasised the importance of supporting indoor scenes along with outdoor ones in such
applications. In their opinion, the outdoor scenes based on digital terrain model (DTM)
should be called 2.5D GIS, while the indoor scenes that are based on the architectural
structure of different floors in upright direction should be considered as true 3D GIS. Coors
(2003) stated that there is a strong need for a 3D GIS to manage 3D geometry and topology,
integrate 3D geometry and semantic information, analyse both spatial and topological
relationships and visualise the data in a suitable form. Zlatanova et al. (2002), in their
overview of trends in 3D GIS, noticed that 3D visualisation requires appropriate means to
visualise 3D spatial analysis – tools to effortlessly explore and navigate through large models
in real time. Their observations on the demand for 3D city models showed user preferences
for photo-true texturing.

The potential of geographically referenced videographic data, i.e. spatial video, as an
additional elemental data type within GIS is examined by Lewis et al. (in press). Research
presented in this paper deals with the integration of 3D GIS and video surveillance systems.
Real-time video monitoring is playing an increasingly significant role in surveillance
systems in various security, law enforcement and military applications (Oner Sebe et al.
2003). A typical outdoor urban surveillance system consists of multiple pan-tilt-zoom (PTZ)
cameras overlooking different areas. However, conventional video monitoring systems have
various problems with multi-point surveillance (Kawasaki and Takai 2002). A typical
system of conventional video monitoring connects each video camera directly to a corre-
sponding display screen. Therefore, we have as many screens as video cameras. In these
kinds of systems, serious problems can occur when the scale of the monitoring system grows
larger than human capacity. Security personnel must mentally map each surveillance
monitor image to the corresponding place in the real world, and this complicated skill
requires experience and training (Kawasaki and Takai 2002). To enable multi-camera
coordination and tracking, Sankaranarayanan and Davis (2008) emphasised the importance
of establishing a common reference frame to which each of these cameras can be mapped.
They suggested the use of GIS as a common frame of reference because it not only provides a
solid ground truth, but more importantly provides semantic information (e.g., locations of
roads, buildings and sensitive areas) for use in applications such as tracking and activity
analysis.

Our solution to this problem relies on the use of augmented reality (AR) techniques
applied to GIS. In this approach, a GIS stores the necessary geospatial and contextual
information about features that can be identified from a camera image. Furthermore, a
GIS-based approach enables the inverse task of selecting and pointing an appropriate camera
to some geo-referenced feature or event. To achieve this goal, we proposed a method for
registration of geospatial data applicable to an outdoor video surveillance system consisting
of several PTZ cameras. The registration is based on using 3D GIS and aligning the camera
image with its virtual scene. In order to accomplish this alignment, it is necessary to establish
a transformation between the relative PTZ camera parameters and the parameters required by
the 3D GIS, such as absolute position, orientation and field of view. The proposed method
does not require any additional camera tracking hardware; therefore, it represents an
inexpensive solution when applied to existing video surveillance systems. Based on this
method and our existing 2D and 3D GIS solutions, we implemented a system called
GeoScopeAVS, which will be described in this article.

The implemented system is used to demonstrate the proposed method, while possible
applications are only suggested. Because we used surveillance cameras, the logical implica-
tion is that the system could be used to enhance video surveillance and support certain

Downloaded by [University of Miami] at 17:22 24 November 2014 International Journal of Geographical Information Science

1417

emergency situations management. The major benefit, regarding surveillance, stems from
the fact that each point in the camera image (assuming there is a good underlying model) is
geo-referenced in 3D space. This information can be used to support various scenarios,
discussed later in the conclusions. Another possible application is assistance in urban
planning. Since the proposed method is based on the aligning of camera image and the
virtual 3D GIS scene, it is possible to extract the visualisation of appropriate 3D virtual
objects and place them over the video. In particular, this could enable investors and architects
to see how their buildings will look in their natural surroundings.

This article is organised as follows: Section 2 presents an overview of AR with regard to
GIS and video surveillance. In Section 3, we describe the architecture of the GeoScopeAVS
system at the sub-system level and give an overview of the system key features. Section 4
deals with the modelling of an observer view in the 3D GIS and discusses transformations
between the relative camera view parameters and this model. Finally, Section 5 presents the
conclusions.

2. Related work

AR techniques aim to combine the real scene viewed by a user and a virtual scene generated
by a computer that augments the scene with additional information. Unlike virtual reality,
which provides the user with a synthetic environment as a replacement for reality, AR
ensures that the user sees the real environment augmented with objects and information from
the virtual environment. In order to better understand the term ‘augmented reality’, the
reality–virtuality continuum defined by Milgram and Kishino (1994) should be considered.
The ‘real world’ and a ‘totally virtual environment’ are at the two ends of this continuum
while the middle region is called mixed reality (MR). AR is near to the real-environment
side, while augmented virtuality (AV) is closer to the virtual-environment side. Unlike AR,
AV adds real images to virtual environment, thereby increasing a virtual object’s reality
degree. Based on the reality–virtuality continuum, Krisp and Ahonen-Rainio (2003) pro-
posed categorisation of different combined representations in order to provide assistance in
choosing the right visualisation for users’ specific needs.

A typical implementation of AR is based on the use of head-mounted displays (HMDs).
However, to avoid reliance on a specific technology for image combination, Azuma (1997),
in his review, defines AR as a system that has the following three characteristics:

(1) Combines real and virtual elements
(2) Is interactive in real time
(3) Is registered in three dimensions

This definition allows other technologies besides HMDs, while retaining the essential
characteristics of AR. In addition to the basic HMD configuration, it is possible to use a
monitor as a device for displaying a composite image. In that case, a real-world image is
gathered from one or two video cameras. A conceptual design of the monitor-based config-
uration is given in Figure 1.

The components necessary for implementing the monitor-based AR include the

following:

l A video camera that provides real-world images
l A device for determining the location and orientation of the camera (tracker)
l A scene generator that uses data about the 3D location and orientation of the camera

Downloaded by [University of Miami] at 17:22 24 November 2014 1418

A. Milosavljevic´ et al.

Figure 1. Monitor-based AR conceptual design (Azuma 1997).

l An image combiner for the real-world images and the generated virtual scene
l A monitor that displays the composite image

Fournier et al. (1993) stated that the successful merging of real images (or video) and
computer-generated images requires solving the problems of common viewing parameters,
common visibility and common illumination. In particular, they presented techniques for
approximating the common global illumination for real video images and computer-
generated images. This pioneering work has shown how the computation of common
illumination between the real and synthetic scene results in a greatly improved graphical
environment with which the user can interact.

Based on the environment in which they are used, AR systems can be divided into two
main categories: indoor AR and outdoor AR. The major difference between them is the way
the camera is tracked. Overviews of different approaches to this problem are presented by
Min et al. (2007). According to them, indoor AR systems usually accomplish registration by
tracking artificial marks. Outdoor AR systems also try to use marks for vision tracking in
order to accomplish stable registration. However, outdoor scenes are usually much more
complex, so it is hard to track natural features stably. They stated that, recently, digital
models and spatial data were taken into account in more and more outdoor AR research
works. In their opinion, any outdoor AR system had to use spatial information as the basic
information to provide a digital platform for the system. Mower (2009) presented an
augmented scene delivery system (ASDS) used to create and distribute annotated carto-
graphic products in real time from captured still imagery. One of the key issues he focused on
is distributing augmented scenes over the Internet. Using GIS data for camera pose estima-
tion was employed by Sourimant et al. (2007) in their research considering 3D building
reconstruction. They presented a method based on the registration of GPS measures, video
sequences and rough 3D models of buildings. By interpreting the Global Positioning System
(GPS) measures with regard to a GIS database, they obtained a coarse estimate of the camera
pose, and they then refined these estimates using suitable visual virtual servoing algorithms.
Nevertheless, in terms of camera tracking, our approach is much simpler. It relies on pose

Downloaded by [University of Miami] at 17:22 24 November 2014 International Journal of Geographical Information Science

1419

estimation by transforming the relative view pan, tilt and zoom parameters of a fixed
mounted camera. Although simple,
lack precision.
Relative PTZ parameters are acquired from a camera with high precision, and once the
camera mounting position is measured, precise pose estimation can be guaranteed.

the proposed approach does not

An excellent idea on how to apply AR to the visualisation of a sub-surface GIS
infrastructure is presented by Schall et al. (2009). Their application, Vidente, uses a hand-
held AR set-up, Vesp´R, built around an ultra-mobile PC that connects a camera, a GPS
receiver and an inertial measurement unit (IMU). Pose estimation precision is limited by the
GPS and IMU precision. They stated that GPS measurements are typically accurate within a
few metres, while the InertiaCube3 measures 3 degrees of freedom orientation with accuracy
below 1 degree.

Another example of integrating AR and GIS has been the subject of Ghadirian and
Bishop’s (2008) research. They presented a GIS-based photo-realistic visualisation method
that uses an offline video-based AR technique to represent GIS model–based landscape
changes in an immersive environment. Unlike our solution, the proposed method, although it
allows for dynamics in supporting both panning and representation of temporal changes in
the landscape, is not real-time.

Addressing the problem of the human ability (or lack thereof) to successfully fuse and
comprehend the information that multi-point video surveillance can provide, Oner Sebe
et al. (2003) proposed a visualisation approach based on an augmented virtual environment
(AVE). The AVE is a virtual reality model augmented by multiple video streams in real time
to help observers comprehend temporal data and imagery from arbitrary views of a scene.
This approach, although promising for some multi-point video surveillance systems, seems
less appropriate for a network of powerful PTZ cameras where camera navigation is as
important as monitoring. Thus, our approach relies on a video stream augmentation by
‘virtual reality’, rather than augmentation of virtual reality by the video stream.

3. GeoScopeAVS system overview

In this section, the architecture and key features of the GeoScopeAVS system for augmented
video surveillance are presented. The diagram depicting the system context is shown in
Figure 2. For real-world image retrieval, the system uses one of several PTZ network
cameras. A PTZ camera has three degrees of freedom due to its ability to pan (rotation),
tilt (pitch) and zoom.

Figure 2. GeoScopeAVS system context.

Downloaded by [University of Miami] at 17:22 24 November 2014 1420

A. Milosavljevic´ et al.

For the system implementation we used AXIS 233D1 cameras. Communication with the
cameras is based on a proprietary Hypertext Transfer Protocol (HTTP)-based protocol called
VAPIX (2008). In addition to the common task of obtaining the current frame, VAPIX
provides an interface for managing the camera and reading its current orientation (setting and
reading pan, tilt and zoom parameters). In this way, the camera itself, if we know its
geographic location and altitude, provides the basis for determining the parameters of a
3D view (‘tracker’ device from Figure 1). However, it should be noted that the pan and tilt
parameters are relative to the camera, so the absolute orientation depends on the way the
camera is mounted. Problems of calculating the absolute parameters of the view and camera
calibration are further discussed in Section 4.

The context of the GeoScopeAVS system, in addition to cameras, includes a repository
of 2D/3D geospatial data (see Figure 2). This repository is used by both the 2D GIS and 3D
GIS sub-systems in order to create a virtual context for the real world seen through the eye of
camera. The repository consists of the following:

l A digital elevation model (DEM) – terrain elevations organised in a grid that is used to

create a virtual relief.

l Two-dimensional geospatial data – standard GIS raster and vector data applied as

textures onto the virtual terrain surface.

l Simple 3D objects – geographic features that are defined using 2D base geometry and
height. They are generated from 2D geospatial data and represented as separate
elements in the 3D scene.

l Complex 3D objects – arbitrary 3D models of geographic features in the COLLADA
(Barnes and Finch 2008) format; these are specially designed in some of the 3D
modelling software that allow export to this format (e.g., Google SketchUp).

l Three-dimensional sub-objects – transparent polygonal surfaces that are attached to
the complex 3D objects, with the linked context data from the appropriate database
tables (e.g., windows of the building with the related individuals).

Based on the described context, the sub-system level architecture of the GeoScopeAVS
is shown in Figure 3. Four sub-systems are identified: 2D GIS, 3D GIS, the sub-system for
communication with cameras and AVS sub-system, whose task is mixing real and virtual
scenes. In the following four subsections, the most important features of each of these sub-
systems are described.

2D GIS sub-system

3.1.
Functionalities of the 2D GIS sub-system are related to geospatial data access, 2D visualisa-
tion, feature querying and identification from the map. The implementation of the 2D GIS
sub-system relies on our previous work summarised by Milosavljevic´ et al. (2008). A
Unified Modelling Language (UML) class diagram that depicts the organisation of the 2D
GIS sub-system is shown in Figure 4.

The geospatial data are organised into layers based on their type, represented by the basic
class GLayer. Virtual methods declared in this abstract class represent interfaces towards
operations implemented in certain derived classes. Some of the methods responsible for 2D
visualisation (method Draw), 3D object access (method Get3DObjects), feature selection
(method Select) and querying (method Query) are shown in Figure 4.

To manage different types of geospatial data we use several GLayer-derived classes.
Class GLayerGroup is responsible for hierarchical organisation of the layers. Classes

Downloaded by [University of Miami] at 17:22 24 November 2014 International Journal of Geographical Information Science

1421

Figure 3. GeoScopeAVS system architecture.

Figure 4. UML class diagram that depicts organisation of the 2D GIS sub-system.

GLayerImage and GLayerCoverage are used to display geo-referenced images (or image
tiles) and coverage data, while GLayerGrid is used to display geographic grids. Classes
feature management, while
GLayerFeature and GLayerMobile are responsible for
GLayerRemote is a special type of grouping layer used to access layers of mobile features
from a server.

The feature layer has an associated feature collection (class GFeatureCollection) that
contains simple features (class GFeatureSimple) described by the adequate feature type
(class GFeatureType). A simple geographic feature is described with its spatial (geometries)
and non-spatial attributes. To define these attributes we use class GAttributeType, and to
manage simple feature geometries we use class GGeometry and its specialisations.

A special type of feature, managed by GLayerMobile class, is the mobile feature (class
GFeatureMobile). This feature class models GPS-acquired positions that correspond to
certain mobile objects (e.g., police patrols).

Class GDef3D, contained by the layer class, is especially highlighted in Figure 4. This
class models a way in which a layer is visualised in the 3D GIS. When this optional object is

Downloaded by [University of Miami] at 17:22 24 November 2014 1422

A. Milosavljevic´ et al.

omitted, we have a default, texture mode of 3D visualisation. The texture generation relies
on the previously mentioned layer method Draw. On the contrary, existence of a GDef3D
object specifies that the layer should be visualised using separate 3D objects. Basic con-
struction of a 3D object relies on using its corresponding feature geometry. If the geometry is
of polygonal type (e.g., building footprint), then GDef3D attribute m_sHeightAttr tells us
how to obtain the object’s height. To specify visualisation mode, where the layer is
represented using both texture and 3D objects, the value of m_bTexture attribute should
be set to true. This kind of visualisation is useful for mobile features, where the current
feature position is indicated using a separate 3D object, while the trace of the previous
positions is shown as a line in a texture.

Finally, a class that represents the front end of the 2D GIS sub-system is GApp. This
class, implemented using a singleton design pattern, encapsulates all described functionality
needed to build a 2D GIS application. The GApp class represents specialisation of a group
layer, which necessarily means that the application inherits all of the methods defined in a
layer context. Additionally, this class contains all feature-type definitions, feature collec-
tions, coverages, feature services, projection definitions and styles referenced by other
classes. Initialisation of a GIS application is done using an Extensible Markup Language
(XML) document whose content corresponds to a previously described organisation.

3D GIS sub-system

3.2.
The 3D GIS sub-system is responsible for the visualisation of the DTM, 2D geospatial data,
as well as simple and complex 3D objects in three dimensions. Two-dimensional geospatial
data are displayed as a texture that is laid over the virtual terrain, while 3D objects are
visualised as separate 3D models. Texture generation, based on the selected layers, is the
responsibility of the 2D GIS sub-system. The 3D GIS sub-system also enables the identifi-
cation of geospatial objects based on screen coordinates in the 2D projection of the 3D scene.
Three-dimensional objects and sub-objects are identified directly using this sub-system,
while ‘texture objects’ are identified using services of the 2D GIS sub-system. Figure 5
shows a scene rendered with the 3D GIS sub-system.

The 3D GIS provides optimised dynamic creation of the scene, which enables unlimited
movement through the virtual 3D space. It is based on our RINGO (Rancˇic´ et al. 2006)
algorithm for massive terrain visualisation. The predominant feature of this algorithm is a
very high frame rate with minimal CPU utilisation (Dimitrijevic´ and Rancˇic´ 2007). It
organises terrain into quadratic blocks with the same spatial dimensions, but with different
resolutions. The resolution of the block depends on its distance from the viewer. Figure 6
illustrates a level of detail (LOD) scheme used by RINGO.

Organisation of the 3D GIS sub-system is shown in Figure 7. The front end of this sub-
system is the singleton class GLRenderer. The primary roles of this class are to create the
Open Graphics Library (OpenGL) rendering context and to manage the scene. In addition, it
serves as a proxy for commands and data requests to and from the 3D GIS sub-system. To
fulfil the selected user interface (UI) operation in a pleasant way, GLRenderer implements
various automatic motion events. Using these motion events, with a selected 3D object in the
scene, a user can choose to do the following:

l Zoom to the object
l Turn to the object and zoom (typically used in a mode that displays camera video)
l Jump to the object (instantly go to the object, so that the object covers most of the

viewing area)

Downloaded by [University of Miami] at 17:22 24 November 2014 International Journal of Geographical Information Science

1423

Figure 5. Virtual reality – scene appearance in 3D GIS.

Figure 6.

Illustration of the level of detail scheme used by the RINGO algorithm.

l Fly to the object
l Rotate around the object

In order to manage the scene, GLRenderer includes two objects: GLCamera and
GLTerrain. The first one models the virtual camera and enables different ways of motion,
while the second encapsulates the scene itself. GLTerrain is implemented as a matrix of
GLTerrainBlock objects. GLTerrain defines an LOD scheme, executes reorganisation of the
blocks whenever the viewer crosses the border of the central block (Rancˇic´ et al. 2006), and
serves as a mediator between GLRenderer and GLTerrainBlock objects.

All 3D objects inside the block (GLTerrainBlock) are contained by the instance of the
class GLFeatureCollection. Inside GLFeatureCollection, 3D objects are classified into an
array of GLFeatureLayer objects, in a similar manner to the 2D GIS sub-system. A class that

Downloaded by [University of Miami] at 17:22 24 November 2014 1424

A. Milosavljevic´ et al.

Figure 7. UML class diagram of the 3D GIS sub-system’s main components.

encapsulates the 3D object is called GLFeature. GLFeature has mandatory bounds to some
2D GIS GFeature-derived instances, but it may also have an optional GLColladaObject that
encapsulates a complex 3D object. Although GLFeatureCollection is assigned to the
GLTerrainBlock, it is in fact bound to the geographic location. During the block’s reorga-
nisation inside the terrain matrix, it is a common case that GLFeatureCollection objects
change the GLTerrainBlock object they reside in.

All stationary 3D objects are placed into GLFeatureCollection instances assigned to
GLTerrainBlock objects. However, there are 3D objects that change their location frequently.
They are referred to as mobile objects and implemented through the 2D GIS
GFeatureMobile class. For storing 3D mobile objects, GLRenderer has its own
GLFeatureCollection. These kinds of objects are typically used for vehicle tracking applica-
tions that can also be supported by this system.

As previously mentioned, another important feature of the 3D GIS sub-system is its
ability to display and interact with complex 3D objects (GLColladaObject class). These
objects are modelled in external programs (e.g. Google SketchUp) and imported into the
scene from COLLADA files. COLLADA is an XML-based format for data exchange
between 3D applications, and it is widely supported. Through COLLADA, very detailed
models can be displayed as a part of the scene. That can be, for example, either an existing
building or architectural designs used in urban planning. Tsai and Lin (2007) proposed a
method that can be used to produce near-photo-realistic texture mappings for these 3D
models.

Based on COLLADA objects, we introduced a new dimension to the geospatial data
representation. The previous 2D GIS applications enable queries based only on the
geospatial location bounded to the Earth’s surface. In three dimensions, we can now
query features that are laid not just on the ground but also on surfaces of complex 3D
objects. To enable this feature, COLLADA models can be supplemented with so-called
3D sub-objects – the transparent polygonal entities laid on the surface of the main object
and linked with arbitrary database entities. Using this paradigm, our system can answer
not just questions such as ‘Who lives in that building?’ but also questions such as ‘Who
lives behind that window?’.

Object

identification in 3D GIS is processed in three stages. First, we need to
determine which 3D objects and sub-objects intersect the frustum defined by a mouse
click. Second, when identifiers of the objects and sub-objects are known, the 3D GIS
sub-system locates those objects and sends them a message to retrieve data related to
identifiers of their sub-objects. Each object and sub-object retrieves related data using
the 2D GIS sub-system services as the third stage of the object identification process. If
the frustum defined by mouse click intersects only with terrain, the corresponding 2D
coordinates are calculated and the identification request is redirected to the 2D GIS sub-
system.

Downloaded by [University of Miami] at 17:22 24 November 2014 International Journal of Geographical Information Science

1425

3.3. Sub-system for communication with cameras

The sub-system for communication with cameras is responsible for the acquisition of the
camera’s image and orientation parameters as well as for controlling the camera. The
communication is based on HTTP protocol and adequate VAPIX requests. A UML activity
diagram that illustrates communication with the camera is shown in Figure 8.

Camera video is acquired by retrieving independent frame images. For retrieving the
current frame, the following request form is used: http://camaddr/jpg/image.jpg. Response
to this request is a JPG-encoded image that represents the current camera frame. This simple
method is used for robustness to different connection speeds and conditions. Nevertheless, to
achieve full usability of the system, the communication infrastructure should support frame/
parameters retrieval loop execution at least 15–20 times per second.

For retrieving current orientation (i.e., PTZ) parameters, we use the following query
request: http://camaddr/axis-cgi/com/ptz.cgi?query=position. As a result of this query, the
camera returns text where each line encodes the value of one parameter in the form
,parameter. = ,value. (e.g., pan = 12.1). In order to know the exact camera orientation
for each frame, this request precedes the image request.

To control camera orientation, the VAPIX protocol defines several commands. An HTTP
request that tells the camera to go to some pan, tilt and zoom has the following form: http://
camaddr/axis-cgi/com/ptz.cgi?pan=0.5&tilt=5&zoom=1. After receiving this command,
the camera starts to move and needs some time to reach a set orientation. This is another
reason why orientation retrieval always precedes frame retrieval.

Figure 8. UML activity diagram that illustrates communication with camera.

Downloaded by [University of Miami] at 17:22 24 November 2014 1426

A. Milosavljevic´ et al.

3.4. AVS sub-system

The AVS sub-system, using services of the two previously described sub-systems, is
responsible for combining camera images and appropriate views into the virtual world of
the 3D scene. Figure 9 illustrates mixing of a real-world image with a computer-generated
virtual scene in the system mode that shows camera video. The responsibilities of this sub-
system include the following:

l Retrieval of frame and PTZ parameters from the sub-system for communication with

l Transformation of relative camera PTZ parameters into adequate absolute parameters

l Setting of calculated view parameters in the 3D GIS and retrieval of the corresponding

cameras

required by the 3D GIS

image

camera image

l Processing of retrieved 3D scene image (setting transparency) and display over

l When a click on the camera image occurs, forwarding the request for object identi-

fication to the 3D GIS sub-system

l When a request to zoom some feature occurs, applying inverse transformation to find
suitable PTZ parameters and sending the command to the camera communication sub-
system

A method for transforming relative camera PTZ parameters into absolute 3D view

parameters is described in the next section.

3.5. Overview of the system key features
GeoScopeAVS main application and all sub-system DLLs2 are implemented in Microsoft
Visual Studio 2008, as Visual C++ Microsoft Foundation Class (MFC) projects. The UI of

Figure 9. Mixing real and virtual reality in application mode that displays camera video.

Downloaded by [University of Miami] at 17:22 24 November 2014 International Journal of Geographical Information Science

1427

the main application consists of three segments represented by adequate geospatial data
views: 2D view, 3D view (see Figure 5) and Camera view (see Figure 9). Besides the central
view, the application interface consists of the layer management panel (top-left), the
navigation map panel (bottom-left) and the selection view panel (right). Additional UI
components are the style editing dialog, the query building and result display dialog and
the camera calibration dialog. The definition of layer hierarchy and parameters that config-
ure them are encoded using XML. A video demonstration of the system can be found at
http://www.youtube.com/watch?v=wGjQXLTtrv0 (accessed 4 October 2009).

The first view we shall describe, also the most common for GIS, is 2D view. 2D view uses
a dynamically generated geographic map to display the selected layers of geospatial data.
Standard GIS functions related to this view are map panning, map zooming, object identi-
fication (i.e., selection) and zooming to object/layer. Implementation of this view is directly
based on the presented 2D GIS sub-system.

The second view, 3D view, displays the selected layers in a custom generated 3D scene.
Depending on a supplied definition, layers are displayed through texture or with separate 3D
objects. The application allows replacement of some automatically generated 3D objects
with complex COLLADA models allowing richer visualisation. Furthermore, using an
additional GSObjectEditor application (see Figure 10), COLLADA models can be supple-
mented with 3D sub-objects. The 3D sub-object is a transparent polygonal entity attached to
the COLLADA object surface, with the linked context data from the appropriate database
tables. The creation and data entities assignment to 3D sub-objects is completely manual. In
order to ‘draw’ 3D sub-object polygonal surfaces, we rely on model geometry and texture,
which should reflect the actual appearance of the corresponding real world object. Three-
dimensional sub-objects are visible only in GSObjectEditor, while in 3D view they remain
invisible. When a user performs a mouse click in the area of some of these ‘invisible
polygons’, the corresponding linked database entities are fetched and shown in the selection
view panel as a sub-hierarchy of the parent 3D object (see Figure 10). Functions for 3D view
manipulation allow moving through the scene (using panning, rotation and lifting), zooming
(changing field of view), object/sub-object identification, jumping/flying/zooming to object
and rotation around object. Implementation of this view is based on the presented 3D GIS
sub-system.

Figure 10. Assigning database entities to sub-objects in GSObjectEditor application and display of
identified sub-object properties in the selection view panel.

Downloaded by [University of Miami] at 17:22 24 November 2014 1428

A. Milosavljevic´ et al.

Figure 11. From real to virtual – illustration of different levels of transparency.

Finally, the third view, Camera view, is used to display a video of a selected surveillance
camera augmented with the described 3D view functionality. Using this view, it is possible to
manipulate the camera by changing pan, tilt and zoom parameters, or using the mouse as a
virtual joystick. As the 3D scene is constantly updated in the background to reflect the
current camera pose, the camera image can be used for object/sub-object identification.
Zooming the camera to an object is also possible for selected objects or objects that represent
query results. The mixing of real-world video and a virtual 3D scene is implemented by
overlaying semi-transparent 3D scene images over video frames. The level of transparency
can be adjusted, as illustrated in Figure 11. Implementation of this view is based on the
presented AVS sub-system.

In order to integrate this system in another city, certain hardware, geodata and
software requirements must be satisfied. Hardware requirements include existence of
several PTZ network cameras that cover the area of interest. A crucial requirement
when choosing the camera model is its ability to retrieve current pan, tilt and zoom
parameters. Geodata required to apply to this system can vary. An accurate DEM for
the area of interest is essential. All other data represent context for the system and can
be divided into 2D features, 2.5D features (features with height info, used for creation
of simple 3D objects), COLLADA models (complex 3D objects) for some features and
complex 3D object surface sub-objects created with GSObjectEditor. Finally, the soft-
ware requirements may include the implementation of camera communication protocol
(currently, we supported only VAPIX protocol), as well as calibrating camera para-
meters. The application data configuration must also be provided, but this is rather
simple because we use XML to encode it. For future development we will consider
supporting CityGML3 as uniform 3D data representation model for urban areas (Kolbe
et al. 2005).

4. Modelling camera view in 3D GIS

An observer view into the 3D GIS is fully determined by the following seven parameters that
can be divided into two groups:

l Position parameters:

(1) Latitude
(2) Longitude
(3) Altitude

Downloaded by [University of Miami] at 17:22 24 November 2014 International Journal of Geographical Information Science

1429

l Orientation parameters:

(4) Azimuth (also called yaw)
(5) Pitch
(6) Roll
(7) Field of view (FOV)

When a PTZ camera is in the role of observer, the first group of parameters is fixed and
determined by the camera mounting position. These parameters need to be measured
(using GPS for example) and provided to the system for further use. Knowing character-
istics of the camera lens, the remaining four parameters are calculated using the retrieved
pan, tilt and zoom parameters.

The current azimuth, pitch and roll are determined from the camera pan and tilt using

three calibration parameters:

l Azimuth of the zero-pan camera position (azimuth0)
l Pitch of the north-pan camera position (pitch0)
l Roll of the north-pan camera position (roll0)

The calibration parameters are determined by the camera mounting. The first parameter
(azimuth0) represents the direction of the camera view when the pan parameter is set to 0.
The other two parameters model small angular deviations in the horizontal plane that arrive
from imperfect mounting, and they are represented as pitch and roll when camera is oriented
to the north. Fine-tuning of the calibration parameters is currently done empirically in the
system working regime that allows display of a semi-transparent virtual scene over the
camera video (see Figure 9).

After the calibration, all the parameters for setting virtual 3D GIS camera orientation are
known, and the rotation matrix for aiming the virtual camera is calculated with the following
formula:

R ¼ Rzð(cid:2)azimuth0Þ · Rxðpitch0Þ · Ryð(cid:2)roll0Þ · Rzð(cid:2)panÞ · RxðtiltÞ

ð1Þ

In the previous formula, R is used to represent rotation matrices (2). The subscript determines
around which axis the rotation is taken, while the parameters in the parentheses are the
angles of rotation measured in an anticlockwise direction. Parameters azimuth0, roll0 and
pitch0 are previously described calibration parameters, while input parameters pan and tilt
determine the current orientation of the camera in its local coordinate system.

RxðjÞ ¼

0

0

cos j (cid:2) sin j
cos j
sin j

3
5; RyðjÞ ¼

2

4

cos j
0

0
1
(cid:2) sin j 0

3
5;

sin j
0
cos j

ð2Þ

2

4

1
0
0
2

4

RzðjÞ ¼

cos j (cid:2) sin j 0
sin j
0
1
0

cos j
0

3

5

Based on the calculated rotation matrix R, vectors that determine camera orientation in the
absolute coordinates of the 3D GIS are calculated with the following formulae:

Downloaded by [University of Miami] at 17:22 24 November 2014 1430

A. Milosavljevic´ et al.

~Vlook ¼ xlook
½
~Vside ¼ xside
½
~Vup ¼ xup
½

ylook
yside

yup

zup

(cid:3)T¼ R · 0
1
(cid:3)T¼ R · 1 0
½

½

zlook
zside
(cid:3)T¼ R · 0

½

0 1

(cid:3)T

(cid:3)T
0
(cid:3)T
0

ð3Þ

ð5Þ

ð6Þ

As the subscripts suggest, the first vector Vlook determines the view direction, while the
second, Vside, and the third, Vup, determine relative right-side and up directions, respectively.
Absolute orientation parameters can be fully determined by two of these three vectors.
Formulae that calculate azimuth, pitch and roll based on Vlook and Vside vectors are the
following:

azimuth ¼ arctan

pitch ¼ arcsin

;

roll ¼ (cid:2) arcsin

ð4Þ

(cid:2)

(cid:3)
;

xlook
ylook

 

!

zlook
j~Vlookj

 

!

zside
j~Vsidej

In order to view a 3D scene in the same way that the actual camera does, we had to model
the virtual camera with the parameters of sensors used in a real camera. To calculate the
angular field of view (FOV) based on the sensor’s dimension (S), a lens’s focal length
(FLmin) and the current zoom factor, we used the following formula (Titus 2005):

FOV ¼ 2 arctan

(cid:2)

(cid:3)

S
2 FLmin zoom

A task inverse to the previously described registration considers aiming the camera to a
geo-referenced target (2D or 3D object). Knowing the target coordinates (centre of the
object’s bounding box), in order to perform aiming, it is necessary to calculate adequate
camera pan and tilt parameters. If we know absolute coordinates of both the target (elements
of a vector VT) and the camera (elements of a vector VC), pan and tilt can be calculated using
the following formulae:

yC

zC

(cid:3)T; ~V ¼ ~VT (cid:2) ~VC

(cid:3)T¼ Ryðroll0Þ · Rxð(cid:2)pitch0Þ · Rzðazimuth0Þ · ~V

~VT ¼ xT
½
~V 0
T ¼ x 0
½

T

pan ¼ arctan

½

zT

(cid:3)T; ~VC ¼ xC
yT
z 0
y 0
T
(cid:2) (cid:3)
x 0
T
;
y 0

tilt ¼ arcsin

T

T

 

!

z 0
T
j~V 0
T j

Elements of the vector V represent the relative position of the target from the camera in the
global coordinate system, while the transformed vector V 0
T represents the same position in
the camera’s local coordinate system. Once the target position is determined in the camera’s
coordinate system (V 0
T ), adequate pan and tilt are easily calculated. Figure 12 illustrates this
coordinate space transform. Since the target is usually some 2D or 3D object and not just a
point in space, the zoom parameter is calculated from Equation (5) to correspond to the
required field of view (FOV).

5. Conclusions

In this article, we have presented the GeoScopeAVS system that integrates GIS and video
surveillance for real-time retrieval of information about viewed geospatial objects.

Downloaded by [University of Miami] at 17:22 24 November 2014 International Journal of Geographical Information Science

1431

Figure 12.
Illustration of coordinate space transformation for aiming camera to the target. Solid line
shows coordinate space of the 3D GIS, while dashed lines correspond to local camera coordinate
space.

Additionally, this system also enables selection and automatic aiming of camera to some
geo-referenced target, as well as mixing of real and virtual scenes. Along with the camera
communication sub-system, GeoScopeAVS is composed of 2D GIS, 3D GIS and AVS sub-
systems, which correspond to different geospatial data views.

Because of some characteristics, GeoScopeAVS can be categorised in a group of
monitor-based outdoor AR systems. To acquire camera 3D view parameters, instead of
using specific tracking devices, we relied on PTZ parameters gathered from the camera
itself. In this article, we presented a method for transforming these relative parameters into
the absolute view parameters required by the 3D GIS sub-system, and vice versa.

Although a presentation of our work is focused on the system we implemented, it should
be seen as a general approach to integration of GIS and video surveillance with suggestion of
some potential applications. The primary goal of the presented approach is to enhance video
surveillance and emergency situation management, but we also suggest potential use in
urban planning.

Downloaded by [University of Miami] at 17:22 24 November 2014 1432

A. Milosavljevic´ et al.

Regarding video surveillance, the major benefit stems from the fact that each point in the
camera image (assuming there is a good underlying model) is geo-referenced in 3D space.
For example, if a camera operator descries some incident, the incident location could be
automatically transmitted to the nearest patrol (assuming that the system has information on
location of the patrols). Further on, if some person(s) initiate escape from the incident site,
the operator can start tracking those fugitives and transmit their current location to the patrol.
Continuing that scenario, when these fugitives are about to leave the current camera scope,
the system could automatically try to find another appropriate camera and switch to it.
Applying certain video processing techniques to enable movable object detection and
tracking, such a system could give even greater assistance to operators.

Another possible application of this system is assistance in urban planning, enabling
investors and architects to see how their buildings will look in their natural surroundings. A
designed model of the building can be easily merged into an image received from the
camera. The 3D GIS sub-system is responsible for accurate positioning and interaction
with the terrain and other objects. Currently, this merging is implemented by introducing
uniform transparency to the image from the 3D GIS that is shown over the camera video. We
intend to extend this segment to achieve the insertion of virtual objects into the real-world
image.

Finally, this system can be considered as an extension to standard GIS. It exposes basic
GIS functionality, not only in 2D view but also in 3D and camera views. A user can click on
the objects and receive their properties, measure distances and orientation, zoom in or zoom
out, perform object identification, or even manipulate data acquisition. Using the 3D sub-
object paradigm, 3D and camera views are enriched with the additional data representation
model.

Acknowledgements
The work described in this article was substantially supported by the Municipality of Rakovica,
Belgrade, Serbia. The authors personally wish to thank Bojan Milic´, chairman of the municipality,
for valuable discussions and many helpful suggestions. We would also like to thank Jugoslav Beljin, IT
manager from Evrogeomatika Ltd., which supplied the geospatial data used for this project.

References
Azuma, R.T., 1997. A survey of augmented reality. Presence: Teleoperators and Virtual Environments,

6 (4), 355–385.

Barnes, M. and Finch, E.L., eds., 2008. COLLADA – Digital asset schema release 1.5.0 specification
[online]. Khronos Group. Available from: http://www.khronos.org/files/collada_spec_1_4.pdf
[Accessed 4 October 2009].

Brooks, S. and Whalley, J.L., 2008. Multilayer hybrid visualization to support 3D GIS. Computers,

Environment and Urban Systems, 32 (4), 278–292.

Chang, K., 2005. Introduction to geographic information systems. 3rd ed. New York: McGraw-Hill.
Coors,V., 2003. 3D-GIS in networking environments. Computers, Environment and Urban Systems,

27, 345–357.

Dimitrijevic´, A. and Rancˇic´, D., 2007. Efficiency of RINGO algorithm for large terrain rendering. In:
Proceedings of the 11th WSEAS international conference on computers, 26–28 July 2007, Agios
Nikolaos, Greece, 463–467.

Fournier, A., Gunawan, A.S., and Romanzin, C., 1993. Common illumination between real and
computer generated scenes. In: Proceedings of graphics interface, May 1993, Toronto, ON,
254–262.

Ghadirian, P. and Bishop, I.D., 2008. Integration of augmented reality and GIS: a new approach to

realistic landscape visualisation. Landscape and Urban Planning, 86, 226–232.

Downloaded by [University of Miami] at 17:22 24 November 2014 International Journal of Geographical Information Science

1433

Kawasaki, N. and Takai, Y., 2002. Video Monitoring system for security surveillance based on
augmented reality. In: Proceedings of the 12th international conference on artificial reality and
telexistence, 4–6 December 2002, Tokyo, Japan, 180–181.

Kolbe, T.H., Gro¨ger, G., and Plu¨mer, L., 2005. CityGML – interoperable access to 3D city models. In:
P. van Oosterom, S. Zlatanova, and E.M. Fendel, eds. Geo-information for disaster management.
Berlin/Heidelberg: Springer-Verlag, 883–900.

Krisp, J. and Ahonen-Rainio, P., 2003. Combining real and virtual components for visualizing
ecological barriers. In: Proceedings of the 21st international cartographic conference (ICC),
10–16 August 2003, Durban, South Africa, 2270–2274.

Lewis, P., Fotheringham, A.S., and Winstanley, A., in press. Spatial video and GIS. International

Journal of Geographical Information Science.

Li, D., et al., 2004. From 2D and 3D GIS for CyberCity. Geo-Spatial Information Science, 7 (1), 1–5.
Milgram, P. and Kishino, F., 1994. A taxonomy of mixed reality visual displays. IEICE Transactions

on Information Systems, E77–D (12), 1321–1329.

Milosavljevic´, A., Ðor(cid:2)devic´-Kajan, S., and Stoimenov, L., 2008. An application framework for rapid
development of web based GIS: GinisWeb. In: J.T. Sample, et al., eds. Geospatial services and
applications for the Internet. New York: Springer, 49–72.

Min, S., et al., 2007. Hybrid tracking for augmented reality GIS registration. In: Proceedings of the
2007 Japan–China joint workshop on frontier of computer science and technology, 1–3 November
2007, Wuhan, China, 139–145.

Mower, J.E., 2009. Creating and delivering augmented scenes. International Journal of Geographical

Information Science, 23 (8), 993–1011.

Oner Sebe, I., et al., 2003. 3D video surveillance with augmented virtual environments. In: First ACM
SIGMM international workshop on video surveillance, 2–8 November 2003, Berkeley, CA,
107–112.

Rancˇic´, D., Dimitrijevic´, A., and Predic´, B., 2006. Spatial coherency and parallelism in blocks
large terrain rendering. WSEAS Transaction on

reorganization of RINGO algorithm for
Computers, 5 (12), 3073–3079.

Sankaranarayanan, K. and Davis, J.W., 2008. A fast linear registration framework for multi-camera
GIS coordination. In: Proceedings of the 5th IEEE international conference on advanced video
and signal based surveillance (AVSS’08), 1–3 September 2008, Santa Fe, NM, 245–251.

Schall, G., et al., 2009. Handheld augmented reality for underground infrastructure visualization.

Personal and Ubiquitous Computing, 13 (4), 281–291.

Sourimant, G., Morin, L., and Bouatouch, K., 2007. GPS, GIS and video registration for building
reconstruction. In: Proceedings of the 2007 IEEE international conference on image processing
(Vol. 6), 16 September–19 October 2007, San Antonio, TX, 401–404.

Titus, J., 2005. Make sense of lens specs [online]. Test & Measurement World. Available from: http://

www.tmworld.com/article/CA6252635.html [Accessed 4 October 2009].

Tsai,F. and Lin, H.-C., 2007. Polygon-based texture mapping for cyber city 3D building models.

International Journal of Geographical Information Science, 21 (9), 965–981.

VAPIX, 2008. VAPIX(cid:2)HTTP API Version 3 [online]. AXIS Communications. Available from: http://
www.axis.com/files/manuals/VAPIX_3_HTTP_API_3_00.pdf [Accessed 4 October 2009].
Zlatanova, S., Abdul Rahman, A., and Pilouk, M., 2002. Trends in 3D GIS development. Journal of

Geospatial Engineering, 4 (2), 1–10.

Downloaded by [University of Miami] at 17:22 24 November 2014 