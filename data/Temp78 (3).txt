Categories are in flux, but their computational 
representations are fixed: That's a problem

Prashant Gupta1  |   Mark Gahegan1,2

DOI: 10.1111/tgis.12602  

R E S E A R C H   A R T I C L E

1Department of Computer Science, 
University of Auckland, Auckland, New 
Zealand

2Centre for eResearch, University of 
Auckland, Auckland, New Zealand

Correspondence
Mark Gahegan, Centre for eResearch, 
University of Auckland, Building 302:585, 
23 Symonds St., Auckland 1142, New 
Zealand.
Email: m.gahegan@auckland.ac.nz

Abstract

As  research  advances,  our  conceptual  understanding  also 

changes.  Computational  approaches  do  little  to  recognize 

the evolution that occurs at the conceptual level during the 

research  process.  This  can  result  in  misunderstanding  be-

tween knowledge producers and consumers and so inhibit 

the reusability of outcomes. In this article, we describe how 

changes at the conceptual level can be represented, along 

with  related  changes  to  data  and  methods,  and  how  ap-

propriate connections between these various artefacts can 

be maintained. To demonstrate these ideas, we show how 

categories  used  in  remote  sensing  and  land  cover  analysis 

change over time and how these changes are linked to vari-

ous  research  activities.  We  present  a  new  system  (called 

AdvoCate)  that  augments  typical  GIS  and  remote  sensing 

functionality with a conceptual model of categories that can 

undergo  change,  and  that  also  captures  the  cause  of  con-

ceptual change and its extent. We argue that concepts and 

categories should be represented explicitly and richly within 

GIS, because without this, we have a poor idea of what our 

modeled entities really mean, and by implication how they 

should be used appropriately. We demonstrate the useful-

ness  of  this  deeper  representation  using  examples  of  cat-

egory evolution from a land cover mapping exercise.

Transactions in GIS. 2020;00:1–24. 

wileyonlinelibrary.com/journal/tgis 

  |  1

© 2020 John Wiley & Sons Ltd2  |    

1 |  I NTRO D U C TI O N

According to Heraclitus, panta rhei—everything is in flux. But what gives that flux its form is the logos—the 

words or signs that enable us to perceive patterns in the flux, remember them, talk about them, and take 

action upon them even while we ourselves are part of the flux we are acting in and on.

J. F. Sowa (2002)

Heraclitus (c. 535–c. 475 BC) recognized that change is the core essence of the universe. Everything changes: both 

the external world and our internal understanding of it (Graham, 2015). To study and understand the world, as Sowa 

suggests, we build patterns (e.g., categories, rules, and theories) that provide a sense of meaning and stability, and 

enable us to apply our understanding to communicate, to solve problems, and to make predictions (Sowa, 2002). Such 

patterns are ephemeral, they respond to changes in the world and in our own understanding. This dynamic interplay 

of permanence and flux has been recognized and emphasized in the philosophy of science at length (Graham, 2015; 

Sowa,  1999;  Whitehead,  1929).  However,  most  of  our  computational  practices  somehow  overlook  this  important 

aspect of being. This article examines the dynamic nature of scientific knowledge from the perspective of categories—

their lifecycle and evolution—and investigates ways to incorporate, manage, and gain insight from this dynamism in 

our computational representation and analysis.

Whether we are creating categories of our own, or operationalizing categories proposed by somebody else, 

how we do it, and how well we do it, are seldom recorded. It is therefore usually impossible for another researcher 

to fully understand how a category came to be. This makes our science opaque and disincentivizes reusability by 

others. When we create equations and models, we are clear about what exactly we mean, and we can show equa-

tions and source code to back this up. When we create and modify categories, our computational systems are not 

up to the task of representing how they came into being and how they have changed over time. The issue pervades 

not only geography, but the whole of the geosciences, where we rely on the human construction of categories to 

help us understand the world and communicate this understanding to others. Ontologies only capture part of this 

meaning, since they are conceptual: they describe what we would like our categories to mean, not how exactly 

they are constructed. But much of their meaning is unfortunately tied up with the process of their construction 

and use (see Section 2 for more detail on this point).

To summarize, the software systems that we use for analysis (such as GIS) are not well equipped to:

•  uncover or propose categories,

•  evaluate the utility of categories,

•  represent and communicate their meaning, and

•  allow them to evolve.

But all four of these aspects are fundamental to categories as they apply in the geographic sciences, and the way 

in which we reason about them (Smith & Samuelson, 1997).

Take  the  example  of  a  forest,  a  category  commonly  used  in  national  and  regional  land  cover  mapping:  see 

Bennett (2001) for an interesting account of what a forest might be, from a philosophical perspective. We know 

that the definition of forest changes from place to place (Chazdon et al., 2016): a forest in New Zealand is defined 

using a higher density of trees than a forest in Australia. But the forest in New Zealand is not static either; it shifts 

subtly in response to improvements in remote sensing platforms, or to the training examples used in image classifi-

cation, or even the kinds of classifiers used. As tools and data get more accurate, we can better separate out forest 

from non-forest, which practically speaking results in a narrower definition of what a forest is, or at the very least, a 

better agreement between the formal definition of forest and the examples of identified forest instances on a map. 

But the meaning of forest also shifts because national land cover mapping is in part at the mercy of other social 

GUPTA And GAHEGAn    |  3

and political drivers that color the science. Definitions of forests have shifted over time as a result of a rising con-

cern about non-indigenous forests (splitting the forest category into two or more parts), and even political efforts 

to maximize apparent revegetation and carbon sequestration by forests—a kind of categorical gerrymandering. In 

short, when we change what we want to see in—or say about—the landscape, our definitions of categories will 

shift too, even if the names we give to them remain fixed. For the most part, this kind of shift goes unrecorded in 

database schema, taxonomies, and ontologies—we tend to treat such categories as endurants (Bittner, Donnelly, 

& Smith, 2004), but they do not exist in nature, so they remain inconveniently ensnared by Heraclitus’ insight. See 

also the work of Ahlqvist (2004) as a means of representing the conceptual spaces that categories are drawn from, 

to understand the differences and similarities between categories.

Earlier work by Brodaric and Gahegan (2007) highlights some of the challenges faced by field scientists trying 

to work collaboratively as their own conceptual understanding of geological categories changes, and the negative 

impacts this can have on creating reliable geological maps. This research goes further, examining the many ways in 

which categories might “shift” during their lifetime, and then building a model of such changes so that the process 

of evolution can be represented and then used to gain a better understanding, finally making possible the transla-

tion of categories between map versions, or from one perspective to another.

The three contributions of this work are:

1.  Incorporating  flux  into  static  knowledge  structures  (categories  in  this  case)  makes  them  more  meaningful 

and  “living”;  thus  representing  and  communicating  scientific  knowledge  as  it  is—a  dynamic  process  of 

evolution.

2.  Connecting products and the process of science provides an epistemological view of scientific evolution, which 

supports a deeper understanding of a scientific enquiry. This helps to bridge the conceptual gaps between knowl-

edge producers and consumers, and aids reproducibility and reusability of such knowledge among communities.

3.  By capturing knowledge as it is constructed and as it evolves (in a dynamic, exploratory manner), the evolution 

model tracks what researchers do at a conceptual level as well as at a procedural level; thus, the computationally 

represented model of science becomes much richer and also more examinable.

We  begin  by  reviewing  the  relevant  literature  to  date,  tying  together  three  distinct  threads:  the  philosophy  of 

science  as  applied  to  dynamic  and  evolving  knowledge;  computational  representations  of  categories  and  their 

change; and the digital tools and technologies supporting the evolution of categories. Then, in Section 3 we syn-

thesize these ideas into a series of design principles for a data model that concurrently represents both informa-

tion and knowledge about that information. Section 4 introduces AdvoCate, the system we built to validate these 

ideas, and a case study in Section 5 shows how this system is able to track the evolution of categories during an 

analytical task (land cover mapping). Some conclusions and future directions are offered in Section 6.

2 |  S C I E NTI FI C K N OW LE D G E A N D  IT S D I G ITA L  CO U NTE R PA RT

Scientific enquiry is an ongoing dynamic, iterative, and exploratory process that tracks the evolution in our un-

derstanding of the world. Despite such inherent complexities, we have a mature understanding of the process 

(Popper,  2002;  Shrager  &  Langley,  1990;  Sowa,  2002),  and  yet  computational  representations  are  struggling 

to catch up. Current approaches to representing scientific knowledge (data, models, workflows, etc.) provide a 

mostly static view of end results as opposed to capturing the dynamic nature of the science process. In addition, 

such knowledge structures are usually captive to their originating computational tools and disconnected from the 

neighboring knowledge structures and analytical processes responsible for their construction, use, and evolution 

(Hinsen, 2014). For example, in the case of a classification exercise (such as land cover mapping) we typically just 

represent and communicate the resulting land cover categories (as labels) and a classified map, separated from the 

GUPTA And GAHEGAn4  |    

understanding that goes into the process, and various choices that might revise and change their meanings and 

structures. The conceptual connections between various knowledge structures and activities during an enquiry, 

and the epistemological aspects of how and why things are done in a specific way, might endure for a while in the 

mind of the researcher responsible, but are easily lost; one might say that our focus shifted from knowledge to in-

formation (an issue raised by many; e.g., Sowa, 2002; Taylor, 1990). We are overdue to re-examine these concerns 

and incorporate them back into the design of computational tools and systems that support science.

2.1 | A being is its becoming

Here, we build on the notion proposed by Whitehead (Sowa, 2002) that the process of becoming a category should 

be regarded as the result of the activities (scientific, computational, physical, and cognitive) interacting together to 

form a unified concrescence. Furthermore, we adopt the view that scientific knowledge is a continuous process (of 

becoming), and the knowledge structures (products) of science are merely “a series of stopping points” (Weinberger, 

2011) in the process of science, or experiences of occasional permanence within the perpetual flux. Although the 

limitations of computational technology at present do not enable us to completely capture and represent the fluidity, 

connectedness, and temporality of the process of science, we can at least capture useful details of an investigation to 

make our knowledge representations, in Waddington's (1977) words, “richer and nearer to nature.”

In contemporary practice, we consider the construction and communication of scientific knowledge as distinct 

activities. Shapin (1984) argues that matter of fact (i.e., scientific knowledge, for example, results generated from 

a scientific experiment) can only be legitimately considered as a reliable item of knowledge if the process of its 

construction is witnessed (by many). This suggests that matter of fact is an epistemological and social category: (a) 

epistemological as it is identified and verified through its process of construction; (b) social as it gains the status 

of a “matter of fact” only when a community witnesses and agrees upon it. Shapin emphasizes three key aspects 

in the “mechanics of fact making”: (a) material technologies or scientific instruments that support the undertaking 

of science, including the production, processing, and analysis of knowledge; (b) literary technologies or information 

technologies that support the virtual witnessing of science, including the recording, dissemination, and access of 

knowledge; and (c) social technologies or organization of scientific communities that outline the conventions of 

their cooperation and dealing with knowledge claims.

While stating these three different aspects, Shapin clarifies that they are not distinct and disconnected; rather, 

the three aspects are interdependent and incorporate each other in their working. For example, a research article 

should not merely represent what was done in an experiment (the approach that current knowledge dissemination 

techniques adopt), but through the virtue of the density of circumstantial details, it should facilitate an experience 

that supports virtual witnessing of material technologies in the process of knowledge construction and the means 

for validating it (Shapin, 1984). This view toward scientific knowledge challenges whether our contemporary digital 

practices—and even our journal publications—really do support the reproducibility of our research. We aim here to 

facilitate such witnessing by allowing the evolution path of a category to be examined by any who would use it.

2.2 | Categories and the missing knowledge layer

We use the term concept to represent the mental representations of classes of things and connect our past ex-

periences to our present interactions with the world, and the term category to represent the classes of objects in 

the world that concepts describe (Medin & Rips, 2005; Murphy, 2002). Categories are generally represented and 

understood via their intension and extension. The intension of a category, grounded in some concept, refers to the 

set of associated attributes or features (its schema) and the extension of a category refers to all the entities or 

items that belong to the category based on adherence to the schema. For data-led categories—which are common 

GUPTA And GAHEGAn    |  5

in the geosciences—intension is often not explicitly represented; rather, a classification model (which we refer to 

as computational intension in this work) acts as a proxy to the intensions of modeled categories. In general, catego-

ries are often woven into a domain taxonomy or ontology, where they form relationships with other categories. 

How a category is incorporated into a taxonomy, or relates to other categories, provides significant clues to its 

deeper meaning. Therefore, the current state of a category can best be represented by the category's intension, 

its  extension,  and  its  place  in  the  taxonomy  or  ontology.  However,  even  these  three  aspects  are  not  explicitly 

represented in our computational models for categories, or if they are, they are represented in different systems 

(e.g., in ontologies, in the scripts and workflows of GIS and remote sensing systems).

The concern we raise here is that contemporary representations of a category do not fully explain its existence 

and identity, nor adequately support its virtual witnessing. The rationale behind how and why a category is what it 

is, and what computational and cognitive elements were involved in the process of its construction and evolution, 

are often not captured and communicated—the knowledge behind these aspects is missing in our representations, 

causing  conceptual  gaps  between  the  producers  and  consumers  of  categories.  Furthermore,  these  conceptual 

gaps  represent  likely  sources  of  error  or  confusion  as  categories  are  applied  in  analysis.  This  is  not  a  problem 

that applies only to the use of categories by novices. The shared names and ontological definitions we give to 

categories right across the geographical and earth sciences hide a multitude of operational differences between 

individuals, times, places, and applications. We need to be clearer about what, precisely, we mean. This becomes 

imperative  as  we  try  to  understand  how  the  environment  is  changing  in  response  to  human  impacts  between 

places and across several decades of change in land cover and land use taxonomies (for example).

To represent and convey a richer model of categories, it is important to broaden our view: from a category as a 

static and atemporal object to a category as a dynamic and living entity that is subject to a lifecycle: it is born. Figure 1 

provides an overview of a category's lifecycle, representing the contemporary facets that express our understanding 

at the time and some key elements of the process responsible for its birth, evolution, and deprication (death). The 

figure shows that data, theories, prior knowledge, and experience interact together and may iterate through several 

cycles of revision before a category becomes stable. The dynamic nature of the category means that its identity and 

structure are subject to ongoing revision due to incremental discoveries, richer observations, and new computing 

(cid:37)(cid:76)(cid:85)(cid:87)(cid:75)(cid:3)(cid:82)(cid:73)(cid:3)(cid:68)(cid:3)(cid:70)(cid:68)(cid:87)(cid:72)(cid:74)(cid:82)(cid:85)(cid:92)

(cid:54)(cid:70)(cid:76)(cid:72)(cid:81)(cid:70)(cid:72)
(cid:83)(cid:85)(cid:82)(cid:70)(cid:72)(cid:86)(cid:86)(cid:72)(cid:86)

(cid:38)(cid:82)(cid:81)(cid:87)(cid:72)(cid:91)(cid:87)(cid:86)(cid:18)
(cid:54)(cid:76)(cid:87)(cid:88)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:86)

(cid:53)(cid:72)(cid:86)(cid:72)(cid:68)(cid:85)(cid:70)(cid:75)(cid:72)(cid:85)(cid:182)(cid:86)
(cid:72)(cid:91)(cid:83)(cid:72)(cid:85)(cid:76)(cid:72)(cid:81)(cid:70)(cid:72)

(cid:39)(cid:68)(cid:87)(cid:68)

(cid:55)(cid:75)(cid:72)(cid:82)(cid:85)(cid:92)

(cid:38)(cid:68)(cid:87)(cid:72)(cid:74)(cid:82)(cid:85)(cid:92)(cid:3)

1.
(cid:44)(cid:81)(cid:87)(cid:72)(cid:81)(cid:86)(cid:76)(cid:82)(cid:81)
2. (cid:40)(cid:91)(cid:87)(cid:72)(cid:81)(cid:86)(cid:76)(cid:82)(cid:81)
3. (cid:51)(cid:82)(cid:86)(cid:76)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:76)(cid:81)(cid:3)(cid:70)(cid:82)(cid:81)(cid:70)(cid:72)(cid:83)(cid:87)(cid:88)(cid:68)(cid:79)(cid:3)

(cid:75)(cid:76)(cid:72)(cid:85)(cid:68)(cid:85)(cid:70)(cid:75)(cid:92)

(cid:38)(cid:82)(cid:81)(cid:70)(cid:72)(cid:83)(cid:87)(cid:88)(cid:68)(cid:79)
(cid:70)(cid:75)(cid:68)(cid:81)(cid:74)(cid:72)

(cid:49)(cid:72)(cid:90)
(cid:82)(cid:69)(cid:86)(cid:72)(cid:85)(cid:89)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)(cid:86)

(cid:40)(cid:89)(cid:82)(cid:79)(cid:88)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)

(cid:54)(cid:82)(cid:70)(cid:76)(cid:72)(cid:87)(cid:68)(cid:79)
(cid:70)(cid:82)(cid:81)(cid:70)(cid:72)(cid:85)(cid:81)(cid:86)

(cid:49)(cid:72)(cid:90)(cid:3)
(cid:70)(cid:82)(cid:80)(cid:83)(cid:88)(cid:87)(cid:76)(cid:81)(cid:74)(cid:3)
(cid:87)(cid:72)(cid:70)(cid:75)(cid:81)(cid:76)(cid:84)(cid:88)(cid:72)(cid:86)

(cid:38)(cid:68)(cid:87)(cid:72)(cid:74)(cid:82)(cid:85)(cid:92)(cid:3)
1.
(cid:44)(cid:81)(cid:87)(cid:72)(cid:81)(cid:86)(cid:76)(cid:82)(cid:81)
2. (cid:40)(cid:91)(cid:87)(cid:72)(cid:81)(cid:86)(cid:76)(cid:82)(cid:81)
3. (cid:51)(cid:82)(cid:86)(cid:76)(cid:87)(cid:76)(cid:82)(cid:81)(cid:3)(cid:76)(cid:81)(cid:3)(cid:70)(cid:82)(cid:81)(cid:70)(cid:72)(cid:83)(cid:87)(cid:88)(cid:68)(cid:79)(cid:3)

(cid:75)(cid:76)(cid:72)(cid:85)(cid:68)(cid:85)(cid:70)(cid:75)(cid:92)

(cid:39)(cid:76)(cid:72)

F I G U R E   1  An overview of the lifecycle of a category, starting from the birth, then evolution, and finally the 
death of the category. The figure displays the various facets—intension, extension, and position in conceptual 
hierarchy—of a category and some of the key elements of the process responsible for its birth and evolution

GUPTA And GAHEGAn6  |    

techniques, along with scientific and societal drivers (Brodaric & Gahegan, 2007). Unfortunately, we neglect to record 

such information during the process of its creation and change. And by the time we need it, the original data, theories, 

and the machine processes created or modified in the category are forgotten.

2.3 | Representing conceptual change

Representing the dynamic aspects of scientific knowledge is challenging. Current solutions go as far as capturing 

the different versions of a category, without any explicit details about how two versions differ and what caused 

the conceptualization to change (Gonçalves, Parsia, & Sattler, 2011; Kirsten, Hartung, Groß, & Rahm, 2009). Such 

a  situation  can  lead  to  problems  of  cognition  related  to  data  reuse  and  taxonomy  (or  ontology)  harmonization 

(Comber, Fisher, & Wadsworth, 2004; Tomas & Lutz, 2013; Würriehausen, Karmacharya, & Müller, 2014). To un-

derstand the evolution of categories, it is essential to know:

1.  What  aspects  or  facets  of  a  category  are  changed  and—if  they  can  be  measured—by  how  much.

2.  What internal or external drivers are responsible for a change.

3.  Any implications the changes have on the category’s identity and possibly any knock-on effects on the defini-

tions of neighboring categories.

As well as capturing changes in categories over time, individuals and communities may have competing, conflict-

ing, or complementary views toward a category at the same time (Brodaric & Gahegan, 2001; Gahegan & Pike, 

2006). Such contested and changing perspectives on categories represent the true nature of our scientific prac-

tices, and may enable us to understand, compare, and communicate communities' views toward the same concept. 

(Ideally, we would allow different versions of categories to exist as multiple, competing hypotheses, so we can 

evaluate them and decide which are the most useful, or simply represent the fact that researchers' conceptualiza-

tions differ and be able to show how they differ.)

2.4 | Modeling the evolution of categories

Knowledge evolution is a well-known problem and has been investigated deeply in various scientific communities. 

In machine learning, concept drift refers to a similar problem, where a classification model becomes inconsistent 

as the underlying data distribution changes (Tsymbal, 2004). Existing solutions, such as ensemble learning and 

instance selection (Tsymbal, 2004), support the revision of a classification model; however, the focus is only on 

improving  the  classification,  rather  than  capturing  and  representing  the  aspects  that  change.  Fanizzi,  d'Amato, 

and Esposito (2008) employ a conceptual clustering technique on populated ontologies to detect concept drift 

and new concept occurrences in a domain with the focus only on the discovery of change. Wang, Schlobach, and 

Klein (2011) introduce concept drift in the context of the Semantic Web, which covers any kind of change in the 

meaning of a concept, including change in its intension, extension, label, and relationships with other concepts. 

However, their work focuses on analyzing changes that have already been formally represented, rather than sup-

porting and capturing the evolution process.

In the semantics community, ontology evolution and versioning is a well-researched topic. Several attempts 

have been made to conceptualize and structure ontology evolution into a process model to provide a better un-

derstanding of ontology evolution and as a design requirement for software frameworks to support ontology evo-

lution (Flouris, Plexousakis, & Antoniou, 2006; Stojanovic, 2004; Zablith, 2011). Current frameworks that support 

the evolution process (or much of it) are the KAON, PROMPT plugin to Protégé, OntoView (Klein & Noy, 2003), and 

Evolva, a NeON toolkit plugin. KAON supports the whole ontology evolution cycle, from discovering changes to 

GUPTA And GAHEGAn    |  7

updating ontologies and propagating changes to the dependent artefacts; however, it does not deal with ontology 

evolution in collaborative environments and considers versioning as a separate task (Stojanovic, 2004). Protégé 

plugins and OntoView support change management for distributed ontologies with a special focus on versioning 

but do not support the change discovery aspect (Klein & Noy, 2003; Noy, Chugh, Liu, & Musen, 2006). Evolva sup-

ports ontology evolution with a focus on discovering changes from existing domain data that are external to an 

ontology; however, it does not support change propagation to dependent artefacts (Zablith, 2009). Each of these 

frameworks have their own process model for ontology evolution, reflecting their perspective and focus. For a 

more detailed comparison of ontology evolution frameworks, the interested reader should refer to Djedidi and 

Aufaure (2010), Hartung, Terwilliger, and Rahm (2011), and Zablith et al. (2014).

Our  focus  here  is  not  just  on  how  well  our  computational  tools  support  ontology  evolution,  but  also  how 

richly they represent a change or connect it to the process of science that caused it. The existing efforts toward 

data-driven and usage-driven change discovery in ontology evolution (Alani, Harris, & O'Neil, 2006; Cimiano & 

Völker, 2005; Javed, Abgaz, & Pahl, 2011; Zablith, d'Aquin, Sabou, & Motta, 2010) are mostly motivated by on-

tology  learning  mechanisms  and  only  suggest  new  concepts  and  instances,  rather  than  changing  the  existing 

concepts  and  instances  at  large.  None  of  these  current  efforts,  to  our  knowledge,  connect  with  the  scientific 

processes to capture and implement changes in domain conceptualization: such changes are considered to be out 

of scope and are often left for ontology engineers to deal with. The above discussion brings a major gap to light. 

The ontology evolution process in current research is useful, but largely disconnected from the process of science 

(i.e., the creation, use, and refinement of the categories in the information systems we use). We want the tools that 

support ontology evolution to be connected to the tools that are used to operationalize these categories. This is a 

major engineering challenge, but a worthy one.

2.5 | Transparency and provenance

The idea of transparency in the context of science requires openness, accessibility, and completeness (or richness) 

that suggests the phenomenon must be described in a “complete-enough” manner to enable its comprehension, 

reproduction, and use in other scientific endeavors (Shapin’s words, virtual witnessing of an enquiry). While there 

exist varied views on provenance, there is a general agreement on the growing significance and need for the adop-

tion of provenance to enhance knowledge development in multiple research domains, including earth science (Di 

& Yue, 2011) and GIScience (Wang, Padmanabhan, Myers, Tang, & Liu, 2008). Ideally, provenance would answer 

questions  raised  by  the  7  “w”s  (who,  what,  where,  when,  why,  which,  and  (w)how),  but  in  practice  we  do  not 

capture such rich descriptions, as there exists no convenient way of doing so, and supplying all that information 

manually would be very time-consuming. Most provenance systems in use today concentrate on capturing what 

was done, sometimes adding in where, when, and who, but seldom touching on how or why in any meaningful way. 

In addition to providing information about the origin of a scientific artefact, provenance information also subtly 

reflects the identity of that artefact. For example, a category's identity is intrinsically bound to the processes by 

which it is formed and used. The factors responsible for causing changes in a category—the decisions made during 

its formation and the processes (social, physical, or machine) used—are jointly responsible for building what the 

category presently is, and also help to explain how and why it is the way it is. This more complete implementation 

of provenance is not yet fully realized in the literature.

3 |  C ATEG O RY E VO LU TI O N  M O D E L

Three key ideas that surfaced from the discussion in the previous section form the basis of our design principles, 

as follows.

GUPTA And GAHEGAn8  |    

1.  Defining  categories  by  their  becoming.  A  change  in  scientific  knowledge  is  shaped  by  the  interactions  of 

multiple  artefacts,  methods,  researchers  (complete  with  their  expertise,  biases,  and  experience),  and  a  host 

of  other  social  drivers.  These  interactions,  although  not  providing  a  complete  account  of  the  scientific 

process  in  a  strict  sense,  capture  key  aspects  and,  importantly,  they  can  be  represented  in  our  com-

putational  systems.  Recall  from  the  previous  section  that  interactions  in  a  change  process,  driven  by  a 

researcher,  shape  the  becoming  of  the  artefact,  in  our  case,  a  category.  We  can  capture  these  interactions 

via  exploration  paths—the  iterative  interplay  between  various  methods,  data,  and  human  understanding 

that  occur  before  a  “stable”  (for  now)  model  of  a  category  is  created.  Our  hypothesis  therefore  is  that 

recording  the  life  of  a  category,  by  connecting  static  versions  and  their  dynamic  exploration  paths,  use-

fully  fills  some  of  the  conceptual  gap  in  our  current  representation  of  knowledge.

2.  Multiple perspectives toward a category. To understand the differences and similarities among different research-

ers' or communities' perspectives toward the same concept, and to support their reusability among those com-

munities, we must support multiple active versions of a category simultaneously. We will also need to support 

deprecated (old) versions that were used to create research artefacts in the past (such as maps), so that they can 

be better understood in the present.

3.  An epistemological view of category evolution. To support a thorough understanding of the transition and evolu-

tion of categories, it is crucial to understand the “how” and “why” aspects of a change, and thus a category 

evolution model needs to support a more epistemological view of category evolution. This requires the evolu-

tion model to incorporate the whole lifecycle of the process of change—where it originates from, how changes 

are conceptualized and validated, what insights motivated the changes, how they are applied, and finally how 

a change in one category affects other categories and related knowledge artefacts—for example, a database or 

GIS may need to change its schema or its rules as a result of one category splitting into two. These five stages 

form the conceptual model of the process of change in this research. In addition, to understand categories and 

their changes at a deeper level, an evolution model needs to explicitly represent categories at the level of their 

different facets (computational intension, extension, and their relationships), along with their connections to 

human conceptualization (concepts) and the process of their construction and evolution (exploration paths).

We now introduce a model of evolution informed from the above design principles. This model has two separate but 

connected components. The first component is a conceptual model of the process of change that describes the whole 

lifecycle of evolution (briefly discussed in the third design principle above). The second component is a schema that 

illustrates  the various  facets  of  concept  and  category,  and  their  relationships  and  dependencies,  and  is  shown  in 

Figure 2. This allows us to study and understand categories and their changes at an appropriate level of granularity.

Referring  to  Figure  2,  the  term  occurrence  denotes  an  individual  entity  (e.g.,  a  pixel  on  a  map)  that  may  be 

assigned to a category, whereas an instance refers to an occurrence that has been placed into a category via a 

classification/categorization process: in other words, it becomes an instance of some category. Every occurrence 

has a spatial and temporal footprint, which may provide further understanding about a category’s computational 

intension  and  extension.  An  exemplar  (training  sample)  is  an  occurrence,  which  is  classified  to  a  category  by  a 

domain expert, and then utilized in the learning process to construct the computational intension of the category. 

A classifier is used both in the learning and classification process, and so is linked to computational intension and 

to every instance classified into a category. Although we conceptually separate the computational intension and 

extension of a category, they are inherently related to each other (Storms, Mechelen, & Boeck, 1994). In a classi-

fication process, we use computational intensions to assign an occurrence to a category. Since it is not possible to 

show this dependency in a Unified Modelling Language (UML) diagram, we connect computational intension and 

extension with a weak dependency link (d1). The computational intension of a category is further dependent on 

the exploration path. Different researchers may try different exploration paths and may come up with different 

computational intensions. However, this dependency cannot be predetermined, so we connect exploration path 

and computational intension with another weak dependency link (d2).

GUPTA And GAHEGAn    |  9

F I G U R E   2  The conceptual model displays the relationships of a taxonomy, concept, and category. It further 
details the various facets of a concept and category, how they relate to each other, and also the aspects related 
to the development of a category's computational intension and extension. The labels “d1” and “d2” represent 
the dependencies of extension on computational intension, and computational intension on exploration path, 
respectively

To avoid any confusion between the terms “intension” and “computational intension” and what (concept or 

category) they refer to, we rename “intension” to “cognitive intension” in this work. This makes it amply clear that 

“cognitive intension” belongs to a (mental) concept, and “computational intension” belongs to a (representation of 

a) category. The computational intension of a category in our case is a mathematical model or a function resulting 

from a learning activity, which is constructed to support a classification activity. It does not necessarily represent 

the meaning of a concept. Comparing the computational intensions of two variants of a category only provides 

a statistical measure of the overlap of their computational models, and does not give any insight into whether or 

how their meanings overlap. Two categories may refer to the same concept while having quite different compu-

tational intensions. In contrast, the cognitive intension of a concept refers to the meaning of the concept and is 

mainly described to support one or more conceptual functions: identity determination, categorization, and con-

cept learning (Magolis & Laurence, 2002). Often, in data-led categories, the cognitive intension of a concept is not 

explicitly defined at the outset. This makes it difficult to examine if the cognitive intension or meaning of a concept 

has drifted, or if its identity has changed. In such cases, where the cognitive intension is not defined explicitly, 

we adopt the notion of agreement (Turney, 1995) to compute change in the cognitive intension of a concept or to 

compare cognitive intensions of similar concepts described in different ontologies or taxonomies. The notion of 

agreement is a measure of overlap in cognitive intension and can be represented as the probability of producing 

the same predictions by two categories (representing the concepts to be compared) over a normal distribution on 

the attribute space—an imperfect measure of course, but nevertheless useful.

Change is the core essence of this model, and so versioning is used to track the evolution of categories. The 

model supports two kinds of category versions: evolutionary and competing. A version of a category that reflects 

evolution over time with respect to the previous version of the category is considered as an evolutionary version. 

In contrast, different variants of a category that exist at the same time due to different viewpoints, task require-

ments, or spatial effects are considered as competing versions. The model supports the following types of change 

operations: birth, merge, split, group, and retire. Note we say retire, not death, because categories that are depre-

cated are still useful in understanding what was done in the past. The model further incorporates multiple kinds 

of drift operations that may occur depending on changes in the cognitive intension of a concept and the computa-

tional intensions and extensions of its related categories. The first one is structural drift, where the computational 

GUPTA And GAHEGAn10  |    

intension of a category is changed, with very minor or no effects on its extension and the cognitive intension of 

the related concept. Often this happens due to change in the classification workflow, or minor changes in the 

training samples. The second kind of drift is efficiency drift, where the computational intension of a category is 

changed as it becomes (say) more accurate, perhaps due to an improved classifier, leading to a noticeable change 

in its extension. However, even in this case the cognitive intension or the meaning of the concept remains un-

changed. The third kind is concept drift, where we see change in all aspects including computational intension, 

extension, and the cognitive intension of the concept. In this case, we may see a change in the concept’s meaning, 

which we can quantify with agreement measures as mentioned above.

We  could  devote  the  remainder  of  this  article  to  describing  each  of  the  concepts  and  relations  shown  in 

Figure  2,  and  arguing  philosophically  or  practically  why  we  made  each  of  those  choices.  Instead,  we  prefer  to 

demonstrate what the resulting model allows us to do. To paraphrase Paul Box (1976), it is our contention that 

there are no “right” models here, just ones that are useful, or not useful. So we have chosen to present a brief de-

scription, then to use the remaining pages to show what this model enables us to do via case studies, with the aim 

of demonstrating its usefulness. We have developed a complete formal model for each of the change operations 

described above, and this can be used to exactly describe the potential consequences of any change. A follow-up 

article will present this more theoretical description of the model and describe, using formal logic, a language of 

change as it applies to the evolution of categories, including the definition of all the different change operators—

there is just not room for that here.

4 |  A DVO C ATE: A DV E NT U R E S O F C ATEG O R I E S

In this section we introduce AdvoCate, which can be understood as a (limited) GIS that has been comprehensively 

extended to also represent changes in concepts and categories and to track the exploratory process of a scientific 

enquiry, and so capture the dynamic interactions and insights lurking within this exploratory process. The tool's 

name, AdvoCate (Adventures of Categories), is an allusion to A. N. Whitehead's Adventures of ideas (Whitehead, 

1933), as the inspiration behind this work comes in large part from Whitehead’s process philosophy described in 

Section 2.1.

4.1 | Category modeling workflow

At the centre of AdvoCate is a process model describing categories and their evolution. In the examples that fol-

low, we have chosen land cover classification as the target domain, for illustrative purposes (the process model is 

generic enough to support other tasks involving categories, outside of remote sensing).

A user might typically start with some training activity and seek validation of a proposed classification model 

through the resulting confusion matrix and other accuracy measures. The user may step back to the previous step 

of editing the training set, or possibly refining the categories to be used if there is not enough separation between 

the modeled categories. Various validation methods, such as the cross-validation method and train-test splits, can 

be used during the training activity to measure the likely accuracies of the modeled categories and the classifica-

tion model. Once the user determines an alignment between the spectral and information classes, (s)he may then 

edit the proposed categories and the training data accordingly and create a classification model. The next step is 

classification, which employs the classification model to classify the unlabeled data into different categories rep-

resented in the training set. At this stage, the user may notice misclassifications and again go back to the previous 

steps to change the training samples or the classifier’s parameters, or use a different classifier. This process con-

tinues until the user finds the set of categories that align with his/her understanding and the training data. Once 

the user is satisfied with the model and the classified map (i.e., the results align with the user’s understanding), 

GUPTA And GAHEGAnthe changes are then accepted and the new categories are minted. During the process, AdvoCate employs the 

two main services—change recognition and change recommendation—which we describe in more detail in Section 5.

    |  11

4.2 | Overview of AdvoCate

We  implement  AdvoCate1   as  an  open  source  web  application  that  offers  the  following  functionality:  (a)  con-

ducting basic classification, including exploratory data analysis, mapping, a selection of classifiers, and evaluation 

tools (including an error matrix, kappa statistic, and category separation statistics); (b) explicit representation of 

taxonomies, and their constituent categories; (c) tracking to capture all analysis activities that result in changes in 

existing taxonomies; and (d) visualizing tools for viewing and comparing existing taxonomies, and their concepts 

and categories.

AdvoCate  was  built  using  the  Python  programming  language  and  makes  extensive  use  of  existing  Python 

libraries. Django, a Python web framework, was used for web development as it provides several useful compo-

nents, such as the session tool and Python logging service to track and capture the provenance of user activities, 

and geo-extensions to support web mapping. Figure 3 illustrates the overall architecture, along with various li-

braries used and the services offered. The architecture consists of three tiers: user interface (UI), service, and 

database tiers. As a researcher explores and experiments with categories, AdvoCate stores the details of these 

activities and the path(s) traversed, and displays this path for the researcher to keep track and understand his/her 

own process of enquiry, and even visually compare competing activities. The system remembers the intermediate 

outputs  produced  and  understands  how  these  activities  are  linked  within  the  overall  research  process  model. 

AdvoCate also provides a visualization interface, so the researcher can visualize taxonomies, concepts, and cate-

gories, and the exploration paths behind their construction and change, along with the resulting classified maps. 

F I G U R E   3  An overview of the three-tiered architecture of AdvoCate, illustrating various UIs and services 
behind them, and the external libraries employed to support various services

GUPTA And GAHEGAn12  |    

The UI employs the JQuery, Bootstrap, and HandsonTable libraries to support dynamic asynchronous interactions 

and visualizations during the investigation process.

The service layer of AdvoCate incorporates four main services: Activity Support, Provenance Tracker, Change 

Recognition,  and  Change  Implementation.  These  services  employ  various  Python  libraries,  such  as  Scikit-learn 

(Pedregosa et al., 2011) for machine-learning tasks, Geospatial Data Abstraction Library (GDAL) for translation 

and  processing  of  geospatial  data  formats,  and  ETE3  (Environment  for  Tree  Exploration;  Huerta-Cepas,  Serra, 

& Bork, 2016) for taxonomy visualization. In addition, the functioning of these services is supported by various 

algorithms and rules (as shown in Figure 3), based on the category evolution model described in Section 3 and as-

sociated change operations and rules. Additional services (recommendation and relationship tracking algorithms) 

were built as we gained insight from the implementation and use of the system. Finally, the database tier stores 

the category versioning data along with the exploration process, in a PostgreSQL server.

4.3 | Supporting the versioning of categories

Change is the key aspect of AdvoCate, so versioning is considered a first-class citizen: categories, taxonomies, 

and all aspects, including training data, concepts, and categories, are all subject to versioning. Figure 4 shows the 

data model incorporated in AdvoCate, informed by the design principles outlined in Section 3. The schema is es-

sentially a metamodel that stores conceptual models (taxonomies and their associated concepts and categories), 

along with the artefacts, activities, and change operations. It explicitly stores exploration paths (chains of activi-

ties in the order of their execution), and connects them to change events representing evolution in categories 

and taxonomies. The concepts within the data model are separated into three different groups: (a) artifacts; (b) 

activities;  and  (c)  change  operations.  However,  in  practice  these  groups  are  deeply  interlinked  (as  explained  in 

Section 3). The data model does not show all the entities and relationships incorporated into AdvoCate due to 

space limitations, but it is complete enough to illustrate the functionality described here.

To support versioning, temporal intervals are added to the tables with a “start-date” and “end-date.” So, for 

example, every time a new version of a category is added, the current category stored in the versioning system 

is expired, and its “end-date” is changed from “Null” to its expiry date. No categories ever become inaccessible, 

even after their expiry; the expired category and its associated information still remain in the system and may be 

needed to explain older data products in which it appears. Figure 4 highlights the temporal tables and versioning 

with <T> and <V> tags, respectively. There are three kinds of temporal attributes used in the data model: (a) T1, the 

transaction time for versioning purposes and for annotating the exploration chain; (b) T2, valid time for activities 

outside the computational system, such as collection of training samples (valid time refers to the time something 

happens in the world); and (c) T3, transaction timestamps for activities performed in AdvoCate (transaction time-

stamps refer to when a row is recorded in the database).

Looking more closely, the taxonomy table has a many-to-many relationship with the concept table, which im-

plies that each concept can belong to multiple taxonomies, and that a taxonomy contains one or more concepts. 

The category table is connected to both the concept table and the taxonomy table, denoting the taxonomy and 

concept that a category relates to. The schema explicitly distinguishes the notions of concept and category, which 

helps us to represent categories that are extensions of the same concept, but in different taxonomies. The con-

cept table has a one-to-many relationship with the category table, which implies that a category always refers to a 

specific concept, but a concept may have one or more categories belonging to one or more taxonomies. Further, 

the  category  table  is  explicitly  connected  to  its  various  facets:  directly  connected  to  the  computational  inten-

sion, extension, and horizontal relationship tables (relationships between different versions of a category); directly 

connected to the concept table, and in turn the hierarchical relationship table; and finally, to its exploration path 

through  the  change  event  table.  All  categories  and  taxonomies  in  AdvoCate  are  connected  to  a  change  event, 

GUPTA And GAHEGAn    |  13

F I G U R E   4  The data model represents how information in AdvoCate is modeled as artifacts, activities, 
change operations, and connections among them. The model also highlights the temporality of artifacts and 
activities—thus the versioned artifacts and activities

which  further  links  to  the  exploration  path  table  and  change  operations  table,  suggesting  there  always  exist  an 

exploration path and change operations behind the construction and change of every category and taxonomy.

4.4 | Services

The Activity Support service forms the interface between the user and the various external libraries, such as Scikit-

learn and GDAL, and other data processing modules to support the required functionalities. The other key aspect 

of this service is change recommendation, which employs a recommender service to examine the intermediate arte-

facts (the confusion matrix and various other statistical functions that can be defined between categories to show 

their overlap, separation, or confusion) and provides suggestions to a user on how to improve the classification 

model, achieve better accuracy of categories, and avoid misclassification.

The Provenance Tracker service constantly interacts with the Activity Support service to capture details of the 

current exploration path and invokes predefined SQL statements to store these details in the database. These 

details are recorded even if the exploration path is unsuccessful or does not result in any change; this information 

can be used for debugging purposes and to understand dead ends in the exploration process.

GUPTA And GAHEGAn14  |    

The  Change  Recognition  service  advises  throughout  the  modeling  process  how  the  proposed  concepts  and 

categories differ from the existing ones. For example, after the classification activity, the service compares the 

extensions of the proposed categories with the existing ones, and quantifies the extensional changes. The service 

employs various statistical measures (refer to Tables 1 and 2) to measure the changes in various facets of concepts 

and categories, and to determine the type of change and the resulting relationships between the proposed and 

existing categories.

Finally, the Change Implementation service gathers the details of the final changes and creates a change event, 

which incorporates the set of change operations to be applied. The service then updates the concepts and cate-

gories within AdvoCate and validates the modified taxonomy and refined categories.

5 |  E X A M PLE S  O F A DVO C ATE I N U S E

The following scenario demonstrates how AdvoCate supports making changes to an existing land cover taxonomy 

to improve its usefulness, resulting in a new taxonomy version and changes to existing concepts and categories, 

via various operations such as splitting, merging, grouping, and drift. In addition to guiding a researcher through 

the  modeling  process,  this  scenario  also  illustrates  how  AdvoCate  continually  compares  the  existing  concepts 

and categories to the proposed new ones, to convey and quantify possible changes. As new sensing devices with 

improved spectral characteristics come online, better image data with more spectral bands become available. This 

opens up an opportunity for researchers to revise (improve accuracy) and broaden (add new specialized concepts) 

the taxonomy of land cover classification. The data used in this scenario is from Landsat TM and ETM + imagery. 

This scenario consists of two distinct tasks, the first of which is as follows:

TA B L E   1  Statistical measures to compare various aspects of evolutionary or competing versions of a 
category. Jaccard index (Real & Vargas, 1996) measures the similarity between two sample sets (training samples 
or extension of categories). Jeffries-Matusita (JM) (Davis et al., 1978) distance is a separability measure and 
is used to measure the separability between the computational intensions of two categories. User's accuracy 
(Congalton & Green, 2008) is the number of occurrences correctly classified for a category divided by the total 
number of occurrences belonging to the category in the classified data. Producer’s accuracy is the number of 
occurrences correctly classified for a category divided by the total number of occurrences belonging to the 
category in the training data. User's and producer's accuracy are measured via the confusion matrix—a square 
array of numbers where the column refers to the number of occurrences assigned to a set of categories through 
reference data or ground truth, and the row represents the number of occurrences assigned through the 
classification method

Intensional comparison

Extensional comparison

What to measure

Overlap in 
cognitive 
intension

Overlap in 
training 
samples

Overlap in 

computational 
intension

User's 

accuracy

Producer's 
accuracy

Overlap

How to measure

Agreement

Jaccard index

JM distance

nii
ni+

njj
n+j

Jaccard 
index

TA B L E   2  Statistical measures to compare categories referring to different concepts from the same taxonomy

What to measure

Overlap in computational intensions

Confusion of A with B

Confusion of 

Intensional comparison

Extensional comparison

How to measure

JM distance

nAB

B with A

nBA

 

 

GUPTA And GAHEGAn    |  15

Bob decides to modify an existing land cover taxonomy “AKL LCDB,” revising the existing concepts and 

categories using improved satellite data with more spectral bands. The existing taxonomy includes the 

following concepts: Built-up Area, Water Bodies, Pasture, Forest, Shadow, and Cloud. He aims to deter-

mine if the existing categories can be improved and if the new spectral bands can better characterize and 

differentiate the specialized concepts in the taxonomy. Bob aims to expand the taxonomy to include the 

following concepts: Built-up Area, Open Space, Grassland, Forest, Scrub, Estuarine Open Water, Inland 

Water, Sea Water, Shadow, and Cloud.

5.1 | Part one: Revising a land cover taxonomy in response to changing external factors

Bob  starts  his  exploration  process  to  revise  the  existing  taxonomy  by  assembling  a  training  set  using  the  new 

richer data with higher spatial and spectral resolution. The interface for assembling a training set allows a user to 

explicitly suggest what (s)he wishes to change in the existing set of concepts. For example, a user may add new 

concepts, refine existing concepts, split an existing concept into multiple, more specialized concepts, or merge 

several concepts into a more generalized concept. Note that these changes are made to the training samples ex-

emplifying the concepts to be modeled—the concepts in the existing taxonomy are not changed yet.

A classification model is generated through a training activity using the “Naive Bayes” classifier and a 10-fold 

cross-validation method. AdvoCate calculates statistical details about the intensional and extensional confusion 

of the proposed new categories. In addition, it also summarizes the intensional and extensional comparison of the 

categories corresponding to the common concepts in the two sets. The training activity results in a classification 

model with a validation score of 0.8, and a kappa value of 0.76, and individual categories with better accuracies 

compared to the original ones. However, Grassland and Open Space, and Forest and Scrub, are shown to be highly 

confused, and so the Activity Support service (mentioned above) suggests merging these concepts.

Bob agrees with the suggestions and merges Grassland and Open Space into Pasture, and Forest and Scrub into 

Woody Vegetation, and remodels the resulting set of concepts. AdvoCate tracks these changes in concepts, and 

recognizes that: (a) the merged concept Pasture is a concept in an existing taxonomy; and (b) Woody Vegetation is 

a generalized concept in relation to the existing concept Forest. Following these changes, the resulting classified 

land cover maps have an accuracy of 0.86, and the Activity Support service provides no further suggestions as the 

accuracies of the modeled categories are above a set threshold.

Bob decides to further explore the modeled categories and check if any of them has a multimodal distribution, 

which may adversely affect the operation of the maximum likelihood classification, and may suggest the need for 

further specialized concepts. A k-means clustering algorithm is used for this purpose. Bob finds that many categories 

(or information classes) have multiple clusters (spectral classes); but either the clusters contain so few points as to be 

meaningless, or so overlapping that they represent a redundant division of data. However, when Bob applies cluster-

ing to the training samples of Built-up Area, it presents a bimodal distribution with both clusters having a reasonable 

number of data points and with their centroids separated by a reasonable distance, as shown in Figure 5.

Often Landsat bands 1, 4, and 5 are good differentiators of different land cover types within a Built-up Area 

and  are  used  to  study  them  (Quinn,  2011).  Figure  5  shows  the  clustered  data  in  combinations  of  these  bands, 

which  suggests  two  distinct  clusters  in  black  and  green,  respectively.  However,  we  can  see  that  the  decision 

boundaries  of  the  two  clusters  overlap,  suggesting  some  confusion  among  these  spectral  classes.  Bob  tries  to 

investigate if Built-up Area has any more spectral classes, but re-clustering with more than two clusters results in 

no new insights. Bob confirms that the two clusters might best represent Urban and Suburban by using existing 

maps and field knowledge, and decides to explore the option of splitting Built-up Area into these two new classes.

After splitting Built-up Area, Bob re-runs the learning activity, and the resulting classification model does not 

show a decrease in accuracy, so the changes would seem to be helpful. The Activity Support service in AdvoCate 

then highlights issues with Estuarine Open Water and Sea Water due to lower accuracies, and suggests merging the 

GUPTA And GAHEGAn16  |    

F I G U R E   5  Scatterplots of the clustered data for Built-up Area  on different band combinations. The black 
and green colors represent the two clusters that emerged when Bob runs a k-means clustering algorithm on the 
training samples of Built-up Area. The centroid of each cluster is highlighted in yellow

two concepts. Bob notices the high omission error in Estuarine Open Water and decides to merge Estuarine Open 

Water and Sea Water into Water. After merging the two concepts, the classification accuracy rises to 0.90, with 

a kappa value of 0.88. The resulting categories and associated map, shown in Figure 6, now align better with his 

understanding and needs.

ones, organized by change type.

As Bob moves to the “change recognition” phase, a summary of his modifications is displayed, as shown in 

Figure 7. The summary shows details of the proposed new categories along with their relationships to existing 

AdvoCate then provides an option to create a new taxonomy version or to make changes to the existing taxon-

omy. Bob decides to create a new version of the taxonomy, which leads to a series of change operations to be applied, 

as summarized by AdvoCate in Figure 8. These change operations create new specialized and generalized concepts 

and categories, the evolutionary versions of categories, and record the horizontal relationships among various cate-

gories in the two taxonomy versions. Bob commits the change event, which saves a new version (2) of the taxonomy 

“AKL LCDB,” and its concepts and categories, along with their hierarchical and horizontal relationships.

5.2 | Part two: Examining and adapting the taxonomic work of others

Later, during the same year, another researcher, Jane, needs to understand the taxonomy built by Bob and 

the exploration path he traversed to model it in order to determine if she can use some of the categories 

GUPTA And GAHEGAn    |  17

F I G U R E   6  Final classified map and the change matrix for part one use case. The change matrix displays the 
overlap of extensions of the proposed categories to the existing ones

for an entirely different purpose related to invasive species and forest extent. In the process, Jane comes 

across some issues that she needs to address, building her own exploration path, and resulting in various 

revisions to the taxonomy.

Jane decides to explore the taxonomy “AKL LCDB—version 2” to understand the concepts, categories, and the explo-

ration path traversed by Bob. AdvoCate allows a user to visualize an existing taxonomy, along with the classified map 

and the exploration path that led to its creation, as shown in Figure 9. The taxonomy and the classified map show the 

final results that Bob achieved, which is all we often see in contemporary scientific communication. But the explora-

tion path also shown enables Jane to understand the steps taken by Bob, along with his decisions. Jane may click on 

any of these activities, and the related details are displayed. For example, the last editing operation in the exploration 

path shows that Estuarine Open Water and Sea Water were merged. Jane may also examine the preceding activities 

to check what other activities might have provided the insight that led to the editing operation. From the exploration 

path, Jane realizes that Bob sacrificed some specialized categories to improve overall accuracy, resulting in the merg-

ing of several pairs of concepts and a simpler map. The exploration path further reveals how Bob realized that Built-up 

Area contains two distinct spectral classes through the findings from clustering various concepts, and the decision 

to split Built-up Area is supported by no negative effects on the accuracy of the classification model. Jane now has a 

better appreciation of Bob’s workflow through the details of his exploration path.

GUPTA And GAHEGAn18  |    

F I G U R E   7  The summary of the current exploration process, and the conceptualized changes in relation to 
the existing set of concepts. The first table displays the common concepts among the proposed taxonomy and 
the existing one, along with the accuracies of the related categories and their extensional similarities. Since 
the proposed categories are built using data with different numbers of spectral bands, JM distance cannot be 
computed. The second table displays the concepts that are generalizations of existing concepts. The notion of 
extensional containment (Brockmans, Haase, & Stuckenschmidt, 2006) means that a concept in one taxonomy 
represents a more specific aspect of the world than a concept in the other taxonomy. The third table displays the 
concepts that are split from an existing concept, along with their accuracies and extensional containment

However, in the process of understanding Bob’s workflow and the classified map, Jane finds that along the 

coastal  region,  the  land  area  covered  under  Mangrove  is  classified  as  Urban  or  Suburban  (rather  than  Woody 

Vegetation). She further discovers that a specific region (the top center of the map in Figure 9) covered by Water 

(containing Sea Water and Estuarine Open Water) is classified as Inland Water. Jane decides to investigate these 

changes, and starts her own exploration process by choosing the existing training set created by Bob and editing 

the training samples of Water and Woody Vegetation to incorporate new training samples from the regions that 

are misclassified. A classification model is then generated through training activity, again using the “Naive Bayes” 

classifier and a 10-fold cross-validation method, and giving an accuracy of 0.87 and a kappa value of 0.85. Jane 

finds the model convincing and decides to classify the map to check if it has any spatial effects on the misclassified 

region. The resulting classified map and the change matrix are shown in Figure 10. The change matrix shows that 

almost one-third of the extension of Inland Water is reclassified as Water, justifying the reduced confusion in the 

new map as compared to the original map in Figure 9.

The change matrix also shows that there is a drift in the extensions of Urban, Suburban, and Woody Vegetation—

an  unintended  consequence  of  these  changes,  but  not  uncommon  when  using  a  normally  distributed  statisti-

cal  model  for  categories.  Some  regions  along  the  coastline,  originally  classified  as  Urban  or  Suburban,  are  now 

GUPTA And GAHEGAn    |  19

F I G U R E   8  The change event history displaying the set of change operations used to create a new version of 
the land cover taxonomy

F I G U R E   9  Visualizations of the taxonomy “AKL LCDB—version 2” along with the classified map and the 
exploration path that Bob traversed to model the taxonomy

GUPTA And GAHEGAn20  |    

F I G U R E   1 0  The classified map and change matrix generated after editing the training samples of Water  and 
Woody Vegetation

reclassified as Woody Vegetation, as can be seen by comparing the maps in Figures 9 and 10. Updating the training 

samples of the two concepts thus resulted in changes to the extensions of several categories, as Jane suspected 

it might. Although the overall accuracy of the classification model has decreased, the changes displayed in the 

extension of various concepts align better with Jane’s analytical needs. Before ending her exploration process 

and committing the changes, Jane groups Urban and Suburban into Built-up Area, and Inland Water and Water into 

Water Bodies. The grouping of concepts creates a new parent concept of the existing concepts that are grouped, 

and the extension of the parent concept is the sum of extensions of its children. This does not require remodeling 

the whole taxonomy again, so ends Jane’s exploration path.

Jane decides to save these changes and moves to the “Change recognition” phase, where AdvoCate displays 

the  change  summary  of  her  exploration  process,  shown  in  Figure  11.  The  first  table  in  the  figure  shows  the 

comparison of the categories modeled to those in the “AKL LCDB – version 2,” and the second table shows the 

concepts that are grouped to create new parent concepts. The statistical details displayed in the table highlight 

some interesting insights about the similarities between categories. As Jane only changed the training samples of 

Water and Woody Vegetation, the computational intensions of only those two categories have drifted, as shown 

GUPTA And GAHEGAn    |  21

F I G U R E   1 1  Summary of the exploration path for part 2 use case, and the conceptualized changes to various 
concepts and categories

by the JM distance values in the first table. However, we see that the extensions of almost all the categories have 

drifted. A good example is Inland Water, for which the value of JM distance is zero, which indicates no change 

in its computational intension, while its extension has changed significantly. This demonstrates that in a Naive 

Bayes model, a category’s extension may change significantly if the computational intensions of its neighboring 

categories are changed. Jane wishes to commit her changes to the existing taxonomy, rather than creating a new 

version. However, she does not wish to overwrite Bob's categories, as she now appreciates the differences in 

Bob's and her own perspective. AdvoCate allows her to commit the new categories as competing versions to the 

categories that Bob modeled. Thus, AdvoCate allows storing and operationalizing of concurrent perspectives on 

a category.

The use-case scenario above shows that AdvoCate supports the process of creating, exploring, and modifying 

categories,  allowing  users  to  change  taxonomies  and  their  member  concepts  and  categories  whilst  simultane-

ously conducting a classification exercise. Having both the procedural (information analysis) aspects of land cover 

analysis and their conceptual counterparts (knowledge representation) all within the same single system means 

that relationships between these two levels of abstraction can be captured and made persistent. Conceptual un-

derstanding and analytical understanding are thus not only explicit, but also kept in sync with each other during 

various science activities.

ate/9868403).

Further examples of AdvoCate in use (including more details on the above scenario) are available as a sep-

arate  download  (https ://auckl and.figsh are.com/artic les/land_Cover_Class ifica tion_Case_Studi es_using_AdvoC 

GUPTA And GAHEGAn22  |    

6 |  CO N C LU S I O N S

In The Rock (Eliot, 1969), T. S. Elliot asked: “Where is the knowledge we have lost in information?” Although written 

long before the information age, this nevertheless seems an apt description of the current state of the art with 

computational methods. Most of our analytical tools, such as GIS and R, operate solely at the abstraction level 

of information. Some newer tools now operate at the knowledge level (ontology systems such as Protégé), but 

none of them operate at both levels simultaneously. But people do, and categories only really make sense when 

both aspects of category identity and formation can be presented, explored, and validated (or refuted). AdvoCate 

shows how we might re-imagine and re-engineer our analysis tools to include both these levels of abstraction into 

a single system. The use-cases presented demonstrate some of the advantages and additional affordances that 

such re-imagining might provide.

We do not claim that the conceptual model developed here is the last word on how categories should be repre-

sented in computational systems—far from it. Rather, it is an example of a model that seems to work well for a specific 

set of tasks relating to remote sensing and land cover, and we make no deeper claims in this regard. It would be very 

useful to see how other such systems, from different application areas, choose to model the interactions between 

concepts, categories, and data, and to learn what might be generalizable from such models. Some obvious example 

areas of application might be in soil or geological mapping, and further afield in taxonomic biology. In human geogra-

phy, new versions of various social indices, used to classify space, are regularly proposed in areas such as deprivation, 

walkability, and happiness. The evolution of these categories could also be modeled with AdvoCate.

The goal of enabling research with information technology and computer science has for too long decomposed 

the activities of science into small pieces that can more easily be implemented. Clearly, this has led to some very 

important progress. But it is not too much to ask that our computational systems adhere more strongly to our 

science processes, which operate simultaneously across multiple levels of abstraction. We should expect more 

from our computational tools.

Prashant Gupta 

 https://orcid.org/0000-0002-4988-384X 

Mark Gahegan 

 https://orcid.org/0000-0001-7209-8156 

O R C I D

E N D N OT E
  1 

R E F E R E N C E S

All the software libraries and components used to construct AdvoCate are open source and freely available. 

The source code for AdvoCate is available at https ://github.com/Prash antgu ptanz/ AdvoCate 

Ahlqvist, O. (2004). A parameterized representation of uncertain conceptual spaces. Transactions in GIS, 8, 493–451.
Alani, H., Harris, S., & O’Neil, B. (2006). Winnowing ontologies based on application use. In Y. Sure & J. Domingue (Eds.), 
The  semantic  web:  Research  and  applications  (Lecture  Notes  in  Computer  Science,  Vol.  4011,  pp.  185–199).  Berlin, 
Germany: Springer.

Bennett, B. (2001). What is a forest? On the vagueness of certain geographic concepts. Topoi, 20, 189–201.
Bittner, T., Donnelly, M., & Smith, B. (2004). Endurants and perdurants in directly depicting ontologies. AI Communications, 

17(4), 247–258.

Box, G. E. (1976). Science and statistics. Journal of the American Statistical Association, 71(356), 791–799.
Brockmans, S., Haase, P., & Stuckenschmidt, H. (2006). Formalism-independent specification of ontology mappings: A 
metamodeling approach. In R. Meersman & Z. Tari (Eds.), On the move to meaningful internet systems 2006: CoopIS, 
DOA, GADA, and ODBASE. OTM 2006 (Lecture Notes in Computer Science, Vol. 4275, pp. 901–908). Berlin, Germany: 
Springer.

Brodaric, B., & Gahegan, M. (2001). Learning geoscience categories in situ: Implications for geographic knowledge rep-
resentation. In Proceedings of the Ninth ACM International Symposium on Advances in Geographic Information Systems, 
Atlanta, GA (pp. 130–135). New York, NY: ACM.

GUPTA And GAHEGAn    |  23

Brodaric,  B.,  &  Gahegan,  M.  (2007).  Experiments  to  examine  the  situated  nature  of  geoscientific  concepts.  Spatial 

Cognition & Computation, 7(1), 61–95.

Chazdon, R. L., Brancalion, P. H. S., Laestadius, L., Bennett-Curry, A., Buckingham, K., Kumar, C., … Wilson, S. J. (2016). 
When is a forest a forest? Forest concepts and definitions in the era of forest and landscape restoration. Ambio, 45(5), 
538–550.

Cimiano,  P.,  &  Völker,  J.  (2005).  Text2Onto:  A  framework  for  ontology  learning  and  data-driven  change  discovery.  In 
A. Montoyo, R. Muńoz, & E. Métais (Eds.), Natural language processing and information systems: NLDB 2005 (Lecture 
Notes in Computer Science, Vol. 3513, pp. 227–238). Berlin, Germany: Springer.

Comber, A., Fisher, P., & Wadsworth, R. (2004). Integrating land-cover data with different ontologies: Identifying change 

from inconsistency. International Journal of Geographical Information Science, 18(7), 691–708.

Congalton, R. G., & Green, K. (2008). Assessing the accuracy of remotely sensed data: Principles and practices. Boca Raton, 

FL: Lewis Publishers.

Davis, S. M., Landgrebe, D. A., Phillips, T. L., Swain, P. H., Hoffer, R. M., Lindenlaub, J. C., & Silva, L. F. (1978). Remote sens-

ing: The quantitative approach. New York, NY: McGraw-Hill.

Di, L., & Yue, P. (2011). Provenance  in  earth  science  cyberinfrastructure. Retrieved from https ://www.earth cube.org/

docum ent/2012/prove nance-earth-scien ce-cyber  infra struc ture-0

Djedidi, R., & Aufaure, M. A. (2010). Ontology evolution: State of the art and future directions. In F. Gargouri & W. Jaziri 

(Eds.), Ontology theory, management and design: Advanced tools and models. pp.179–207.Hershey, PA: IGI Global.

Eliot, T. S. (1969). The complete poems and plays of T. S. Eliot. London, UK: Faber & Faber.
Fanizzi,  N.,  d’Amato,  C.,  &  Esposito,  F.  (2008).  Conceptual  clustering  and  its  application  to  concept  drift  and  novelty 
detection. In S. Bechhofer, M. Hauswirth, J. Hoffmann, & M. Koubarakis (Eds.), The semantic web: Research and appli-
cations, ESWC 2008 (Lecture Notes in Computer Science, Vol. 5021, pp. 318–332). Berlin, Germany: Springer.

Flouris, G., Plexousakis, D., & Antoniou, G. (2006). Evolving ontology evolution. In J. Wiedermann, G. Tel, J. Pokorný, M. 
Bieliková, & J. Štuller (Eds.), OFSEM 2006: theory and practice of computer science, 32nd Conference on Current Trends 
in Theory and Practice of Computer Science, Merin, Czech Republic, Proceedings (pp. 14–29). Berlin, Germany: Springer.

Gahegan, M., & Pike, W. (2006). A situated knowledge representation of geographical information. Transactions in GIS, 

10(5), 727–749.

Gonçalves, R. S., Parsia, B., & Sattler, U. (2011). Analysing multiple versions of an ontology: A study of the NCI thesaurus. 

In Proceedings of the 24th International Workshop on Description Logics. Barcelona, Spain.

Graham, D. W. (2015). Heraclitus. Retrieved from http://plato.stanf ord.edu/archi ves/fall2 015/entri es/herac litus/ 
Hartung, M., Terwilliger, J., & Rahm, E. (2011). Recent advances in schema and ontology evolution. In Z. Bellahsene, A. 

Bonifati, & E. Rahm (Eds.), Schema matching and mapping (pp. 149–190). Berlin, Germany: Springer.

Hinsen, K. (2014). Computational science: Shifting the focus from tools to models. Retrieved from https ://f1000 resea rch.

Huerta-Cepas, J., Serra, F., & Bork, P. (2016). ETE 3: Reconstruction, analysis, and visualization of phylogenomic data. 

com/artic les/3-101/v2

Molecular Biology & Evolution, 33(6), 1635–1638.

Javed,  M.,  Abgaz,  Y.  M.,  &  Pahl,  C.  (2011).  Graph-based  discovery  of  ontology  change  patterns.  In  Proceedings  of  the 
International  Semantic  Web  Conference  Workshops:  Joint  Workshop  on  Knowledge  Evolution  and  Ontology  Dynamics. 
Bonn, Germany.

Kirsten,  T.,  Hartung,  M.,  Groß,  A.,  &  Rahm,  E.  (2009).  Efficient  management  of  biomedical  ontology  versions.  In  R. 
Meersman, P. Herrero, & T. Dillon (Eds.), On the move to meaningful internet systems: OTM 2009 Workshops (Lecture 
Notes in Computer Science, Vol. 5872, pp. 574–583). Berlin, Germany: Springer.

Klein,  M.,  &  Noy,  N.  F.  (2003).  A  component-based  framework  for  ontology  evolution.  In  Proceedings  of  the  IJCAI-03 

Workshop on Ontologies and Distributed Systems. Acapulco, Mexico.

Magolis, E., & Laurence, S. (2002). Concepts. In S. P. Stich & T. A. Warfield (Eds.), The Blackwell guide to philosophy of mind 

(pp. 190–213). Oxford, UK: Blackwell.

Medin, D. L., & Rips, L. J. (2005). Concepts and categories: Memory, meaning, and metaphysics. In K. J. Holyoak & R. G. 
Morrison (Eds.), The Cambridge handbook of thinking and reasoning (pp. 17–70). Cambridge, UK: Cambridge University 
Press.

Murphy, G. L. (2002). The big book of concepts. Cambridge, MA: MIT Press.
Noy, N. F., Chugh, A., Liu, W., & Musen, M. A. (2006). A framework for ontology evolution in collaborative environments. 

In Proceedings of the Fifth International Conference on the Semantic Web (pp. 544–558). Athens, GA.

Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., … Duchesnay, E. (2011). Scikit-learn: Machine 

learning in Python. Journal of Machine Learning Research, 12, 2825–2830.

Popper, K. (2002). The logic of scientific discovery. Hove, UK: Psychology Press.
Quinn, J. W. (2011). Band combinations. Retrieved from http://web.pdx.edu/~emch/ip1/bandc ombin ations.html
Real, R., & Vargas, J. M. (1996). The probabilistic basis of Jaccard's index of similarity. Systematic Biology, 45(3), 380–385.
Shapin, S. (1984). Pump and circumstance: Robert Boyle's literary technology. Social Studies of Science, 14(4), 481–520.

GUPTA And GAHEGAn24  |    

Kaufmann.

Learning.

signp roc.htm

Karlsruhe, Germany.

Shrager, J., & Langley, P. (1990). Computational models of scientific discovery and theory formation. San Mateo, CA: Morgan 

Smith, L. B., & Samuelson, L. K. (1997). Perceiving and remembering: Category stability, variability and development. In 
K. Lamberts & D. R. Shanks (Eds.), Knowledge, concepts and categories (pp. 161–195). Hove, UK: Psychology Press.
Sowa, J. F. (1999). Knowledge representation: Logical, philosophical, and computational foundations. Boston, MA: Cengage 

Sowa, J. F. (2002). Signs, processes, and language games: Foundations for ontology. Retrieved from http://jfsowa.com/pubs/

Stojanovic,  L.  (2004).  Methods  and  tools  for  ontology  evolution  (Unpublished  M.Sc.  thesis).  University  of  Karlsruhe, 

Storms, G., Mechelen, I. V., & Boeck, P. D. (1994). Structural analysis of the intension and extension of semantic concepts. 

European Journal of Cognitive Psychology, 6(1), 43–75.

Taylor, P. J. (1990). Editorial comment GKS. Political Geography Quarterly, 9(3), 211–212.
Tomas, R., & Lutz, M. (2013). Key pillars of data interoperability in earth sciences: INSPIRE and beyond. In Proceedings of 

the 15th EGU General Assembly Conference (p. 13767). Vienna, Austria.

Tsymbal,  A.  (2004).  The  problem  of  concept  drift:  Definitions  and  related  work  (Report  No.  TCD-CS-2004-15).  Dublin, 

Ireland: Computer Science Department, Trinity College.

Turney, P. (1995). Technical note: Bias and the quantification of stability. Journal of Machine Learning, 20(1–2), 23–33.
Waddington, C. H. (1977). Whitehead and modern science. In J. Cobb & G. David (Eds.), Mind in nature: The interface of 

science and philosophy (pp. 143–146). Washington, DC: University Press of America.

Wang, S., Padmanabhan, A., Myers, J. D., Tang, W., & Liu, Y. (2008). Towards provenance-aware geographic information 
systems. In Proceedings of the 16th ACM SIGSPATIAL International Conference on Advances in Geographic Information 
Systems, Irvine, CA. New York, NY: ACM.

Wang, S., Schlobach, S., & Klein, M. (2011). Concept drift and how to identify it. Journal of Web Semantics, 9, 247–265.
Weinberger, D. (2011). Too big to know. New York, NY: Basic Books.
Whitehead, A. N. (1929). Process and reality: An essay in cosmology. New York, NY: Free Press.
Whitehead, A. N. (1933). Adventures of ideas. Cambridge, UK: Cambridge University Press.
Würriehausen, F., Karmacharya, A., & Müller, H. (2014). Using ontologies to support land-use spatial data interoperability. 
In R. Murgante, S. Misra, A. Maria, A. C. Rocha, C. TorreJorge, G. Rocha, … O. Gervasiet (Eds.), Computational Science 
and Its Applications: ICCSA 2014 (Lecture Notes in Computer Science, Vol. 8580, pp. 453–468). Cham, Switzerland: 
Springer.

Zablith, F. (2009). Evolva: A comprehensive approach to ontology evolution. In Proceedings of the Sixth European Semantic 

Web Conference (pp. 944–948). Crete, Greece.

Zablith, F. (2011). Harvesting online ontologies for ontology evolution. Retrieved from http://oro.open.ac.uk/54231/ 
Zablith, F., Antoniou, G., d'Aquin, M., Flouris, G., Kondylakis, H., Motta, E., …Sabou, M. (2014). Ontology evolution: A 

process-centric survey. Knowledge Engineering Review, 30(1), 45–75.

Zablith,  F.,  d’Aquin,  M.,  Sabou,  M.,  &  Motta,  E.  (2010).  Using  ontological  contexts  to  assess  the  relevance  of  state-
ments in ontology evolution. In Proceedings of the International Conference on Knowledge Engineering and Knowledge 
Management. Valencia, Spain.

How to cite this article: Gupta P, Gahegan M. Categories are in flux, but their computational 

representations are fixed: That's a problem. Transactions in GIS. 2020;00:1–24. 

https ://doi.org/10.1111/tgis.12602 

GUPTA And GAHEGAn