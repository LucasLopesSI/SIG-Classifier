Geoinformatica (2014) 18:537–567
DOI 10.1007/s10707-013-0189-8

An assisting, constrained 3D navigation technique
for multiscale virtual 3D city models

Dieter Hildebrandt · Robert Timm

Received: 12 February 2013 / Revised: 14 July 2013 /
Accepted: 14 August 2013 / Published online: 7 September 2013
© Springer Science+Business Media New York 2013

Abstract Virtual 3D city models serve as integration platforms for complex geospa-
tial and georeferenced information and as medium for effective communication
of spatial information. In order to explore these information spaces, navigation
techniques for controlling the virtual camera are required to facilitate wayfinding and
movement. However, navigation is not a trivial task and many available navigation
techniques do not support users effectively and efficiently with their respective skills
and tasks. In this article, we present an assisting, constrained navigation technique
for multiscale virtual 3D city models that is based on three basic principles: users
point to navigate, users are lead by suggestions, and the exploitation of semantic,
multiscale, hierarchical structurings of city models. The technique particularly sup-
ports users with low navigation and virtual camera control skills but is also valuable
for experienced users. It supports exploration, search, inspection, and presentation
tasks, is easy to learn and use, supports orientation, is efficient, and yields effective
view properties. In particular, the technique is suitable for interactive kiosks and
mobile devices with a touch display and low computing resources and for use
in mobile situations where users only have restricted resources for operating the
application. We demonstrate the validity of the proposed navigation technique by
presenting an implementation and evaluation results. The implementation is based
on service-oriented architectures, standards, and image-based representations and
allows exploring massive virtual 3D city models particularly on mobile devices
with limited computing resources. Results of a user study comparing the proposed
navigation technique with standard techniques suggest that the proposed technique
provides the targeted properties, and that it is more advantageous to novice than to
expert users.

D. Hildebrandt (B) · R. Timm
Hasso-Plattner-Institut at University of Potsdam,
Prof.-Dr.-Helmert-Str. 2-3, 14482 Potsdam, Germany
e-mail: dieter.hildebrandt@hpi.uni-potsdam.de

R. Timm
e-mail: robert.timm@hpi.uni-potsdam.de

538

Geoinformatica (2014) 18:537–567

Keywords Virtual 3D city model · Multiscale modeling · View navigation ·
Virtual camera control · Mobile device · Distributed 3D geovisualization

1 Introduction

Virtual 3D city models (V3DCMs) serve as integration platforms for complex geospa-
tial and georeferenced information and as medium for effective communication of
spatial information. They can can be provided to users as parts of 3D geovirtual
environments (3DGeoVEs). In order to enable users to explore, analyze, and present
these information spaces and to enable users to conduct wayfinding, moving and
controlling the virtual camera within 3DGeoVEs, navigation techniques are required.
However, navigation in 3DGeoVEs is not a trivial task [17, 23, 48] and many
navigation techniques do not support users effectively and efficiently with their
respective skills and tasks in this specific type of virtual environment. Still the
most commonly provided techniques in 3DGeoVEs and 3D computer graphics
applications using 2-degree of freedom (DOF) devices include pan, zoom, orbit, look,
f ly, and goto [17, 23]. These standard techniques exhibit several shortcomings that
they share with many other techniques.

As a common issue, navigation techniques are not easy to learn and use. Nav-
igation in 3DGeoVEs is a learned skill. Learning can be error-prone and con-
fusing requiring significant effort even for experienced users [17, 23]. Controlling
simultaneously up to seven camera DOFs requires dexterity and is often found
problematic [13]. Navigation techniques can be difficult to understand, provide
insufficient information to the user, and may be ineffective in preventing users from
making errors [13, 17]. A core problem for users when navigating in 3DGeoVEs is
disorientation [8, 9, 17, 23]. It affects in particular inexperienced but also experienced
users [8, 17] and is a problem in particular in multiscale environments [30]. Moreover,
navigation techniques can permit generating views of 3DGeoVEs with ineffective
view properties [13] that can arise in the form of unsteady and discontinuous camera
motion, awkward viewing angles presenting the model in poor light or missing
important features, unwanted views, “clunky or visually jarring” views (e.g., due to
frequent mouse clutching), distracting transitions, a low or inconsistent level of visual
and interactive quality [8–10, 13, 24, 38]. Furthermore, we can observe suboptimal
ef f iciency of navigation techniques regarding human (e.g., time, physical efforts)
and computing resources. Completing a single navigation task often requires a col-
lection of navigation techniques resulting in frequent control switches, in particular
when using standard navigation techniques [17, 23]. Each control switch divides
the action into separate chunks; leading to a higher separation in subtasks and a
higher cognitive load, higher time consumption, inefficient movement trajectories,
and ineffective view properties [9, 10, 23, 38]. Many navigation techniques map
input device DOFs to camera DOFs in real-time requiring real-time 3D rendering,
although this is not crucial for many applications using V3DCMs [16, 23]. In this
case, alternative, selective navigation techniques can be more computer resource
efficient [16].

To address these issues, in this article, we present an assisting, constrained
navigation technique for multiscale V3DCMs that is based on three basic principles:
users point to navigate, users are lead by suggestions, and the exploitation of semantic,

Geoinformatica (2014) 18:537–567

539

multiscale, hierarchical structurings of city models (Fig. 1). The technique is intended
to support particularly users with low navigation and virtual camera control skills but
also experienced users. It is intended to support exploration, search, inspection, and
presentation tasks, to be easy to learn and use, support orientation, to be efficient,
and yield effective view properties. In particular, the technique is intended to be
suitable for interactive kiosks and mobile devices with a touch display and low
computing resources and for use in mobile situations where users only have restricted
resources for operating the application. We demonstrate the validity of the proposed
navigation technique by presenting an implementation and an evaluation including a
user study. The implementation is based on service-oriented architectures, standards,
and image-based representations (SSI) [26] and allows exploring massive V3DCMs
particularly on mobile devices with limited computing resources.

This article first reviews related work (Section 2) and then provides the following
contributions: a novel navigation technique for multiscale V3DCMs based on seman-
tic navigation hierarchies, suggestions, and pointing (Section 3), an implementation
of the navigation technique based on service-oriented architectures, standards, and
image-based representations and its integration into an existing 3D geovisualization
system (3DGeoVS) (Section 4), an image-based representation of a navigation
hierarchy (Section 4.1), an optimizing preprocessing pipeline and algorithms for the
automated, generic generation of a navigation hierarchy from a V3DCM including

Fig. 1 Screenshot of the 3D navigation technique displaying a 2D overlay on top of an independently
created 3D view of a V3DCM. The overlay provides movement and view suggestions, navigation aids
(overview map, compass, node and path labels) and divides the view into a focus and a context region
with one node being selected

540

Geoinformatica (2014) 18:537–567

an implementation (Section 4.2), an interactive runtime processing pipeline and
algorithms using image-based representations provided by services as the sole repre-
sentation of a 3DGeoVE (Section 4.3), an algorithm for the view-dependent, level-
of-scale-dependent selection of nodes from an image-based navigation hierarchy
(Section 4.3), a view overlay visualization technique structuring an independently
generated view (Section 4.3), and an evaluation of effectiveness and efficiency
aspects including a user study (Section 5). Section 6 concludes this article.

2 Related work

Constrained navigation can be defined as “the restriction of viewpoint generation
and rendering parameters to goal-driven subclasses whose access is specified by
the application designer” [25]. It is a general principle that can be found in a
multitude of approaches and is applied successfully to improve navigation [1, 8, 29].
Previous approaches have limited the camera’s position to viewpoints [47], 1D tra-
jectories [10, 16, 29, 38], 1D trajectories connected through graphs [1, 41, 46], 2D sur-
faces [9, 25, 32], and 3D volumes [5]. Moreover, approaches imposed constraints on
the camera’s velocity [38, 51] and orientation [41]. User control can be completely
absent [14] or provided locally with varying degrees [9, 25]. Constraints can essen-
tially be authored manually [1, 9, 10] or computed automatically [32, 46]. This work
constrains the camera to automatically computed 1D trajectories, velocities, and
orientations connected through a graph with user control restricted to decisions at
nodes. In contrast to previous work, the constraints are automatically computed from
a semantic V3DCM with the intent to support specifically navigation in multiscale
V3DCMs from the city level down to buildings without allowing explicitly to travel
street networks.

Automatic camera control refers to the automated computation of static and
dynamic camera parameters according to goal specifications typically without any di-
rect user input [13]. Goals define the desired view properties and can be specified on
(a) the camera parameters directly such as focal length, vantage angles, distances to
objects [24], and collision avoidance [5], (b) camera path properties such as collision
avoidance, path and frame coherence [12, 24], and cinematographic properties [12],
or (c) properties relating to the projected image content such as absolute [24] or
relative locations of objects [42], framing of objects [4, 42], occlusion [4, 24], and
properties relating to aesthetics and cognition [21] with canonical views [6] as one
relevant concept. In this work, we apply established view properties such as vantage
angles, collision avoidance, path and frame coherence, framing of objects, occlusion,
and cognitive properties. However, we select, instance, and apply a specific subset of
properties for the specific case of multiscale, hierarchical V3DCMs.

Pointing in this work refers to users indicating a single 2D position in screen space
on the display by using a pointing input device. It is employed for navigation and
camera motion in different ways such as to specify a direction, destination POI or
path either discretely in time for selective navigation [16] in the current view [39],
on a map or World in Miniature [46], using widgets [23], or continuously in the
current view to specify real-time [38, 40] or selective navigation, e.g., by sketching
paths or gestures [16]. It is employed for specifying camera parameters [23] and

Geoinformatica (2014) 18:537–567

541

to trigger complex actions [10, 17]. Pointing can be used as the single, unified
UI method [10] as can other simple methods such as dragging [9]. For specifying
directions or destinations, other methods such as framing [39], gazing [40], or directly
controlling camera parameters [52] can be used as well. In addition, continuous
direct [45] or indirect [50] object manipulation techniques using one and more points
including rotate-scale-translate interactions [45] can be used for indirect camera
control. This work uses single-point pointing as the single, unified UI method to
specify camera motion discretely. It differs from previous work in that it at the
same time allows navigating in large-scale, multiscale V3DCMs, is used to specify
directions, destinations, camera parameters, and complex actions, and does not
require any authoring.

Suggestions in this work are referred to as navigation actions offered to the user
for explicit, discrete selection in the presented 3D view (see Section 3.2). Sugges-
tions can be represented as 3D models [44], 3D widgets [17], 2D thumbnails [10],
2D buttons [1], or text [28]. The actions triggered typically include authored camera
animations [10], computed camera animations between authored [1, 44] or com-
puted control points [14, 17], “discontinuous” camera animation [44], and telepor-
tation [47]. Suggestions triggering a camera to move from a current to a destination
point in 3D space can also be called 3D hyperlinks [28, 47] additionally differing
in attributes such as duration, preview information, and visual continuity [47]. In
this work, suggestions are represented as highlighted parts of a 3D model and
2D buttons triggering camera animations that are continuous, bidirectional, and com-
puted completely automatically, have distance-dependent durations and minimized
rotational offsets, have strong preview information, visual continuity, and directional
cues, are integrated into a continuous Euclidean 3D space, and connect spatially
adjacent and non-adjacent locations. Differing from previous work, we use such
suggestions as the single, unified method to facilitate camera motion modeled with
viewing graphs [19], provide suggestions to specifically support motion in large-scale,
multiscale V3DCMs, compute and offer suggestions automatically, and provide a
unique representation based on outlining and shading regions of the V3DCM in
the view.

Semantic, hierarchical, multiscale modeling of geodata can be utilized to support
navigation. The open standard CityGML [22] facilitates modeling V3DCMs with se-
mantics and restricted hierarchical, multiscale characteristics limited to constituents
and LOD representations of individual buildings. Döllner et al. [16] exploit V3DCM
single, small-scale semantics to interpret navigation commands. Hierarchies of vi-
sual representations of V3DCMs can be automatically generated representing the
V3DCM with a range of levels of detail (LOD) and/or levels of abstraction (LOA)
based on geometry and semantics (e.g., buildings, street networks) with the aim
to improve perception (LOA) [11, 20] or rendering efficiency (LOD) [11]. Hier-
archical structurings of navigable information spaces can be modeled with viewing
graphs [19], and are employed to structure and guide 3D navigation [3, 14, 33, 44, 48].
Navigation in multiscale 3D models is found to cause disorientation potentially [30],
is supported by authored hierarchies to model multiscale environments in the
geospatial domain [44] and anatomy [3, 33], and can be improved by offering
adaptive or automatic control over movement velocity [38, 39, 51, 53]. This work
uses an automatically computed semantic, hierarchical, multiscale structuring of

542

Geoinformatica (2014) 18:537–567

a V3DCM modeled using viewing graphs to support navigation. In contrast to
previous work, the generated hierarchy structures the 2D city area, employs semantic
features beyond buildings and streets such as administrative borders and geological
features, and is generated and represented based on images. In addition, we use a
semantic, multiscale hierarchy of a V3DCM at the same time to structure, guide,
and constrain the navigation process, to automatically control movement velocity
and view properties, and to support navigation with a specific visualization and
navigation aids.

Image-based representations can be used to implement various functionalities of
navigation techniques such as occlusion [24], collision [15, 39], computing view-
points with specific properties [49], or camera velocity control [39, 51]. In these
approaches, images are typically generated by a GPU and encode information such
as geometry [15, 39], object identif ication (OID) [24], or geographic attributes [54].
In contrast to previous work, we represent a hierarchical model as images and use
this to support navigation: at preprocessing time as a stack of OID images and at
runtime as view-dependent OID images encoding hierarchy in a unique manner. The
navigation technique uses depth images as the sole representation of model geometry
for computing camera animations, automatic framing (together with OID images),
collision, and occlusion.

3 An assisting, constrained 3D navigation technique

In this Section, we describe the concept of the proposed navigation technique and its
rationale. We introduce the technique with an overview, example, and characteriza-
tion. Then, we present the concept in detail by explaining the three underlying prin-
ciples, semantic, hierarchical, multiscale model (Section 3.1), navigation suggestions
(Section 3.2), and point to navigate (Section 3.3), along with supplemental techniques
(Section 3.4).

The proposed navigation technique (Fig. 1) allows the user to navigate in mul-
tiscale V3DCMs. For each view, it adaptively presents to the user a finite set of
visual representations of navigation suggestions that represent specific potential user
navigation intentions. The suggestions are derived from a semantic, hierarchical,
multiscale structuring of a V3DCM. The user selects one of the presented suggestions
by pointing at its representation. This triggers the technique to attempt to satisfy the
intention represented by the suggestion by moving the camera continuously over
time through 3D space to an appropriate target position. Once the target position is
reached, this process starts again from the beginning. Figure 2 depicts an example
navigation usage sequence descending from the highest to the lowest level of scale of
a V3DCM.

The navigation technique can be characterized as selective [16], constrained
[16, 25, 29], automatic [13], and assisting. In contrast to real-time navigation where
the user controls the camera continuously over time using an input device, the user
selectively, discretely issues navigation commands by pointing at visual representa-
tions of suggestions triggering camera movement to appropriate target positions. By
applying constraints, we limit the camera movement to trajectories and orientations
along these trajectories that are automatically computed to meet certain goals. We
limit the users DOFs and, thus, where users can go with the intent to make it easier

Geoinformatica (2014) 18:537–567

543

Fig. 2 Example navigation usage sequence descending from the highest to the lowest level of scale
of a V3DCM (left to right, top to bottom)

for them to get where they want to go [29]. The system assists users by automatically,
adaptively presenting navigation suggestions.

3.1 Semantic, hierarchical, multiscale model

The navigation technique utilizes a semantic, hierarchical, multiscale structuring of
a V3DCM in order to support a user in navigating in a multiscale V3DCM. The
structuring hierarchically subdivides a V3DCM by subdividing its 2D ground area
guided primarily by semantics (e.g., successively subdividing a city area into districts,
quarters, etc., Fig. 3). The navigation technique uses such a hierarchy as a skeleton
structure to guide the user’s navigation process and to constrain camera movement.
For V3DCMs, various different types of hierarchies exist or can be conceived. Each is
assumed suitable only for specific application domains and user tasks. For instance,
for a tourism application, the V3DCM could be subdivided into districts, quarters,
clusters of points of interest, and points of interest.

For a proof of concept case study, we use a type of hierarchy that is aimed
at application domains such as city planning or city marketing. We are concerned
with hierarchies that can be represented as rooted, directed, k-ary trees. Each node
holds geometric (e.g., occupied 2D area), semantic (e.g., representing a district),
and scale information (e.g., specifying scale of phenomena represented by node).
Node geometries are partitions of their children’s geometries and are disjoint from
each other on each level. We refer to each level i of the tree as level of scale i
(LOSi). The proposed hierarchy has seven LOS: city (root node), districts, quarters,
boroughs, neighborhoods, blocks, and buildings. In Section 4.2, we demonstrate how
to generate this type of hierarchy automatically from a given V3DCM. Figure 3
depicts an automatically generated, specific instance of this type of hierarchy for the
city of Berlin.

Using a semantic, hierarchical, multiscale structuring to guide navigation in
multiscale V3DCMs supports meeting the goals stated in Section 1 in several ways.

544

Geoinformatica (2014) 18:537–567

Fig. 3 Image-based representation of a semantic, hierarchical, multiscale structuring of the 2D area
of the city of Berlin. The structuring consists of seven LOSs (left to right, top to bottom): city (not
depicted), districts, quarters, boroughs, neighborhoods, blocks, and buildings. Each image represents
a LOS where each node is represented by a set of pixels identified by an arbitrary, unique color

First, research suggests that humans naturally organize their cognitive map into a
semantic hierarchy [27]. Hence, using an explicit semantic hierarchy for visualization
and navigation can help the user in correlating the presented V3DCM with his
already existing cognitive map and in further developing his cognitive map. Second,
carving up the V3DCM efficiently with large-scale semantics (e.g., dividing the city
area into a set of named districts, etc.) yields a structure that users can read to find
“good” paths to their desired target and, thus, supports wayfinding [19]. For this, a
semantic structuring is assumed more effective than a structuring that is primarily
guided by geometric criteria (e.g., quadtrees) [19, 44]. Third, a semantic, hierarchical
structure scales well to large, multiscale environments and allows a user to traverse
efficiently large models [19, 44]. A V3DCM can be represented with only seven LOSs
(Section 4.2). The paths in the hierarchy between any two nodes tend to be short (e.g.,
for a balanced tree the maximum path length is logarithmic with the count of nodes).
Fourth, we can use an explicit, discrete structuring of the V3DCM for visualization
to structure explicitly and discretely the views of the V3DCM presented to the user.
Such visualization has the potential to ease cognitive acquisition and processing. In
addition, we can utilize the nodes of the structure for selective navigation [16] as basis
for stationary camera positions. Finally, geodata often contains emergent behavior at
different scales [37]. A semantic, multiscale structuring can make the contained scale-
dependent structures and information explicit and exploitable for visualization and
navigation.

3.2 Navigation suggestions

A navigation suggestion represents an executable navigation action that is offered by
the navigation technique to the user for explicit, discrete selection in the presented

Geoinformatica (2014) 18:537–567

545

3D view. It realizes a specific potential user navigation intention for a specific
stationary view. The navigation technique presents to the user for each stationary
view a finite set of visual representations of automatically, adaptively computed
navigation suggestions (Fig. 1). These are represented as 2D overlay elements
displayed on top of a 2D projection of a V3DCM. The user can select a navigation
suggestion by pointing at its visual representation triggering camera motion. We
distinguish two types of navigation suggestions:

Movement suggestions (MS) represent the intention of a user to move the camera
towards a part of the model as indicated by the visual representation of the MS.
They are primarily intended to support search and exploration tasks. A MS is
visually represented either by an outlined and shaded region of the displayed
2D projection of the V3DCM decorated with a textual label if available or by a
GUI button.

View suggestions (VS) represent the intention of a user to acquire a different static
or dynamic view on a currently focused region of the V3DCM. They are primarily
intended to support object inspection tasks and are visually represented by GUI
buttons.

We can model the navigable space that can be visited by using navigation
suggestions and the provided views with their view properties with the concept of
viewing graphs [19]. Each node holds information on geometry, semantics, scale,
and camera 3D position and orientation. Each edge holds information on a camera
position and orientation animation over time from the source to the target node.
Thus, the viewing graph represents navigation suggestions as outgoing edges and
view properties indirectly as camera parameters held in nodes and edges.

The basic navigation process using viewing graphs can be described as view
traversal [19]. The camera is located at any given time either stationary at one node
or is moving controlled by the navigation technique on the edge from a source to a
target node. At any current node, the technique presents to the user a specific view
of the node with a set of navigation suggestions corresponding to outgoing edges. By
selecting a suggestion, the user triggers the camera to move automatically along the
edge to the target node.

We construct a viewing graph from a given semantic, hierarchical, multiscale
structuring (Section 3.1) by first adopting and then modifying its tree. Each node’s
camera parameters are set up to look from the south at the node geometry’s
center (automatic north heading), setting the pitch as a function of the node’s LOS
(from −90◦ at LOS0 for 2D map-like views to −35◦ at LOS6 for oblique views)
(automatic pitch) and framing the node’s geometry’s 3D axis-aligned bounding box
(AABB) (automatic framing). The automatic camera animations on the edges are
set up with the goals to provide steady and continuous camera motion with high
orientation values [8], the adequate level of visual information complexity required
for supporting orientation, and rapid, controlled camera movement [38].

The navigation technique offers four types of VSs that are each implemented by
adding nodes and/or edges to the viewing graph: yaw (view on focused region from
indicated directions, e.g., 120◦ differences, “Look” button), pitch (views at pitches
−90◦, −35◦, 0◦, and pedestrian view, “Top”, “High”, “Mid”, and “Ped” buttons,
Fig. 4), good view (automatic canonical views on focused region intended to be
preferred by most users for a wide range of tasks [6]), and turntable (automatic

546

Geoinformatica (2014) 18:537–567

p

p

n

v00

n=
v01

v02

v03

v10

v11

v12

v13

v23

v22

v21

v20

c0

c1

c0

c1

Fig. 4 Augmenting the viewing graph at node n with VSs of type yaw and pitch represented by
the nodes vij =: nvij and connected edges (indicated by dashed lines). Each node vij represents a
combination of yaw 0 ≤ i ≤ nyaw and pitch 0 ≤ j ≤ n pitch with nyaw = 2, n pitch = 3 in this example

360◦ camera animation focused at region center with constant pitch and distance for
overviews).

As additional MSs, we add to each node n additional edges to nodes visible from
its view (Fig. 5) based on a projected screen space criterion (Section 4.3). As effects,
the view is segmented into regions that tend to have equal sizes not falling below a
threshold (Fig. 1), regions that are more distant are represented by nodes on lower
LOSs (Fig. 6), visual clutter is reduced, the count and sizes of MSs in a view are
controlled, and paths in the viewing graph are shortened. The intuition is that when
a user points at a more distant location, he wants to move to a region around the
location pointed at with a size that he can easily oversee from his current view instead
of moving to that exact location. When the user selects a MS at node n to an adjacent

0

2

1

3

0

1

0

1

Fig. 5 Augmenting the viewing graph at node n with additional edges (indicated by dashed lines) to
hierarchy nodes as additional MSs

Geoinformatica (2014) 18:537–567

547

Fig. 6 View-dependent level of scale selection. Nodes on lower LOSs represent more distant regions
illustrated by color-coding nodes according to their LOS (from LOS6 to LOS1: original, magenta,
cyan, blue, green, red)

node m, a target node is automatically chosen that has the smallest yaw and pitch
difference to n, if n and m are spatially close. Otherwise, m is selected. Finally, the
user can access the history of previously visited nodes (“Back” button) and move to
a node’s parent region (“Up” button).

Navigation suggestions as presented support meeting the goals stated in Section 1
as follows. First, users can easily understand suggestions as a means to guide them.
Suggestions are solely and consistently represented in two different, straightforward
ways (outlining model regions and buttons) and selected in the same way always
triggering camera movement. Suggestions can ease decision-making, one of the
essential components of navigation. The navigation technique always presents to the
user visually a clear, discrete, limited set of available next actions. In particular, views
that cannot be represented by directly outlining a region in the current view (e.g.,
same region from different direction), are made explicit and directly selectable
as buttons without requiring complex manual steering. Structuring the views into
semantic regions with edges can support perception, cognitive map building and
wayfinding: The structures are comparable to Lynch’s [35] districts, nodes, paths,
and edges, delineated by edges as important elements of views [36], and can act as
morphological elements of the model that are found to largely define navigation
behavior [2]. Second, user orientation is improved by providing error prevention
through presenting for each view only a set of defined, assumable useful suggestions
derived from the current view, error recovery (“Back” button), and continuous
camera motion exclusively without any potentially disorienting teleportation [7].
Third, since the navigation technique takes control over possible camera positions,
it can guarantee a certain level of quality and effectiveness for the view properties
and never displays awkward views. Fourth, suggestions are efficient allowing rapid,

548

Geoinformatica (2014) 18:537–567

controlled movement over long distances and selective navigation without requiring
real-time rendering. Finally, the presented suggestions support exploration, search,
inspection, and presentation tasks as well as wayfinding and motion. Furthermore,
the suggestions conform to reported, observed user preferences: Users prefer man-
ually tilted views at 3D objects only when they are more experienced, consistently
return to overviews, return to north orientations after a short period of time, divide
travel into motion and stationary phases for processing visual cues, and prefer to
delegate navigation work to the system [2].

3.3 Point to navigate

The navigation technique provides pointing as the single, unified user input method.
Users point at suggestions provided by the technique to communicate their intentions
and, thus, to issue navigation commands. Pointing refers to users indicating a single
2D position in screen space on the display by using a pointing input device (e.g.,
click with mouse) or a touch input device (e.g., one single finger tap). Pointing
inputs are discrete in time and space. The mapping of input device parameters to
camera parameters is not direct but instead decoupled and mediated by suggestions.
Pointing at a suggestion represented by a button immediately triggers the action it
represents. Pointing at an unselected, outlined region selects this region indicated
by a blue coloring, pointing at a selected region triggers its action. In addition, the
navigation technique provides automatic view magnif ication when the user points at
or near a suggestion with a small screen space projection area a (i.e., a < φmin, with
φmin = 0.85 cm2 when using touch [43]).

Pointing supports meeting the goals stated in Section 1 as follows. First, pointing
as an input method for navigation is easy to learn and use [10, 23, 38]. Users
understand the concept of pointing to navigate very quickly [10, 23]. A single
suggestion replaces and eliminates potentially complex motion and steering tasks.
The navigation technique automatically, intelligently, and transparently performs
and combines standard techniques (e.g., pan, zoom, and orbit to establish a view of
an object) to control the camera DOFs. Thus, the user is relieved from the burden to
control several techniques, switches between them and arbitrary navigation directly
but instead controls navigation on a higher, more abstract level. Second, replacing
direct camera control by higher-level suggestions transfers more control from the
user to the navigation technique and, thus, enables it to improve user orientation and
view property effectiveness. Moreover, pointing to navigate can be more efficient
than standard techniques since for the same motion task it requires a smaller number
of input events and less physical effort. Pointing is particularly suited for users with
low camera control skills but is also useful for experienced users [23]. It conforms to
the strong user expectation of being able to point at objects of the scene to trigger
object-related actions [9, 10]. Finally, pointing only requires a generic pointing input
device and no specific hardware.

3.4 Supplemental techniques

Spatial information is provided by an auto-zooming 2D overview map with a fixed
true-north orientation aligned with the camera’s automatic north heading (Fig. 1)
and with an arrow indicating the camera’s position and direction. Hierarchical

Geoinformatica (2014) 18:537–567

549

information is provided by a textual path label (indicating the hierarchy path from
the root to the current node) and adaptive, hierarchical styling of the 2D overview
map showing outlines of at most the three hierarchy LOSs starting at the current
node n and highlighting via FNC nodes on the path from n to the root.

4 System design and implementation

In this Section, we present the system design and implementation of the proposed
navigation technique. We provide a system architecture overview, introduce hierar-
chical object IDs as representation (Section 4.1), explain the automatic generation
of a semantic, multiscale hierarchy for a V3DCM (Section 4.2), and the interactive,
runtime processing pipeline (Section 4.3).

Figure 7 depicts an overview of a 3DGeoVS implementing the navigation concept
introduced in Section 3. We use as basis an already existing 3D geovisualization
system based on service-oriented architectures, standards, and image-based repre-
sentations [26]. This system allows the interactive visualization of massive V3DCMs
in particular on mobile devices with limited computing resources. We extend the
system with support for the introduced navigation concept as follows. The Geodata
Mapping service transforms V3DCM representations from the CityGML Storage
service and the OSM Storage service into an image-based hierarchy and with this
augments data in the Proprietary Model Storage. From the therein contained
data, the 3D Rendering service generates 2D images of projected views of the
V3DCM encoding various information such as depth or OID as requested. The 3D
Visualization Client implements a 3D Client Rendering Technique
and the 3D Navigation Technique that implements the proposed naviga-
tion technique on the client-side. The 3D Navigation Technique requests

Interaction Layer

User

3D Visualization Client

3D Navigation Technique

3D Client Rendering Technique

Process Layer

Functionality
Layer

Geodata Mapping
(WPS)

R

Data Layer

R

R

R

R

3D Rendering
(WVS-I)

CityGML
Storage
(WFS)

OSM
Storage

Thematic
Data Storage
(WFS)

Proprietary Model
Storage (File System)

Geometry Textures

Fig. 7 Architecture overview of a service-oriented 3DGeoVS system implementing the proposed
navigation technique. For this, an existing system is extended with two components (green)

550

Geoinformatica (2014) 18:537–567

depth (encoding geometry) and OID (encoding hierarchy) images from the 3D
Rendering service and additional hierarchy related data from the Thematic
Data Storage service.

4.1 Hierarchical Object Identification (HOID)

The hierarchy is represented with hierarchical object identif ications (HOIDs) and
HOID images. An object identification (OID) is an integer value uniquely identi-
fying an “object” or feature respectively within a 3DGeoVE. Let T = (N, E) be a
rooted, directed, k-ary tree with root node r ∈ N. Further, let lid : N\{r} → N\{0}
map each node to an integer obtained by numbering for each tree node its child
nodes locally with integers continuously starting from 1 with +1 increments, and
b its : N → N, v (cid:6)→ (cid:7)log2
(v)(cid:8) + 1 the minimum number of bits required to represent
i−1
j=1 b its(lid(n j))
v. Then, we define hoid : N\{r} → N\{0}, n (cid:6)→ lid(n1) +
with Pn := (n0 = r, n1, . . . , nm = n) and m ≥ 1, ∀i, j, 0 ≤ i < j ≤ m : (ni, n j) ∈ E be-
ing a path from r to n. Thus, hoid maps each node n (cid:12)= r to an (OID) whose value
encodes node n and the path from r to n. A HOID is an (OID) calculated with
function hoid. Though we limit HOIDs to encoding paths in rooted trees, we refer
to this representation as hierarchical to stress this aspect. Frisken and Perry [18]
proposes a similar encoding for quadtrees and octrees. Figure 8 depicts the allocation
of bits in a HOID (32-bit integer) to the six levels of a specific hierarchy and a
mapping of this integer to a RGBA color with the same size for use in images. Given
a hierarchy T = (N, E), a HOID image is a georeferenced 2D image of a projected
view of a 3DGeoVE where each pixel encodes the HOID value of a node n ∈ N
whose geometry is projected to this pixel (Fig. 12).

i=2 lid(ni) ∗ 2

(cid:2)

(cid:2)

m

4.2 Navigation hierarchy generation

In this Section, we describe the requirements for and design of a navigation hier-
archy for a specific application domain and the implementation and service-based
provisioning of its generation.

Case study As a case study, we create a navigation hierarchy for the domain of city
planning and city marketing (Section 3.1). The hierarchy must allow traversing the
complete city area from the city level down to individual buildings in a multiscale
manner. We generate a hierarchy for the city of Berlin covering 892 km2. Data from
two different data sets is used, since neither contains complete sets of all required
features: a CityGML data set [31] for the approximately 550,000 buildings and
OpenStreetMap (OSM) for the remaining features. We use dimensions of 16,3842

Channel

LOS

Byte

Bit

Building,
LOS6, 6 bit
Red
3

Block,
LOS5, 6 bit

Neighborhood,
LOS4, 5 bit

Quarter, 
LOS2, 5 bit

District,
LOS1, 4 bit

Borough,
LOS3, 5 bit
Blue
1

Alpha
0

Green
2

31 30 29 28 27 26 25 24 23 22 21 20 19 18 17 16 15 14 13 12 11 10 9 8 7 6 5 4 3 2 1 0

Fig. 8 Hierarchical object IDs (HOID). Allocation of bits to hierarchy levels in the case study and
mapping to RGBA color values

Geoinformatica (2014) 18:537–567

551

pixel for representing the hierarchy and individual LOS as images with sufficient
resolution.

Requirements We identify requirements for the hierarchy that influence the
effectiveness and efficiency of the navigation technique it is used in. When the
camera is stationary at an inner node framing its geometry, all its children are
visible and selectable in the view as MSs, covering on average 23 % of the screen
space (Section 5.2). We suggest the children count to be in the range of αmin = 4
and αmax = 40 with a mean in the range of 14 ± 2 (αmean = 14, αmeandelta = 2) for
display sizes of minimum 7” diagonal and touch input. A minimum of 4 guarantees
a nontrivial number of choices at each node and a sufficiently small number of steps
from the root to a leaf and, thus, sufficient “movement speed”. A maximum of
40 yields average suggestion display areas on targeted displays that are above the
minimum size that can be efficiently selected with touch input (0.85 cm2, [43]) and
is assumed to yield still clear enough sets of choices. The targeted means of 14 ± 2
result from taking what the 7 ± 2 rule suggests [34] and doubling it in an attempt to
follow the recommendation to prefer broad-shallow menu tree structures [34]. To
allow for efficient touch-based selection, nodes offered for selection should have a
minimum display size of φmin = 0.85 cm2 [43] assuming approximately square node
display projections. The hierarchy must be build from existing geometric properties
of the V3DCM that are important in the targeted application domain and support
user orientation. Hierarchy modifications, e.g., for enforcing child node properties as
presented above, should preserve the original structure as much as possible.

Hierarchy design For the case study, a hierarchy with seven LOS can appropriately
represent an approximately balanced tree with 550,000 building leaf nodes and
αmean = 14. For the seven LOS, we select features as follows and, thus, create
structures corresponding to Lynch’s [35] districts, nodes, paths, and edges: city
(administrative boundaries), districts (ditto), quarters (ditto), boroughs (major
streets and water bodies), neighborhoods (regular streets and railways), blocks
(minor streets and ways), and buildings (building footprints). Each LOS forms a
subdivision of its next higher LOS, if available, by combining its selected features
with the features from its next higher LOS. Every area enclosed by outlines from the
selected features represents a node of the respective LOS.

Automated generation: processing and algorithm The Geodata Mapping service
generates a navigation hierarchy from the V3DCM data and transforms it into
a representation optimized for real-time 3D rendering. The service implements a
generic, automated preprocessing pipeline as depicted in Fig. 9. The service provides
an interface based on the Web Processing Service (WPS) standard. It takes as
input V3DCM data from the CityGML Storage and OSM Storage services,
V3DCM triangle mesh data from the Proprietary Model Storage, and a set of
configuration parameters (specifying data sources, LOS count, LOS feature selection
rules, thresholds, etc.). The output is a set of multi-resolution terrain texture tiles and
the input V3DCM triangle mesh augmented with vertex attributes.

The OSM LOS Visualization processing stage generates an Outline
Image for each LOS from level 1 to 5 by filtering, mapping, rendering OSM
City Model Data. Each generated Outline Image is a 2D image that con-
tains the rasterized, top-down orthographic projections of all node geometries of

552

Geoinformatica (2014) 18:537–567

Outline Images

LOS Images

ID 
Assignment,
Artifacts 
Removal

Tree 
Extraction

Outline Image

Tree

OSM City 
Model Data

OSM
LOS 
Visualization

CityGML 
Data

CityGML
LOS 
Visualization

Configuration
Parameters

LOS Images

HOID Image

Optimization

Merge

HOID 
Terrain 
Texture 
Tiles

Triangle 
Mesh
with HOID

Terrain
Texture
Generation

Triangle 
Mesh
Vertex
Assignment

Triangle 
Mesh

gniredneR,gnippaM,gniretliF

gniretliF

Mapping

Fig. 9 Service-based, generic, automated preprocessing pipeline generating a navigation hierarchy
from the V3DCM data and transforming it into a representation optimized for real-time 3D render-
ing. Italic element names denote external input/output

a specific LOS. Similarly, the CityGML LOS Visualization stage generates an
Outline Image for LOS6 containing buildings from CityGML Data. The ID
Assignment, Artifacts Removal stage generates a LOS Image from each
Outline Image by filling each outline area with a color representing an OID,
removing outlines, and removing rasterization artifacts and very small features
(Fig. 10).

The properties of the hierarchy constructed up to this point depend primarily on
the used data set and LOS feature selection rules. As will be evident (Section 5.2), the
stated requirements (see above) are not met automatically. Therefore, we propose
a greedy optimization algorithm that carefully modifies the hierarchy in order to
meet better the stated requirements while preserving its essential structure. As
preparation, the Tree Extraction stage extracts an explicit, pointer-based tree
data structure from the LOS images to allow for a more convenient and efficient
implementation of the optimization algorithm. The Optimization stage takes
as input the original LOS images and tree data structure, outputs optimized LOS
images, and implements the algorithm presented in Listing 1 (procedure Optimize).
This algorithm modifies the tree only by splitting nodes. Splitting is more con-
servative than joining in the sense that it preserves all original geometry boundaries
while only introducing new ones. Splitting a node n with parent p into c0 and c1 as
described generally results in increasing p’s children count by one, on average an
even distribution of the area and children of n among c0 and c1 (halving the children

(a)

(b)

Fig. 10 LOS image artifacts removal. For each LOS image, unintended a small areas (yellow) and
b slightly larger, oblong areas (yellow, middle), e.g., created by street lane outlines (left), are removed
by growing their proximity (right)

Geoinformatica (2014) 18:537–567

553

Listing 1 Pseudo-code of the greedy hierarchy optimization algorithm

count), decreasing the tree’s average children count, and aiming for OBB side length
aspect ratios of one for c0’s and c1’s geometries. In this case study, the algorithm is
configured to not allow splitting nodes on LOS1 and LOS2 (administrative entities)
and LOS6 (buildings) since splitting a node on these LOSs would replace an entity
directly defined in the input data set with clearly defined semantics (e.g., a certain
district) by new entities with unclear, probably undefined semantics. In contrast, we
allow splitting nodes on the other LOSs since they do not directly represent input
entities but instead are defined by the outlines of such entities (e.g., streets). To
allow for avoiding and limiting the number of child node splits while creating natural
split border geometries aligned with the children geometries, we “fit” the two node
geometries resulting from a split to their respective children’s geometries instead
of splitting the children, if possible (Fig. 11). The algorithm terminates eventually
when no more splits can be performed because each node is either too small for
splitting, is on a LOS that prohibits splitting, or has already been considered for
splitting. To allow terminating the algorithm earlier and faster with an approximation
of the end result, the algorithm can be extended to rate the importance of all
known pending splits, process them according to importance, and terminate when
the highest importance falls below a user-defined threshold. The algorithm has a
complexity of O(n) with n being the tree’s node count. To extensively preserve the
original structures and avoid excessive splitting, we relax the requirements for the
mean children count and minimum area size by allowing means in the range [4, 11]
and smaller area sizes. We intend to compensate smaller area sizes with automatic
view magnification (Section 3.3).

The Merge stage combines the set of optimized LOS images into a single HOID
Image (Fig. 12). The Triangle Mesh Vertex Assignment stage assigns to

554

Geoinformatica (2014) 18:537–567

Fig. 11 Comparing examples of two methods for splitting nodes. a Geometry of one LOSi node
and d its LOSi+1 children with buildings overlayed for illustration. b Geometries resulting from 3
splits in sequence on LOSi and e propagated splits to LOSi+1. c Geometries resulting from 3 splits
in sequence on LOSi after fitting node geometries to their respective children, resulting in c natural
border geometries on LOSi and f no splits on LOSi+1

each input building triangle mesh vertex an attribute representing the HOID of
the building from the HOID Image. The Terrain Texture Generation stage
removes LOS6 from the HOID image and computes a tiled (e.g., 5122) image
pyramid (mipmaps). We implemented a prototype of the preprocessing pipeline

Fig. 12 Computed HOID
image encoding the navigation
hierarchy for the city of Berlin

Geoinformatica (2014) 18:537–567

555

as an extension of the Geodata Mapping service in C++ using a single CPU
core and no GPU as cross-platform software (operating on Linux, Mac OS X,
Windows) in approximately 10k lines of codes (LOCs) reusing as notable third-
party software PostgreSQL (as OpenStreetMap(OSM) data base), Mapnik (for
OpenStreetMap(OSM) outline image rendering), and GDAL (for terrain texture
generation).

4.3 Interactive runtime processing pipeline

In this Section, we present the service-based implementation of the proposed 3D
Navigation Technique as part of the 3D Visualization Client. The nav-
igation technique implements an interactive, runtime processing pipeline as depicted
in Fig. 13.

Initially, the camera’s current view node is set to the viewing graph node represent-
ing the root of the navigation hierarchy. The corresponding initial image requests and
camera parameters are passed to the Image Retrieval and Control Camera
processing stages respectively. The Image Retrieval stage takes as input an
image request specifying in particular camera parameters and image layers, and
outputs the requested images. Typically, pairs of depth and HOID images of the
same perspectively projected view of a V3DCM are requested. They are retrieved
according to availability either from the local image cache, the remote image cache
of the 3D Rendering service, or are generated on demand by the 3D Rendering
service.

The Suggestion Determination stage derives the suggestions (Section 3.2)
for the current view node from the HOID image and state information. The complete
viewing graph is not represented explicitly but instead is created locally, partially, and
on demand at each hierarchy node. To derive MSs represented as outlined regions
in the view, we propose a view-dependent LOS selection algorithm. Its purpose is to
view-dependently select from the HOID image for the current view node the nodes
that represent MSs and to prune the subtrees below the selected nodes. For the
current view node, we determine the current hierarchy node n (e.g., node n = v01
in Fig. 4). We select nodes that either are children of n or are on the same or
lower LOS than n according to a hierarchical screen space criterion. The algorithm
is structured in three phases: First, an explicit, pointer-based tree data structure is
extracted from the HOID image for a more convenient and efficient processing.
When the data structure is constructed, nodes above the LOS of n except n’s children

Initial Image 
Requests

Image 
Retrieval

HOID Image

Suggestion 
Determination

Pruned
HOID Image

Suggestion 
Display

Overlay

Suggestion 
Selection

Selected 
Suggestion

Suggestion 
Execution

Image 
Requests

Initial Camera 
Parameters

Button 
Specifcations

Camera 
Parameters

Depth Image

HOID Image

Control 
Camera

Filtering, Mapping, Rendering

Filtering

Mapping, Rendering

Interactivity / Interaction Processing

Fig. 13 Interactive, runtime processing pipeline of the navigation technique. Italic element names
denote external input/output

556

Geoinformatica (2014) 18:537–567

and subtrees below the children of n are already pruned from the tree. Then, the tree
data structure is recursed and a node is selected if it is either a leaf node or it is
not on a path to n and the mean projected screen space area of its children falls
below a given threshold υmin. Finally, the HOID image is pruned by setting to zero in
each HOID value the bits that represent nodes below selected nodes. Depending on
the capabilities of the 3D Rendering service, the view-dependent LOS selection is
performed either by the 3D Rendering service or locally by the client. Figure 14
demonstrates how the algorithm selects a set of nodes from the hierarchy represented
view-dependently in the HOID image and how complexity is removed from a view.
On average, the algorithm removes 98 % of the nodes from a HOID image. Figure 6
illustrates how nodes from different LOSs are systematically selected for a given
view.

the

The Suggestion Display stage displays

identified suggestions
as a 2D overlay computed from the Pruned HOID Image and Button
Specifications. To generate the outlined and shaded regions, a quad is
positioned in 3D space to cover exactly the screen when the viewpoints of the
camera and HOID image match. It is rendered with a fragment shader that takes as
primary input the pruned HOID image, the current hierarchy node’s HOID, and the
user-selected node’s HOID. The shader detects and draws edges between different
nodes identified by their HOIDs, colorizes the user-selected node, and applies a
FNC vignetting technique by increasing the brightness of the current hierarchy

Fig. 14 View-dependent level of scale selection. From an example original view (top, left) and HOID
image (top, right), nodes are removed (υmin = 1.85 cm2) resulting in a less complex view (bottom, left)
and pruned HOID image (bottom, right)

Geoinformatica (2014) 18:537–567

557

node’s children as the focus and decreasing the brightness of all other nodes as
context. Textual node labels are displayed centered at projected node centers when
available in the data once retrieved at startup from the Thematic Data Storage
service.

In the Suggestion Selection stage, the user selects a suggestion by pointing
at its visual representation that is then executed in the Suggestion Execution
stage. Automatic view magnification is implemented by animating the camera’s FOV
and orientation. Selecting a suggestion triggers a camera animation for which the
positions and orientations of the target view node and trajectory must be computed.
For this, information on the environment is solely retrieved from pairs of depth
and HOID images of three different view types used depending on the suggestion
type: For MSs, current perspective views (retrieved for each current view node)
or global overviews (for “Up”; top-down orthographic projection of the entire city
area; retrieved once; e.g., 10242 resolution) are used. For VSs, current perspective
views or local overviews (top-down orthographically projected views enclosing all
VS nodes of the current hierarchy node; retrieved for current hierarchy nodes; e.g.,
322 resolution) are used according to availability. Local and global overviews are
retrieved additionally to provide information on small and large scales that may not
be contained in the current view due to occlusion or the limited FOV. For computing
the camera trajectory to the target node, its (AABB) is computed from the respective
depth and HOID images by back-projecting depth samples identified by HOID val-
ues and building the AABB from them, the camera’s target position and orientation
is computed relative to the AABB, and approximate occlusion management and
collision avoidance is performed based on the assumption of a 2.5D V3DCM by
projecting 3D lines of sight and movement into respective depth images. Once the
camera parameters are computed, the camera animation is started and the images for
the target view node are requested. We implemented a cross-platform prototype of
the runtime pipeline as an extension of the 3D Visualization Client in C++
for tablet computers and smartphones (iOS) and PCs (Linux, Mac OS X, Windows)
in approximately 14k lines of codes (LOCs) reusing as notable third-party software
OpenSceneGraph (OSG) and OpenGL ES.

5 Evaluation

We assess to what degree the proposed approach meets goals and requirements
stated in Sections 1 and 4.2. For all experiments, we use as a case study the V3DCM
of Berlin (Section 4.2) and as algorithm parameters αmin = 4, αmax = 40, αmean = 14,
αmeandelta = 2, σmin = 20 pixel, τmin = 0.001, υmin = 1.85 cm2, φmin = 0.85 cm2, φmin =
0.92 cm. For the server, we use a workstation with an Intel Xeon W3520, 2.67 GHz, 4
cores, 12 GiB main memory, GeForce GTX 275, Seagate ST32000641AS, Windows 7
64-bit. For the clients, we use an Apple iPad2 and a workstation with the same
specification as the server computer.

5.1 Computer resource efficiency

Figure 15 presents the preprocessing pipeline mean execution times of the complete
process (66:08 minutes) and its stages. We estimate that the execution times can be

558

Geoinformatica (2014) 18:537–567

Fig. 15 Preprocessing pipeline
mean execution times of the
complete process (66:08
minutes) and its stages

considerably reduced by optimizing our prototype implementation and by exploiting
parallelism. Main memory requirements primarily depend on the count, dimensions,
and OID ranges of LOS images and are below 3.0 GiB for the case study.

Figure 16 depicts the mean execution times of the four most time consuming parts
of the runtime processing pipeline on three different client hardware configurations.
The client requires at any time main memory for one or two perspective views, zero
to two local overviews, and one global overview, and memory and/or disk space for
an image cache of arbitrary size. The client retrieves pairs of depth and HOID images
from the 3D Rendering service. The requested image dimensions depend on the
view type and screen dimensions. Table 1 reports mean network image transmission
sizes for requested image dimensions. Executing LOS selection on the service-side
relieves the client from its most time consuming processing stage and reduces the
network transmission volume for perspective view HOID images by 90 %.

5.2 Suggestion count and size

The optimization algorithm significantly improves the properties of the initial hierar-
chy (Table 2) towards the targeted properties (Section 4.2). However, achieving the
targeted properties cannot be guaranteed for every input hierarchy since required
node splits could not be performed in every case. At runtime, the LOS selection
algorithm (Section 4.3) removes on average 98 % of the nodes (9.7” display at
1024x768). Inside the focus, mean node display sizes (4.6 cm2 ≥ φmin) and mean node
counts (12 in αmean ± αmeandelta) are in the targeted ranges, whereas minimum node
display sizes and node counts are outside the targeted ranges due to runtime factors
(e.g., occlusion, perspective shortening, framing, viewport clipping).

Goals and hypotheses
In order to evaluate the general usability of the navigation
technique, we conducted a user study. We evaluate to what extent the proposed

5.3 User study

Fig. 16 Mean execution times
of the four most time
consuming parts of the
runtime processing pipeline
on three different client
configurations

Geoinformatica (2014) 18:537–567

559

Table 1 Mean network image transmission sizes (byte)

View type

Image type

Perspective view

Global overview

Local overview

HOID unpruned
HOID pruned
Depth
HOID unpruned
Depth
HOID pruned
Depth

Image size

Image size

Viewport dimensions
1024 × 768
Image dim.
1024 × 768
1024 × 768
512 × 384
1024 × 1024
512 × 512
32 × 32
32 × 32

76,935
8,725
136,686
214,997
5,332
504
1,275

2048 × 1536

Image dim.
2048 × 1536
2048 × 1536
512 × 384
1024 × 1024
512 × 512
32 × 32
32 × 32

176,873
17,048
136,686
214,997
5,332
504
1,275

navigation technique (HSP, for hierarchy, suggestions, pointing) meets requirements
identified in Section 1 for novice and experienced users in comparison to standard
navigation techniques (STD). We summarize our research hypotheses as follows:
There is a difference when using STD or HSP for novices or experts between each of
the following criteria: ease of learning and use, orientation, effective view properties,
efficiency, speed, precision, freedom, satisfaction, and task preference. The first four
main criteria are derived from the requirements stated in Section 1, whereas the
remaining are added criteria for a more extensive assessment.

Experimental design We adopt a 2 × 2 split-plot design with two independent
variables: The navigation technique (STD or HSP) is the within-subject factor and
subject experience with 3D interaction (novice or expert) is the between-group
factor. Table 3 lists the assessed 9 dependent criteria (first column) and associated
14 dependent variables with their metrics (second column). The 3 variables for
interaction efficiency in the search/inspection task are measured as finger tap count,
direct manipulation time (total time at least one finger touches the display), and task
completion time. All other 11 variables are measured based on subjective ratings
using a questionnaire (questions listed in Table 3, second column) employing a 7-
point Likert scale ranging from 1 (strongly disagree) to 7 (strongly agree), presented
as an approximate to an interval-scale. Subjects performed three different types of
tasks each in a given, delimited area of the V3DCM: In the first task type, a subject
observed a presentation given by an experimenter resembling a virtual city tour

Table 2 Navigation hierarchy node counts for the seven LOSs and the overall hierarchy before (left)
and after (right) the preprocessing optimization stage

Level Semantics

Before optimization

Final, after optimization

Node
count Min Max Mean Med count Min Max Mean Med

Children

Children

Node

0
1
2
3
4
5
6
0–6

City
District
Quarter
Borough
Neighborhood
Block
Building
Tree overall

1
12
96
708
1,357
13,199
541,871
557,244

12
2
1
1
1
0

–

0

12
15
20
12
146
2,231
–
2,231

12.0
8.0
7.3
1.9
9.7
41.0
–
36.2

12
7
6
1
5
26
–
21

1
12
96
772
3,450
31,794
541,871
577,996

12
2
4
4
4
0
–
0

12
15
40
40
40
39
–
40

12.0
8.0
8.0
4.4
9.2
17.0
–
16.0

12
7
6
4
4
19
–
17

560

Geoinformatica (2014) 18:537–567

t
s
e
t

a

f
o

s
n
o
i
t
a
n
i
b
m
o
C

.
)
t
n
a
c
i
f
i
n
g
i
s

t
o
n

:
s
n

,
t
n
a
c
i
f
i
n
g
i
s

:
s

,
t
s
e
t

y
e
n
t
i
h
W
-
n
n
a
M

:

U

,
t
s
e
t
-
t

t
n
e
d
n
e
p
e
d

:
t

,
t
s
e
t

k
n
a
r
-
d
e
n
g
i
s
n
o
x
o
c
l
i

W

:
z
(

e
l
b
a
i
r
a
v

t
n
e
d
n
e
p
e
d

f
o

s
r
i
a
p

g
n
i
r
a
p
m
o
c

r
o
f

s
t
s
e
t

e
c
n
a
c
i
f
i
n
g
i
s

f
o

s
t
l
u
s
e
R

.

}
P
S
H

,

D
T
S
{

t
r
e
p
x
E

,

e
c
i
v
o
N

{

s
n
o

i
t
i

d
n
o
c

r
u
o
f

e
h
t

f
o

h
c
a
e

r
o
f

e
l
a
c
s

t
r
e
k
i
L

t
n
i
o
p
-
7
(

s
t
n
e
m
e
r
u
s
a
e
m

r
o
f

e
u
l
a
v

n
a
e
m
a

,
e
l
b
a
i
r
a
v

t
n
e
d
n
e
p
e
d

h
c
a
e

.
s
c
i
r
t
e
m
d
n
a

s
e
l

b
a
i
r
a
v

t
n
e
d
n
e
p
e
d

o
t
n

i

d
e
n

i
f
e
r

e
r
a

a
i
r
e
t
i
r
c

n
o
i
t
a
u
l
a
v
E

r
o
F

×

}

5
0
.

r
o
f

<
p

d
e
y
a
l
p
s
i
d

s
i

)
s
n
o
i
t
s
e
u
q

t
a

d
e
t
r
o
p
e
r

e
r
a

s
n
a
e
m

)
t
h
g
i
r
n
a
h
t

r
e
t
t
e
b

t
f
e
l
d
n
a
s

:

d
e
r

,
s
n

:

w
o
l
l
e
y
,
t
f
e
l
n
a
h
t

r
e
t
t
e
b
t
h
g
i
r
d
n
a
s

:

n
e
e
r
g
(
g
n
i
d
o
c
-
r
o
l
o
c
h
t
i

w
d
e
t
a
r
t
s
u

l
l
i

e
r
a
s
n
a
e
m
o
w

t

f
o
n
o
s
i
r
a
p
m
o
c

e
h
t
d
n
a
t
l
u
s
e
r

y
d
u
t
s

r
e
s
u
e
h
t

f
o
s
t
l

u
s
e
r

e
v
i
t
a
t
i
t
n
a
u
Q

3
e
l
b
a
T

Geoinformatica (2014) 18:537–567

561

visiting 15 sites consecutively. The second task type comprises exploring the V3DCM
for 5 minutes and then as motivation to acquire spatial knowledge drawing a map
of major structural elements of the V3DCM and notable spots or landmarks. In
the third task type, a subject searches the V3DCM for 15 buildings each marked
with two signs on its facade: a large sign and a small sign each showing two letters.
Typically, after a large sign is discovered from a distance, the subject moves closer to
the building and performs a building inspection to discover the small sign. For each
building, the subject notes the letters from both signs on paper.

Apparatus We use an iPad2 mounted on a table, inclined towards a sitting user.
In order to eliminate network access latencies, all data required by the system is
precached on the device. For use in the trials, we identified two disjoint areas in
the V3DCM of Berlin each covering LOS2 to LOS6 with similar ground area and
complexity. For STD, pan (one-finger drag), orbit (two-finger drag), zoom (two-
finger pinch), and goto (double-tap) are offered. In addition, an overview map
(Section 3.4) is provided with the restriction to outline only the extent of the current
task’s area.

Participants Twenty subjects participate in the experiment. Ten subjects (four
female, aged 25 to 46) are classified as novices (not using interactive 3D applications
in a typical week), the remaining ten (three female, aged 24 to 39) as experts (daily
or weekly use). All subjects are right-handed, hold at least one university degree, and
rate their degree of being familiar with the two used city areas below 3 on a 7-point
Likert scale.

Procedure After a subject is welcomed and introduced, the subject first performs
the presentation task using technique A and area A, and then using technique B and
area B. Then, technique A is explained to the subject followed by a practice session,
in which the subject is explicitly asked to adjust the technique’s animation speed or
sensitivity to fit personal preferences. With technique A in area A, the subject first
performs the exploration task followed by a practice run and an actual run of the
combined search/inspection task. Then, the procedure of introducing a technique and
performing the exploration and search/inspection tasks is repeated with technique B
and area B. After completing all tasks, the subject fills out a short questionnaire and
receives a small gratification. Subjects are given a break after finishing each task.
We randomize the assignment of technique STD and HSP to A and B, the two
available areas to area A and B, the subject order, and sets of sites and buildings
for the exploration and search/inspection tasks.

Results Table 3 and Fig. 17 present the quantitative experiment results and can
be summarized as follows. Regarding subjective dependent variables measured with
questions Q1 to Q11 using a 7-point Likert scale, statistically significant results sug-
gest that for novices (Table 3, column Novice) HSP in comparison to STD is easier to
learn and use, allows better orientation, yields more effective view properties, gives
the impression of higher navigation speed, is more satisfactory, and is preferred over
STD as a default technique for V3DCMs. However, novices feel navigating less freely
with HSP than with STD. On the contrary, results indicate that experts do not profit
from HSP as much as novices do. For experts (column Expert), HSP in comparison
to STD is easier to use, and yields more effective view properties when observing a

562

Geoinformatica (2014) 18:537–567

Fig. 17 Quantitative results of the user study displayed as a bar chart of dependent variable
means with error bars displaying 95 % confidence intervals. Each subjective (questions Q1 to Q11,
7-point Likert scale) and each objective, efficiency-related dependent variable (values normalized
to dependent variable maximum and then to six) is measured once for each of the four conditions
{Novice, Expert} × {ST D, HSP}

presentation. However, experts feel navigating less fast and less freely with HSP than
with STD. In all remaining dependent variables, no significant difference between
HSP and STD is found. When comparing novices and experts using STD (column
STD), experts rate higher all dependent variables except view properties when
observing a presentation, speed of movement and general preference for V3DCMs.
When using HSP (column HSP), there is no significant difference between novices
and experts concerning ease, orientation, view properties, direct manipulation time,
subjective precision, and general satisfaction. However, novices feel navigating faster
and more freely and prefer HSP as default technique stronger than experts do,
whereas experts value HSP higher for specific tasks. Regarding objective, efficiency-
related dependent variables, statistically significant results suggest that for both
novices and experts HSP in comparison to STD is more efficient regarding physical
demands. However, no significant difference in task completion time is found for
both novices and experts for HSP compared to STD. When comparing novices and
experts, experts performed significantly more efficient when using either STD or
HSP regarding physical demand and task completion time.

5.4 Limitations

The usability of the navigation technique largely depends on how well the provided
suggestions match the user’s requirements. They can be insufficient in several
regards. Suggestions for required views could be missing, e.g., when certain features
are only visible in narrow ranges of yaw and pitch values and viewing these features
is required but not covered by provided VSs. To mitigate this, suitable VSs could be
computed adaptively for each feature based on a feature analysis and the navigation
technique could be augmented with constrained (e.g., to 1D path or 2D surface) or
unconstrained direct camera control techniques to allow for more precise control.
Features required as navigation targets could not be represented as suggestions. They
could generally not be represented as suggestions (such as streets in the case study)
hindering or prohibiting to focus or inspect these features. To mitigate this, MSs
could be extended to node boundary edges and intersections for improved inspection

Geoinformatica (2014) 18:537–567

563

of paths, edges, and nodes [35]. Alternatively, targeted features could be represented
as suggestions that are not selectable at the current view but are clearly visible (such
as a large building visible from a great distance). Instead of directly traveling to
the feature, this could result in traveling through multiple intermediate nodes and
visually following the target feature “jumping” to a different screen location at each
node. To mitigate this, the navigation technique could be augmented with framed
zooming [39] or direct camera control techniques. Required scales could be missing.
The case study is limited to a scale range from city to building level and does not
include indoor or pedestrian navigation. To support indoor or pedestrian navigation,
the viewing graph could be extended with a ground-level routing network with edges
along walkable linear structures (e.g., streets, corridors) and nodes at junctions.

Furthermore, the navigation technique only decorates an independently gener-
ated view with an overlay (de-/) emphasizing features. Generating views and appro-
priately representing multiscale information from the V3DCM at various scales is an
orthogonal problem that the navigation technique does not address. Additionally,
given V3DCM data could result in suboptimal navigation hierarchies even after
optimization (Section 4.2). Finally, since the hierarchy generation is parametrized by
the target display size and input device type, changing these parameters considerably
could require rebuilding the hierarchy.

6 Conclusions

In this article, we introduced the concept, implementation, and evaluation of a
novel assisting, constrained navigation technique for multiscale V3DCMs based
on semantic navigation hierarchies, suggestions, and pointing. We demonstrated
how a navigation technique can be built based on service-oriented architectures,
standards, and image-based representations that supports novice and expert users
in exploration, search, inspection, and presentation tasks, and that is easy to learn
and use, improves orientation, is efficient, and yields effective view properties. The
proposed navigation technique is more computer resource efficient than standard
techniques when integrated into the proposed 3D geovisualization system based on
service-oriented architectures, standards, and image-based representations since it
limits camera trajectories and views to finite, a priori known sets. The proposed
navigation technique uses standards-based, image-based representations provided
by services as the sole representation of a 3DGeoVE. Results of a user study suggest
that the proposed navigation technique provides the targeted usability properties
with the drawback of subjectively moving less fast and freely compared to standard
techniques, and that it is more advantageous to novice than to expert users. The
proposed navigation technique is not assumed to replace or generally be more
advantageous than standard techniques. Instead, it can be seen as an alternative
that could be more appropriate when facing specific requirements such as targeting
novice users at interactive kiosks or on mobile devices in 3DGeoVSs built on SSI.

In future work, the approach can be further extended and evaluated by overcom-
ing current limitations as discussed in Section 5.4, applying the overlay structuring
visualization technique demonstrated for service-oriented architectures, standards,
and image-based representations and selective navigation to real-time rendering and
standard navigation techniques, evaluating different types of semantic hierarchies

564

Geoinformatica (2014) 18:537–567

and different V3DCMs, and conducting further user studies to more extensively
assess the proposed navigation technique.

Acknowledgement The authors thank Jürgen Döllner for support.

References

1. Abásolo MJ, Della JM (2007) Magallanes: 3D navigation for everybody. In: GRAPHITE 2007:
proceedings of the 5th international conference on computer graphics and interactive techniques
in Australia and Southeast Asia. ACM Press, pp 135–142

2. Abend P, Thielmann T, Ewerth R, Seiler D, Mühling M, Döring J, Grauer M, Freisleben B
(2012) Geobrowsing behaviour in google earth: a semantic video content analysis of on-screen
navigation. In: Proceedings of geoinformatics forum. Salzburg, Austria

3. Bacim F, Bowman D, Pinho M (2009) Wayfinding techniques for multiscale virtual environ-

ments. In: IEEE symposium on 3D user interfaces, pp 67–74

4. Bares W, McDermott S, Boudreaux C, Thainimit S (2000) Virtual 3D camera composition
from frame constraints. In: Proceedings of the ACM international conference on multimedia
(ACMMM ’00). ACM Press, pp 177–186

5. Beckhaus S, Ritter F, Strothotte T (2001) Guided exploration with dynamic potential fields: the

cubicalpath system. Comput Graph Forum 20(4):201–210

6. Blanz V, Tarr MJ, Bülthoff HH (1999) What object attributes determine canonical views?

Perception 28(5):575–599

7. Bowman DA, Koller D, Hodges LF (1997) Travel in immersive virtual environments: an evalua-
tion of viewpoint motion control techniques. In: Proceedings of the 1997 Virtual Reality Annual
International Symposium (VRAIS 1997). IEEE Computer Society, pp 45–52

8. Buchholz H, Bohnet J, Döllner J (2005) Smart and physically-based navigation in 3D geovirtual
environments. In: Proceedings of the ninth international conference on information visualisation.
IEEE Computer Society, pp 629–635

9. Burtnyk N, Khan A, Fitzmaurice G, Balakrishnan R, Kurtenbach G (2002) StyleCam: interactive
stylized 3D navigation using integrated spatial & temporal controls. In: UIST ’02: proceedings of
the 15th annual ACM symposium on user interface software and technology. ACM, pp 101–110
10. Burtnyk N, Khan A, Fitzmaurice G, Kurtenbach G (2006) ShowMotion: camera motion based
3D design review. In: Proceedings of the 2006 symposium on interactive 3D graphics and games,
I3D ’06. ACM, pp 167–174

11. Chang R, Wessel G, Kosara R, Sauda E, Ribarsky W (2007) Legible cities: focus-dependent
multi-resolution visualization of urban relationships. Trans Vis Comput Graph 13:1169–1175
12. Christie M, Languénou E (2003) A constraint-based approach to camera path planning. In: Pro-
ceedings of the third international symposium on Smart Graphics (SG ’03), vol 2733. Springer,
pp 172–181

13. Christie M, Olivier P, Normand JM (2008) Camera control in computer graphics. Comput Graph

Forum 27(8):2197–2218

14. Dennis BM, Healey CG (2002) Assisted navigation for large information spaces. In: Proceedings

of the conference on visualization ’02, VIS ’02. IEEE Computer Society, pp 419–426

15. Diepenbrock S, Ropinski T, Hinrichs KH (2011) Context-aware volume navigation. In: IEEE

pacific visualization symposium (PacificVis 2011)

16. Döllner J, Hagedorn B, Schmidt S (2005) An approach towards semantics-based navigation in
3D city models on mobile devices. In: Gartner G (ed) Proceedings of the 3rd symposium on LBS
& teleCartography 2005, vol 74. TU Wien, pp 171–176

17. Fitzmaurice G, Matejka J, Mordatch I, Khan A, Kurtenbach G (2008) Safe 3D navigation.
In: Proceedings of the 2008 symposium on interactive 3D graphics and games, I3D ’08. ACM,
pp 7–15

18. Frisken SF, Perry RN (2003) Simple and efficient traversal methods for quadtrees and octrees.

J Graph Tools 7(3):1–11

19. Furnas GW (1997) Effective view navigation. In: Pemberton S (ed) Human factors in computing

systems, CHI 1997 conference proceedings. ACM, pp 367–374

20. Glander T, Döllner J (2009) Abstract representations for interactive visualization of virtual 3D

city models. Comput Environ Urban Syst 33(5):375–387

Geoinformatica (2014) 18:537–567

565

21. Gooch B, Reinhard E, Moulding C, Shirley P (2001) Artistic composition for image creation. In:
Proceedings of the 12th eurographics workshop on rendering techniques. Springer, pp 83–88
22. Gröger G, Kolbe TH, Nagel C, Häfele K (eds) (2012) OGC City Geography Markup Language

(CityGML) encoding standard, version 2.0.0. Open Geospatial Consortium Inc

23. Hachet M, Decle F, Knödel S, Guitton P (2009) Navidget for 3D interaction: camera positioning

and further uses. Int J Hum Comput Stud 67(3):225–236

24. Halper N, Helbing R, Strothotte T (2001) A camera engine for computer games: managing the
trade-off between constraint satisfaction and frame coherence. Comput Graph Forum 20(3):174–
183

25. Hanson AJ, Wernert EA (1997) Constrained 3D navigation with 2D controllers. In: Proceedings

of the 8th conference on visualization ’97. IEEE Computer Society Press, pp 175–182

26. Hildebrandt D, Döllner J (2010) Service-oriented, standards-based 3D geovisualization: poten-

tial and challenges. J Comput Environ Urban Syst 34(6):484–495

27. Hommel B, Gehrke J, Knuf L (2000) Hierarchical coding in the perception and memory of spatial

layouts. Psychol Res 64:1–10

28. Jankowski J, Decker S (2012) A dual-mode user interface for accessing 3D content on the world
wide web. In: Mille A, Gandon FL, Misselis J, Rabinovich M, Staab S (eds) WWW. ACM,
pp 1047–1056

29. Jul S (2003) This is a lot easier!: constrained movement speeds navigation. In: CHI ’03 extended

abstracts on human factors in computing systems, CHI EA ’03. ACM, pp 776–777

30. Jul S, Furnas GW (1998) Critical zones in desert fog: aids to multiscale navigation. In: ACM

symposium on user interface software and technology, pp 97–106

31. Kada M (2009) The 3D Berlin project. In: Fritsch D (ed) Photogrammetric Week 2009, pp 331–

340. Wichmann Verlag, Heidelberg

32. Khan A, Komalo B, Stam J, Fitzmaurice G, Kurtenbach G (2005) HoverCam: interactive 3D
navigation for proximal object inspection. In: Proceedings of the 2005 symposium on interactive
3D graphics and games, I3D ’05. ACM, pp 73–80

33. Kopper R, Ni T, Bowman DA, Pinho M (2006) Design and evaluation of navigation techniques
for multiscale virtual environments. In: Proceedings of the IEEE conference on Virtual Reality,
VR 2006. IEEE Computer Society, pp 175–182

34. Lazar J, Feng JH, Hochheiser H (2010) Research methods in human-computer interaction. Wiley
35. Lynch K (1960) The image of the city. MIT Press, Cambridge
36. MacEachren AM (ed) (2004) How maps work: representation, visualization, and design, 2nd

37. MacEachren AM, Kraak MJ (2001) Research challenges in geovisualization. Cartogr Geogr Inf

edn. Guilford Press

Sci 28(1):3–12

38. Mackinlay JD, Card SK, Robertson GG (1990) Rapid controlled movement through a virtual
3D workspace. In: SIGGRAPH 1990: proceedings of the 17th annual conference on computer
graphics and interactive techniques. ACM, pp 171–176

39. McCrae J, Mordatch I, Glueck M, Khan A (2009) Multiscale 3D navigation. In: Proceedings of

the 2009 symposium on interactive 3D graphics and games, I3D ’09. ACM, pp 7–14

40. Mine MR (1995) Virtual environment interaction techniques. Tech. Rep. TR95-018, University

of North Carolina at Chapel Hill

41. Nurminen A, Oulasvirta A (2008) Designing interactions for navigation in 3D mobile maps. In:
Meng L, Zipf A, Winter S (eds) Map-based mobile services: design, interaction and usability.
Lecture notes in geoinformation and cartography. Springer, pp 198–224

42. Olivier P, Halper N, Pickering J, Luna P (1999) Visual composition as optimisation. In: AISB

symposium on AI and creativity in entertainment and visual art, pp 22–30

43. Parhi P, Karlson AK, Bederson BB (2006) Target size study for one-handed thumb use on small
touchscreen devices. In: Proceedings of the 8th conference on human-computer interaction with
mobile devices and services, MobileHCI 2006. ACM, pp 203–210

44. Pierce JS, Pausch R (2004) Navigation with place representations and visible landmarks. In:
Proceedings of the IEEE virtual reality conference 2004, VR 2004. IEEE Computer Society,
pp 173–180

45. Reisman JL, Davidson PL, Han JY (2009) A screen-space formulation for 2D and 3D direct
manipulation. In: Proceedings of the 22nd annual ACM symposium on User Interface Software
and Technology, UIST ’09. ACM, pp 69–78

46. Ropinski T, Steinicke F, Hinrichs KH (2005) A constrained road-based VR navigation technique
for travelling in 3D city models. In: Proceedings of the 15th international conference on artificial
reality and telexistence (ICAT05), pp 228–235

566

Geoinformatica (2014) 18:537–567

47. Ruddle RA, Howes A, Payne SJ, Jones DM (2000) The effects of hyperlinks on navigation in

virtual environments. Int J Hum Comput Stud 53(4):551–581

48. Santos CRd, Gros P, Abel P, Loisel D, Trichaud N, Paris JP (2000) Metaphor-aware 3D naviga-
tion. In: Proceedings of the IEEE symposium on Information Vizualization 2000, INFOVIS ’00.
IEEE Computer Society, pp 155–165

49. Vazquez PP (2009) Automatic view selection through depth-based view stability analysis. Vis

Comput 25(5–7):441–449

50. Walther-Franks B, Herrlich M, Malaka R (2011) A multi-touch system for 3D modelling and
animation. In: Dickmann L, Volkmann G, Malaka R, Boll S, Krüger A, Olivier P (eds) Smart
graphics. Lecture notes in computer science, vol 6815. Springer, pp 48–59

51. Ware C, Fleet D (1997) Context sensitive flying interface. In: Proceedings of the 1997 Symposium

on Interactive 3D graphics, SI3D ’97. ACM Press, pp 127–130

52. Ware C, Osborne S (1990) Exploration and virtual camera contro in virtual three dimensional
environments. In: Proceedings of the 1990 Symposium on Interactive 3D graphics (SI3D ’90).
ACM Press, pp 175–183

53. Zhang X (2009) Multiscale traveling: crossing the boundary between space and scale. Virtual

Reality 13(2):101–115

54. Zhao H, Shneiderman B (2005) Colour-coded pixel-based highly interactive web mapping for

georeferenced data exploration. Int J Geogr Inf Sci 19(4):413–428

Dieter Hildebrandt
is a research scientist at the Computer Graphics Systems department of the
Hasso-Plattner-Institut, Potsdam, Germany, where he is currently pursuing a Ph.D. in Computer
Science. He received a Diploma degree in Computer Science from the University of Oldenburg,
Germany. For more than five years, he was head of the Software Engineering Department of Piranha
Bytes GmbH, Germany, and company partner. For one and a half year, he was a research assistant at
the Business Information Management Division of the OFFIS Institute for Information Technology,
Germany. His research interests include service-oriented architectures and 3D geovisualization. He
is a member of the German Association for Computer Science GI.

Geoinformatica (2014) 18:537–567

567

Robert Timm is a research assistant at the Computer Graphics Systems department of the Hasso-
Plattner-Institut, Potsdam, Germany. He received a Bachelor and a Master degree in IT Systems
Engineering from the University of Potsdam. His research interests include service-oriented archi-
tectures, 3D GIS, and tangible 3D user interfaces.

