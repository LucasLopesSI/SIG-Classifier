International Journal of Geographical Information Science
Vol. 25, No. 11, November 2011, 1809–1827

Evaluation of inductive logic programming for information extraction
from natural language texts to support spatial data recommendation
services

Domen Smolea*, Marjan ˇCehb and Tomaž Podobnikarb,c

aDFG CONSULTING, Ltd., Ljubljana, Slovenia; bDepartment of Geodetic Engineering, Faculty of
Civil and Geodetic Engineering, University of Ljubljana, Ljubljana, Slovenia; cInstitute of
Anthropological and Spatial Studies, Scientiﬁc Research Centre of the Slovenian Academy of
Sciences and Arts, Ljubljana, Slovenia

(Received 4 May 2009; ﬁnal version received 9 January 2011)

In this article we analyze a well-known and extensively researched problem: how to
ﬁnd all datasets, on the one hand, and on the other hand only those that are of value
to the user when dealing with a speciﬁc spatially oriented task. In analogy with exist-
ing approaches to a similar problem from other ﬁelds of human endeavor, we call this
software solution ‘a spatial data recommendation service.’ In its ﬁnal version, this ser-
vice should be capable of matching requests created in the user’s mind with the content
of the existing datasets, while taking into account the user’s preferences obtained from
the user’s previous use of the service. As a result, the service should recommend a list
of datasets best suited to the user’s needs. In this regard, we consider metadata, par-
ticularly natural language deﬁnitions of spatial entities, a crucial piece of the solution.
To be able to use this information in the process of matching the user’s request with
the dataset content, this information must be semantically preprocessed. To automate
this task we have applied a machine learning approach. With inductive logic program-
ming (ILP) our system learns rules that identify and extract values for the ﬁve most
frequent relations/properties found in Slovene natural language deﬁnitions of spatial
entities. The initially established quality criterion for identifying and extracting infor-
mation was met in three out of ﬁve examples. Therefore we conclude that ILP offers
a promising approach to developing an information extraction component of a spatial
data recommendation service.

Keywords: metadata; spatial entities; semantics; inductive logic programming

Introduction

1.
Today, looking at the Internet as the ‘information superhighway’ is out of date. It is much
more like a platform for peer-to-peer collaboration. If thousands of people can collabo-
rate to create an operating system or an encyclopedia, what’s next (Tapscott and Williams
2007)? In the domain of spatial data, peer-to-peer work is sometimes called geocollabora-
tion. Researchers at Pennsylvania State University deﬁne the activity of geocollaboration as
a group working together to solve geographic problems facilitated by geospatial informa-
tion technologies. Others view geocollaboration as any form of collaboration that involves
geospatial data (Bortenschlager et al. 2007). Tools enabling geocollaboration are rarely

*Corresponding author. Email: domen.smole@dfgcon.si

ISSN 1365-8816 print/ISSN 1362-3087 online
© 2011 Taylor & Francis
http://dx.doi.org/10.1080/13658816.2011.556640
http://www.tandfonline.com

1810

D. Smole et al.

available. This is due to the lack of technological advances in achieving feasible GIS-based
solutions that would efﬁciently support geocollaboration (Cai 2005).

Comber et al. (2006) propose that to realize the beneﬁts of the new cyber infrastructures
for spatial data, we need to expand metadata to include free text descriptions and develop
text-mining tools that would enable users to estimate the data ﬁtness for their tasks. A sim-
ilar observation has been made by Kavouras (2005), who proposed a methodology for the
integration of heterogeneous ontologies to achieve semantic interoperability. The method-
ology consists of three prerequisite processes, with information extraction as the ﬁrst one,
followed by concept comparison, and ﬁnally concept integration. A similar methodology is
to be followed when constructing a solution for searching or recommending the appropri-
ate spatial data to the user. Therefore, we ﬁnd this methodology appropriate for our task.
At the same time, we agree that we should address ﬁrst things ﬁrst. As a consequence, we
decided to focus on the ﬁrst step of the above-mentioned methodology, namely information
extraction from natural texts.

The ﬁrst of the three processes mentioned above, information extraction from natural
texts representing metadata, is a non-trivial task (Kimball and Caserta 2004). This has been
demonstrated in other articles, for example Kavouras et al. (2005) and Karalopoulos et al.
(2004). Our present study describes an attempt to learn rules for identifying and extract-
ing information from deﬁnitions of geographic entities in natural language by utilizing a
machine learning approach called inductive logic programming, or ILP for short. Sections
1 and 2 explain the background and the objective of this work, the latter in the form of a
use-case scenario. Section 3 introduces the basic concepts of the ILP approach. Developing
the information extraction component of a spatial data recommendation service consists of
two parts. Section 4 describes the ﬁrst part, which comprises the four vital steps that have
to be carried out before the ILP learning takes place: the selection of spatial databases
and dictionaries with deﬁnitions of spatial entities, morphosyntactical analysis or part-of-
speech (POS) tagging, shallow syntactical analysis or chunking, and semantic role labeling.
The results of the ﬁrst part are direct input for the second part of the research work, which
is described in Section 5. The second part deals with the four variants of ILP learning: ILP
with a procedural classiﬁcation, ILP and cost-sensitive learning, ILP and boosting, and ILP
with a naïve Bayesian classiﬁcation. Finally, Section 6 discusses the results and provides
some ideas for further research.

2. The objective of the research

The objective of this research is to explore a machine learning approach, namely ILP, for
the so-called information extraction process. In a broader sense, the acquired knowledge
can be used for implementing an automated recommendation service for recommending
semantically similar spatial datasets based on user’ requests. The best way to illustrate the
objective of our research is through the presentation of a use-case scenario.

Let us assume that there is an employee working for some company. This company
would like to expand its production capacities to a selected foreign country and they
are looking for the right location. The company already has a long list of potential loca-
tions. But what they would like to arrive at is a short list. High on the priority list of the
requirements a short-listed location should satisfy is well-maintained infrastructure, such
as roads, railways, and telecommunications. The problem the employee now faces is that
he knows little or nothing about the available datasets offering this kind of information for
the selected foreign country which the company is expanding to.

However, at his disposal the employee has a dataset of roads, along with the correspond-
ing metadata, for the country he lives in, but not for the selected foreign country. We will

International Journal of Geographical Information Science

1811

name this dataset DH_R, where DH_R stands for ‘DatasetHome_Roads’. The employee
knows that this dataset could solve his problem only if it covered the geographical area of
the selected foreign country. Included in the metadata is the following information about
the entered road entities:

(cid:129) an artiﬁcial surface which, in accordance with the law, is used for trafﬁc by various

vehicles.

The ﬁrst step in the two-step process of ﬁnding semantically similar dataset for the foreign
country would be to obtain the following information about the content of dataset DH_R:

(cid:129) a more general entity is an artiﬁcial surface,
(cid:129) the purpose of the entity is to enable trafﬁc.

This information could also be provided directly by the employee if there was no such
dataset as DH_R.

Now let us assume that there exists a list of spatial datasets for the foreign country. Not
only the list, but also the corresponding metadata for each dataset is publicly available. The
employee wants to know which dataset from the list could help him solve the problem. We
will name the ﬁrst dataset from the list DF_1, where DF_1 stands for ‘DatasetForeign_1’.
The metadata attached to DF_1 includes the following information describing a set of
entities included in it:

(cid:129) a wide and purpose-built path, especially for vehicle trafﬁc.

Again we can obtain the following information about the content of dataset DF_1:

(cid:129) the more general entity is a wide and purpose-built path,
(cid:129) the purpose of the entity is to enable vehicle trafﬁc.

The second step requires comparing the extracted information:

(cid:129) a more general entity of the described entity:

(cid:129) DH_R: an artiﬁcial surface,
(cid:129) DF_1: a wide purpose-built path,

(cid:129) the described entity has a particular purpose:

(cid:129) DH_R: to enable trafﬁc,
(cid:129) DF_1: to enable vehicle trafﬁc.

From this comparison the software might conclude that among many available datasets for
the foreign country, dataset DF_1 is appropriate for further analysis aimed at formulating
a short list of potential locations out of the long one. However, this further analysis of the
content of DF_1 is yet to be done by the employee by means of some GIS tool.

The core assumption of our research is that the solution to the complex task described in
the use-case could be automated with software that we call a spatial data recommendation
service. The aim of this service would be to recommend the list of datasets which are
semantically similar to the user’s request, while taking into account the user’s preferences
obtained from previous use of the service. A simple example of such a preference would be
that the user ﬁnds the datasets of a particular owner more useful than semantically similar
datasets of other users. This information need not be explicitly provided by the user, as

1812

D. Smole et al.

the user might not at all be aware of this. It can be learned by the service itself based on
the feedback the user provides for each of the recommended datasets. For example, the
feedback might take the form of a rating ranging from 1 to 5. If given a rating of 1, the user
considers this particular recommended dataset useless, and if given a rating of 5, the user
considers this particular recommended dataset extremely useful.

In this research project, we have been focusing on the implementation of a spatial
data recommendation service component for the semantic preprocessing of metadata only.
This component should automatically extract knowledge in a structured form from meta-
data that describes the spatial entities of a database in natural language. For this particular
task, we have decided to apply an ILP approach, which has been used for similar pur-
poses already, for example, in Aitken (2002) and Sasaki and Matsuo (2000). The entire
research project was originally carried out with research material in the Slovene language.
For this reason this article might be more easily understandable for Slovene native speak-
ers. Furthermore, some papers, for example Mark and Turk (2003), suggest that the existing
cultural differences can also have an inﬂuence on understanding the presented research.

To provide a ﬁnal or fully operational version of a spatial data recommendation ser-
vice, we would have to implement two other software components. The ﬁrst additional
component would compare previously extracted information from the user’s request with
information extracted from other available spatial datasets. The second additional compo-
nent would need to learn the user’s preferences based on his or her feedback regarding the
list of previously recommended datasets. Due to the complexity of the information extrac-
tion component, developing and implementing these two components has not been part of
this research.

3. A short introduction to ILP
A detailed introduction to ILP can be found in Džeroski and Lavraˇc (2001). Without going
into too much detail, ILP is a machine learning approach where the goal is to generate
new knowledge in the form of a model or a hypothesis. The input for the machine learning
algorithm consists of a set of data and background knowledge. As a result of learning, the
combination of learned knowledge and background knowledge describes and explains the
data.

In ILP a hypothesis consists of rules. Rules are induced from a set of positive and
negative learning examples and background knowledge. ILP requires a set of positive
examples which satisfy the hypothesis, and a set of negative examples which do not sat-
isfy the hypothesis. One can optionally introduce other known relations, called background
knowledge, which the algorithm considers when learning the hypothesis. It must be noted
that background knowledge must not explain positive examples. Furthermore, before the
learning process starts, one has the ability to introduce syntactical limits the algorithm has
to comply with when learning the hypothesis. The aim of the algorithm is to ﬁnd the sim-
plest hypothesis H that explains all the positive examples E+ and none of the negative ones
E−, given the background knowledge B (Džeroski and Lavraˇc 2001):

and

e ∈ E+ : B & H = e (H is complete)

e ∈ E− : B & H = e (H is consistent)

International Journal of Geographical Information Science

1813

ILP is known for the rich expressiveness it offers with regard to describing background
knowledge and learned hypotheses (Džeroski and Lavraˇc 2001). Learning examples, back-
ground knowledge, and the learned hypotheses are presented as a set of logic programs
in Prolog syntax, which is a subset of the ﬁrst-order predicate logic. Logic programs con-
sist of sentences or clauses, where the ﬁrst part of the sentence, called the head, deﬁnes
the conclusion, and the rest of the sentence, called the body, deﬁnes the conditional part.
Both the head and the body of the sentence consist of atoms or literals, where an atom is a
predicate with zero or more arguments (Bratko 2001, Džeroski and Lavraˇc 2001).

The following is an ILP example. Let our target be the relation: ancestor(X,Y). We read
this as person ‘X’ is an ancestor of person ‘Y.’ To learn the target rule, we have to deﬁne
positive and negative examples of this relation, as well as background knowledge in the
form of Prolog facts using standard Prolog symbols.

Some positive examples are as follows:

Some negative examples are as follows:

Background knowledge is as follows:

(cid:129) ancestor(aka,ana).
(cid:129) ancestor(ana,bor).
(cid:129) ancestor(bor,mik).

(cid:129) ancestor(aka,aka).
(cid:129) ancestor(ana,aka).
(cid:129) ancestor(bor,ana).

(cid:129) parent(aka,ana).
(cid:129) parent(ana,bor).
(cid:129) parent(bor,mik).

ancestor(X,Y) :-
parent(X,Y).
ancestor(X,Y) :-
parent(X,Z),
ancestor(Z,Y).

An ILP algorithm can derive the following correct deﬁnition of a target relation or a hypoth-
esis that generalizes the data and background knowledge, again using standard Prolog
symbols:

In our case, the hypothesis will consist of rules for annotating parts of the deﬁnition with
the proper relation or property. The characteristics of learning, testing examples, and the
background knowledge will be presented in subsequent sections after we describe the
methodology of our approach.

4. The proposed methodology

4.1. Deﬁnitions of spatial concepts in natural language
The primary role of any language is to convey ideas or concepts between participants
in communication. For example, in our case metadata descriptions are the medium for

1814

D. Smole et al.

communication between a spatial data provider and a spatial data user. From this point of
view, language can be regarded as a bridge connecting a message in the form of voice or
text and the meaning. When communicating in the same language, the participant who is
speaking or writing converts the meaning into voice or text (coding), and the participant
listening or reading tries to re-establish the idea or concept in his or her mind (decoding).
Ideal communication is characterized by a situation wherein the consequence the message
has on the decoding participant exactly ﬁts the purpose the coding participant wanted to
achieve (Hofmann 1993).

The deﬁnition of a term connects the voice or text, which we sometimes name the form
or symbol, with the meaning, which we sometimes name the content. The meaning of a
word differs from the meaning of a sentence. Most sentences are propositions and can be
either true or false, which cannot be said for the meaning of a word. In our case, we intend
to deal with deﬁnitions in the form of sentences or propositions.

As stated in Karalopoulos et al. (2004), geographic concept deﬁnitions are a rich and
invaluable source of knowledge with special structure and content, and as such reﬂect the
scientiﬁc knowledge of a domain. Similar observations have been made in other studies,
for example in Kavouras et al. (2005) and Comber et al. (2006). Furthermore, efforts to
automatically derive and present knowledge from deﬁnitions are described, for example, in
Nichols et al. (2005), Hovy et al. (2003), and O’Hara (2005).

The explanation of the term ‘deﬁnition’ states that a deﬁnition is an exact description of
the symbol with regard to its crucial parameters. Usually, deﬁnitions of spatial entities are
found in object catalogues or metadata documents. For the purpose of this work, we have
selected 1308 deﬁnitions of spatial entities. To illustrate, the following is the deﬁnition of
the spatial entity ‘Sewerage’ from the Object Catalogue of Digital Topography Database
(GURS 2003):

(cid:129) ‘Sewerage is a system of ditches and canals for supplying and draining off water.’

Examples of deﬁnitions were randomly selected from the following three sources:

(cid:129) Object catalogue of Digital Topography Database, scale 1:5000 (GURS 2003),
(cid:129) Dictionary of Geographical Terms (ZRC SAZU 2005),
(cid:129) Dictionary of Mountain Walking Terms (ZRC SAZU 2002).

Spatial databases containing a metadata description in the Slovene language are scarce.
This is the reason we have chosen two dictionaries that describe spatial entities.

4.2. Morphosyntactical analysis

We have analyzed deﬁnitions of spatial concepts written in the Slovene language. This
is important, because POS taggers depend on the particular language. Our text was ana-
lyzed with a proprietary rule-based POS tagger for the Slovene language provided by the
company Amebis, d.o.o. During the process of POS tagging, every word is assigned a syn-
tactical category, including all its subcategories. For example, for the Slovene word ‘miza’
(i.e., ‘table’ in English) the following information is assigned (only here in English too,
after the ‘–’ symbol):

(cid:129) miza – the word miza in the nominative case,
(cid:129) sam – noun (other examples: gla – verb, prd – adjective, etc.),

International Journal of Geographical Information Science

1815

(cid:129) obcn – common noun,
(cid:129) zsp – female gender (other examples: msp – male gender, etc.),
(cid:129) ed – singular (other examples: dv – dual, mn – plural),
(cid:129) i – nominative case (other examples: r – genitive case, etc.).

Words of other syntactical categories (verbs, adjectives, etc.) are tagged in a similar way
as the above noun example. The input for POS tags consisted of an ASCII ﬁle containing
1308 deﬁnitions with 17,749 words altogether. The results of POS tagging was an XML
ﬁle with POS-tagged words. We are fully aware that to be comprehensible to an English
speaker, we should provide an English example here. However, that would probably require
an English language deﬁnition and a POS tagger able to process English texts. As this was
not part of our research, we provide an example of a POS-tagged deﬁnition of the selected
entity ‘bajar’ (‘pond’ in English) in the Slovene language, with added English translation
in italics as follows:

(cid:129) sentence(1, [pos(1, bajar – a pond, bajer, sam(obcn, msp, ed, i)), pos(2, je – is, biti,
gla(vezn, pove, sed, tre, edn, nez)), pos(3, vecja – big, velik, prd(kak, prim, zsp, ed,
i)), pos(4, kotanja – a sinkhole, kotanja, sam(obcn, zsp, ed, i)), pos(5, z – with, z,
pre(pre, enos, o)), pos(6, vodo – water, voda, sam(obcn, zsp, ed, o)), pos(7, (cid:3),(cid:3), pun),
pos(8, ribnik – a ﬁshpond, ribnik, sam(obcn, msp, ed, i)), pos(9, (cid:3).(cid:3), pun)]).

As is evident from the above example, the ﬁnal result of a deﬁnition takes the form of a
valid Prolog sentence with the sentence/2 predicate, where arguments have the following
meaning:

(cid:129) the ﬁrst argument is the identiﬁer of the deﬁnition or sentence;
(cid:129) the second argument determines a list of elements or a data structure of type

pos(X,Y,Z,W) with the parameters as follows:
(cid:129) X: the identiﬁer of a word inside a given deﬁnition,
(cid:129) Y: the POS-tagged word,
(cid:129) Z: the POS-tagged headword of the word,
(cid:129) W: the data structure representing morphosyntactical information.

The results of this morphosyntactical analysis have been examined carefully because POS
tagging can have a signiﬁcant impact on the quality of subsequent processing. Therefore,
one must make sure errors in POS tagging are eliminated to the highest degree possible.
In our case, the overall performance of the POS tagger was acceptable. However, there
was some manual work required to remove certain errors that were mostly a consequence
of inherent semantic ambiguity. For example, the Slovene word ‘prst’ has two meanings,
ﬁnger and soil. If the wrong morphosyntactical category is assigned to the word, there may
be errors in determining its morphosyntactical subcategories, which may lead to further
errors during subsequent processing.

4.3. Shallow parsing
Shallow or partial parsing is a natural language processing technique that attempts to pro-
vide some machine understanding of the structure of a sentence without parsing it fully into
a parsed tree form. The latter is a characteristic of a deep parsing approach. Shallow pars-
ing most often refers to the task of chunking. The aim of chunking is to divide the words

1816

D. Smole et al.

of a sentence into a non-overlapping series of words according to the available morphosyn-
tactical information. Most often, the task of building a chunker is a complex task which
makes a machine learning approach an attractive option in comparison to the handcrafting
of the rules (Hammerton et al. 2002).

In this work, we have implemented a tool for chunking natural language deﬁnitions
written in the Slovene language. The basic rule of chunking says that the end of any chunk
is determined by a noun or a noun phrase. The ﬁnal result, where the symbol ‘|’ represents
the starting or the ending point of a chunk, may be in the following form:

(cid:129) |Sewerage| is a system of ditches and canals | for supplying, draining off water.|

The rules for chunking deﬁnitions in the Slovene language are quite simple and do not
require the application of a machine learning approach. They accurately chunk (more than
95%) of all deﬁnitions of the spatial entities we have come across. In general, the rules for
chunking consider a morphosyntactical analysis of nouns as well as the existence and the
location of verbs and prepositional words inside a deﬁnition.

4.4.

Identifying semantic relations and properties

Once the chunks of a deﬁnition are correctly determined, we can start annotating them with
semantic relations and properties. The approach we have followed resembles the approach
of frame semantics and semantic roles (Fillmore and Baker 2001). Minsky (1975) was
the ﬁrst to argue that knowledge can be organized in the form of data structures describ-
ing stereotyped scenarios or objects called frames. Certain frames are evoked by certain
words. In our case, verbs play the central role. They are the core of the sentence to which
other syntactic constituents, such as subjects, objects, and propositions, are linked. In this
manner, the meaning of the sentence is expressed by means of a verb or a predicate of
a given sentence and the number of arguments this predicate relates to in a given typical
situation or a frame. For example, the transitive verb ‘to see’ deﬁnes the relation between
two persons or things, ‘X sees Y.’ X and Y are participants or arguments and play speciﬁc
semantic role in a given frame evoked by the verb ‘to see.’ There exist one, two, three,
or more parameter relations. In addition to verbs, nouns, adjectives, and prepositions can
also play the role of a predicate and are as such of great importance when presenting the
meaning of a given sentence (Hofmann 1993, Hurford and Heasley, 1993).

During our research work we have identiﬁed 26 different relations/properties. Figure 1
presents a frequency graph of each of the 26 relations/properties appearing in the 1308
analyzed deﬁnitions.

In this work we have focused only on identifying the ﬁve most

frequent

relations/properties. These are as follows:

(cid:129) ‘isA’: something is a more general entity of a given spatial entity,
(cid:129) ‘isLocated’: the location of a given spatial entity,
(cid:129) ‘hasPurpose’: the purpose of a given spatial entity,
(cid:129) ‘isResultOf ’: what the given spatial entity is a consequence of,
(cid:129) ‘hasParts’: the parts of a given spatial entity.

We view different semantic relations/properties as different quality dimensions. The notion
of quality dimensions originates from the cognitive semantics approach, which claims that
the meanings of terms reside in people’s heads. Meanings are therefore mappings to con-
ceptual structures, which themselves refer to real-world entities (Raubal and Kuhn 2004).

International Journal of Geographical Information Science

1817

Figure 1. Twenty-six relations/properties and the respective number of occurrences in the 1308
analyzed deﬁnitions.

Conceptual structures exist within conceptual spaces, which are frameworks for represent-
ing information on the conceptual level with geometrical structures based on a number of
quality dimensions (Gaerdenfors 2000). The following is an example of a sentence where
each chunk holds the semantic information of a given quality dimension.

We can look at an example sentence: ‘| Koliševka:1 | is:2 a:3 large:4 karst:5 hole:6 | in:7
the:8 ground:9 | that:10 is:11 the:12 result:13 of:14 the:15 collapse:16 of:17 a:18 cave:19
ceiling:20 .:21 |’

Table 1 shows the same example, this time with chunks having a manually assigned

quality dimension or relation/property.

The following is a short explanation of the abbreviations used:

(cid:129) slotName: the name of the subject/object part of a relation/property,
(cid:129) sentNdx: the identiﬁer of a deﬁnition,
(cid:129) lowNdx: the identiﬁer of the ﬁrst word within a chunk, which is assigned a given

(cid:129) highNdx: the identiﬁer of the last word within a chunk, which is assigned a given

relation/property,

relation/property.

The result of our example in a human readable form is as follows:

(cid:129) ‘koliševka’
(cid:129) ‘a large karst hole’
(cid:129) ‘a large karst hole’

‘is’
‘is located’
‘is a result of ’

‘a large karst hole,’
‘in the ground,’
‘the collapse of a cave ceiling.’

Table 1. Learning examples for deﬁnition number 905.

slotName

sentNdx

lowNdx

highNdx

isA_subject
isA_object
isLocated_subject
isLocated_object
isResultOf_subject
isResultOf_object

(S 905,
(S 905,
(S 905,
(S 905,
(S 905,
(S 905,

L 1,
L 2,
L 2,
L 7,
L 2,
L 10,

H 1)
H 6)
H 6)
H 9)
H 6)
H 21)

1818

D. Smole et al.

We have manually assigned relations/properties to all chunks of the selected 1308 deﬁ-
nitions. This is how we obtained a training and testing set for the ILP learning. In other
words, this means that we successfully completed all four tasks of the ﬁrst part of this
research project. These four tasks comprise the selection of spatial entity deﬁnitions, POS
tagging, chunking, and ﬁnally, semantic role labeling.

5. Learning the rules for identifying semantic relations and properties by
means of ILP
Now we move on to the second part of this research project, which concerns learning the
rules for extracting information – the input for this learning are the results of the ﬁrst part –
for as yet unseen deﬁnitions of spatial entities.

Introduction to the different settings of the ILP learning procedures

5.1.
In machine learning, searching for new knowledge in the data usually means we would like
to describe the data in a more compact and generalized form. In our case, we have used a
well-known implementation of ILP called Progol. A detailed description of how it works
can be found in Muggleton (1995). Generalization in Progol works in an iterative mode
using a so-called covering algorithm. This means that the algorithm ﬁrst selects a learning
example and then tries to generalize it in the form of Prolog or the ﬁrst-order predicate
logic clause. To construct such a clause Progol uses ‘mode’ and ‘type’ declarations. With
‘mode’ declarations we tell Progol which atoms can appear in the head of the clause (modeh
parameter) and which in the body of the clause (modeb parameter). Furthermore, with
‘type’ declarations we tell Progol what kind of argument types are acceptable for each atom
of the clause. Argument types describe the categories of objects, for example, numbers and
lists. We can also deﬁne whether the argument is a constant or a variable and whether the
argument is known before (the input argument) or if it should be computed by the atom (the
output argument). In our case, modeb and modeh declarations are used only when declaring
target relations and not in the case of POS-tagged deﬁnitions. A practical example of input
data for Progol is provided in Section 5.2. Besides preparing learning examples, setting
‘mode’ and ‘type’ declarations is crucial when working with Progol. ‘Mode’ and ‘type’
declarations also shorten the time of learning signiﬁcantly and improve the quality of the
learned rules.

The covering algorithm loops until all or at least most of the positive and none of the
negative examples are covered. The possible solution is constrained to the most speciﬁc
and most general clause. When searching the constrained space, Progol uses an A∗ algo-
rithm. This ensures that the algorithm learns the shortest and the simplest clauses possible.
It accomplishes this by computing the evaluation number for each clause of the rule by
taking into account four parameters: the number of positive examples covered, the number
of negative examples covered, the length of the clause, and the number of atoms needed
to deﬁne the clause. Subtracting the last three parameters from the ﬁrst one yields the
evaluation number. The higher the number is, the better the clause.

ILP learning has been carried out with four different settings. The aim was to
apply each of these variants to assess which of them yields the best results in terms of
classiﬁcation accuracy and simplicity of the learned rules. These variants are as follows:

(cid:129) ILP and procedural classiﬁcation,
(cid:129) ILP and cost-sensitive learning,

International Journal of Geographical Information Science

1819

(cid:129) ILP and boosting,
(cid:129) ILP and naïve Bayesian classiﬁcation.

For each of these four ILP variants, we followed the standard procedure applied in machine
learning as follows:

(cid:129) to learn a hypothesis we use a CProgol4.4 algorithm with a manually prepared
learning and testing set of examples, each consisting of positive and negative
examples;

(cid:129) apply a 10-fold cross-validation technique for estimating the performance of a

predictive model or a hypothesis:
(cid:129) one round of cross-validation involves partitioning a sample of data into comple-
mentary subsets, performing the analysis on one subset (called the training set),
and validating the analysis on the other subset (called the testing set). We repeat
this 10 times;

(cid:129) measure the time complexity required to learn each hypothesis;
(cid:129) measure the performance of each learned hypothesis with the following parame-

ters:
(cid:129) sensitivity: this measures the rate of correctly classiﬁed positive examples,
(cid:129) speciﬁcity: this measures the rate of correctly classiﬁed negative examples,
(cid:129) classiﬁcation accuracy: this measures the rate of correctly classiﬁed positive and

negative examples.

In the following sections, we will describe four variants of the ILP learning approach and
their performance. When describing the ﬁrst variant, ILP, and the procedural classiﬁcation,
we will go deeper than in the case of the other three ILP variants because the core concepts
remain the same in the remaining three cases.

ILP and the procedural classiﬁcation

5.2.
ILP and the procedural classiﬁcation is the ﬁrst and most basic variant of learning rules for
identifying semantic information that we applied. For illustration purposes, we will provide
an example here. First, we deﬁne a target relation in Progol syntax which states that a chunk
of words numbered from lNum to hNum of a given deﬁnition numbered sNum belong to
the isA relation. Note that lNum, hNum, and sNum are user-deﬁned object types (Progol
enables this using the above-mentioned type declarations). The target relation the algorithm
tries to learn is the following:

(cid:129) :-modeh(1,isA_object(+sNum, +lNum, +hNum))?

Second, we deﬁne background predicates, which the learner can use to express or deﬁne
the upper target relation. For example, a chunk of words is assigned a target relation if a
speciﬁc word belongs to it:

(cid:129) :-modeb(∗,listHasWord(+sNum, +lNum, +hNum, #ofWord))?

The listHasWord is again an example of a predicate which uses user-deﬁned argument
types. Note that there are many more predicates the learner can use to deﬁne the target
relation.

1820

D. Smole et al.

In our case, another piece of information the learning algorithm considers to be a part
of background knowledge is a list of chunks given for every deﬁnition. This information is
a result of the shallow parsing process described in Section 4.3:

(cid:129) sentDivided(s1,[l1,h1,l2,h5,l6,h7,l8,h10,l11,h14,l15,h17,l18,h22,l23,h26]).

The upper sentence says that the deﬁnition numbered s1 consists of eight chunks, where
the ﬁrst chunk consists of one word (from index 1 to index 1), the second chunk consists
of four words (from index 2 to index 5), and so on.

For Progol, the next and the largest piece of background knowledge consists of POS-
tagged deﬁnitions, such as the one describing ‘ablacija’ in Slovene or ‘ablation’ in English.
This information is a result of the morphosyntactical analysis described in Section 4.2:

(cid:129) sentence(s2, [pos(1, ablacija, ablacija, sam(obcn, zsp, ed, i)), pos(2, je, biti, gla(vezn,
pove, sed, tre, ed, nez)), pos(3, zmanjsevanje, zmanjsevanje, sam(obcn, ssp, ed, i)),
pos(4, ledenika, ledenik, sam(obcn, msp, ed, r)), pos(5, ali, ali, vez(noDataVez)),
pos(6, snezne, snezen, prd(kak, osno, zsp, ed, r)), pos(7, odeje, odeja, sam(obcn, zsp,
ed, r)), pos(8, zaradi, zaradi, pre(pre, enos, r)), pos(9, taljenja, taljenje, sam(obcn,
ssp, ed, r)), pos(10, (cid:3),(cid:3), (cid:3),(cid:3), pun), pos(11, izhlapevanja, izhlapevanje, sam(obcn, ssp,
ed, r)), pos(12, ali, ali, vez(noDataVez)), pos(13, mehanskega, mehanski, prd(vrs,
osno, ssp, ed, r)), pos(14, odnasanja, odnasanje, sam(obcn, ssp, ed, r)), pos(15, (cid:3).(cid:3), (cid:3).(cid:3),
pun)]).

Finally, there are positive and negative examples of a given target relation. A positive and
negative learning example is as follows:

(cid:129) isA_object(s524,l2,h6).
(cid:129) :-isA_object(s834,l21,h24).

Reading a positive learning example would determine that a chunk of ﬁve words, from
index 2 to index 6 of deﬁnition numbered s524 belong to the ‘isA’ relation. Similarly,
reading a negative learning example would determine that a chunk of four words, from
index 21 to index 24 of deﬁnition numbered s834 does not belong to the ‘isA’ relation.

This is a common input for all four variants of ILP learning. However, the output or the
results of such learning differ. The following is only a part of the learned rule for the isA
target relation. It says that if a given chunk of words from index B to index C of deﬁnition
A has the word ‘je’ (or ‘is’ in English), the chunk is a representative of the isA relation.
Please also note that this is the relation with the highest achieved accuracy, because it is
the simplest of all ﬁve selected relations/properties:

(cid:129) isA_object(A,B,C) :- listHasWord(A,B,C,je).

Before proceeding to the results of this approach, we present a short explanation of the
abbreviations used for the crucial parameters describing the settings and the results of the
learning procedure used throughout the text as follows:

(cid:129) #e+ : the number of positive examples,
(cid:129) #e- : the number of negative examples,
(cid:129) time: the average amount of time required to learn the target relation,

International Journal of Geographical Information Science

1821

(cid:129) sens: the value of the sensitivity parameter,
(cid:129) spec: the value of the speciﬁcity parameter,
(cid:129) acc: the value of the classiﬁcation accuracy parameter.

Table 2 includes the values of crucial parameters for the basic ILP variant.

Before starting with an interpretation of the values, we should review the meaning
of all three parameters for estimating the performance of the applied ILP method. In our
case, the meaning of the sensitivity parameter can be less formally interpreted as follows:
if a chunk of words is a representative of an isA relation, then the sensitivity parameter
determines the probability that it will actually be assigned to the isA relation during the
classiﬁcation process using the learned hypothesis. Similar holds true for the speciﬁcity
parameter as follows: if a chunk of words is not a representative of an isA relation, then the
speciﬁcity parameter determines the probability that it will actually not be assigned to the
isA relation during the classiﬁcation process using the learned hypothesis. Classiﬁcation
accuracy can be interpreted as the probability that the randomly selected chunk of words
will be classiﬁed correctly, which means that it either belongs to the given relation or not.
We can draw at least two important conclusions from Table 2. First, we can observe
that high values of a speciﬁcity parameter are a common characteristic of all relations and
properties. The values of the sensitivity parameter are consistently lower. The reason for
this lies in the fact that the number of negative examples is far greater than the number
of positive examples. This makes us believe that the speciﬁcity parameter conveys higher
importance when estimating the performance of the learned rules, possibly even higher
than classiﬁcation accuracy, which is, in essence, a function of the speciﬁcity parameter.
Second, in terms of the sensitivity parameter, the learned rules for the isA relation identi-
ﬁcation achieved the highest result. We speculate that this is due to the simplicity of this
particular relation.

To achieve better results in terms of the sensitivity parameter, we decided to apply
three other variants of ILP learning. Short descriptions, results, and the most important
conclusions regarding such are presented in the following sections.

5.3.

ILP and cost-sensitive learning

When dealing with classiﬁcation problems, often one has to focus on minimizing the cost
of incorrect classiﬁcation. This can be just the opposite of maximizing the overall classiﬁ-
cation accuracy. Most algorithms do not include this possibility by default. Cost-sensitive
learning is applied when one or more classes, in our case the number of positive examples
for a given relation or property, are in the minority. Properly weighting the examples from
rare classes is a simple method to force the learning algorithm to focus on minimizing
the cost of incorrect classiﬁcation instead of maximizing classiﬁcation accuracy. Examples
of rare classes are assigned a greater weight. In our case, positive examples form a rare

Table 2. Basic ILP variant with the procedural classiﬁcation.

Rel./Prop. name

isA_object
isLocated_object
hasPurpose_object
hasParts_object
isResultOf_object

#e+

1 423
826
252
246
195

#e−

4 298
4 895
5 469
5 475
5 526

time

3 h 2 min
5 h 30 min
1 h 53 min
4 h 58 min
1 h 26 min

sens

71.6
32.0
41.8
18.9
30.3

spec

99.4
99.1
99.9
99.8
99.9

acc

92.6
89.4
97.3
96.3
97.5

1822

D. Smole et al.

Table 3. Cost-sensitive ILP variant.

Rel./Prop. name

isA_object
isLocated_object
hasPurpose_object
hasParts_object
isResultOf_object

#e+

3 843
4 464
4 994
4 884
4 928

#e−

3 869
4 406
4 923
4 928
4 974

time

4 h 5 min
3 h 36 min
1 h 23 min
3 h 0 min
1 h 4 min

sens

41.1
49.6
56.7
32.4
48.7

spec

99.0
97.7
99.3
99.2
99.7

acc

84.8
90.9
97.4
96.4
97.9

class. The consequence of applying the cost-sensitive learning approach is that we obtain
a similar (and higher) number of positive and negative learning examples.

Table 3 includes the values of crucial parameters for the cost-sensitive ILP variant.
By comparing the results of the cost-sensitive method to the results achieved in the
basic ILP approach with procedural classiﬁcation, we can observe from Table 3 that the
values of the sensitivity parameter have increased except for the isA relation. In some cases
this signiﬁcant increase, for example, in the case of isResultOf (from 30.3% to 48.7%), is
probably a consequence of a new distribution regarding the number of positive and negative
learning examples in favor of the former.

ILP and boosting

5.4.
The ﬁrst attempts to combine a boosting principle with the ILP method were introduced
by Quinlan (1996). The idea of boosting lies in assigning weights to examples according
to how difﬁcult they are for learning. In this case, the algorithm should know how to deal
with differently weighted examples of the learning and testing sets. The more problem-
atic examples are assigned higher weight and vice versa. A higher weight means a higher
probability of being selected by the learning algorithm. Starting with the ﬁrst iteration, the
algorithm learns the hypothesis with equally weighted examples, that is, every example
has a weight of 1. When classifying the examples, the hypothesis predicts a class for each
example. When preparing weights for the next iteration, correctly classiﬁed examples are
assigned a lower weight and examples not classiﬁed correctly are assigned a higher weight.
The ﬁnal hypothesis classiﬁes examples according to the sum of votes, where each partial
hypothesis contributes a certain amount of votes for a given class (Quinlan 1996).

There are two main disadvantages to this approach. The ﬁrst disadvantage is that this
method yields a complex hypothesis which makes its interpretation more difﬁcult. The
second disadvantage is the larger processing times the learning process requires in this
case (Hoche 2004).

Again, we can draw some important conclusions from Table 4. First, when comparing
the values of the sensitivity parameter, it is clear that this method outperforms cost-sensitive
learning (see Table 3). In all ﬁve cases, the value of the sensitivity parameter has increased.
This is most notably true for the isLocated_object property (from 32.0% to 66.3%). Second,

Table 4. Boosting ILP variant.

Rel./Prop. name

isA_object
isLocated_object
hasPurpose_object
hasParts_object
isResultOf_object

#e+

1 423
826
252
246
195

#e−

4 298
4 895
5 469
5 475
5 526

time

5 h 41 min
10 h 19 min
2 h 57 min
7 h 17 min
2 h 45 min

sens

87.6
66.3
57.8
26.7
39.6

spec

97.6
96.1
99.4
98.7
99.9

acc

95.1
91.8
97.5
95.6
97.9

International Journal of Geographical Information Science

1823

the learning process has taken twice as much time as the standard ILP learning approach.
At the same time, the hypothesis C∗, which is a combination of all partial hypotheses, is
more difﬁcult to understand. However, in our case we prefer higher values of the sensi-
tivity parameter values over the time complexity of the learning process or even over the
complexity of the learned hypothesis. The reason for this is that the learning process takes
place only once and thus is not a problem if it takes a bit longer to learn the hypothesis. On
the contrary, the higher the value of the sensitivity parameter, the better is the end-user’s
experience with the results in the form of suggested spatial databases.

5.5.

ILP and naïve Bayesian classiﬁcation

It is believed that an ILP learner could improve its overall performance, classiﬁcation accu-
racy, and noise resistance by combining probabilistic answers to satisﬁed clauses. Pompe
and Kononenko (1995) have proposed such a method based on a naïve Bayesian approach.
The foundation of this approach is the Bayesian equation, which can be used for this pur-
pose under the assumption that the rules of a hypothesis are independent. A more detailed
explanation of this method is beyond the scope of this article.

The results we achieved with this method equal the results obtained by the ILP method
with the procedural classiﬁcation, see Table 2. This means that no improvement was
achieved by introducing probability concepts to the basic ILP method. We assume that
this is because the independence condition of the learned rules was not satisﬁed.

Interpretation of the results

5.6.
Table 5 summarizes the results achieved when learning hypotheses for assigning relations
and properties to chunks of words extracted from the spatial entity deﬁnition in natural
language.

According to Table 5, the achieved classiﬁcation accuracy of the learned rules for the
relation and property annotation is high: 90% or higher for all the variants of all the
relations and properties, namely ‘isA,’ ‘isLocated,’ ‘hasPurpose,’ ‘hasParts,’ and ‘isRe-
sultOf.’ Here ‘isA’ is a minor exception, because the classiﬁcation accuracy achieved varies
between 84.8% and 95.1%.

It must be noted that the sensitivity parameter is even more important than the classiﬁ-
cation accuracy parameter. According to Figure 2, boosting yielded the best results in three
out of ﬁve of the described learning tasks: ‘isA’ (87.6%), ‘isLocated’ (66.3%), and ‘hasPur-
pose’ (57.8%). The cost-sensitive approach has proved to be the best for the remaining two
relations or properties: ‘isResultOf ’ (48.7%) and ‘hasParts’ (32.4%), whereas boosting was
the second best choice in these two cases. The ILP with the procedural classiﬁcation and
the ILP with the naïve Bayesian classiﬁcation yielded the same results. The ILP method
in combination with procedural classiﬁcation proved to be a good choice only in one case,

Table 5. Summary results of ILP variants.

ILP procedural

ILP cost-sensitive

ILP boosting

ILP naïve Bayes

sens

spec

acc

Sens

spec

acc

sens

spec

acc

sens

spec

acc

isA
isLocated
hasPurpose
hasParts
isResultOf

71.6
32.0
41.8
18.9
30.3

99.4
99.1
99.9
99.8
99.9

92.6
89.4
97.3
96.3
97.5

41.1
49.6
56.7
32.4
48.7

99.0
97.7
99.3
99.2
99.7

84.8
90.9
97.4
96.4
97.9

87.6
66.3
57.8
26.7
39.6

97.6
96.1
99.4
98.7
99.9

95.1
91.8
97.5
95.6
97.9

71.6
32.0
41.8
18.9
30.3

99.4
99.1
99.9
99.8
99.9

92.6
89.4
97.3
96.3
97.5

1824

D. Smole et al.

Figure 2. The values of the sensitivity parameter for each ILP variant.

when learning the rules for ‘isA.’ It achieved second place after the boosting and before
the cost-sensitive learning approach.

To improve the values of all three parameters for all types of information extraction

rules, we believe the following has to be done:

(cid:129) re-check the learning set and the background knowledge, especially the results of
the morphosyntactical analysis, which has not yet been done for every deﬁnition
because this requires a substantial amount of manual work, and remove any hidden
errors,

(cid:129) increase the number of learning examples, and if needed upgrade the background

(cid:129) improve the performance of the POS tagger or try to utilize a deep parsing instead

(cid:129) when learning the rules, take advantage of the information stored in a semantically

knowledge,

of a shallow parsing approach,

tagged corpora, if available.

In the event these recommendations are followed, we speculate that the values of the sensi-
tivity parameter would improve in all ﬁve cases, especially in the case of ‘isResultOf ’ and
‘hasParts,’ which currently have the lowest values.

6. Conclusions and further research
Not surprisingly, there is a gap in communication between the spatial data producer and
the spatial data user. If they communicate at all, they speak their own language, consisting
of different vocabularies. The question is whether there exists an efﬁcient approach that

International Journal of Geographical Information Science

1825

could be applied to bridge this gap. Currently a great deal of research in the GIS commu-
nity focuses on methods that would enable the automatic interpretation of the meaning of
different vocabularies. An ontology-based approach to resolving semantic heterogeneity
problems is among the most commonly cited approaches (Kuhn 2001, Fonseca et al. 2002,
Hakimpour and Timpf 2002, Kuhn 2003, ˇCeh et al. 2004, ˇCeh et al. 2006). This and other
studies, for example Zhang et al. (2010), suggest that an ontological approach should be
combined with NLP techniques.

The aim of this research was to explore the feasibility of an ILP approach to extracting
information from deﬁnitions of spatial terms. According to the results, this task can be
automated when extracting three out of ﬁve of the selected types of semantic information.
The boosting variant of the ILP approach yields reasonable results for the ‘isA,’ ‘isLocated,’
and ‘hasPurpose’ relations/properties. Please note that in the initial phase of this research,
we agreed that reasonable results would entail that the sensitivity parameter values reach
50% or higher, whereas for the speciﬁcity parameter and classiﬁcation accuracy, values
over 95% and 90%, respectively, were considered reasonable. Now, after gaining some
experience with the ILP method and taking into consideration the suggested improvements
in Section 5.6, an even higher value of the sensitivity parameter, for example 70%, could be
set and we speculate that this could even be reached. This is why we believe an ILP method
could be of practical use and can therefore play an important role in different ontological
approaches.

Applying a machine learning approach offers a variety of other interesting challenges
in this ﬁeld of research. Therefore, we encourage other researchers to consider researching
certain tasks, namely the following:

(cid:129) the introduction of new relations and properties to this or a similar natural language

processing approach and testing the achieved quality of the learned rules;

(cid:129) the application of the described methodology for identifying relations and properties

of spatial concepts written in other natural languages;

(cid:129) the expansion of this method to non-deﬁnition parts of metadata descriptions to
derive other potentially useful information with little or no work on the side of the
user;

(cid:129) the application of a multidisciplinary approach to interpret the learned rules for
identifying relations and properties to acquire new, possibly useful, and not yet
discovered knowledge; and ﬁnally

(cid:129) the extensive research and subsequent development of the two additional compo-
nents of a spatial data recommendation service (one for the comparison of semantic
information of different spatial datasets and one for learning the user’s individual
preferences) by applying a machine learning approach.

We believe that by realizing these considerations in practice it is possible to move toward
signiﬁcantly better semantic interoperability.

Acknowledgements
We are very grateful to Prof. Dr. Igor Kononenko from the Faculty of Computer and Information
Science, University of Ljubljana, for his support and guidance. We thank the Amebis d.o.o. company
for kindly allowing us to use the POS tagger for the Slovene language that they developed. We also
thank the Slovene Ministry of Higher Education, Science and Technology, which made this work
ﬁnancially feasible.

1826

D. Smole et al.

References
Aitken, J.S., 2002. Learning information extraction rules: an inductive logic programming approach.
In: F. van Harmelen, ed. The proceedings of the 15th European conference on artiﬁcial
intelligence. Amsterdam, Netherlands: IOS Press, 355–359.

Bortenschlager, M., et al., 2007. Towards a P2P-based geocollaboration system for disaster

management. In: GI-days 2007, 9–12 September 2007. Muenster, Germany, 167–172.

Bratko, I., 2001. Prolog programming for artiﬁcial intelligence. 3rd ed. Harlow: Addison-Wesley.
Cai, G., 2005. Extending distributed GIS to support geocollaborative crisis management.

Geographical Information Science, Special issue on distributed GIS, 11 (1), 4–14.

ˇCeh, M., Podobnikar, T., and Smole, D., 2006. Semantic similarity measures within the seman-
tic framework of the universal ontology of geographical space. In: A. Riedl, W. Kainz and
G. Elmes, ed. Progress in spatial data handling: 12th international symposium on spatial data
handling, 12–14 July 2006. Berlin: Springer-Verlag, 417–434.

ˇCeh, M., Smole, D., and Podobnikar, T., 2004. Geodata – are they accessible and useful? In: F. Toppen
and P. Prastacos, ed. 7th conference on geographic information science, AGILE, conference
proceedings, 29 April–1 May 2004. Heraklion: Crete University, 789–794.

Comber, A.J., et al., 2006. Using metadata to link uncertainty and data quality assessments. In:
A. Riedl, W. Kainz, and G. Elmes, eds. Progress in spatial data handling, 12th international
symposium on spatial data handling, 12–14 July 2006. Berlin: Springer-Verlag, 279–292.
Fillmore, C.J. and Baker, C.F., 2001. Frame semantics for text understanding. In: Proceedings of

WordNet and other lexical resources workshop, June 2001. Pittsburgh: NAACL, 59–64.

Fonseca, F., et al., 2002. Using ontologies for integrated geographic information systems.

Gaerdenfors, P., 2000. Conceptual spaces – the geometry of thought. Cambridge, MA: Bradford

Transactions in GIS, 6 (3), 231–257.

Books, MIT Press.

GURS, 2003. DTK5: object catalogue of the digital topography database, scale 1:5000. Ljubljana:

Surveying and Mapping Authority of the Republic of Slovenia.

Hakimpour, F. and Timpf, S., 2002. A step towards geodata integration using formal ontologies.
In: M. Ruiz, M. Gould, and J. Ramon, eds. 5th AGILE conference on geographic information
science, 25–27 April 2002. Illes Balears: Universitat de les Illes Balears, 25–27.

Hammerton, J., et al., 2002. Introduction to special issue on machine learning approaches to shallow

parsing. The Journal of Machine Learning Research, 2 (4), 551–558.

Hoche, S., 2004. Active relational rule learning in a constrained conﬁdence-rated boosting frame-

work. Thesis (PhD). Rheinische Friedrich-Wilhelms-Universitaet Bonn.

Hofmann, T.R., 1993. Realms of meaning – an introduction to semantics. New York: Longman

Publishing.

Press.

Hovy, E., et al., 2003. Extending metadata deﬁnitions by automatically extracting and organizing
glossary deﬁnitions. In: Proceedings of the NSF’s dg.o, 18–21 May 2003. Digital Government
Society of North America, 1–6.

Hurford, J.R. and Heasley, B., 1993. Semantics: a coursebook. Cambridge: Cambridge University

Karalopoulos, A., Kokla, M., and Kavouras, M., 2004. Geographic knowledge representation using
conceptual graphs. In: F. Toppen and P. Prastacos, eds. AGILE 2004 conference proceedings.
Heraklion, Crete, Greece: Crete University Press, 511–521.

Kavouras, M., 2005. A uniﬁed ontological framework for semantic integration. In: P. Agouris and
A. Croitoru, eds. Next generation geospatial information. ISPRS book series. London: A.A.
Balkema Publishers – Taylor & Francis, 147–156.

Kavouras, M., Kokla, M., and Tomai, E., 2005. Comparing categories among geographic ontologies.

Computers & Geosciences, 31, 145–154.

Kimball, R. and Caserta, J., 2004. The data warehouse ETL toolkit: practical techniques for

extracting, cleaning, conforming, and delivering data. Indianapolis,IN: John Wiley.

Kuhn, W., 2001. Ontologies in support of activities in geographical space. International Journal of

Geographic Information Science, 15 (7), 613–631.

Kuhn, W., 2003. Semantic reference systems. International Journal of Geographical Information

Science, 17 (5), 405–409.

Mark, D.M., and Turk, A.G., 2003. Landscape categories in Yindjibarndi: ontology, environment, and
language. In: W. Kuhn, M. Worboys., and S. Timpf, eds. Spatial information theory. Foundations
of geographic information science. International Conference, COSIT 2003, Springer, 28–45.

International Journal of Geographical Information Science

1827

Minsky, M.L., 1975. A framework for representing knowledge. In: P.H. Winston, ed. The psychology

of computer vision. New York: McGraw-Hill, 211–277.

Muggleton, S., 1995. Inverse entailment and Progol. New Generation Computing Journal, 13,

245–286.

Nichols, E., Bond, F., and Flickinger, D., 2005. Robust ontology acquisition from machine-readable
dictionaries. In: L.P. Kaelbling and A. Safﬁotti, eds. Proceedings of the international joint con-
ference on artiﬁcial intelligence IJCAI-2005, 30 July–5 August 2005. San Francisco: Morgan
Kaufmann Publishers Inc., 1111–1116.

O’Hara, T.P., 2005. Empirical acquisition of conceptual distinctions via dictionary deﬁnitions. Thesis

(PhD). NMSU CS.

Pompe, U. and Kononenko, I., 1995. Naive Bayesian classiﬁer within ILP-R. In: L. De Raedt,
ed. Proceedings of the 5th international workshop on inductive logic programming. Leuven:
Department of Computer Science, Katholieke Universiteit Leuven, 417–436.

Quinlan, J.R., 1996. Boosting ﬁrst-order learning. In: S. Arikawa and A.K. Sharma, eds. Proceedings
of the 7th international workshop on algorithmic learning theory, vol. 1160 of lecture notes in
computer science. Berlin: Springer-Verlag, 143–155.

Raubal, M. and Kuhn, W., 2004. Ontology-based task simulation. Spatial Cognition & Computation,

4 (1), 15–37.

Sasaki, Y. and Matsuo, Y., 2000. Learning semantic-level information extraction rules by type-
oriented ILP. In: M. Kay, ed. Proceedings of the 18th international conference on computational
linguistics, 31 July–4 August 2000. San Francisco, CA: Morgan Kaufmann, 698–704.

Tapscott, D. and Williams, A.D., 2007. Wikinomics: how mass collaboration changes everything.

London: Atlantic Books.

Zhang, C., Tian, T., and Li, W., 2010. Automatic search of geospatial features for disaster and emer-
gency management. International Journal of Applied Earth Observation and Geoinformation,
12 (6), 409–418.

ZRC SAZU, 2002. In: V. Likar, ed. Mountain walking dictionary/Planinski terminološki slovar.

Ljubljana: ZRC SAZU, Fran Ramovš Institute for Slovene Language.

ZRC SAZU, 2005. In: D. Kladnik et al., eds. Dictionary of geographical

terms/Geografski

terminološki slovar. Ljubljana: ZRC SAZU, Anton Melik Institute for Geography.

Copyright of International Journal of Geographical Information Science is the property of Taylor & Francis Ltd

and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright

holder's express written permission. However, users may print, download, or email articles for individual use.

